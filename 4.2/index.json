[
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/domain-on-pv/overview/",
	"title": "Overview",
	"tags": [],
	"description": "Learn how to create a domain on a persistent volume.",
	"content": "Contents Overview High-level use case WebLogic Deploy Tooling models Runtime behavior Runtime updates Overview Domain on persistent volume (Domain on PV) is an operator domain home source type, which requires that the domain home exists on a persistent volume. The domain home can be created either manually or automatically by specifying the section, domain.spec.configuration.initializeDomainOnPV, in the domain resource YAML file. The initial domain topology and resources are described using WebLogic Deploy Tooling (WDT) models.\nNOTE: The initializeDomainOnPV section provides a one time only domain home initialization. The operator creates the domain when the domain resource is first deployed. After the domain is created, this section is ignored. Subsequent domain lifecycle updates must be controlled by the WebLogic Server Administration Console, the WebLogic Remote Console, WebLogic Scripting Tool (WLST), or other mechanisms. See the High-level use case.\nThe initializeDomainOnPv section:\nCreates the PersistentVolume (PV) and/or PersistenVolumeClaim (PVC), if needed. Creates the RCU schema, if needed. Creates the WebLogic domain home on the persistent volume based on the provided WDT models. If you are running WebLogic Scripting Tool (WLST) inside a server pod, then please refer to Use kubectl exec in the WLST documentation for very important information related to memory usage.\nHigh-level use case The typical Domain on PV use case is for an application life cycle that requires persisting changes to the permanent file system.\nFor example, you might use frameworks like Metadata Services (MDS), Oracle Application Development Framework (ADF), or Oracle Web Services Manager (OWSM). These frameworks require a running domain and the lifecycle operations are persisted to the file system. Typically, after the initial domain is created, you use tools like Fusion Middleware Control, product-specific WLST functions, the WebLogic Server Administration Console, the WebLogic Remote Console, or JDeveloper for lifecycle operations. The changes are managed by these tools; the data and operations cannot be described using WDT models.\nWebLogic Deploy Tooling models WDT models are a convenient and simple alternative to WLST configuration scripts. They compactly define a WebLogic domain using model files, variable properties files, and application archive files. For more information about the model format and its integration, see Usage and Working with WDT Model files. The WDT model format is fully described in the open source, WebLogic Deploy Tooling GitHub project.\nRuntime behavior When you deploy a Domain on PV domain resource YAML file:\nThe operator will run a Kubernetes Job, called an introspector job, that:\nMerges your WDT model files. Runs the WDT Create Domain Tool to create a domain home. After the introspector job completes:\nThe operator creates one or more ConfigMaps following the pattern DOMAIN_UID-weblogic-domain-introspect-cm***. The operator subsequently boots your domain\u0026rsquo;s WebLogic Server pods. Runtime updates You control runtime updates to the WebLogic domain configuration using tools, such as Fusion Middleware Control, product-specific WLST functions, the WebLogic Server Administration Console, the WebLogic Remote Console, or JDeveloper. After the initial domain is created, subsequent updates to the source of the WDT model files or any referenced macros will be ignored.\nSome changes may require triggering an introspector job. For example:\nAfter you change the WebLogic domain credential in the WebLogic Server Administration Console, in the domain resource YAML file, you must:\nUpdate the credentials in domain.spec.webLogicCredentialsSecret. Update the value of domain.spec.introspectVersion. If you change any WebLogic domain topology, such as using the WebLogic Server Administration Console to add clusters or servers, you must:\nUpdate the value of domain.spec.introspectVersion in the domain resource YAML file. Optionally, if you want to fine tune their life cycle or replica counts, then update the domain resource YAML file to add the new clusters or servers. "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-operators/overview/",
	"title": "Overview",
	"tags": [],
	"description": "An introduction to the operator runtime.",
	"content": "An operator runtime is a process that runs in a container deployed into a Kubernetes Pod and that automatically manages domain resources. A domain resource references WebLogic domain configuration, a WebLogic installation image, Kubernetes secrets, and anything else necessary to run a particular WebLogic domain. The operator requires Helm for its installation and tuning.\nA single operator instance is capable of managing multiple domains in multiple namespaces depending on how it is configured. A Kubernetes cluster can host multiple operators, but no more than one per namespace, and two operators cannot manage domains in the same namespace. You can deploy, delete, and manage domain resources while an operator is running.\nA completely installed and running WebLogic Kubernetes Operator environment includes:\nA Kubernetes cluster. A pair of Kubernetes custom resource definitions (CRD) for domain and cluster resource that, when installed, enables the Kubernetes API server and the operator to monitor and manage their resource instances. One or more operator runtimes, each deployed to a different namespace, that monitor Kubernetes namespaces for domain resources. A WebLogic domain resource conversion webhook that runs in a single namespace in the Kubernetes cluster, that is shared by the operator runtimes, and that handles the automatic conversion of older versions of the domain resource. Each operator is associated with a local Kubernetes service account for security purposes. The service account is deployed to the same namespace as the operator. When an operator runtime detects a domain, it will first run a short-lived Kubernetes job (the \u0026ldquo;introspector job\u0026rdquo;) that reads and checks the domain\u0026rsquo;s WebLogic configuration, and then it will generate and deploy the domain\u0026rsquo;s pods, services, and potentially other resources. The operator will also monitor the domain for changes, such as a request to change the number of pods in a WebLogic cluster, will update status fields on the domain\u0026rsquo;s domain resource, and will generate Kubernetes events for the domain in the domain\u0026rsquo;s namespace. If the operator detects that a domain is deleted, then it will shut down any running pods associated with the domain and delete the resources that it has deployed for the domain. If an operator is shut down, then its domains\u0026rsquo; pods, services, and such, will remain running but changes to a domain resource will not be detected and honored until the operator is restarted.\nOptionally, you can monitor an operator and its log using an Elastic Stack (previously referred to as the ELK Stack, after Elasticsearch, Logstash, and Kibana). For an example, see the operator Elastic Stack sample.\nFor advanced users, the operator provides an optional REST server that you can use as an alternative method for getting a list of WebLogic domains and clusters that an operator manages, and to initiate scaling operations (instead of directly performing such operations using the Kubernetes API or the Kubernetes command line). See the operator REST services.\nReferences:\nFor a full overview of how an operator runtime and its domain resources work together, see the terms, design philosophy, and architecture documentation. For information about using a Helm chart to install, update, or upgrade the operator, its CRD, or its service account, see the operator Prepare for installation and Installation guides. All operator Helm chart configuration options are documented in the operator Configuration Reference. For a detailed description of configuring the namespaces which an operator manages, plus preparing a namespace for operator management, see Namespace management. For an example of installing the operator, setting the namespace that it monitors, deploying a domain resource to its monitored namespace, and uninstalling the operator, see the Quick Start.\nThere can be multiple operators in a Kubernetes cluster, and in that case, you must ensure that the namespaces managed by these operators do not overlap. At most, a namespace can be managed by one operator. In addition, you cannot deploy more than operator to a particular namespace. See Common mistakes and solutions.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/elastic-stack/operator/",
	"title": "Operator",
	"tags": [],
	"description": "Sample for configuring the Elasticsearch and Kibana deployments and services for the operator&#39;s logs.",
	"content": "The operator Helm chart includes the option of installing the necessary Kubernetes resources for Elastic Stack integration.\nYou are responsible for configuring Kibana and Elasticsearch, then configuring the operator Helm chart to send events to Elasticsearch. In turn, the operator Helm chart configures Logstash in the operator deployment to send the operator\u0026rsquo;s log contents to that Elasticsearch location.\nElastic Stack per-operator configuration As part of the Elastic Stack integration, Logstash configuration occurs for each deployed operator instance. You can use the following configuration values to configure the integration:\nSet elkIntegrationEnabled is true to enable the integration. Set logStashImage to override the default version of Logstash to be used (logstash:6.8.23). Set elasticSearchHost and elasticSearchPort to override the default location where Elasticsearch is running (elasticsearch2.default.svc.cluster.local:9201). This will configure Logstash to send the operator\u0026rsquo;s log contents there. Set createLogStashConfigMap to true to use the default Logstash configuration, or set it to false and create a ConfigMap named weblogic-operator-logstash-cm in the operator\u0026rsquo;s namespace with your own Logstash pipeline configuration. Optionally, create a secret named logstash-certs-secret in the operator\u0026rsquo;s namespace with certificates for communicating with secured Elasticsearch. The certificates will be placed under the /usr/share/logstash/config/certs/ directory in the Logstash container. For additional details, see Elastic Stack integration Helm commands.\nSample to configure Elasticsearch and Kibana This sample configures the Elasticsearch and Kibana deployments and services. It\u0026rsquo;s useful for trying out the operator in a Kubernetes cluster that doesn\u0026rsquo;t already have them configured.\nIt runs the Elastic Stack on the same host and port that the operator\u0026rsquo;s Helm chart defaults to, therefore, you only need to set elkIntegrationEnabled to true in your values.yaml file.\nTo control Elasticsearch memory parameters (Heap allocation and Enabling/Disabling swapping), open the file elasticsearch_and_kibana.yaml, search for env variables of the Elasticsearch container and change the values of the following:\nES_JAVA_OPTS: value may contain, for example, -Xms512m -Xmx512m to lower the default memory usage (please be aware that this value is applicable for demonstration purposes only and it is not the one recommended by Elasticsearch). bootstrap.memory_lock: value may contain true (enables the usage of mlockall, to try to lock the process address space into RAM, preventing any Elasticsearch memory from being swapped out) or false (disables the usage of mlockall). To install Elasticsearch and Kibana, use:\n$ kubectl apply -f kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml To remove them, use:\n$ kubectl delete -f kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/faq/newbie/",
	"title": "Answers for newcomers",
	"tags": [],
	"description": "Answers to commonly asked newcomer questions.",
	"content": "What is the WebLogic Kubernetes Operator, how can I get started with it, where is its documentation? It\u0026rsquo;s all here.\nHow much does it cost? The WebLogic Kubernetes Operator (the “operator”) is open source and free, licensed under the Universal Permissive license (UPL), Version 1.0.\nWebLogic Server is not open source. Licensing is required for each running WebLogic Server instance, just as with any deployment of WebLogic Server. Licensing is free for a single developer desktop development environment.\nFor more information, see Pricing and licensing.\nHow can I get help? You are welcome to get in touch with us to ask questions, provide feedback, or give suggestions. To learn how, see Get help.\nWebLogic Server Certification Q: Which Java EE profiles are supported/certified on Kubernetes, only Web Profile or WLS Java EE full blown?\nA: We support the full Java EE Profile.\nWebLogic Server Configuration Q: How is the WebLogic Server domain configured in a container (for example, databases, JMS, and such) that is potentially shared by many domains?\nA: In a Kubernetes and container environment, the WebLogic domain home can be externalized to a persistent volume, or supplied in an image (by using a layer on top of a WebLogic Server image). For WebLogic domains that are supplied using an image, the domain logs and store locations optionally can be located on a persistent volume. See the samples in this project.\nWhen using the operator, each deployed domain is specified by a domain resource that you define which describes important aspects of the domain. These include the location of the WebLogic Server image you wish to use, a unique identifier for the domain called the domain-uid, any PVs or PVC the domain pods will need to mount, the WebLogic clusters and servers which you want to be running, and the location of its domain home.\nBeginning with operator 4.0, WebLogic clusters that are part of the domain configuration may be associated with a cluster resource. The cluster resource makes it easier to scale the number of member servers currently running using kubectl scale, the Kubernetes built-in Horizontal Pod Autoscaling, or similar tools. This cluster resource must be referenced from the domain resource.\nMultiple deployments of the same domain are supported by specifying a unique domain-uid string for each deployed domain and specifying a different domain resource. The domain-uid is in turn used by the operator as the name-prefix and/or label for the domain\u0026rsquo;s Kubernetes resources that the operator deploys for you. The WebLogic configuration of a domain\u0026rsquo;s deployments optionally can by customized by specifying configuration overrides in the domain resource \u0026ndash; which, for example, is useful for overriding the configuration of a data source URL, user name, or password.\nThe operator does not specify how a WebLogic domain home configuration is created. You can use WLST, REST, or a very convenient new tool called WebLogic Deploy Tooling (WDT). WDT allows you to compactly specify WebLogic configuration and deployments (including JMS, data sources, applications, authenticators, and such) using a YAML file and a ZIP file (which include the binaries). The operator samples show how to create domains using WLST and using WDT.\nQ: Is the Administration Server required? Node Manager?\nA: Certification of both WebLogic running in containers and WebLogic in Kubernetes consists of a WebLogic domain with the Administration Server. The operator configures and runs Node Managers for you within a domain\u0026rsquo;s pods - you don\u0026rsquo;t need to configure them yourself - so their presence is largely transparent.\nCommunications Q: How is location transparency achieved and the communication between WLS instances handled?\nA: Inside the Kubernetes cluster, the operator generates a Kubernetes ClusterIP service for each WebLogic Server pod called DOMAINUID-WEBLOGICSERVERNAME and for each WebLogic cluster called DOMAINUID-cluster-CLUSTERNAME. The operator also overrides your WebLogic network listen address configuration to reflect the service names so that you don\u0026rsquo;t need to do this. The services act as DNS names, and allow the pods with WebLogic Servers to move to different nodes in the Kubernetes cluster and continue communicating with other servers.\nQ: How is communication from outside the Kubernetes cluster handled?\nA:\nHTTP communication to your applications from locations outside the cluster: Typically, this is accomplished by deploying a load balancer that redirects traffic to your domain\u0026rsquo;s Kubernetes services (the operator automatically deploys these services for you); see Ingress. For an example, see the Quick Start, Install the operator and ingress controller.\nJMS, EJB, and other types of RMI communication with locations outside of the Kubernetes cluster: This is typically accomplished by tunneling the RMI traffic over HTTP through a load balancer or, less commonly accomplished by using T3 or T3S directly with Kubernetes NodePorts; see External WebLogic clients.\nAccess the WebLogic Server Administration Console: This can be done through a load balancer; see the Model in Image sample. Or, this can be done through a Kubernetes NodePort service; run $ kubectl explain domain.spec.adminServer.adminService.channels.\nAccess the WebLogic Remote Console: This can be done using a load balancer or Kubernetes NodePort service; see Use the Remote Console.\nQ: Are clusters supported on Kubernetes using both multicast and unicast?\nA: Only unicast is supported. Most Kubernetes network fabrics do not support multicast communication. Weave claims to support multicast but it is a commercial network fabric. We have certified on Flannel and Calico, which support unicast only.\nQ: For binding EJBs (presentation/business-tier), are unique and/or dynamic domain-names used?\nA: We do not enforce unique domain names. If you deploy two domains that must interoperate using RMI/EJB/JMS/JTA/and such, or that share RMI/EJB/JMS/JTA/and such clients, which concurrently communicate with both domains, then, as usual, the domain names must be configured to be different (even if they have different domain-uids).\nLoad Balancers Q: Load balancing and failover inside a DataCenter (HTTPS and T3s)?\nA: We originally certified on Traefik with the Kubernetes cluster; this is a very basic load balancer. We have also certified other more sophisticated load balancers. See Ingress.\nLife Cycle and Scaling Q: How to deal with grow and shrink? Cluster and non-cluster mode.\nA: You can scale and shrink a configured WebLogic cluster (a set of preconfigured Managed Servers) or a dynamic WebLogic cluster (a cluster that uses templated Managed Servers) using different methods. See Scaling.\nManually, using Kubernetes command-line interface, kubectl. WLDF rules and policies; when the rule is met the Administration Server sends a REST call to the operator which calls the Kubernetes API to start a new pod/container/server. We have developed and made open source the WebLogic Monitoring Exporter which exports WebLogic metrics to Prometheus and Grafana. In Prometheus, you can set rules similar to WLDF and when these rules are met, a REST call is made to the operator which invokes a Kubernetes API to start a new pod. Q: Container life cycle: How to properly spin up and gracefully shut down WLS in a container?\nA: The operator manages container/pod/WebLogic Server life cycle automatically; it uses the Node Manager (internally) and additional approaches to do the following operations:\nEntrypoint - start WebLogic Server. Liveliness probe – check if the WebLogic Server is alive. Readiness probe – query if the WebLogic Server is ready to receive requests. Shutdown hook – gracefully shut down WebLogic Server. These operations also can be done manually using the Kubernetes command-line interface, kubectl.\nFor more information, see the Domain life cycle documentation.\nPatching and Upgrades Q: How can I get a patched Oracle container image for running in Kubernetes (one-off patches, overlays, CPUs, etc.)?\nA: Download pre-patched images from the Oracle Container Registry or use WebLogic Image Tool to create your own. For additional information on WebLogic container images on Oracle Container registry, see Obtain images from the Oracle Container Registry.\nFor additional information on creating new container images with the WebLogic Image Tool, see Create custom images.\nQ: How do I apply patches without impacting my application\u0026rsquo;s uptime?\nA: See Rolling restarts.\nQ: How do I integrate patching with my continuous integration and continuous deployment processes?\nA: See CI/CD considerations.\nDiagnostics and Logging Q: Integration with ecosystems: logging, monitoring (OS, JVM and application level), and such.\nA: WebLogic Server stdout logging is echoed to their pod logs by default, and WebLogic Server file logs are optionally persisted to an external volume. We are working on a project to integrate WebLogic Server logs with the Elastic Stack. See Elastic Stack.\nWith regards to monitoring, all the tools that are traditionally used to monitor WebLogic Server can be used running in containers and Kubernetes. In addition, as mentioned previously, we have developed the WebLogic Monitoring Exporter, which exports WebLogic metrics in a format that can be read and displayed in dashboards like Prometheus and Grafana.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/cicd/layering/",
	"title": "Container image layering",
	"tags": [],
	"description": "Learn about container image layering and why it is important.",
	"content": "Container images are composed of layers, as shown in the following diagram. If you download the standard weblogic:12.2.1.4 image from the Oracle Container Registry, then you can see these layers using the command docker inspect container-registry.oracle.com/middleware/weblogic:12.2.1.4 (the domain layer will not be there). You are not required to use layers, but efficient use of layers is considered a best practice.\nWhy is it important to maintain the layering of images? Layering is an important technique in container images. Layers are important because they are shared between images. Let\u0026rsquo;s consider an example. In the following diagram, we have two domains that we have built using layers. The second domain has some additional patches that we needed on top of those provided in the standard WebLogic image. Those are installed in their own layer, and then the second domain is created in another layer on top of that.\nLet\u0026rsquo;s assume we have a three-node Kubernetes cluster and we are running both domains in this cluster. Sooner or later, we will end up with servers in each domain running on each node, so eventually all of the image layers are going to be needed on all of the nodes. Using the approach shown (that is, standard image layering techniques) we are going to need to store all six of these layers on each node. If you add up the sizes, then you will see that it comes out to about 1.5GB per node.\nNow, let\u0026rsquo;s consider the alternative, where we do not use layers, but instead, build images for each domain and put everything in one big layer (this is often called \u0026ldquo;squashing\u0026rdquo; the layers). In this case, we have the same content, but if you add up the size of the images, you get 2.9GB per node. That’s almost twice the size!\nWith only two domains, you start to see the problem. In the layered approach, each new domain is adding only a relatively very small increment. In the non-layered approach, each new domain is essentially adding the entire stack over again. Imagine if we had ten domains, now the calculation looks like this:\nWith Layers Without Layers Shared Layers 1.4GB 0GB Dedicated/different layers 10 x 10MB = 100MB 10 x 1.5GB = 15GB Total per node 1.5GB 15GB You can see how the amount of storage for images really starts to add up, and it is not just a question of storage. When Kubernetes creates a container from an image, the size of the image has an impact on how long it takes to create and start the container.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/security/domain-security/pod-and-container/",
	"title": "Pod and container security",
	"tags": [],
	"description": "Pod and container security.",
	"content": "The WebLogic Kubernetes Operator enforces pod and container security best practices for the pods and containers that the operator creates for WebLogic Server instances, the init container for auxiliary images, sidecar containers for Fluentd or the WebLogic Monitoring Exporter, and the introspection job.\nThe operator adds the following pod-level securityContext content:\nsecurityContext: seccompProfile: type: RuntimeDefault The operator also adds the following container-level securityContext content to each container:\nsecurityContext: runAsUser: 1000 runAsNonRoot: true privileged: false allowPrivilegeEscalation: false capabilities: drop: - ALL On OpenShift environments, the operator omits the runAsUser element.\nCustomers can configure pod and container generation for WebLogic Server instances using the serverPod element in the Domain resource. If specified, the operator will use the serverPod.podSecurityContext or serverPod.containerSecurityContext content from the Domain resource rather than using the default content shown previously.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/security/certificates/",
	"title": "Certificates",
	"tags": [],
	"description": "Operator SSL/TLS certificate handling.",
	"content": "This document is now located in the operator user guide, see REST Services.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/accessing-the-domain/weblogic-admin-console/",
	"title": "Use the Administration Console",
	"tags": [],
	"description": "Use the WebLogic Server Administration Console with domains running in Kubernetes.",
	"content": "Contents Introduction Use a load balancer Configure ingress path routing rules for a non-SSL port Configure ingress path routing rules for an SSL port and enable WebLogic Plugin Enabled Use an Administration Server NodePort Use a kubectl port-forward connection Test Introduction You can access the WebLogic Server Administration Console external to the Kubernetes cluster using the following approaches:\nUse a load balancer\nUse an Administration Server NodePort\nUse a kubectl port-forward connection\nNOTES:\nFor production use cases, Oracle recommends using a load balancer with ingress path routing rules and an SSL port to access the WebLogic Server Administration Console.\nTo verify that your load balancer, NodePort, or kubectl port-forward setup is working as expected, see Test.\nDo not use the WebLogic Server Administration Console to start or stop servers, or for scaling clusters. See Starting and stopping servers and Scaling.\nIf your domain home type is either Domain in Image or Model in Image, then do not use the Administration Console to make changes to the WebLogic domain configuration because these changes are ephemeral and will be lost when servers restart. See Choose a domain home source type.\nExternally exposing administrative, RMI, or T3 capable WebLogic channels using a Kubernetes NodePort, load balancer, port forwarding, or a similar method can create an insecure configuration. For more information, see External network access security.\nUse a load balancer To access the WebLogic Server Administration Console through a load balancer, first set up an Ingress. This, in combination with SSL, is the best practice approach for production use cases.\nThe following path-routing ingress instructions do not apply when you need to concurrently access multiple domains in the same Kubernetes cluster through the same external load balancer port. For the multiple domain use case, see the Host-based routing sample and make sure that the host names are resolvable by your DNS server (for example, domain1.org and domain2.org in the sample).\nConfigure ingress path routing rules for a non-SSL port The following example sets up an ingress path routing rule to access a WebLogic Server Administration Console through a non-SSL port.\nSet up a path-routing YAML file for a Traefik load balancer:\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: annotations: kubernetes.io/ingress.class: traefik name: traefik-pathrouting-1 namespace: weblogic-domain spec: routes: - kind: Rule match: PathPrefix(`/console`) services: - kind: Service name: domain1-adminserver namespace: weblogic-domain port: 7001 To access the WebLogic Server Administration Console, open the following URL from your browser:\nhttp://${HOSTNAME}:${LB_PORT}/console Where:\n${HOSTNAME} is where the ingress load balancer is running.\nTo determine the ${LB_PORT} when using a Traefik load balancer:\n$ export LB_PORT=$(kubectl -n traefik get service traefik-operator -o jsonpath='{.spec.ports[?(@.name==\u0026quot;web\u0026quot;)].nodePort}')\nIf you have an FMW Infrastructure domain, then you can add an ingress path routing rule for the PathPrefix /em and access Fusion Middleware Control (Enterprise Manager) using the following URL:\nhttp://${HOSTNAME}:${LB_PORT}/em Configure ingress path routing rules for an SSL port and enable WebLogic Plugin Enabled The following example sets up load balancer routing for access to the WebLogic Server Administration Console through an SSL port.\nEnable the WebLogic Plugin Enabled setting in the WebLogic configuration:\nThe WebLogic configuration setting WebLogic Plugin Enabled, when set to true, informs WebLogic Server about the presence of a load balancer proxy. Failure to have this setting enabled causes unexpected results in cases where the client IP address is required or when SSL terminates at the load balancer.\nWhen using WDT to configure a WebLogic domain, use the resource section at the domain level in a model YAML file:\nresources: WebAppContainer: WeblogicPluginEnabled: true When using a WLST script to configure a WebLogic domain, use these commands:\n# Configure the Administration Server cd(\u0026#39;/Servers/AdminServer\u0026#39;) set(\u0026#39;WeblogicPluginEnabled\u0026#39;,true) ... cd(\u0026#39;/Clusters/%s\u0026#39; % cluster_name) set(\u0026#39;WeblogicPluginEnabled\u0026#39;,true) Configure an ingress path routing rule and update the ingress resource with a customRequestHeaders value:\nFor example, see the following path-routing YAML file for a Traefik load balancer. In the case of SSL termination, Traefik must pass a custom header WL-Proxy-SSL:true to the WebLogic Server endpoints.\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: annotations: kubernetes.io/ingress.class: traefik name: traefik-console-tls namespace: weblogic-domain spec: entryPoints: - websecure routes: - kind: Rule match: PathPrefix(`/console`) middlewares: - name: tls-console-middleware namespace: weblogic-domain services: - kind: Service name: domain1-adminserver namespace: weblogic-domain port: 7002 --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: tls-console-middleware namespace: weblogic-domain spec: headers: customRequestHeaders: WL-Proxy-SSL: \u0026#34;true\u0026#34; sslRedirect: true Access the WebLogic Server Administration Console using the HTTPS port:\nGet the SSL port from the Kubernetes service:\n# Get the ingress controller secure web port $ SSLPORT=$(kubectl -n traefik get service traefik-operator -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;websecure\u0026#34;)].nodePort}\u0026#39;) From your browser, use the following URL to access the WebLogic Server Administration Console:\nhttps://${HOSTNAME}:${SSLPORT}/console If you have an FMW Infrastructure domain, then you can add an ingress path routing rule for the PathPrefix /em and access Fusion Middleware Control (Enterprise Manager) using the following URL:\nhttps://${HOSTNAME}:${SSLPORT}/em Use an Administration Server NodePort Use the following steps to configure a NodePort to access the WebLogic Server Administration Console:\nUpdate the WebLogic Administration Server configuration to add a Network Access Point (custom channel) with the HTTP protocol, and expose this channel on a NodePort service using the domain.spec.adminServer.adminService.channels attribute.\nFor an example of setting up the NodePort on an Administration Server, see Use a NodePort. For information about the NodePort Service on an Administration Server, see the Domain resource document.\nFrom your browser, use the following URL to access the WebLogic Server Administration Console:\nhttp://hostname:adminserver-NodePort/console The adminserver-NodePort is the port number of the Administration Server outside the Kubernetes cluster.\nIf you have an FMW Infrastructure domain, then you can also access Fusion Middleware Control (Enterprise Manager) using the following URL:\nhttp://hostname:adminserver-NodePort/em Use a kubectl port-forward connection A Kubernetes port forward command is convenient for development use cases but is not recommended for production use cases. It creates a local process external to a Kubernetes cluster that accepts external traffic on a dedicated local port and forwards this traffic to a specific pod and port in the Kubernetes cluster. If you have multiple domains, then each domain will require its own dedicated port forward command and a separate local port.\nForward a local port (that is external to Kubernetes) to the administration port of the Administration Server Pod according to these instructions.\nNOTE: If you plan to access the WebLogic Server Administration Console from a browser on a different machine than the port forwarding command, then the port forwarding command needs to specify an --address parameter with an externally accessible IP address for the machine that is running the command.\nIn the browser, use the following URL:\nhttp://${HOSTNAME}:${LOCAL_PORT}/console Where:\n${HOSTNAME} is the DNS address or the IP address of the machine where the kubectl port-forward command is running. This is customizable using the --address parameter and is localhost or 127.0.0.1, by default.\n${LOCAL_PORT} is the local port specified on the kubectl port-forward command line.\nIf you have an FMW Infrastructure domain, then you can also access Fusion Middleware Control (Enterprise Manager) using the following URL:\nhttp://${HOSTNAME}:${LOCAL_PORT}/em Test To verify that your WebLogic Server Administration Server URL is correct, and to verify that that your load balancer, NodePort, or kubectl port-forward are working as expected, run the following curl command at the same location as your browser:\n$ curl http://${HOSTNAME}:${LB_PORT}/console \u0026gt; /dev/null \u0026amp;\u0026amp; echo \u0026#34;Connection succeeded.\u0026#34; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 271 100 271 0 0 90333 0 --:--:-- --:--:-- --:--:-- 90333 Connection succeeded. If successful, then you will see the Connection succeeded message in the output from the command.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/accessing-the-domain/remote-admin-console/",
	"title": "Use the Remote Console",
	"tags": [],
	"description": "Use the WebLogic Remote Console with domains running in Kubernetes.",
	"content": "Contents Introduction Setup Use an Administration Server NodePort Configure ingress path routing rules Use a kubectl port-forward connection Test Introduction The WebLogic Remote Console is a lightweight, open source console that does not need to be collocated with a WebLogic Server domain. It is an alternative to the WebLogic Server Administration Console. You can install and run the Remote Console anywhere. For an introduction, read the blog, \u0026ldquo;The NEW WebLogic Remote Console\u0026rdquo;. For detailed documentation, see the WebLogic Remote Console.\nA major benefit of using the Remote Console is that it runs in your browser or as a desktop application, and can be used to connect to different WebLogic Server instances. You can use the Remote Console with WebLogic Server slim installers, available on the OTN or OSDC. Slim installers reduce the size of WebLogic Server downloads, installations, container images, and Kubernetes pods. For example, a WebLogic Server 12.2.1.4 slim installer download is approximately 180 MB smaller.\nThe Remote Console is deployed as a standalone application, which can connect to multiple WebLogic Server Administration Servers using REST APIs. You connect to the Remote Console and, when prompted, supply the WebLogic Server login credentials along with the URL of the WebLogic Server Administration Server\u0026rsquo;s administration port to which you want to connect.\nNOTES:\nAn Administration Server administration port typically is the same as its default port unless either an SSL port or an administration port is configured and enabled. If your domain home type is either Domain in Image or Model in Image, then do not use the WebLogic Remote Console to make changes to the WebLogic domain configuration because these changes are ephemeral and will be lost when servers restart. See Choose a domain home source type. Externally exposing administrative, RMI, or T3 capable WebLogic channels using a Kubernetes NodePort, load balancer, port forwarding, or a similar method can create an insecure configuration. For more information, see External network access security.\nSetup To set up access to WebLogic Server domains running in Kubernetes using the Remote Console:\nInstall, configure, and start the Remote Console according to these instructions.\nNOTE: These instructions assume that you are installing and running the Remote Console externally to your Kubernetes cluster.\nFor additional functionality, incorporate and deploy the WebLogic Remote Console extension in your 12.2.1.4 14.1.1, and 14.1.2 domains. NOTE: As a best practice, make sure that you are using the same versions of the WebLogic Remote Console and the WebLogic Remote Console Extension, otherwise you might lose functionality.\na. From https://github.com/oracle/weblogic-remote-console/releases, download the Remote Console extension WAR file, console-rest-ext-[version].war.\nb. Using the WebLogic Deploy Tooling (WDT) Archive Helper Tool, modify the WDT application archive to include the Remote Console Extension downloaded in the previous step. For example:\n/Directory to WDT/weblogic-deploy/bin/archiveHelper.sh add weblogicRemoteConsoleExtension -archive_file=/Directory to WDT application archive/archive.zip -source=/Directory to Remote Console Extension/console-rest-ext[version].war For more information, see the Archive Helper Tool documentation.\nc. With the WebLogic Image Tool, create an auxiliary image and include the archive modified in the previous step.\nd. Provision or update the domain using the new auxiliary image.\nWhen you first launch the Remote Console, it will prompt you with a login dialog for a WebLogic Server Administration Server URL. To give the Remote Console access to an Administration Server running in Kubernetes, you can:\nUse an Administration Server NodePort.\nDeploy a load balancer with ingress path routing rules.\nUse a kubectl port-forward connection.\nNOTE: If you want the Remote Console to use SSL to connect to the WebLogic Server Administration Server, then see Connect to a WebLogic domain using SSL/TLS.\nUse an Administration Server NodePort For the Remote Console to connect to the Kubernetes WebLogic Server Administration Server’s NodePort, use the following URL after you have launched the Remote Console and it prompts for the location of your WebLogic Server Administration Server:\nhttp://hostname:adminserver-NodePort/ The adminserver-NodePort is the port number of the Administration Server outside the Kubernetes cluster. For information about the NodePort Service on an Administration Server, see the Domain resource document. For an example of setting up the NodePort on an Administration Server, see Use a NodePort for WLST.\nConfigure ingress path routing rules Configure an ingress path routing rule. For information about ingresses, see the Ingress documentation.\nFor an example, see the following path-routing YAML file for a Traefik load balancer:\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: annotations: kubernetes.io/ingress.class: traefik name: traefik-pathrouting-1 namespace: weblogic-domain spec: routes: - kind: Rule match: PathPrefix(`/`) services: - kind: Service name: domain1-adminserver namespace: weblogic-domain port: 7001 After you have connected to the Remote Console with your browser, it will prompt for the location of your WebLogic Server Administration Server. For the Remote Console to connect to the Kubernetes WebLogic Server Administration Server, supply a URL that resolves to the load balancer host and ingress that you supplied in the previous step. For example:\nhttp://${HOSTNAME}:${LB_PORT}/ Where:\n${HOSTNAME} is where the ingress load balancer is running.\nTo determine the ${LB_PORT} when using a Traefik load balancer:\n$ export LB_PORT=$(kubectl -n traefik get service traefik-operator -o jsonpath='{.spec.ports[?(@.name==\u0026quot;web\u0026quot;)].nodePort}')\nUse a kubectl port-forward connection Forward a local port (that is external to Kubernetes) to the administration port of the Administration Server Pod according to these instructions.\nNOTE: If you plan to run the Remote Console on a different machine than the port forwarding command, then the port forwarding command needs to specify a --address parameter with the IP address of the machine that is hosting the command.\nAfter you have connected to the Remote Console with your browser, it will prompt you for the location of your WebLogic Server Administration Server. Supply a URL using the local hostname or IP address from the port-forward command in the first step, plus the local port from this same command. For example:\nhttp://${LOCAL_HOSTNAME}:${LOCAL_PORT}/ Where:\n${LOCAL_HOSTNAME} is the hostname or the defined IP address of the machine where the kubectl port-forward command is running. This is customizable on the port-forward command and is localhost or 127.0.0.1, by default.\n${LOCAL_PORT} is the local port where the kubectl port-forward command is running. This is specified on the port-forward command.\nTest To verify that your WebLogic Server Administration Server URL is correct, and to verify that that your load balancer, NodePort, or kubectl port-forward are working as expected, run the following curl commands at the same location as your browser:\n$ curl --user username:password \\ http://${HOSTNAME}:${LB_PORT}/management/weblogic/latest/domainRuntime?fields=name\\\u0026amp;links=none ; echo $ curl --user username:password \\ http://${HOSTNAME}:${LB_PORT}/management/weblogic/latest/serverRuntime?fields=name\\\u0026amp;links=none ; echo These commands access the REST interface of the WebLogic Server Administration Server in a way that is similar to the Remote Console\u0026rsquo;s use of REST. If successful, then the output from the two commands will be {\u0026quot;name\u0026quot;: \u0026quot;your-weblogic-domain-name\u0026quot;} and {\u0026quot;name\u0026quot;: \u0026quot;your-weblogic-admin-server-name\u0026quot;}, respectively.\nIf you want to see the full content of the domainRuntime and serverRuntime beans, then rerun the commands but remove ?fields=name\\\u0026amp;links=none, which is appended at the end of each URL.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/credentials/",
	"title": "Credentials",
	"tags": [],
	"description": "Sample for creating a Kubernetes Secret that contains the Administration Server credentials. This Secret can be used in creating a WebLogic Domain YAML file.",
	"content": "Creating credentials for a WebLogic domain This sample demonstrates how to create a Kubernetes Secret containing the credentials for a WebLogic domain. The operator expects this secret to be named following the pattern domainUID-weblogic-credentials, where domainUID is the unique identifier of the domain. It must be in the same namespace that the domain will run in.\nTo use the sample, run the command:\n$ ./create-weblogic-credentials.sh -u \u0026lt;username\u0026gt; -p \u0026lt;password\u0026gt; -d domainUID -n namespace -s secretName The parameters are as follows:\n-u user name, must be specified. -p password, must be provided using the -p argument or user will be prompted to enter a value. -d domainUID, optional. The default value is domain1. If specified, the secret will be labeled with the domainUID unless the given value is an empty string. -n namespace, optional. Use the default namespace if not specified. -s secretName, optional. If not specified, the secret name will be determined based on the domainUID value. This creates a generic secret containing the user name and password as literal values.\nYou can check the secret with the kubectl get secret command. The following example is shown, including the output:\n$ kubectl -n domain-namespace-1 get secret domain1-weblogic-credentials -o yaml apiVersion: v1 data: username: \u0026lt;user name\u0026gt; password: \u0026lt;password\u0026gt; kind: Secret metadata: creationTimestamp: 2018-12-12T20:25:20Z labels: weblogic.domainName: domain1 weblogic.domainUID: domain1 name: domain1-weblogic-credentials namespace: domain-namespace-1 resourceVersion: \u0026#34;5680\u0026#34; selfLink: /api/v1/namespaces/domain-namespace-1/secrets/domain1-weblogic-credentials uid: 0c2b3510-fe4c-11e8-994d-00001700101d type: Opaque where \u0026lt;user name\u0026gt; and \u0026lt;password\u0026gt; are to be replaced with their actual values.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/domain-home-on-pv/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "Follow these prerequisite steps for WLS and JRF domain types.",
	"content": "Contents Prerequisites for WLS and JRF domain types Additional prerequisites for JRF domains Prerequisites for WLS and JRF domain types Choose the type of domain you’re going to use throughout the sample, WLS or JRF.\nThe JAVA_HOME environment variable must be set and must reference a valid JDK 8 or 11 installation.\nGet the operator source and put it in /tmp/weblogic-kubernetes-operator.\nFor example:\n$ cd /tmp $ git clone --branch v4.2.20 https://github.com/oracle/weblogic-kubernetes-operator.git NOTE: We will refer to the top directory of the operator source tree as /tmp/weblogic-kubernetes-operator; however, you can use a different location.\nFor additional information about obtaining the operator source, see the Developer Guide Requirements.\nCopy the Domain on PV sample to a new directory; for example, use directory /tmp/sample.\n$ mkdir -p /tmp/sample $ cp -r /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain/domain-on-pv/* /tmp/sample NOTE: We will refer to this working copy of the sample as /tmp/sample; however, you can use a different location.\nCopy the wdt-artifacts directory of the sample to a new directory; for example, use directory /tmp/sample/wdt-artifacts.\n$ mkdir -p /tmp/sample/wdt-artifacts $ cp -r /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain/wdt-artifacts/* /tmp/sample/wdt-artifacts Download the latest WebLogic Deploy Tooling (WDT) and WebLogic Image Tool (WIT) installer ZIP files to your /tmp/sample/wdt-artifacts directory. Both WDT and WIT are required to create the images.\n$ cd /tmp/sample/wdt-artifacts $ curl -m 120 -fL https://github.com/oracle/weblogic-deploy-tooling/releases/latest/download/weblogic-deploy.zip \\ -o /tmp/sample/wdt-artifacts/weblogic-deploy.zip $ curl -m 120 -fL https://github.com/oracle/weblogic-image-tool/releases/latest/download/imagetool.zip \\ -o /tmp/sample/wdt-artifacts/imagetool.zip To set up the WebLogic Image Tool, run the following commands:\n$ cd /tmp/sample/wdt-artifacts $ unzip imagetool.zip $ ./imagetool/bin/imagetool.sh cache deleteEntry --key wdt_latest $ ./imagetool/bin/imagetool.sh cache addInstaller \\ --type wdt \\ --version latest \\ --path /tmp/sample/wdt-artifacts/weblogic-deploy.zip Note that the WebLogic Image Tool cache deleteEntry command does nothing if the wdt_latest key doesn\u0026rsquo;t have a corresponding cache entry. It is included because the WIT cache lookup information is stored in the $HOME/cache/.metadata file by default, and if the cache already has a version of WDT in its --type wdt --version latest location, then the cache addInstaller command would fail. For more information about the WIT cache, see the WIT Cache documentation.\nThese steps will install WIT to the /tmp/sample/wdt-artifacts/imagetool directory, plus put a wdt_latest entry in the tool\u0026rsquo;s cache which points to the WDT ZIP file installer. You will use WIT and its cached reference to the WDT installer later in the sample for creating model images.\nTo set up the WebLogic Deploy Tooling that we will use later for the archive helper, run the following command:\n$ unzip /tmp/sample/wdt-artifacts/weblogic-deploy.zip Make sure an operator is set up to manage the namespace, sample-domain1-ns. Also, make sure a Traefik ingress controller is managing the same namespace and listening on port 30305. To do this, follow the same steps as the Quick Start guide up through the Prepare for a domain step.\nMake sure you stop when you complete the \u0026ldquo;Prepare for a domain\u0026rdquo; step and then resume following these instructions.\nSet up ingresses that will redirect HTTP from the Traefik port 30305 to the clusters in this sample\u0026rsquo;s WebLogic domains.\nRun kubectl apply -f on each of the ingress YAML files that are already included in the sample source directory:\n$ kubectl apply -f https://raw.githubusercontent.com/oracle/weblogic-kubernetes-operator/release/4.2/kubernetes/samples/scripts/create-weblogic-domain/ingresses/traefik-ingress-sample-domain1-admin-server.yaml $ kubectl apply -f https://raw.githubusercontent.com/oracle/weblogic-kubernetes-operator/release/4.2/kubernetes/samples/scripts/create-weblogic-domain/ingresses/traefik-ingress-sample-domain1-cluster-cluster-1.yaml NOTE: We give each cluster ingress a different host name that is decorated using both its operator domain UID and its cluster name. This makes each cluster uniquely addressable even when cluster names are the same across different clusters. When using curl to access the WebLogic domain through the ingress, you will need to supply a host name header that matches the host names in the ingress.\nFor more information on ingresses and load balancers, see Ingress.\nObtain the WebLogic 12.2.1.4 image that is referenced by the sample\u0026rsquo;s Domain resource YAML.\na. Use a browser to access the Oracle Container Registry.\nb. Choose an image location: for JRF domains, select Middleware, then fmw-infrastructure; for WLS domains, select Middleware, then weblogic.\nc. Select Sign In and accept the license agreement.\nd. Use your terminal to log in to the container registry: docker login container-registry.oracle.com.\ne. Later in this sample, when you run WebLogic Image Tool commands, the tool will use the image as a base image for creating model images. Specifically, the tool will implicitly call docker pull for one of the previous licensed images as specified in the tool\u0026rsquo;s command line using the --fromImage parameter. For JRF, this sample specifies container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4, and for WLS, the sample specifies container-registry.oracle.com/middleware/weblogic:12.2.1.4.\nThe example base images are General Availability (GA) images that are suitable for demonstration and development purposes only where the environments are not available from the public Internet; they are not acceptable for production use. In production, you should always use CPU (patched) images from OCR or create your images using the WebLogic Image Tool (WIT) with the --recommendedPatches option. For more guidance, see Apply the Latest Patches and Updates in Securing a Production Environment for Oracle WebLogic Server.\nAdditional prerequisites for JRF domains NOTE: If you\u0026rsquo;re using a Domain on PV, WLS domain type, skip this section and continue on to Build the domain creation image for the Domain on PV sample.\nJRF prerequisites Introduction to JRF setups Set up and initialize an infrastructure database Important considerations for RCU model attributes, Domain fields, and secrets Introduction to JRF setups NOTE: The requirements in this section are in addition to Prerequisites for WLS and JRF domain types.\nA JRF domain requires an infrastructure database, and configuring your domain to access this database. For more details, see JRF domains in the user documentation. You must perform all these steps before you create your domain.\nSet up and initialize an infrastructure database A JRF domain requires an infrastructure database and requires initializing this database with a schema and a set of tables for each different domain. The following example shows how to set up a database. The database is set up with the following attributes:\nAttribute Value database Kubernetes namespace default database Kubernetes pod oracle-db database image container-registry.oracle.com/database/enterprise:12.2.0.1-slim database password MY_DBA_PASSWORD infrastructure schema prefix FMW1 (for domain1) infrastructure schema password MY_RCU_SCHEMA_PASSWORD database URL oracle-db.default.svc.cluster.local:1521/devpdb.k8s Ensure that you have access to the database image, and then create a deployment using it:\nUse a browser to log in to https://container-registry.oracle.com, select Database -\u0026gt; enterprise and accept the license agreement.\nGet the database image:\nIn the local shell, docker login container-registry.oracle.com. In the local shell, docker pull container-registry.oracle.com/database/enterprise:12.2.0.1-slim. Deploy a secret named oracle-db-secret with your desired Oracle DBA password for its SYS account.\nIn the local shell: $ kubectl -n default create secret generic oracle-db-secret \\ --from-literal=\u0026#39;password=MY_DBA_PASSWORD\u0026#39; Replace MY_DBA_PASSWORD with your desired value. Oracle Database passwords can contain upper case, lower case, digits, and special characters. Use only _ and # as special characters to eliminate potential parsing errors in Oracle connection strings. NOTE: Record or memorize the value you chose for MY_DBA_PASSWORD. It will be be needed again in other parts of this sample. Use the sample script in /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oracle-db-service to create an Oracle database running in the pod, oracle-db.\n$ cd /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oracle-db-service $ start-db-service.sh This script will deploy a database in the default namespace with the connect string oracle-db.default.svc.cluster.local:1521/devpdb.k8s, and administration password MY_DBA_PASSWORD.\nThis step is based on the steps documented in Run a Database.\nNOTE: If your Kubernetes cluster nodes do not all have access to the database image in a local cache, then deploy a Kubernetes docker secret to the default namespace with login credentials for container-registry.oracle.com, and pass the name of this secret as a parameter to start-db-service.sh using -s your-image-pull-secret. Alternatively, copy the database image to each local Docker cache in the cluster. For more information, see the Cannot pull image FAQ.\nWARNING: The Oracle Database images are supported only for non-production use. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1).\nImportant considerations for RCU model attributes, Domain fields, and secrets To allow the operator to access the database and OPSS wallet, you must create an RCU access secret containing the database connect string, user name, and password that\u0026rsquo;s referenced from your model and an OPSS wallet password secret that\u0026rsquo;s referenced from your Domain before deploying your domain. It\u0026rsquo;s also necessary to define an RCUDbInfo stanza in your model.\nThe sample includes examples of JRF models and Domain YAML files in the /tmp/sample/wdt-artifacts/wdt-model-files and /tmp/sample/domain-resources directories, and instructions in the following sections will describe setting up the RCU and OPSS secrets.\nWhen you follow the instructions in the samples, avoid instructions that are WLS only, and substitute JRF for WLS in the corresponding model image tags and Domain YAML file names.\nFor example, in this sample:\nJRF Domain YAML file has an configuration.opss.walletPasswordSecret field that references a secret named sample-domain1-opss-wallet-password-secret, with a walletPassword of your choice.\nJRF domain creation image models have the following domainInfo -\u0026gt; RCUDbInfo stanza that references a sample-domain1-rcu-access secret with the appropriate values for attributes, rcu_prefix, rcu_schema_password, and rcu_db_conn_string, for accessing the Oracle database that you deployed to the default namespace as one of the prerequisite steps.\nRCUDbInfo: rcu_prefix: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_prefix@@\u0026#39; rcu_schema_password: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_schema_password@@\u0026#39; rcu_db_conn_string: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_db_conn_string@@\u0026#39; For important JRF domain information, refer to JRF domains.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/model-in-image/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "Follow these prerequisite steps for WLS domain type.",
	"content": "Prerequisites The JAVA_HOME environment variable must be set and must reference a valid JDK 8 or 11 installation.\nGet the operator source and put it in /tmp/weblogic-kubernetes-operator.\nFor example:\n$ cd /tmp $ git clone --branch v4.2.20 https://github.com/oracle/weblogic-kubernetes-operator.git NOTE: We will refer to the top directory of the operator source tree as /tmp/weblogic-kubernetes-operator; however, you can use a different location.\nFor additional information about obtaining the operator source, see the Developer Guide Requirements.\nCopy the Model in Image sample to a new directory; for example, use directory /tmp/sample.\n$ mkdir -p /tmp/sample $ cp -r /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain/model-in-image/* /tmp/sample NOTE: We will refer to this working copy of the sample as /tmp/sample; however, you can use a different location.\nCopy the wdt-artifacts directory of the sample to a new directory; for example, use directory /tmp/sample/wdt-artifacts.\n$ mkdir -p /tmp/sample/wdt-artifacts $ cp -r /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain/wdt-artifacts/* /tmp/sample/wdt-artifacts Download the latest WebLogic Deploy Tooling (WDT) and WebLogic Image Tool (WIT) installer ZIP files to your /tmp/sample/wdt-artifacts directory. Both WDT and WIT are required to create the images.\n$ cd /tmp/sample/wdt-artifacts $ curl -m 120 -fL https://github.com/oracle/weblogic-deploy-tooling/releases/latest/download/weblogic-deploy.zip \\ -o /tmp/sample/wdt-artifacts/weblogic-deploy.zip $ curl -m 120 -fL https://github.com/oracle/weblogic-image-tool/releases/latest/download/imagetool.zip \\ -o /tmp/sample/wdt-artifacts/imagetool.zip To set up the WebLogic Image Tool, run the following commands:\n$ cd /tmp/sample/wdt-artifacts $ unzip imagetool.zip $ ./imagetool/bin/imagetool.sh cache deleteEntry --key wdt_latest $ ./imagetool/bin/imagetool.sh cache addInstaller \\ --type wdt \\ --version latest \\ --path /tmp/sample/wdt-artifacts/weblogic-deploy.zip Note that the WebLogic Image Tool cache deleteEntry command does nothing if the wdt_latest key doesn\u0026rsquo;t have a corresponding cache entry. It is included because the WIT cache lookup information is stored in the $HOME/cache/.metadata file by default, and if the cache already has a version of WDT in its --type wdt --version latest location, then the cache addInstaller command would fail. For more information about the WIT cache, see the WIT Cache documentation.\nThese steps will install WIT to the /tmp/sample/wdt-artifacts/imagetool directory, plus put a wdt_latest entry in the tool\u0026rsquo;s cache which points to the WDT ZIP file installer. You will use WIT and its cached reference to the WDT installer later in the sample for creating model images.\nTo set up the WebLogic Deploy Tooling that we will use later for the archive helper, run the following command:\n$ unzip /tmp/sample/wdt-artifacts/weblogic-deploy.zip Make sure an operator is set up to manage the namespace, sample-domain1-ns. Also, make sure a Traefik ingress controller is managing the same namespace and listening on port 30305. To do this, follow the same steps as the Quick Start guide up through the Prepare for a domain step.\nMake sure you stop when you complete the \u0026ldquo;Prepare for a domain\u0026rdquo; step and then resume following these instructions.\nSet up ingresses that will redirect HTTP from the Traefik port 30305 to the clusters in this sample\u0026rsquo;s WebLogic domains.\nRun kubectl apply -f on each of the ingress YAML files that are already included in the sample source directory:\n$ kubectl apply -f https://raw.githubusercontent.com/oracle/weblogic-kubernetes-operator/release/4.2/kubernetes/samples/scripts/create-weblogic-domain/ingresses/traefik-ingress-sample-domain1-admin-server.yaml $ kubectl apply -f https://raw.githubusercontent.com/oracle/weblogic-kubernetes-operator/release/4.2/kubernetes/samples/scripts/create-weblogic-domain/ingresses/traefik-ingress-sample-domain1-cluster-cluster-1.yaml $ kubectl apply -f https://raw.githubusercontent.com/oracle/weblogic-kubernetes-operator/release/4.2/kubernetes/samples/scripts/create-weblogic-domain/ingresses/traefik-ingress-sample-domain2-cluster-cluster-1.yaml NOTE: We give each cluster ingress a different host name that is decorated using both its operator domain UID and its cluster name. This makes each cluster uniquely addressable even when cluster names are the same across different clusters. When using curl to access the WebLogic domain through the ingress, you will need to supply a host name header that matches the host names in the ingress.\nFor more information on ingresses and load balancers, see Ingress.\nObtain the WebLogic 12.2.1.4 image that is referenced by the sample\u0026rsquo;s Domain resource YAML.\na. Use a browser to access the Oracle Container Registry.\nc. Select Sign In and accept the license agreement.\nd. Use your terminal to log in to the container registry: docker login container-registry.oracle.com.\ne. Later in this sample, when you run WebLogic Image Tool commands, the tool will use the image as a base image for creating model images. Specifically, the tool will implicitly call docker pull for one of the previous licensed images as specified in the tool\u0026rsquo;s command line using the --fromImage parameter.\nThe example base images are General Availability (GA) images that are suitable for demonstration and development purposes only where the environments are not available from the public Internet; they are not acceptable for production use. In production, you should always use CPU (patched) images from OCR or create your images using the WebLogic Image Tool (WIT) with the --recommendedPatches option. For more guidance, see Apply the Latest Patches and Updates in Securing a Production Environment for Oracle WebLogic Server.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/storage/",
	"title": "Storage",
	"tags": [],
	"description": "Sample for creating a PV or PVC that can be used by a Domain YAML file as the persistent storage for the WebLogic domain home or log files.",
	"content": "Sample PersistentVolume and PersistentVolumeClaim The sample scripts demonstrate the creation of a Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC), which can then be used in a Domain YAML file as a persistent storage for the WebLogic domain home or log files.\nA PV and PVC can be shared by multiple WebLogic domains or dedicated to a particular domain.\nPrerequisites Before you begin, read this document, Persistent storage.\nUsing the scripts to create a PV and PVC Prior to running the create-pv-pvc.sh script, make a copy of the create-pv-pvc-inputs.yaml file, and uncomment and explicitly configure the weblogicDomainStoragePath property in the inputs file.\nRun the create script, pointing it at your inputs file and an output directory:\n$ ./create-pv-pvc.sh \\ -i create-pv-pvc-inputs.yaml \\ -o /path/to/output-directory The create-pv-pvc.sh script will create a subdirectory pv-pvcs under the given /path/to/output-directory directory. By default, the script generates two YAML files, namely weblogic-sample-pv.yaml and weblogic-sample-pvc.yaml, in the /path/to/output-directory/pv-pvcs. These two YAML files can be used to create the Kubernetes resources using the kubectl create -f command.\n$ kubectl create -f weblogic-sample-pv.yaml $ kubectl create -f weblogic-sample-pvc.yaml As a convenience, the script can optionally create the PV and PVC resources using the -e option.\nThe usage of the create script is as follows:\n$ sh create-pv-pvc.sh -h usage: create-pv-pvc.sh -i file -o dir [-e] [-h] -i Parameter inputs file, must be specified. -o Output directory for the generated yaml files, must be specified. -e Also create the Kubernetes objects using the generated yaml files -h Help If you copy the sample scripts to a different location, make sure that you copy everything in the \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts directory together into the target directory, maintaining the original directory hierarchy.\nConfiguration parameters The PV and PVC creation inputs can be customized by editing the create-pv-pvc-inputs.yaml file.\nParameter Definition Default domainUID ID of the Domain to which the generated PV and PVC will be dedicated. Leave it empty if the PV and PVC are going to be shared by multiple domains. no default namespace Kubernetes Namespace to create the PVC. default baseName Base name of the PV and PVC. The generated PV and PVC will be \u0026lt;baseName\u0026gt;-pv and \u0026lt;baseName\u0026gt;-pvc respectively. weblogic-sample weblogicDomainStoragePath Physical path of the storage for the PV. When weblogicDomainStorageType is set to HOST_PATH, this value should be set the to path to the domain storage on the Kubernetes host. When weblogicDomainStorageType is set to NFS, then weblogicDomainStorageNFSServer should be set to the IP address or name of the DNS server, and this value should be set to the exported path on that server. Note that the path where the domain is mounted in the WebLogic containers is not affected by this setting; that is determined when you create your domain. no default weblogicDomainStorageReclaimPolicy Kubernetes PVC policy for the persistent storage. The valid values are: Retain, Delete, and Recycle. Retain weblogicDomainStorageSize Total storage allocated for the PVC. 10Gi weblogicDomainStorageType Type of storage. Legal values are NFS and HOST_PATH. If using NFS, weblogicDomainStorageNFSServer must be specified. HOST_PATH weblogicDomainStorageNFSServer Name or IP address of the NFS server. This setting only applies if weblogicDomainStorateType is NFS. no default Shared versus dedicated PVC By default, the domainUID is left empty in the inputs file, which means the generated PV and PVC will not be associated with a particular domain, but can be shared by multiple Domains in the same Kubernetes Namespaces as the PV and PVC. If the PV/PVC is being shared across domains, then, as a best practice, you should specify a unique baseName.\nFor the use cases where dedicated PV and PVC are desired for a particular domain, the domainUID needs to be set in the create-pv-pvc-inputs.yaml file. The presence of a non-empty domainUID in the inputs file will cause the generated PV and PVC to be associated with the specified domainUID. The association includes that the names of the generated YAML files and the Kubernetes PV and PVC objects are decorated with the domainUID, and the PV and PVC objects are also labeled with the domainUID.\nVerify the results The create script will verify that the PV and PVC were created, and will report a failure if there was any error. However, it may be desirable to manually verify the PV and PVC, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nGenerated YAML files with the default inputs The content of the generated weblogic-sample-pvc.yaml:\n# Copyright 2018, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: weblogic-sample-pvc namespace: default storageClassName: weblogic-sample-storage-class accessModes: - ReadWriteMany resources: requests: storage: 10Gi The content of the generated weblogic-sample-pv.yaml:\n# Copyright 2018, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. apiVersion: v1 kind: PersistentVolume metadata: name: weblogic-sample-pv # labels: # weblogic.domainUID: spec: storageClassName: weblogic-sample-storage-class capacity: storage: 10Gi accessModes: - ReadWriteMany # Valid values are Retain, Delete or Recycle persistentVolumeReclaimPolicy: Retain hostPath: # nfs: # server: %SAMPLE_STORAGE_NFS_SERVER% path: \u0026#34;/scratch/k8s_dir\u0026#34; Generated YAML files for dedicated PV and PVC The content of the generated domain1-weblogic-sample-pvc.yaml when domainUID is set to domain1:\n# Copyright 2018, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: domain1-weblogic-sample-pvc namespace: default labels: weblogic.domainUID: domain1 spec: storageClassName: domain1-weblogic-sample-storage-class accessModes: - ReadWriteMany resources: requests: storage: 10Gi The content of the generated domain1-weblogic-sample-pv.yaml when domainUID is set to domain1:\n# Copyright 2018, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. apiVersion: v1 kind: PersistentVolume metadata: name: domain1-weblogic-sample-pv labels: weblogic.domainUID: domain1 spec: storageClassName: domain1-weblogic-sample-storage-class capacity: storage: 10Gi accessModes: - ReadWriteMany # Valid values are Retain, Delete or Recycle persistentVolumeReclaimPolicy: Retain hostPath: # nfs: # server: %SAMPLE_STORAGE_NFS_SERVER% path: \u0026#34;/scratch/k8s_dir\u0026#34; Verify the PV and PVC objects You can use this command to verify the PersistentVolume was created. Note that the Status field should have the value Bound, indicating the that PersistentVolume has been claimed:\n$ kubectl describe pv weblogic-sample-pv Name: weblogic-sample-pv Annotations: pv.kubernetes.io/bound-by-controller=yes StorageClass: weblogic-sample-storage-class Status: Bound Claim: default/weblogic-sample-pvc Reclaim Policy: Retain Access Modes: RWX Capacity: 10Gi Message: Source: Type: HostPath (bare host directory volume) Path: /scratch/k8s_dir HostPathType: Events: \u0026lt;none\u0026gt; You can use this command to verify the PersistentVolumeClaim was created:\n$ kubectl describe pvc weblogic-sample-pvc Name: weblogic-sample-pvc Namespace: default StorageClass: weblogic-sample-storage-class Status: Bound Volume: weblogic-sample-pv Annotations: pv.kubernetes.io/bind-completed=yes pv.kubernetes.io/bound-by-controller=yes Finalizers: [] Capacity: 10Gi Access Modes: RWX Events: \u0026lt;none\u0026gt; "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/developerguide/contributing/",
	"title": "Contribute to the operator",
	"tags": [],
	"description": "Learn how to contribute to the operator project.",
	"content": "Oracle welcomes contributions to this project from anyone. Contributions may be reporting an issue with the operator or submitting a pull request. Before embarking on significant development that may result in a large pull request, it is recommended that you create an issue and discuss the proposed changes with the existing developers first.\nIf you want to submit a pull request to fix a bug or enhance an existing feature, please first open an issue and link to that issue when you submit your pull request.\nIf you have any questions about a possible submission, feel free to open an issue too.\nContributing to the WebLogic Kubernetes Operator repository Pull requests can be made under The Oracle Contributor Agreement (OCA), which is available at https://oca.opensource.oracle.com/.\nFor pull requests to be accepted, the bottom of the commit message must have the following line, using the contributor’s name and e-mail address as it appears in the OCA Signatories list.\nSigned-off-by: Your Name \u0026lt;you@example.org\u0026gt; This can be automatically added to pull requests by committing with:\n$ git commit --signoff Only pull requests from committers that can be verified as having signed the OCA can be accepted.\nPull request process Fork the repository. Create a branch in your fork to implement the changes. We recommend using the issue number as part of your branch name, for example, 1234-fixes. Ensure that any documentation is updated with the changes that are required by your fix. Ensure that any samples are updated if the base image has been changed. Submit the pull request. Do not leave the pull request blank. Explain exactly what your changes are meant to do and provide simple steps on how to validate your changes. Ensure that you reference the issue you created as well. We will assign the pull request to 2-3 people for review before it is merged. Introducing a new dependency Please be aware that pull requests that seek to introduce a new dependency will be subject to additional review. In general, contributors should avoid dependencies with incompatible licenses, and should try to use recent versions of dependencies. Standard security vulnerability checklists will be consulted before accepting a new dependency. Dependencies on closed-source code, including WebLogic Server, will most likely be rejected.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/developerguide/requirements/",
	"title": "Requirements",
	"tags": [],
	"description": "Review the software requirements to obtain and build the operator.",
	"content": "In addition to the requirements listed here, the following software is also required to obtain and build the operator:\nGit (1.8 or later recommended) Java Developer Kit (11 required, 11.0.2 recommended) Apache Maven (3.5.3 min, 3.6 recommended) The operator is written primarily in Java, BASH shell scripts, and WLST scripts.\nBecause the target runtime environment for the operator is Oracle Linux, no particular effort has been made to ensure the build or tests run on any other operating system. Please be aware that Oracle will not provide support, or accept pull requests to add support for other operating systems.\nObtaining the operator source code The operator source code is published on GitHub at https://github.com/oracle/weblogic-kubernetes-operator. Developers may clone this repository to a local machine or, if desired, create a fork in their personal namespace and clone the fork. Developers who are planning to submit a pull request are advised to create a fork.\nTo clone the repository from GitHub, issue this command:\n$ git clone https://github.com/oracle/weblogic-kubernetes-operator.git "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/domain-lifecycle/startup/",
	"title": "Startup and shutdown",
	"tags": [],
	"description": "There are fields on the Domain that specify which WebLogic Server instances should be running, started, or restarted. To start, stop, or restart servers, modify these fields on the Domain.",
	"content": "This document describes approaches for stopping, starting, rolling, and restarting WebLogic Server instances in a Kubernetes environment.\nContents Introduction Starting and stopping servers serverStartPolicy rules Available serverStartPolicy values Administration Server start and stop rules Standalone Managed Server start and stop rules Clustered Managed Server start and stop rules Common starting and stopping scenarios Normal running state Shut down all the servers Only start the Administration Server Shut down a cluster Shut down a specific standalone server Force a specific clustered Managed Server to start Shutdown options Shutdown environment variables shutdown rules Restarting servers Fields that cause servers to be restarted Rolling restarts Draining a node and PodDisruptionBudget Common restarting scenarios Using restartVersion to force the operator to restart servers Restart all the servers in the domain Restart all the servers in the cluster Restart the Administration Server Restart a standalone or clustered Managed Server Full domain restarts Domain lifecycle sample scripts Introduction There are fields on the Domain and the Cluster that specify which servers should be running, which servers should be restarted, and the desired initial state. To start, stop, or restart servers, modify these fields on the Domain or the Cluster (for example, by using kubectl or the Kubernetes REST API). The operator will detect the changes and apply them.\nStarting and stopping servers The serverStartPolicy and replicas fields of the Domain and the Cluster control which servers should be running, where a Cluster replicas field defaults to the corresponding Domain value. The operator monitors these fields and creates or deletes the corresponding WebLogic Server instance Pods.\nDo not use the WebLogic Server Administration Console to start or stop servers.\nserverStartPolicy rules You can specify the serverStartPolicy property at the domain.spec Domain level, the cluster.spec Cluster level, the domain.spec.managedServers Managed Server level, or the domain.spec.adminServer Administration Server level. Each level supports a different set of values.\nAvailable serverStartPolicy values Level Default Value Supported Values Domain IfNeeded IfNeeded, AdminOnly, Never Cluster IfNeeded IfNeeded, Never Server IfNeeded IfNeeded, Always, Never Administration Server start and stop rules Domain Admin Server Started / Stopped Never any value Stopped AdminOnly, IfNeeded Never Stopped AdminOnly, IfNeeded IfNeeded, Always Started Standalone Managed Server start and stop rules Domain Standalone Server Started / Stopped AdminOnly, Never any value Stopped IfNeeded Never Stopped IfNeeded IfNeeded, Always Started Clustered Managed Server start and stop rules Domain Cluster Clustered Server Started / Stopped AdminOnly, Never any value any value Stopped IfNeeded Never any value Stopped IfNeeded IfNeeded Never Stopped IfNeeded IfNeeded Always Started IfNeeded IfNeeded IfNeeded Started if needed to get to the cluster\u0026rsquo;s replicas count Servers configured as Always count toward the cluster\u0026rsquo;s replicas count.\nIf more servers are configured as Always than the cluster\u0026rsquo;s replicas count, they will all be started and the replicas count will be exceeded.\nCommon starting and stopping scenarios Normal running state Normally, the Administration Server, all of the standalone Managed Servers, and enough Managed Servers members in each cluster to satisfy its replicas count, should be started. In this case, the Domain does not need to specify serverStartPolicy, or list any clusters or servers, but it does need to specify a replicas count.\nFor example:\nkind: Domain metadata: name: domain1 spec: image: ... replicas: 3 The domain.spec.replicas field is the default for all clusters. Individual clusters may customize their replicas using cluster.spec.replicas.\nShut down all the servers Sometimes you need to completely shut down the domain (for example, take it out of service).\nkind: Domain metadata: name: domain1 spec: serverStartPolicy: Never ... Only start the Administration Server Sometimes you want to start the Administration Server only, that is, take the Managed Servers out of service but leave the Administration Server running so that you can administer the domain.\nkind: Domain metadata: name: domain1 spec: serverStartPolicy: AdminOnly ... Shut down a cluster To shut down a cluster (for example, take it out of service), add it to the Domain and set its serverStartPolicy to Never.\nkind: Cluster metadata: name: domain1-cluster1 spec: clusterName: \u0026#34;cluster1\u0026#34; serverStartPolicy: Never ... A Cluster resource must be referenced from the domain.spec.clusters and must have a .spec.clusterName that matches the corresponding cluster in the domain\u0026rsquo;s WebLogic configuration.\nShut down a specific standalone server To shut down a specific standalone server, add it to the Domain and set its serverStartPolicy to Never.\nkind: Domain metadata: name: domain1 spec: managedServers: - serverName: \u0026#34;server1\u0026#34; serverStartPolicy: Never ... The Administration Server can be shut down by setting the serverStartPolicy of the adminServer to Never. Care should be taken when shutting down the Administration Server. If a Managed Server cannot connect to the Administration Server during startup, it will try to start up in Managed Server Independence (MSI) mode but this could fail due to reasons such as no accessible Authentication Provider from the Managed Server pod.\nForce a specific clustered Managed Server to start Normally, all of the Managed Servers members in a cluster are identical and it doesn\u0026rsquo;t matter which ones are running as long as the operator starts enough of them to get to the cluster\u0026rsquo;s replicas count. However, sometimes some of the Managed Servers are different (for example, support some extra services that the other servers in the cluster use) and need to always be started.\nThis is done by adding the server to the Domain and setting its serverStartPolicy to Always.\nkind: Domain metadata: name: domain1 spec: managedServers: - serverName: \u0026#34;cluster1_server1\u0026#34; serverStartPolicy: Always ... The server will count toward the cluster\u0026rsquo;s replicas count. Also, if you configure more than the replicas servers count to Always, they will all be started, even though the replicas count will be exceeded.\nShutdown options The Domain and Cluster YAML files include the field serverPod that is available under domain.spec, domain.adminServer, each entry of domain.spec.managedServers, and cluster.spec. The serverPod field controls many details of how Pods are generated for WebLogic Server instances.\nThe shutdown field of serverPod controls how managed servers will be shut down and has the following four properties: shutdownType, timeoutSeconds, ignoreSessions and waitForAllSessions. The operator runtime monitors these properties but will not restart any server pods solely to adjust the shutdown options. Instead, server pods created or restarted because of another property change will be configured to shutdown, at the appropriate time, using the shutdown options set when the WebLogic Server instance Pod is created.\nField Default Value Supported Values Description shutdownType Graceful Graceful or Forced Specifies how the operator will shut down server instances. timeoutSeconds 30 Whole number in seconds where 0 means no timeout. For graceful shutdown only, number of seconds to wait before aborting in-flight work and shutting down the server. ignoreSessions false true or false Boolean indicating if active sessions should be ignored; only applicable if shutdown is graceful. waitForAllSessions false true or false For graceful shutdown only, set to true to wait for all HTTP sessions during in-flight work handling; false to wait for non-persisted HTTP sessions only during in-flight work handling. The waitForAllSessions property does not apply when the ignoreSessions property is true. When the ignoreSessions property is false then waitForAllSessions property is taken into account during the WebLogic graceful shutdown process. When thewaitForAllSessions is true, the graceful shutdown process will wait for all HTTP sessions to complete or be invalidated before proceeding. When waitForAllSessions is false, the graceful shutdown process will wait only for non-persisted HTTP sessions to complete or be invalidated before proceeding.\nShutdown environment variables The operator configures shutdown behavior with the use of the following environment variables. Users may instead simply configure these environment variables directly. When a user-configured environment variable is present, the operator will not override the environment variable based on the shutdown configuration.\nEnvironment Variables Default Value Supported Values SHUTDOWN_TYPE Graceful Graceful or Forced SHUTDOWN_TIMEOUT 30 Whole number in seconds where 0 means no timeout SHUTDOWN_IGNORE_SESSIONS false Boolean indicating if active sessions should be ignored; only applicable if shutdown is graceful SHUTDOWN_WAIT_FOR_ALL_SESSIONS false true to wait for all HTTP sessions during in-flight work handling; false to wait for non-persisted HTTP sessions only ; only applicable if shutdown is graceful shutdown rules You can specify the serverPod field, including the shutdown field, at the domain, cluster, and server levels. If shutdown is specified at multiple levels, such as for a cluster and for a member server that is part of that cluster, then the shutdown configuration for a specific server is the combination of all of the relevant values with each field having the value from the shutdown field at the most specific scope.\nFor instance, given the following Domain YAML and Cluster YAML files:\nkind: Domain metadata: name: domain1 spec: serverPod: shutdown: shutdownType: Graceful timeoutSeconds: 45 clusters: - name: \u0026#34;domain1-cluster1\u0026#34; managedServers: - serverName: \u0026#34;cluster1_server1\u0026#34; serverPod: shutdown: timeoutSeconds: 60 ignoreSessions: false ... kind: Cluster metadata: name: domain1-cluster1 labels: weblogic.domainUID: sample-domain1 spec: clusterName: cluster1 replicas: 2 serverPod: shutdown: ignoreSessions: true Graceful shutdown is used for all servers in the domain because this is specified at the domain level and is not overridden at any cluster or server level. The \u0026ldquo;cluster1\u0026rdquo; cluster defaults to ignoring sessions; however, the \u0026ldquo;cluster1_server1\u0026rdquo; server instance will not ignore sessions and will have a longer timeout.\nRestarting servers The operator automatically recreates (restarts) WebLogic Server instance Pods when fields on the Domain that affect Pod generation change (such as image, volumes, and env). The restartVersion field on the Domain lets you force the operator to restart a set of WebLogic Server instance Pods.\nThe operator does rolling restarts of clustered servers so that service is maintained.\nFields that cause servers to be restarted The operator will restart servers when any of the follow fields on the Domain that affect the WebLogic Server instance Pod generation are changed:\nauxiliaryImages auxiliaryImageVolumes containerSecurityContext domainHome domainHomeSourceType env image imagePullPolicy imagePullSecrets includeServerOutInPodLog logHomeEnabled logHome livenessProbe nodeSelector podSecurityContext readinessProbe resources restartVersion startupProbe volumes volumeMounts For Model in Image, a change to the introspectVersion field, which causes the operator to initiate a new introspection, will result in the restarting of servers if the introspection results in the generation of a modified WebLogic domain home. See the documentation on Model in Image runtime updates for a description of changes to the model or associated resources, such as Secrets, that will cause the generation of a modified WebLogic domain home.\nIf the only change detected is the addition or modification of a customer-specified label or annotation, the operator will patch the Pod rather than restarting it. Removing a label or annotation from the Domain will cause neither a restart nor a patch. It is possible to force a restart to remove such a label or annotation by modifying the restartVersion.\nRolling restarts Clustered servers that need to be restarted are gradually restarted (for example, \u0026ldquo;rolling restarted\u0026rdquo;) so that the cluster is not taken out of service and in-flight work can be migrated to other servers in the cluster.\nThe maxUnavailable field on the Cluster determines how many of the cluster\u0026rsquo;s servers may be taken out of service at a time when doing a rolling restart. It can be specified at the cluster level and defaults to 1 (that is, by default, clustered servers are restarted one at a time).\nWhen using in-memory session replication, Oracle WebLogic Server employs a primary-secondary session replication model to provide high availability of application session state (that is, HTTP and EJB sessions). The primary server creates a primary session state on the server to which the client first connects, and a secondary replica on another WebLogic Server instance in the cluster. Specifying a maxUnavailable property value of 1 protects against inadvertent session state loss which could occur if both the primary and secondary servers are shut down at the same time during the rolling restart process.\nIf you are supplying updated models or secrets for a running Model in Image domain, and you want the configuration updates to take effect using a rolling restart, consult Modifying WebLogic Configuration and Runtime updates before consulting this document.\nDraining a node and PodDisruptionBudget A Kubernetes cluster administrator can drain a Node for repair, upgrade, or scaling down the Kubernetes cluster.\nBeginning in version 3.2, the operator takes advantage of the PodDisruptionBudget feature offered by Kubernetes for high availability during a Node drain operation. The operator creates a PodDisruptionBudget (PDB) for each WebLogic cluster in the Domain namespace to limit the number of WebLogic Server pods simultaneously evicted when draining a node. The maximum number of WebLogic cluster\u0026rsquo;s server pods evicted simultaneously is determined by the maxUnavailable field on the Cluster resource. The .spec.minAvailable field of the PDB for a cluster is calculated from the difference of the current replicas count and maxUnavailable value configured for the cluster. For example, if you have a WebLogic cluster with three replicas and a maxUnavailable of 1, the .spec.minAvailable for PDB is set to 2. In this case, Kubernetes ensures that at least two pods for the WebLogic cluster\u0026rsquo;s Managed Servers are available at any given time, and it only evicts a pod when all three pods are ready. For details about safely draining a node and the PodDisruptionBudget concept, see Safely Drain a Node and PodDisruptionBudget.\nCommon restarting scenarios Using restartVersion to force the operator to restart servers The restartVersion property lets you force the operator to restart servers.\nEach time you want to restart some servers, you need to set restartVersion to a different value. The specific value does not matter so most customers use whole number values.\nThe operator will detect the new value and restart the affected servers (using the same mechanisms as when other fields that affect the WebLogic Server instance Pod generation are changed, including doing rolling restarts of clustered servers).\nThe restartVersion property can be specified at the domain, cluster, and server levels. A server will be restarted if any of these three values change.\nThe servers will also be restarted if restartVersion is removed from the Domain (for example, if you had previously specified a value to cause a restart, then you remove that value after the previous restart has completed).\nRestart all the servers in the domain Set restartVersion at the domain level to a new value.\nkind: Domain metadata: name: domain1 spec: restartVersion: \u0026#34;5\u0026#34; ... Restart all the servers in the cluster Set restartVersion at the cluster level to a new value.\nkind: Cluster metadata: name: domain1-cluster1 spec: clusterName : \u0026#34;cluster1\u0026#34; restartVersion: \u0026#34;5\u0026#34; maxUnavailable: 2 ... A Cluster resource must be referenced from the domain.spec.clusters and must have a .spec.clusterName that matches the corresponding cluster in the domain\u0026rsquo;s WebLogic configuration.\nRestart the Administration Server Set restartVersion at the adminServer level to a new value.\nkind: Domain metadata: name: domain1 spec: adminServer: restartVersion: \u0026#34;5\u0026#34; ... Restart a standalone or clustered Managed Server Set restartVersion at the managedServer level to a new value.\nkind: Domain metadata: name: domain1 spec: managedServers: - serverName: \u0026#34;standalone_server1\u0026#34; restartVersion: \u0026#34;1\u0026#34; - serverName: \u0026#34;cluster1_server1\u0026#34; restartVersion: \u0026#34;2\u0026#34; ... Full domain restarts To do a full domain restart, first shut down all servers (Administration Server and Managed Servers), taking the domain out of service, then restart them. Unlike rolling restarts, the operator cannot detect and initiate a full domain restart; you must always manually initiate it.\nTo manually initiate a full domain restart:\nChange the domain-level serverStartPolicy on the Domain to Never. kind: Domain metadata: name: domain1 spec: serverStartPolicy: Never ... Wait for the operator to stop ALL the servers for that domain.\nTo restart the domain, set the domain level serverStartPolicy back to IfNeeded. Alternatively, you do not have to specify the serverStartPolicy as the default value is IfNeeded.\nkind: Domain metadata: name: domain1 spec: serverStartPolicy: IfNeeded ... The operator will restart all the servers in the domain. Domain lifecycle sample scripts See the Lifecycle sample scripts for scripts that help with initiating domain lifecycle operations.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/base-images/ocr-images/",
	"title": "OCR images",
	"tags": [],
	"description": "Obtain and inspect base images for WebLogic Server or Fusion Middleware Infrastructure deployments from the Oracle Container Registry (OCR).",
	"content": "Contents Overview Understand Oracle Container Registry images Compare General Availability to Critical Patch Updates images WebLogic distribution installer type Compare \u0026ldquo;dated\u0026rdquo; and \u0026ldquo;undated\u0026rdquo; images Example OCR image names Obtain images from the Oracle Container Registry Inspect images Ensure you are using recently patched images Use or create WebLogic images depending on the domain home source type Overview A container image with WebLogic Server or Fusion Middleware Infrastructure is required to run WebLogic domains in Kubernetes. Oracle recommends obtaining these WebLogic images from the Oracle Container Registry (OCR) or creating custom images using the WebLogic Image Tool. Note that all of the OCR images that are described in this document are built using the WebLogic Image Tool (WIT). You can also use WIT to build your own WebLogic Server or Fusion Middleware Infrastructure images. For more information, see Create custom images.\nThis document describes how to obtain and inspect container images with WebLogic Server or Fusion Middleware Infrastructure from the Oracle Container Registry (OCR).\nUnderstand Oracle Container Registry images The Oracle Container Registry (OCR) is located at https://container-registry.oracle.com/ and contains images for licensed commercial Oracle software products that may be used in your enterprise for deployment using a container engine and Kubernetes.\nOCR contains WebLogic Server images, which have a pre-installed Oracle Home with Oracle WebLogic Server and Coherence. OCR, also, contains Fusion Middleware Infrastructure images, which have a pre-installed Oracle Home with Oracle WebLogic Server, Coherence, Fusion Middleware Control, and Java Required Files (JRF). NOTE: Oracle strongly recommends that you use only images with the latest set of recommended patches applied.\nAs of June, 2023, Oracle WebLogic Server 12.2.1.3 is no longer supported. The last Critical Patch Updates (CPU) images for WebLogic Server 12.2.1.3 were published in April, 2023. As of December, 2022, Fusion Middleware 12.2.1.3 is no longer supported. The last CPU images for FMW Infrastructure 12.2.1.3 were published in October, 2022.\nSee the following sections for information about OCR images:\nCompare General Availability to Critical Patch Updates images WebLogic distribution installer type Compare \u0026ldquo;dated\u0026rdquo; and \u0026ldquo;undated\u0026rdquo; images Example OCR image names Compare General Availability to Critical Patch Updates images General Availability (GA) images:\nLocated in the OCR repositories middleware/weblogic and middleware/fmw-infrastructure. Updated quarterly with Oracle Linux and Oracle Java security updates. GA images are free to use and are subject to Oracle Technology Network (OTN) Developer License Terms, which include, but are not limited to: Must be used only for the purpose of developing, testing, prototyping, and demonstrating applications. Must not be used for any data processing, business, commercial, or production purposes. Critical Patch Updates (CPU) images:\nLocated in the OCR repositories middleware/weblogic_cpu and middleware/fmw-infrastructure_cpu. Updated quarterly with Oracle Linux and Oracle Java security updates. Updated quarterly with the latest Patch Set Update (PSU) for WebLogic Server or Fusion Middleware, and all of the recommended security patches for products included in that distribution. Suitable for production use. WebLogic Server GA images and Fusion Middleware Infrastructure GA images on OCR do not include the latest security patches for WebLogic Server or Fusion Middleware Infrastructure. Oracle strongly recommends using images with the latest set of recommended patches applied, such as the Critical Patch Updates (CPU) images provided quarterly on OCR or custom generated images using the WebLogic Image Tool (WIT) with the --recommendedPatches option. See Ensure you are using recently patched images.\nWebLogic distribution installer type OCR image tags may include keywords like generic, slim, and such. This reflects the type of WebLogic distribution installed in the image\u0026rsquo;s Oracle Home. There are multiple types and usually, the type can be determined by examining the image name and tag:\n.../weblogic...:...generic...\nThe WebLogic generic image. Contains the same binaries as those installed by the WebLogic generic installer. .../weblogic...:...slim...:\nThe WebLogic slim image. To reduce image size, it contains a subset of the binaries included in the WebLogic generic image: The WebLogic Server Administration Console, WebLogic examples, WebLogic clients, Maven plug-ins, and Java DB have been removed. All binaries that remain included are the same as those in the WebLogic generic image. If there are requirements to monitor the WebLogic configuration, then: You should address them using Prometheus and Grafana, or other alternatives. Note that you can use the open source WebLogic Remote Console as an alternative for the WebLogic Server Administration Console. .../weblogic...:...dev...:\nThe WebLogic developer image. To reduce image size, it contains a subset of the binaries included in the WebLogic generic image: WebLogic examples and Console help files have been removed (the WebLogic Server Administration Console is still included). All binaries that remain included are the same as those in the WebLogic generic image. This image type is primarily intended to provide a container image that is consistent with the WebLogic \u0026ldquo;quick installers\u0026rdquo; intended for development only. NOTE: Production WebLogic domains should use the WebLogic generic, WebLogic slim, or Fusion Middleware Infrastructure images. .../fmw-infrastructure...:...:\nThe Fusion Middleware (FMW) Infrastructure image. Contains the same binaries as those installed by the WebLogic generic installer and adds Fusion Middleware Control and Java Required Files (JRF). None of the above\nIf the tag portion of a .../weblogic... OCR image name does not include a keyword like slim, dev, or generic, then you can assume that the image contains the same binaries as those installed by the WebLogic generic installer. Compare \u0026ldquo;dated\u0026rdquo; and \u0026ldquo;undated\u0026rdquo; images OCR images are \u0026ldquo;dated\u0026rdquo; or \u0026ldquo;undated\u0026rdquo; depending on whether the name tags include an embedded date stamp in the form YYMMDD, which represent a specific version that was released on a specific date. Unlike dated images, undated images are periodically updated to the latest available versions of their GA or CPU equivalents. Therefore, undated images change over time in the repository even though their name and tag remain the same.\nExample OCR image names Here are some example WebLogic Server Oracle Container Repository (OCR) images, where the names are abbreviated to omit their container-registry.oracle.com/middleware/ prefix:\nAbbreviated Name Description weblogic:12.2.1.4 GA image with latest JDK 8, latest Oracle Linux 7, and the GA Oracle WebLogic Server 12.2.1.4 generic distribution. Note that this image has no date stamp, so it can change over time with potential updates to JDK 8 and Oracle Linux 7. weblogic:12.2.1.4-YYMMDD GA image with JDK 8, Oracle Linux 7, and the GA Oracle WebLogic Server 12.2.1.4 generic distribution for the given date. weblogic_cpu:12.2.1.4-generic-jdk8-ol7 CPU image with latest JDK 8, latest Oracle Linux 7, and the GA Oracle WebLogic Server 12.2.1.4 generic distribution CPU. Note that this image has no date stamp, so it can change over time with potential updates to JDK 8, to Oracle Linux 7, and to the latest CPU. weblogic_cpu:12.2.1.4-slim-jdk8-ol8-220204 CPU image with latest JDK 8, latest Oracle Linux 8, and the GA Oracle WebLogic Server 12.2.1.4 slim distribution, January 2022 CPU. Obtain images from the Oracle Container Registry The Oracle Container Registry (OCR) contains images for licensed commercial Oracle software products that you may use in your enterprise. To access the Oracle Registry Server, you must have an Oracle Single Sign-On (SSO) account. OCR provides a web interface that allows an administrator to authenticate and then to select the images for the software that your organization wishes to use. You must agree to the Oracle Standard Terms and Restrictions using the web interface. Then, you can pull images of the software from OCR using the standard docker pull command while using your SSO for your docker login credentials.\nFor example, to use Docker to pull an image from OCR:\nAccept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy:\nIn a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On (SSO) authentication service. If you do not already have SSO credentials, then at the top of the page, click Sign In to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy:\nClick Middleware.\nSelect one of weblogic, weblogic_cpu, fmw-infrastructure_cpu, or such, depending in your image type.\nFor example, if you are following the operator Quick Start guide (which uses WebLogic GA images), then select weblogic. NOTE: GA images are suitable for demonstration and development purposes only where the environments are not available from the public Internet; they are not acceptable for production use. In production, you should always use CPU (patched) images from OCR or create your images using the WebLogic Image Tool (WIT) with the --recommendedPatches option.\nClick Continue.\nFollow the prompts to sign in with your SSO and accept the terms.\nThe newly available patched images in OCR require accepting a second, different Terms and Restrictions agreement. Your acceptance of these terms is stored in a database that links the software images to your Oracle Single Sign-On login credentials. This database is automatically checked when you use docker pull to obtain images from OCR.\nNOTE: This step is needed only once for each image name (not the tag level). For example, if you accept the terms for weblogic_cpu in the middleware repository, then the acceptance applies to all versions of WebLogic CPU images.\nProvide Docker with credentials for accessing the Oracle Container Registry. For example, the following command will prompt for your SSO credentials:\n$ docker login container-registry.oracle.com Use Docker to pull the desired image:\n$ docker pull container-registry.oracle.com/middleware/weblogic_cpu:12.2.1.4-generic-jdk8-ol8 Use Docker to display an inventory of your local image cache:\n$ docker images If desired, then you can inspect the content of the image.\nNOTES:\nIf you are using a multi-node Kubernetes cluster, or your Kubernetes cluster is remote from your locally created or pulled domain image, then additional steps are usually required to ensure that your Kubernetes cluster can access the image. See Access domain images. The operator requires domain images to contain WebLogic Server 12.2.1.4.0 or later. Inspect images If you have local access to a WebLogic Server or Fusion Middleware Infrastructure image and the image originates from the Oracle Container Registry or was created using the WebLogic Image Tool, then you can use the following commands to determine their contents:\nCheck the WLS version:\n$ docker run \\ container-registry.oracle.com/middleware/weblogic_cpu:12.2.1.4-generic-jdk8-ol8 \\ sh -c \u0026#39;source $ORACLE_HOME/wlserver/server/bin/setWLSEnv.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; java weblogic.version\u0026#39; Check the WLS patches:\n$ docker run \\ container-registry.oracle.com/middleware/weblogic_cpu:12.2.1.4-generic-jdk8-ol8 \\ sh -c \u0026#39;$ORACLE_HOME/OPatch/opatch lspatches\u0026#39; If you have installed the WebLogic Image Tool (WIT), then you can obtain useful version and patch information using the WIT inspect command with the --patches option. For example:\n$ /tmp/imagetool/bin/imagetool.sh inspect \\ --image=container-registry.oracle.com/middleware/weblogic_cpu:12.2.1.4-generic-jdk8-ol8 \\ --patches Ensure you are using recently patched images You should not use images without the latest set of recommended patches applied. Please review the following guidance to ensure that you are using recently patched images:\nFor production deployments, Oracle requires using fully patched custom images that you generate yourself or Critical Patch Update (CPU) images from the Oracle Container Registry (OCR). CPU images contain _cpu in their image name, for example container-registry.oracle.com/middleware/weblogic_cpu:TAG.\nGeneral Availability (GA) images do not include the latest security patches for WebLogic Server or Fusion Middleware Infrastructure. They are not licensable and are not suitable for production use.\nLocally cached OCR images that do not have a date stamp embedded in their tag:\nMay have a corresponding newer version in the registry. If so, then such images will remain out of date until one of the following occurs: The images are explicitly pulled again on every host machine with such a cached image. The images are explicitly deleted from every host machine with such a cached image. The images are implicitly pulled again due to spec.image referencing a repository with an updated image, and having a domain resource spec.imagePullPolicy of Always when a pod starts. For detailed information about OCR image naming and the differences between GA and CPU images, see Understand Oracle Container Registry images.\nTo determine the patches and versions of software within a particular image, see Inspect images.\nFor information about licensed access to WebLogic patches and CPU images, see Supported environments.\nUse or create WebLogic images depending on the domain home source type For information relevant to your chosen domain home source type, refer to the following documentation:\nModel in Image domains that use auxiliary images:\nUnderstand Oracle Container Registry images Obtain images from the Oracle Container Registry Create a custom image with patches applied Auxiliary images Model in Image domains that do not use auxiliary images: NOTE: Model in Image without auxiliary images (the WDT model and installation files are included in the same image with the WebLogic Server installation) is deprecated in WebLogic Kubernetes Operator version 4.0.7. Oracle recommends that you use Model in Image with Auxiliary images. See Auxiliary images.\nDomain in Image domains: NOTE: The Domain in Image domain home source type is deprecated in WebLogic Kubernetes Operator version 4.0. Oracle recommends that you choose either Domain on PV or Model in Image, depending on your needs.\nCreate a custom image with your domain inside the image Domain on Persistent Volume (PV) domains:\nUnderstand Oracle Container Registry images Obtain images from the Oracle Container Registry Create a custom image with patches applied Domain creation images "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/persistent-storage/pv-pvc/",
	"title": "PersistentVolumes and PersistentVolumeClaims",
	"tags": [],
	"description": "Use a Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC) to store WebLogic domain homes and log files.",
	"content": "This document show you how to set up a Kubernetes PersistentVolume and PersistentVolumeClaim, which can be used as storage for WebLogic domain homes and log files. A PersistentVolume can be shared by multiple WebLogic domains or dedicated to a particular domain.\nPrerequisites The following prerequisites must be fulfilled before proceeding with the creation of the volume:\nCreate a Kubernetes Namespace for the PersistentVolumeClaim unless the intention is to use the default namespace. Note that a PersistentVolumeClaim has to be in the same namespace as the Domain that uses it. Make sure that all the servers in the WebLogic domain are able to reach the storage location. Make sure that the host directory that will be used, already exists and has the appropriate file permissions set. Persistent volume storage locations PersistentVolumes can point to different storage locations, for example NFS servers or a local directory path. For a list of available options, see the Kubernetes documentation.\nNote regarding HostPath: In a single-node Kubernetes cluster, such as may be used for testing or proof of concept activities, HOST_PATH provides the simplest configuration. In a multinode Kubernetes cluster, a HOST_PATH that is located on shared storage mounted by all nodes in the Kubernetes cluster is the simplest configuration. If nodes do not have shared storage, then NFS is probably the most widely-available option. There are other options listed in the referenced table.\nThe operator provides a sample script to create the PersistentVolume and PersistentVolumeClaim for the domain. This script must be executed before creating the domain. Beginning with operator version 4.1.0, for the Domain on PV domain home source type, the operator provides options to create the PV and PVC during the domain initialization. See the Domain on PV documentation or the domain.spec.configuration.initializeDomainOnPV section in the domain resource schema for more details.\nPersistent volumes using HostPath approach The HOST_PATH provider is the simplest case for creating a PersistentVolume. It requires creating a directory on the Kubernetes master and ensuring that it has the correct permissions:\n$ mkdir -m 777 -p /path/to/domain1PersistentVolume YAML files Persistent volumes and claims are described in YAML files. For each PersistentVolume, you should create one PersistentVolume YAML file and one PersistentVolumeClaim YAML file. In the following example, you will find two YAML templates, one for the volume and one for the claim. As stated previously, they either can be dedicated to a specific domain, or shared across multiple domains. For the use cases where a volume will be dedicated to a particular domain, it is a best practice to label it with weblogic.domainUID=[domain name]. This makes it easy to search for, and clean up resources associated with that particular domain.\nFor sample YAML templates, refer to the PersistentVolumes example.\nFor more details, refer to Kubernetes PV/PVC examples here.\nVerify the results To confirm that the PersistentVolume was created, use these commands:\n$ kubectl describe pv \u0026lt;persistent volume name\u0026gt; $ kubectl describe pvc -n NAMESPACE \u0026lt;persistent volume claim name\u0026gt; Common problems This section provides details of common problems that might occur while running the script and how to resolve them.\nPersistentVolume provider not configured correctly Possibly the most common problem experienced during testing was the incorrect configuration of the PersistentVolume provider. The PersistentVolume must be accessible to all Kubernetes Nodes, and must be able to be mounted as Read/Write/Many. If this is not the case, the PersistentVolume creation will fail.\nThe simplest case is where the HOST_PATH provider is used. This can be either with one Kubernetes Node, or with the HOST_PATH residing in shared storage available at the same location on every node (for example, on an NFS mount). In this case, the path used for the PersistentVolume must have its permission bits set to 777.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/manage-domains/",
	"title": "About WebLogic domains",
	"tags": [],
	"description": "An overview about managing WebLogic domains and clusters in Kubernetes.",
	"content": "This document is an overview of managing WebLogic domains and clusters in Kubernetes.\nContents Creating and managing WebLogic domains Modifying domain configurations Managing lifecycle operations Scaling clusters About domain events Accessing and monitoring domains Logging Meet Kubernetes resource name restrictions Important considerations for WebLogic domains in Kubernetes Creating and managing WebLogic domains Domain resources reference WebLogic domain configuration, a WebLogic install, images, and anything else necessary to run the domain. Beginning with operator 4.0, WebLogic clusters that are within a WebLogic domain configuration may optionally be associated with a Cluster resource in addition to a Domain resource. For more information, see Domain and Cluster resources.\nYou can locate a WebLogic domain either on a persistent volume (Domain on PV), inside the container only (Model in Image), or in an image (Domain in Image). For an explanation of each, see Choose a domain home source type. For examples of each, see the WebLogic Kubernetes Operator samples.\nThe Domain in Image domain home source type is deprecated in WebLogic Kubernetes Operator version 4.0. Oracle recommends that you choose either Domain on PV or Model in Image, depending on your needs.\nIf you want to create your own container images, for example, to choose a specific set of patches or to create a domain with a specific configuration or applications deployed, then you can create the domain custom resource manually to deploy your domain. This process is documented in this sample.\nNOTE: After you are familiar with the basics, it is recommended to review important considerations and resource name restrictions.\nModifying domain configurations You can modify the WebLogic domain configuration for Domain on PV, Domain in Image, and Model in Image before deploying a Domain YAML file:\nWhen the domain is on a persistent volume, you can use WLST or WDT to change the configuration.\nFor Domain in Image and Domain on PV, you can use Configuration overrides.\nConfiguration overrides allow changing a configuration without modifying its original config.xml or system resource XML files, and supports parameterizing overrides so that you can inject values into them from Kubernetes Secrets. For example, you can inject database user names, passwords, and URLs that are stored in a secret. However, note the scenarios for which configuration overrides are not supported.\nFor Model in Image, you use Runtime Updates.\nManaging lifecycle operations You can perform lifecycle operations on WebLogic Servers, clusters, or domains. This includes starting, stopping, and rolling domains, clusters, or individual servers, plus detecting failures and tuning retry behavior. See Domain life cycle.\nScaling clusters The operator lets you initiate scaling of clusters in various ways:\nUsing kubectl to edit a Cluster resource Using Kubernetes scale commands Using a Kubernetes Horizontal Pod Autoscaler Using the operator\u0026rsquo;s REST APIs Using WLDF policies Using a Prometheus action See Domain life cycle scaling.\nAbout domain events The operator generates Kubernetes events at key points during domain processing. For more information, see Domain events.\nAccessing and monitoring domains To access the domain using WLST, console, T3, or a load balancer, or to export Prometheus-compatible metrics, see Access and monitor domains.\nLogging To tune log file location and rotation, see Log Files.\nTo export operator or domain log files, see the Elastic Stack examples.\nMeet Kubernetes resource name restrictions Kubernetes requires that the names of some resource types follow the DNS label standard as defined in DNS Label Names and RFC 1123. This requirement restricts the characters that are allowed in the names of these resources, and also limits the length of these names to no more than 63 characters.\nThe following is a list of such Kubernetes resources that the operator generates when a domain resource is deployed, including how their names are constructed.\nA domain introspector job named \u0026lt;domainUID\u0026gt;-\u0026lt;introspectorJobNameSuffix\u0026gt;. The default suffix is -introspector, which can be overridden using the operator\u0026rsquo;s Helm configuration introspectorJobNameSuffix (see WebLogic domain management). A ClusterIP type service and a pod for each WebLogic Server named \u0026lt;domainUID\u0026gt;-\u0026lt;serverName\u0026gt;. A ClusterIP type service for each WebLogic cluster named \u0026lt;domainUID\u0026gt;-cluster-\u0026lt;clusterName\u0026gt;. An optional NodePort type service, also known as an external service, for the WebLogic Administration Server named \u0026lt;domainUID\u0026gt;-\u0026lt;adminServerName\u0026gt;-\u0026lt;externalServiceNameSuffix\u0026gt;. The default suffix is -ext, which can be overridden using the operator\u0026rsquo;s Helm configuration externalServiceNameSuffix (see WebLogic domain management). The operator puts in place certain validation checks and conversions to prevent these resources from violating Kubernetes restrictions.\nAll the names previously described can contain only the characters A-Z, a-z, 0-9, -, or _, and must start and end with an alphanumeric character. Note that when generating pod and service names, the operator will convert configured names to lowercase and substitute a hyphen (-) for each underscore (_). A domainUID is required to be no more than 45 characters. WebLogic domain configuration names, such as the cluster names, Administration Server name, and Managed Server names must be kept to a legal length so that the resultant resource names do not exceed Kubernetes\u0026rsquo; limits. When a domain resource or WebLogic domain configuration violates the limits, the domain startup will fail, and actual validation errors are reported in the domain resource\u0026rsquo;s status.\nImportant considerations for WebLogic domains in Kubernetes Be aware of the following important considerations for WebLogic domains running in Kubernetes:\nDomain Home Location: The WebLogic domain home location is determined by the Domain YAML file domainHome, if specified; otherwise, a default location is determined by the domainHomeSourceType setting.\nIf the Domain domainHome field is not specified and domainHomeSourceType is Image (the default), then the operator will assume that the domain home is a directory under /u01/oracle/user_projects/domains/, and report an error if no domain is found or more than one domain is found. If the Domain domainHome field is not specified and domainHomeSourceType is PersistentVolume, then the operator will assume that the domain home is /shared/domains/DOMAIN_UID. Finally, if the Domain domainHome field is not specified and the domainHomeSourceType is FromModel, then the operator will assume that the domain home is /u01/domains/DOMAIN_UID. Oracle strongly recommends storing an image containing a WebLogic domain home (domainHomeSourceType is Image) as private in the registry (for example, Oracle Cloud Infrastructure Registry, GitHub Container Registry, and such). A container image that contains a WebLogic domain has sensitive information including keys and credentials that are used to access external resources (for example, the data source password). For more information, see WebLogic domain in container image protection.\nLog File Locations: The operator can automatically override WebLogic Server, domain, and introspector log locations. This occurs if the Domain logHomeEnabled field is explicitly set to true, or if logHomeEnabled isn\u0026rsquo;t set and domainHomeSourceType is set to PersistentVolume. When overriding, the log location will be the location specified by the logHome setting. For additional log file tuning information, see Log files.\nListen Address Overrides: The operator will automatically override all WebLogic domain default, SSL, admin, or custom channel listen addresses (using Configuration overrides). These will become domainUID followed by a hyphen and then the server name, all lowercase, and underscores converted to hyphens. For example, if domainUID=domain1 and the WebLogic Server name is Admin_Server, then its listen address becomes domain1-admin-server.\nDomain, Cluster, Server, and Network-Access-Point Names: WebLogic domain, cluster, server, and network-access-point (channel) names must contain only the characters A-Z, a-z, 0-9, -, or _, and must be kept to a reasonable length. This ensures that they can be safely used to form resource names that meet Kubernetes resource and DNS1123 naming requirements. For more details, see Meet Kubernetes resource name restrictions.\nNode Ports: If you choose to expose any WebLogic channels outside the Kubernetes cluster using a NodePort, for example, the administration port or a T3 channel to allow WLST access, you need to ensure that you allocate each channel a unique port number across the entire Kubernetes cluster. If you expose the administration port in each WebLogic domain in the Kubernetes cluster, then each one must have a different port number. This is required because NodePorts are used to expose channels outside the Kubernetes cluster. Exposing administrative, RMI, or T3 capable channels using a Kubernetes NodePort can create an insecure configuration. In general, only HTTP protocols should be made available externally and this exposure is usually accomplished by setting up an external load balancer that can access internal (non-NodePort) services. For more information, see WebLogic T3 and administrative channels.\nHost Path Persistent Volumes: If using a hostPath persistent volume, then it must be available on all worker nodes in the cluster and have read/write/many permissions for all container/pods in the WebLogic Server deployment. Be aware that many cloud provider\u0026rsquo;s volume providers may not support volumes across availability zones. You may want to use NFS or a clustered file system to work around this limitation.\nSecurity Note: The USER_MEM_ARGS environment variable defaults to -Djava.security.egd=file:/dev/./urandom in all WebLogic Server pods and the WebLogic introspection job. It can be explicitly set to another value in your Domain or Cluster YAML file using the env attribute under the serverPod configuration.\nJVM Memory and Java Option Arguments: The following environment variables can be used to customize the JVM memory and Java options for both the WebLogic Server Managed Servers and Node Manager instances:\nJAVA_OPTIONS - Java options for starting WebLogic Server USER_MEM_ARGS - JVM memory arguments for starting WebLogic Server NODEMGR_JAVA_OPTIONS - Java options for starting a Node Manager instance NODEMGR_MEM_ARGS - JVM memory arguments for starting a Node Manager instance For more information, see JVM memory and Java option environment variables.\nNode Manager environment variables: You can use the following environment variables to specify the logging files limit.\nNODEMGR_LOG_FILE_MAX: Maximum size of the Node Manager Log specified as an integer. When this limit is reached, a new log file is started. Default: 0, no limit. NODEMGR_LOG_LEVEL: Severity level of logging used for the Node Manager log. Node Manager uses the standard logging levels from the java.util.logging.level package. Default: FINEST. NODEMGR_LOG_COUNT: Maximum number of log files to create when LogLimit is exceeded. Default: 1. The following features are not certified or supported in this release:\nWhole server migration Consensus leasing Node Manager (although it is used internally for the liveness probe and to start WebLogic Server instances) Multicast Multitenancy Production redeployment Mixed clusters (configured servers targeted to a dynamic cluster) For up-to-date information about the features of WebLogic Server that are supported in Kubernetes environments, see My Oracle Support Doc ID 2349228.1.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/introduction/terms/",
	"title": "Important terms",
	"tags": [],
	"description": "Define important terms used throughout this documentation.",
	"content": "This documentation uses several important terms which are intended to have a specific meaning.\nTerm Definition Cluster A WebLogic cluster is a group of WebLogic Managed Servers that together host some application or component and which are able to share load and state between them; a single WebLogic domain can define multiple WebLogic clusters. The 4.0 release adds a cluster resource which is a Kubernetes resource that is of custom resource type Cluster. Cluster resources are monitored by the WebLogic Kubernetes operator. A cluster resource references a WebLogic cluster to control how many member servers are running and potentially additional Kubernetes resources that are specific to that WebLogic cluster. A Kubernetes cluster is a group of machines (“Nodes”) that all host Kubernetes resources, like Pods and Services, and which appear to the external user as a single entity. If the term “cluster” is not prefixed, then it should be assumed to mean a Kubernetes cluster. Domain A WebLogic domain is a group of related applications and configuration information necessary to run them with or without Kubernetes. A domain resource is a Kubernetes resource that is of custom resource type Domain. Domain resources are monitored by the WebLogic Kubernetes operator. A domain resource references a WebLogic domain\u0026rsquo;s WebLogic install, WebLogic domain configuration, and potentially additional Kubernetes resources that are necessary for running the WebLogic domain. If the term “domain” is not prefixed or suffixed, then it should be assumed to mean a Kubernetes domain resource. Domain UID A Domain UID identifies a particular Domain resource and is a unique string that is minimally unique within the scope of a Kubernetes namespace. If the Domain UID is not explicitly set in a Domain resource\u0026rsquo;s spec.domainUID field, then the Domain UID defaults to being the metadata.Name of the resource. As a convention, any resource that is associated with a particular Domain UID is typically assigned a Kubernetes label weblogic.domainUID that is assigned to that name. Ingress A Kubernetes Ingress provides access to applications and services in a Kubernetes environment to external clients. An Ingress may also provide additional features like load balancing. Labels A Kubernetes Label is a name and value pair that is associated with any Kubernetes resource. A resource can have multiple arbitrary labels and labels are often used to allow resources to reference each other. For example, a Service may define a selector that matches the service to Pods with a given label. Namespace A Kubernetes Namespace is a named scope within a Kubernetes cluster that can be used to group together related Kubernetes resources, for example, Pods and Services. Same named resources of different types can reside in the same Kubernetes namespace, but same named resources of the same type must be targeted to different namespaces. Note that some Kubernetes resources are global in scope and cannot targeted to a specific namespace (a Kubernetes ClusterRole for example). Operator A Kubernetes operator is software that performs management of complex applications. The WebLogic Kubernetes operator is an operator that manages Domain and Cluster resources. If the term “operator” is not prefixed or suffixed, then it should be assumed to mean a WebLogic Kubernetes Operator. Pod A Kubernetes Pod contains one or more containers and is the object that provides the execution environment for an instance of an application component, such as a web server or database. Job A Kubernetes Job is a type of controller that creates one or more Pods that run to completion to complete a specific task. Secret A Kubernetes Secret is a named object that can store secret information like user names, passwords, X.509 certificates, or any other arbitrary data. Service A Kubernetes Service exposes application network endpoints inside a Pod to other Pods, or outside the Kubernetes cluster. A Service may also provide additional features like load balancing. The name of service is used to generate a DNS name that applications can use to access the service. Kubernetes requires that the names of some resource types, including services, follow the DNS label standard as defined in DNS Label Names and RFC 1123. Therefore, the operator enforces that the names of the Kubernetes resources do not exceed Kubernetes limits (see Meet Kubernetes resource name restrictions; for example, when the operator generates a service for you, it generates service names that are lowercase and that have their underscores _ converted to hyphens -. "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/quickstart/install/",
	"title": "Install the operator and ingress controller",
	"tags": [],
	"description": "",
	"content": "Use Helm to install the operator and Traefik ingress controller. First, install the operator.\nCreate a namespace for the operator.\n$ kubectl create namespace sample-weblogic-operator-ns Create a service account for the operator in the operator\u0026rsquo;s namespace.\n$ kubectl create serviceaccount -n sample-weblogic-operator-ns sample-weblogic-operator-sa Set up Helm with the location of the operator Helm chart using this format: helm repo add \u0026lt;helm-chart-repo-name\u0026gt; \u0026lt;helm-chart-repo-url\u0026gt;\n$ helm repo add weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts --force-update Install the operator using this format: helm install \u0026lt;helm-release-name\u0026gt; \u0026lt;helm-chart-repo-name\u0026gt;/weblogic-operator ...\n$ helm install sample-weblogic-operator weblogic-operator/weblogic-operator \\ --namespace sample-weblogic-operator-ns \\ --set serviceAccount=sample-weblogic-operator-sa \\ --wait This Helm release deploys the operator with the default behavior of managing Domains in all Kubernetes namespaces with the label weblogic-operator=enabled.\nVerify that the operator\u0026rsquo;s pod is running by listing the pods in the operator\u0026rsquo;s namespace. You should see one for the operator and one for the conversion webhook, a singleton Deployment in your Kubernetes cluster that automatically and transparently upgrades domain resources.\n$ kubectl get pods -n sample-weblogic-operator-ns Verify that the operator is up and running by viewing the operator pod\u0026rsquo;s log.\n$ kubectl logs -n sample-weblogic-operator-ns -c weblogic-operator deployments/weblogic-operator Create a Traefik ingress controller. Set up Helm with the location of the Traefik Helm chart using this format: helm repo add \u0026lt;helm-chart-repo-name\u0026gt; \u0026lt;helm-chart-repo-url\u0026gt;\n$ helm repo add traefik https://helm.traefik.io/traefik --force-update Create a namespace for the ingress controller.\n$ kubectl create namespace traefik Install Traefik using this format: helm install \u0026lt;helm-release-name\u0026gt; \u0026lt;helm-chart-repo-name\u0026gt;/traefik ...\n$ helm install traefik-operator traefik/traefik \\ --namespace traefik \\ --set \u0026#34;ports.web.nodePort=30305\u0026#34; \\ --set \u0026#34;ports.websecure.nodePort=30443\u0026#34; \\ --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; This deploys the Traefik controller with plain text node port 30305, SSL node port 30443, and kubernetes.namespaces specifically set.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/quickstart/",
	"title": "Quick Start",
	"tags": [],
	"description": "",
	"content": "The Quick Start guide provides a simple tutorial to help you get the operator up and running quickly. Use this Quick Start guide to create a WebLogic Server deployment in a Kubernetes cluster with the WebLogic Kubernetes Operator (the \u0026ldquo;operator\u0026rdquo;). Please note that this walk-through is for demonstration purposes only, not for use in production. These instructions assume that you are already familiar with Kubernetes. If you need more detailed instructions, please refer to Manage operators.\nFor this exercise, you’ll need a Kubernetes cluster. If you need help setting one up, check out our cheat sheet. This guide assumes a Kubernetes cluster with no operator installation.\nThe operator uses Helm to create and deploy the necessary resources and then run the operator in a Kubernetes cluster. For detailed Helm installation and usage information, see the Configuration reference.\nAll Kubernetes distributions and managed services have small differences. In particular, the way that persistent storage and load balancers are managed varies significantly. You may need to adjust the instructions in this guide to suit your particular flavor of Kubernetes.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/azure-kubernetes-service/domain-on-pv/",
	"title": "Domain home on a PV",
	"tags": [],
	"description": "Sample for creating a WebLogic domain home on an existing PV or PVC on the Azure Kubernetes Service.",
	"content": "This sample demonstrates how to use the WebLogic Kubernetes Operator (hereafter \u0026ldquo;the operator\u0026rdquo;) to set up a WebLogic Server (WLS) cluster on the Azure Kubernetes Service (AKS) using the domain on PV approach. After going through the steps, your WLS domain runs on an AKS cluster and you can manage your WLS domain by accessing the WebLogic Server Administration Console.\nContents Prerequisites Prepare parameters Oracle Container Registry Sign in with Azure CLI Download the WebLogic Kubernetes Operator sample Create Resource Group Create the AKS cluster Create storage Create a domain creation image Image creation prerequisites Image creation - Introduction Understanding your first archive Staging a ZIP file of the archive Staging model files Creating the image with WIT Pushing the image to Azure Container Registry Install WebLogic Kubernetes Operator into the AKS cluster Create WebLogic domain Create secrets Enable Weblogic Operator Create WebLogic Domain Automation Access sample application Validate NFS volume Clean up resources Troubleshooting Useful links Prerequisites This sample assumes the following prerequisite environment.\nIf you don\u0026rsquo;t have an Azure subscription, create a free account before you begin. It\u0026rsquo;s strongly recommended that the Azure identity you use to sign in and complete this article has either the Owner role in the current subscription or the Contributor and User Access Administrator roles in the current subscription. If your identity has very limited role assignments, ensure you have the following role assignments in the AKS resource group and AKS node resource group. Contributor role and User Access Administrator role in the resource group that runs AKS cluster. This requires asking a privileged user to assign the roles before creating resource in the resource group. Contributor role in the AKS node resource group whose name starts with \u0026ldquo;MC_\u0026rdquo;. This requires asking a privileged user to assign the role after the AKS instance is created. Operating System: GNU/Linux, macOS, or Windows Subsystem for Linux (WSL). Note: the Docker image creation steps will not work on a Mac with Apple Silicon. Git; use git --version to test if git works. This document was tested with version 2.25.1. Azure CLI; use az --version to test if az works. This document was tested with version 2.58.0. Docker for Desktop. This document was tested with Docker version 20.10.7. kubectl; use kubectl version to test if kubectl works. This document was tested with version v1.21.2. Helm, version 3.1 and later; use helm version to check the helm version. This document was tested with version v3.6.2. A JDK, version 8 or 11. Azure recommends Microsoft Build of OpenJDK. Ensure that your JAVA_HOME environment variable is set correctly in the shells in which you run the commands. Ensure that you have the zip/unzip utility installed; use zip/unzip -v to test if zip/unzip works. You will need an Oracle account. Prepare parameters Set required parameters by running the following commands.\n# Change these parameters as needed for your own environment export ORACLE_SSO_EMAIL=\u0026lt;replace with your oracle account email\u0026gt; export ORACLE_SSO_PASSWORD=\u0026#34;\u0026lt;replace with your oracle password\u0026gt;\u0026#34; # Specify a prefix to name resources, only allow lowercase letters and numbers, between 1 and 7 characters export BASE_DIR=~ export NAME_PREFIX=wls export WEBLOGIC_USERNAME=weblogic export WEBLOGIC_PASSWORD=Secret123456 export domainUID=domain1 # Used to generate resource names. export TIMESTAMP=`date +%s` export AKS_CLUSTER_NAME=\u0026#34;${NAME_PREFIX}aks${TIMESTAMP}\u0026#34; export AKS_PERS_RESOURCE_GROUP=\u0026#34;${NAME_PREFIX}resourcegroup${TIMESTAMP}\u0026#34; export AKS_PERS_LOCATION=eastus export AKS_PERS_STORAGE_ACCOUNT_NAME=\u0026#34;${NAME_PREFIX}storage${TIMESTAMP}\u0026#34; export AKS_PERS_SHARE_NAME=\u0026#34;${NAME_PREFIX}-weblogic-${TIMESTAMP}\u0026#34; export SECRET_NAME_DOCKER=\u0026#34;${NAME_PREFIX}regcred\u0026#34; export ACR_NAME=\u0026#34;${NAME_PREFIX}acr${TIMESTAMP}\u0026#34; Oracle Container Registry The following steps will direct you to accept the license agreement for WebLogic Server. Make note of your Oracle Account password and email. This sample pertains to 12.2.1.4, but other versions may work as well.\nIn a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them. The Oracle Container Registry provides a WebLogic 12.2.1.4 General Availability (GA) installation image that is used in this sample. In the Oracle Container Registry, navigate to Middleware, then weblogic. On the left, choose a language and accept the license agreement. You will then see a message such as: \u0026ldquo;You last accepted the Oracle Standard Terms and Restrictions on 08/10/2020 at 06:12 AM Coordinated Universal Time (UTC).\u0026rdquo; NOTE: General Availability (GA) images are suitable for demonstration and development purposes only where the environments are not available from the public Internet; they are not acceptable for production use. In production, you should always use CPU (patched) images from the OCR or create your images using the WebLogic Image Tool (WIT) with the --recommendedPatches option. For more guidance, see Apply the Latest Patches and Updates in Securing a Production Environment for Oracle WebLogic Server. Ensure that Docker is running. Find and pull the WebLogic 12.2.1.4 installation image: $ docker login container-registry.oracle.com -u ${ORACLE_SSO_EMAIL} -p ${ORACLE_SSO_PASSWORD} $ docker pull container-registry.oracle.com/middleware/weblogic:12.2.1.4 If you have problems accessing the Oracle Container Registry, you can build your own images from the Oracle GitHub repository.\nSign in with Azure CLI The steps in this section show you how to sign in to the Azure CLI.\nOpen a Bash shell.\nSign out and delete some authentication files to remove any lingering credentials.\n$ az logout $ rm ~/.azure/accessTokens.json $ rm ~/.azure/azureProfile.json Sign in to your Azure CLI.\n$ az login Set the subscription ID. Be sure to replace the placeholder with the appropriate value.\n$ export SUBSCRIPTION_ID=\u0026#34;\u0026lt;your-sub-id\u0026gt;\u0026#34; $ az account set -s $SUBSCRIPTION_ID Download the WebLogic Kubernetes Operator sample Download the WebLogic Kubernetes Operator sample ZIP file. We will use several scripts in this zip file to create a WebLogic domain. This sample was tested with v4.2.8, but should work with the latest release.\n$ cd $BASE_DIR $ mkdir sample-scripts $ curl -m 120 -fL https://github.com/oracle/weblogic-kubernetes-operator/releases/download/v4.2.8/sample-scripts.zip \\ -o ${BASE_DIR}/sample-scripts/sample-scripts.zip $ unzip ${BASE_DIR}/sample-scripts/sample-scripts.zip -d ${BASE_DIR}/sample-scripts The following sections of the sample instructions will guide you, step-by-step, through the process of setting up a WebLogic cluster on AKS - remaining as close as possible to a native Kubernetes experience. This lets you understand and customize each step. If you wish to have a more automated experience that abstracts some lower level details, you can skip to the Automation section.\nCreate Resource Group Create the resource group by issuing the following commands.\n$ az extension add --name resource-graph $ az group create --name $AKS_PERS_RESOURCE_GROUP --location $AKS_PERS_LOCATION Create the AKS cluster This sample doesn\u0026rsquo;t enable application routing. If you want to enable application routing, follow Managed nginx Ingress with the application routing add-on in AKS.\nRun the following command to create the AKS cluster.\n$ az aks create \\ --resource-group $AKS_PERS_RESOURCE_GROUP \\ --name $AKS_CLUSTER_NAME \\ --node-count 2 \\ --generate-ssh-keys \\ --nodepool-name nodepool1 \\ --node-vm-size Standard_DS2_v2 \\ --location $AKS_PERS_LOCATION \\ --enable-managed-identity Successful output will be a JSON object with the entry \u0026quot;type\u0026quot;: \u0026quot;Microsoft.ContainerService/ManagedClusters\u0026quot;.\nAfter the deployment finishes, run the following command to connect to the AKS cluster. This command updates your local ~/.kube/config so that subsequent kubectl commands interact with the named AKS cluster.\n$ az aks get-credentials --resource-group $AKS_PERS_RESOURCE_GROUP --name $AKS_CLUSTER_NAME Successful output will look similar to:\nMerged \u0026#34;wlsaks1596087429\u0026#34; as current context in /home/username/.kube/config After your Kubernetes cluster is up and running, run the following commands to make sure kubectl can access the Kubernetes cluster:\n$ kubectl get nodes -o wide Successful output will look like the following.\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME aks-nodepool1-15679926-vmss000000 Ready agent 118s v1.25.6 10.224.0.4 \u0026lt;none\u0026gt; Ubuntu 22.04.2 LTS 5.15.0-1041-azure containerd://1.7.1+azure-1 aks-nodepool1-15679926-vmss000001 Ready agent 2m8s v1.25.6 10.224.0.5 \u0026lt;none\u0026gt; Ubuntu 22.04.2 LTS 5.15.0-1041-azure containerd://1.7.1+azure-1 NOTE: If you run into VM size failure, see Troubleshooting - Virtual Machine size is not supported.\nCreate storage Our usage pattern for the operator involves creating Kubernetes \u0026ldquo;persistent volumes\u0026rdquo; to allow the WebLogic Server to persist its configuration and data separately from the Kubernetes Pods that run WebLogic Server workloads.\nYou will create an external data volume to access and persist data. There are several options for data sharing as described in Storage options for applications in Azure Kubernetes Service (AKS).\nYou will dynamically create and use a persistent volume with Azure Files NFS share. For details about this full featured cloud storage solution, see the Azure Files Documentation.\nCreate an Azure Storage account and NFS share Create an Azure Storage Account.\nCreate a storage account using the Azure CLI. Make sure the following values are specified:\nOption name Value Notes name $AKS_PERS_STORAGE_ACCOUNT_NAME The storage account name can contain only lowercase letters and numbers, and must be between 3 and 24 characters in length. sku Premium_LRS Only Premium_LRS and Premium_ZRS work for NFS share, see the Azure Files NFS Share Documentation. https-only false You can\u0026rsquo;t mount an NFS file share unless you disable secure transfer. default-action Deny For security, we suggest that you deny access by default and choose to allow access from the AKS cluster network. $ az storage account create \\ --resource-group $AKS_PERS_RESOURCE_GROUP \\ --name $AKS_PERS_STORAGE_ACCOUNT_NAME \\ --location $AKS_PERS_LOCATION \\ --sku Premium_LRS \\ --kind FileStorage \\ --https-only false \\ --default-action Deny Successful output will be a JSON object with the entry \u0026quot;type\u0026quot;: \u0026quot;Microsoft.Storage/storageAccounts\u0026quot;.\nCreate an NFS share.\nWe strongly recommend NFS instead of SMB. NFS evolved from the UNIX operating system, and other variants such as GNU/Linux. For this reason, when using NFS with container technologies such as Docker, it is less likely to have problems for concurrent reads and file locking.\nPlease be sure to enable NFS v4.1. Versions lower than v4.1 will have problems.\nTo create the file share, you must use NoRootSquash to allow the operator to change the ownership of the directory in the NFS share.\nOtherwise, you will get an error like chown: changing ownership of '/shared': Operation not permitted.\nThe following command creates an NFS share with 100GiB:\n# Create NFS file share $ az storage share-rm create \\ --resource-group $AKS_PERS_RESOURCE_GROUP \\ --storage-account $AKS_PERS_STORAGE_ACCOUNT_NAME \\ --name ${AKS_PERS_SHARE_NAME} \\ --enabled-protocol NFS \\ --root-squash NoRootSquash \\ --quota 100 The command provisions an NFS file share with NFS 4.1 or above.\nAssign the AKS cluster Contributor role to access the storage account.\nYou must configure role assignment allowing access from the AKS cluster to the storage account.\nGet the objectId of the AKS cluster with the following command and save it with the variable AKS_OBJECT_ID:\n$ AKS_OBJECT_ID=$(az aks show --name ${AKS_CLUSTER_NAME} --resource-group ${AKS_PERS_RESOURCE_GROUP} --query \u0026#34;identity.principalId\u0026#34; -o tsv) Get the Id of the storage account with the following command:\n$ STORAGE_ACCOUNT_ID=$(az storage account show --name ${AKS_PERS_STORAGE_ACCOUNT_NAME} --resource-group ${AKS_PERS_RESOURCE_GROUP} --query \u0026#34;id\u0026#34; -o tsv) Now, you are able to create a role assignment to grant the AKS cluster the Contributor role in the scope of the storage account. Then, the AKS cluster is able to access the file share.\n$ az role assignment create \\ --assignee-object-id \u0026#34;${AKS_OBJECT_ID}\u0026#34; \\ --assignee-principal-type \u0026#34;ServicePrincipal\u0026#34; \\ --role \u0026#34;Contributor\u0026#34; \\ --scope \u0026#34;${STORAGE_ACCOUNT_ID}\u0026#34; Successful output will be a JSON object like the following:\n{ \u0026#34;condition\u0026#34;: null, \u0026#34;conditionVersion\u0026#34;: null, \u0026#34;createdBy\u0026#34;: \u0026#34;d6fe7d09-3330-45b6-ae32-4dd5e3310835\u0026#34;, \u0026#34;createdOn\u0026#34;: \u0026#34;2023-05-11T04:13:04.922943+00:00\u0026#34;, \u0026#34;delegatedManagedIdentityResourceId\u0026#34;: null, \u0026#34;description\u0026#34;: null, \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/wlsresourcegroup1683777168/providers/Microsoft.Storage/storageAccounts/wlsstorage1683777168/providers/Microsoft.Authorization/roleAssignments/93dae12d-21c8-4844-99cd-e8b088356af6\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;93dae12d-21c8-4844-99cd-e8b088356af6\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;95202c6f-2073-403c-b9a7-7d2f1cbb4541\u0026#34;, \u0026#34;principalName\u0026#34;: \u0026#34;3640cbf2-4db7-43b8-bcf6-1e51d3e90478\u0026#34;, \u0026#34;principalType\u0026#34;: \u0026#34;ServicePrincipal\u0026#34;, \u0026#34;resourceGroup\u0026#34;: \u0026#34;wlsresourcegroup1683777168\u0026#34;, \u0026#34;roleDefinitionId\u0026#34;: \u0026#34;/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/providers/Microsoft.Authorization/roleDefinitions/b24988ac-6180-42a0-ab88-20f7382dd24c\u0026#34;, \u0026#34;roleDefinitionName\u0026#34;: \u0026#34;Contributor\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/wlsresourcegroup1683777168/providers/Microsoft.Storage/storageAccounts/wlsstorage1683777168\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Authorization/roleAssignments\u0026#34;, \u0026#34;updatedBy\u0026#34;: \u0026#34;d6fe7d09-3330-45b6-ae32-4dd5e3310835\u0026#34;, \u0026#34;updatedOn\u0026#34;: \u0026#34;2023-05-11T04:13:04.922943+00:00\u0026#34; } Configure network security.\nYou must configure network security allowing access from the AKS cluster to the file share.\nFirst, you must get the virtual network name and the subnet name of the AKS cluster.\nRun the following commands to get network information:\n# get the resource group name of the AKS managed resources $ aksMCRGName=$(az aks show --name $AKS_CLUSTER_NAME --resource-group $AKS_PERS_RESOURCE_GROUP -o tsv --query \u0026#34;nodeResourceGroup\u0026#34;) $ echo ${aksMCRGName} # get network name of AKS cluster $ aksNetworkName=$(az graph query -q \u0026#34;Resources \\ | where type =~ \u0026#39;Microsoft.Network/virtualNetworks\u0026#39; \\ | where resourceGroup =~ \u0026#39;${aksMCRGName}\u0026#39; \\ | project name = name\u0026#34; --query \u0026#34;data[0].name\u0026#34; -o tsv) $ echo ${aksNetworkName} # get subnet name of AKS agent pool $ aksSubnetName=$(az network vnet subnet list --resource-group ${aksMCRGName} --vnet-name ${aksNetworkName} -o tsv --query \u0026#34;[*].name\u0026#34;) $ echo ${aksSubnetName} # get subnet id of the AKS agent pool $ aksSubnetId=$(az network vnet subnet list --resource-group ${aksMCRGName} --vnet-name ${aksNetworkName} -o tsv --query \u0026#34;[*].id\u0026#34;) $ echo ${aksSubnetId} You must enable the service endpoint Microsoft.Storage for the subnet using the following command:\n$ az network vnet subnet update \\ --resource-group $aksMCRGName \\ --name ${aksSubnetName} \\ --vnet-name ${aksNetworkName} \\ --service-endpoints Microsoft.Storage It takes several minutes to enable the service endpoint; successful output will be a JSON object like the following:\n\u0026#34;serviceEndpoints\u0026#34;: [ { \u0026#34;locations\u0026#34;: [ \u0026#34;eastus\u0026#34;, \u0026#34;westus\u0026#34; ], \u0026#34;provisioningState\u0026#34;: \u0026#34;Succeeded\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;Microsoft.Storage\u0026#34; } Now you must create a network rule to allow access from the AKS cluster. The following command enables access from the AKS subnet to the storage account:\n$ az storage account network-rule add \\ --resource-group $AKS_PERS_RESOURCE_GROUP \\ --account-name $AKS_PERS_STORAGE_ACCOUNT_NAME \\ --subnet ${aksSubnetId} Successful output will be a JSON object with a virtual network rule like:\n\u0026#34;virtualNetworkRules\u0026#34;: [ { \u0026#34;action\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;Succeeded\u0026#34;, \u0026#34;virtualNetworkResourceId\u0026#34;: \u0026#34;${aksSubnetId}\u0026#34; } ] Create SC and PVC Generated configuration files Use the following command to generate configuration files.\ncat \u0026gt;azure-csi-nfs-${TIMESTAMP}.yaml \u0026lt;\u0026lt;EOF # Copyright (c) 2018, 2023, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: azurefile-csi-nfs provisioner: file.csi.azure.com parameters: protocol: nfs resourceGroup: ${AKS_PERS_RESOURCE_GROUP} storageAccount: ${AKS_PERS_STORAGE_ACCOUNT_NAME} shareName: ${AKS_PERS_SHARE_NAME} reclaimPolicy: Delete volumeBindingMode: Immediate allowVolumeExpansion: true EOF cat \u0026gt;pvc-${TIMESTAMP}.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: wls-azurefile-${TIMESTAMP} spec: accessModes: - ReadWriteMany storageClassName: azurefile-csi-nfs resources: requests: storage: 5Gi EOF Use the kubectl command to create the Storage Class and persistent volume claim in the default namespace.\n$ kubectl apply -f azure-csi-nfs-${TIMESTAMP}.yaml $ kubectl apply -f pvc-${TIMESTAMP}.yaml Use the following command to verify:\n$ kubectl get sc Example of kubectl get sc output:\n$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE azurefile file.csi.azure.com Delete Immediate true 30m azurefile-csi file.csi.azure.com Delete Immediate true 30m azurefile-csi-nfs file.csi.azure.com Delete Immediate true 24m azurefile-csi-premium file.csi.azure.com Delete Immediate true 30m azurefile-premium file.csi.azure.com Delete Immediate true 30m ... $ kubectl get pvc Example of kubectl get pvc output:\n$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE wls-azurefile-1693900684 Bound pvc-1f615766-0f21-4c88-80e1-93c9bdabb3eb 5Gi RWX azurefile-csi-nfs 46s Create a domain creation image This sample requires Domain creation images. For more information, see Domain on Persistent Volume.\nImage creation prerequisites The JAVA_HOME environment variable must be set and must reference a valid JDK 8 or 11 installation.\nCopy the sample to a new directory; for example, use the directory /tmp/dpv-sample. In the directory name, dpv is short for \u0026ldquo;domain on pv\u0026rdquo;. Domain on PV is one of three domain home source types supported by the operator. To learn more, see Choose a domain home source type.\n$ rm -rf /tmp/dpv-sample $ mkdir /tmp/dpv-sample $ cp -r $BASE_DIR/sample-scripts/create-weblogic-domain/domain-on-pv/* /tmp/dpv-sample NOTE: We will refer to this working copy of the sample as /tmp/dpv-sample; however, you can use a different location.\nCopy the wdt-artifacts directory of the sample to a new directory; for example, use directory /tmp/dpv-sample/wdt-artifacts\n$ cp -r $BASE_DIR/sample-scripts/create-weblogic-domain/wdt-artifacts/* /tmp/dpv-sample $ export WDT_MODEL_FILES_PATH=/tmp/dpv-sample/wdt-model-files Download the latest WebLogic Deploying Tooling (WDT) and WebLogic Image Tool (WIT) installer ZIP files to your ${WDT_MODEL_FILES_PATH} directory. Both WDT and WIT are required to create your Model in Image images.\n$ curl -m 120 -fL https://github.com/oracle/weblogic-deploy-tooling/releases/latest/download/weblogic-deploy.zip \\ -o ${WDT_MODEL_FILES_PATH}/weblogic-deploy.zip $ curl -m 120 -fL https://github.com/oracle/weblogic-image-tool/releases/latest/download/imagetool.zip \\ -o ${WDT_MODEL_FILES_PATH}/imagetool.zip Set up the WebLogic Image Tool, run the following commands:\n$ unzip ${WDT_MODEL_FILES_PATH}/imagetool.zip -d ${WDT_MODEL_FILES_PATH} $ ${WDT_MODEL_FILES_PATH}/imagetool/bin/imagetool.sh cache deleteEntry --key wdt_latest $ ${WDT_MODEL_FILES_PATH}/imagetool/bin/imagetool.sh cache addInstaller \\ --type wdt \\ --version latest \\ --path ${WDT_MODEL_FILES_PATH}/weblogic-deploy.zip These steps will install WIT to the ${WDT_MODEL_FILES_PATH}/imagetool directory, plus put a wdt_latest entry in the tool’s cache which points to the WDT ZIP file installer. You will use WIT later in the sample for creating model images.\nImage creation - Introduction The goal of image creation is to demonstrate using the WebLogic Image Tool to create an image tagged as wdt-domain-image:WLS-v1 from files that you will stage to ${WDT_MODEL_FILES_PATH}/WLS-v1/.\nThe directory where the WebLogic Deploy Tooling software is installed (also known as WDT Home) is expected in an image’s /auxiliary/weblogic-deploy directory. WDT model YAML (model), WDT variable (property), and WDT archive ZIP (archive) files are expected in directory /auxiliary/models. Understanding your first archive See Understanding your first archive.\nStaging a ZIP file of the archive Delete any possible existing archive.zip in case we have an old leftover version.\n$ rm -f ${WDT_MODEL_FILES_PATH}/WLS-v1/archive.zip Create a ZIP file of the archive in the location that we will use when we run the WebLogic Image Tool.\n$ cd /tmp/dpv-sample/archives/archive-v1 $ zip -r ${WDT_MODEL_FILES_PATH}/WLS-v1/archive.zip wlsdeploy Staging model files In this step, you explore the staged WDT model YAML file and properties in the ${WDT_MODEL_FILES_PATH}/WLS-v1 directory. The model in this directory references the web application in your archive, configures a WebLogic Server Administration Server, and configures a WebLogic cluster. It consists of only two files, model.10.properties, a file with a single property, and, model.10.yaml, a YAML file with your WebLogic configuration.\nHere is the WLS model.10.properties:\nCLUSTER_SIZE=5 Here is the WLS model.10.yaml:\ndomainInfo: AdminUserName: \u0026#39;@@SECRET:__weblogic-credentials__:username@@\u0026#39; AdminPassword: \u0026#39;@@SECRET:__weblogic-credentials__:password@@\u0026#39; ServerStartMode: \u0026#39;prod\u0026#39; topology: Name: \u0026#39;@@ENV:CUSTOM_DOMAIN_NAME@@\u0026#39; AdminServerName: \u0026#39;admin-server\u0026#39; Cluster: \u0026#39;cluster-1\u0026#39;: DynamicServers: ServerTemplate: \u0026#39;cluster-1-template\u0026#39; ServerNamePrefix: \u0026#39;managed-server\u0026#39; DynamicClusterSize: \u0026#39;@@PROP:CLUSTER_SIZE@@\u0026#39; MaxDynamicClusterSize: \u0026#39;@@PROP:CLUSTER_SIZE@@\u0026#39; MinDynamicClusterSize: \u0026#39;0\u0026#39; CalculatedListenPorts: false Server: \u0026#39;admin-server\u0026#39;: ListenPort: 7001 ServerTemplate: \u0026#39;cluster-1-template\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 appDeployments: Application: myapp: SourcePath: \u0026#39;wlsdeploy/applications/myapp-v1\u0026#39; ModuleType: ear Target: \u0026#39;cluster-1\u0026#39; The model file:\nDefines a WebLogic domain with:\nCluster cluster-1 Administration Server admin-server An EAR application, targeted to cluster-1, located in the WDT archive ZIP file at wlsdeploy/applications/myapp-v1 Leverages macros to inject external values:\nThe property file CLUSTER_SIZE property is referenced in the model YAML file DynamicClusterSize and MaxDynamicClusterSize fields using a PROP macro. The model file domain name is injected using a custom environment variable named CUSTOM_DOMAIN_NAME using an ENV macro. You set this environment variable later in this sample using an env field in its Domain. This conveniently provides a simple way to deploy multiple differently named domains using the same model image. The model file administrator user name and password are set using a weblogic-credentials secret macro reference to the WebLogic credential secret. This secret is in turn referenced using the webLogicCredentialsSecret field in the Domain. The weblogic-credentials is a reserved name that always dereferences to the owning Domain actual WebLogic credentials secret name. An image can contain multiple properties files, archive ZIP files, and model YAML files but in this sample you use just one of each. For a complete description of WDT model file naming conventions, file loading order, and macro syntax, see Model files in the user documentation.\nCreating the image with WIT At this point, you have all of the files needed for image wdt-domain-image:WLS-v1 staged; they include:\n/tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/model.10.yaml /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/model.10.properties /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/archive.zip Now, you use the Image Tool to create an image named wdt-domain-image:WLS-v1. You’ve already set up this tool during the prerequisite steps.\nRun the following command to create the image and verify that it worked.\n$ ${WDT_MODEL_FILES_PATH}/imagetool/bin/imagetool.sh createAuxImage \\ --tag wdt-domain-image:WLS-v1 \\ --wdtModel ${WDT_MODEL_FILES_PATH}/WLS-v1/model.10.yaml \\ --wdtVariables ${WDT_MODEL_FILES_PATH}/WLS-v1/model.10.properties \\ --wdtArchive ${WDT_MODEL_FILES_PATH}/WLS-v1/archive.zip This command runs the WebLogic Image Tool to create the domain creation image and does the following:\nBuilds the final container image as a layer on a small busybox base image. Copies the WDT ZIP file that\u0026rsquo;s referenced in the WIT cache into the image. Note that you cached WDT in WIT using the keyword latest when you set up the cache during the sample prerequisites steps. This lets WIT implicitly assume it\u0026rsquo;s the desired WDT version and removes the need to pass a -wdtVersion flag. Copies the specified WDT model, properties, and application archives to image location /auxiliary/models. When the command succeeds, it should end with output like the following:\n[INFO ] Build successful. Build time=70s. Image tag=wdt-domain-image:WLS-v1 Verify the image is available in the local Docker server with the following command.\n$ docker images | grep WLS-v1 The output will show something similar to the following:\nwdt-domain-image WLS-v1 012d3bfa3536 5 days ago 1.13GB The imagetool.sh is not supported on macOS with Apple Silicon. See Troubleshooting - exec format error.\nYou may run into a Dockerfile parsing error if your Docker buildkit is enabled, see Troubleshooting - WebLogic Image Tool failure.\nPushing the image to Azure Container Registry AKS can pull images from any container registry, but the easiest integration is to use Azure Container Registry (ACR). In addition to simplicity, using ACR simplifies high availability and disaster recovery with features such as geo-replication. For more information, see Geo-replication in Azure Container Registry. In this section, we will create a new Azure Container Registry, connect it to our pre-existing AKS cluster and push the image built in the preceding section to it. For complete details, see Azure Container Registry documentation.\nLet\u0026rsquo;s create an instance of ACR in the same resource group we used for AKS. We will use the environment variables used during the steps above. For simplicity, we use the resource group name as the name of the ACR instance.\n$ az acr create --resource-group $AKS_PERS_RESOURCE_GROUP --name $ACR_NAME --sku Basic --admin-enabled true Closely examine the JSON output from this command. Save the value of the loginServer property aside. It will look something like the following.\n\u0026#34;loginServer\u0026#34;: \u0026#34;contosoresourcegroup1610068510.azurecr.io\u0026#34;, Use this value to sign in to the ACR instance. Note that because you are signing in with the az CLI, you do not need a password because your identity is already conveyed by having done az login previously.\n$ export LOGIN_SERVER=$(az acr show -n $ACR_NAME --resource-group $AKS_PERS_RESOURCE_GROUP --query \u0026#34;loginServer\u0026#34; -o tsv) $ az acr login --name $LOGIN_SERVER Successful output will include Login Succeeded.\nPush the wdt-domain-image:WLS-v1 image created while satisfying the preconditions to this registry.\n$ docker tag wdt-domain-image:WLS-v1 $LOGIN_SERVER/wdt-domain-image:WLS-v1 $ docker push ${LOGIN_SERVER}/wdt-domain-image:WLS-v1 Finally, connect the AKS cluster to the ACR. For more details on connecting ACR to an existing AKS, see Configure ACR integration for existing AKS clusters.\n$ export ACR_ID=$(az acr show -n $ACR_NAME --resource-group $AKS_PERS_RESOURCE_GROUP --query \u0026#34;id\u0026#34; -o tsv) $ az aks update --name $AKS_CLUSTER_NAME --resource-group $AKS_PERS_RESOURCE_GROUP --attach-acr $ACR_ID Successful output will be a JSON object with the entry \u0026quot;type\u0026quot;: \u0026quot;Microsoft.ContainerService/ManagedClusters\u0026quot;.\nIf you see an error that seems related to you not being an Owner on this subscription, please refer to the troubleshooting section Cannot attach ACR due to not being Owner of subscription.\nInstall WebLogic Kubernetes Operator into the AKS cluster The WebLogic Kubernetes Operator is an adapter to integrate WebLogic Server and Kubernetes, allowing Kubernetes to serve as a container infrastructure hosting WLS instances. The operator runs as a Kubernetes Pod and stands ready to perform actions related to running WLS on Kubernetes.\nKubernetes Operators use Helm to manage Kubernetes applications. The operator’s Helm chart is located in the kubernetes/charts/weblogic-operator directory. Please install the operator by running the corresponding command.\n$ helm repo add weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts --force-update $ helm repo update $ helm install weblogic-operator weblogic-operator/weblogic-operator The output will show something similar to the following:\n$ helm install weblogic-operator weblogic-operator/weblogic-operator NAME: weblogic-operator LAST DEPLOYED: Tue Jan 18 17:07:56 2022 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None Verify the operator with the following command; the STATUS must be Running. The READY must be 1/1.\n$ kubectl get pods -w NAME READY STATUS RESTARTS AGE weblogic-operator-69794f8df7-bmvj9 1/1 Running 0 86s weblogic-operator-webhook-868db5875b-55v7r 1/1 Running 0 86s You will have to press Ctrl-C to exit this command due to the -w flag.\nCreate WebLogic domain Now that you have created the AKS cluster, installed the operator, and verified that the operator is ready to go, you can ask the operator to create a WLS domain.\nCreate secrets You will use the $BASE_DIR/sample-scripts/create-weblogic-domain-credentials/create-weblogic-credentials.sh script to create the domain WebLogic administrator credentials as a Kubernetes secret. Please run the following commands:\ncd $BASE_DIR/sample-scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u ${WEBLOGIC_USERNAME} -p ${WEBLOGIC_PASSWORD} -d domain1 The output will show something similar to the following:\nsecret/domain1-weblogic-credentials created secret/domain1-weblogic-credentials labeled The secret domain1-weblogic-credentials has been successfully created in the default namespace. You will use the kubernetes/samples/scripts/create-kubernetes-secrets/create-docker-credentials-secret.sh script to create the Docker credentials as a Kubernetes secret. Please run the following commands:\n$ cd $BASE_DIR/sample-scripts/create-kubernetes-secrets $ ./create-docker-credentials-secret.sh -s ${SECRET_NAME_DOCKER} -e ${ORACLE_SSO_EMAIL} -p ${ORACLE_SSO_PASSWORD} -u ${ORACLE_SSO_EMAIL} The output will show something similar to the following:\nsecret/wlsregcred created The secret wlsregcred has been successfully created in the default namespace. Verify secrets with the following command:\n$ kubectl get secret The output will show something similar to the following:\nNAME TYPE DATA AGE domain1-weblogic-credentials Opaque 2 2m32s sh.helm.release.v1.weblogic-operator.v1 helm.sh/release.v1 1 5m32s weblogic-operator-secrets Opaque 1 5m31s weblogic-webhook-secrets Opaque 2 5m31s wlsregcred kubernetes.io/dockerconfigjson 1 38s NOTE: If the NAME column in your output is missing any of the values shown above, please review your execution of the preceding steps in this sample to ensure that you correctly followed all of them.\nEnable Weblogic Operator Run the following command to enable the operator to monitor the namespace.\nkubectl label namespace default weblogic-operator=enabled Create WebLogic Domain Now, you deploy a sample-domain1 domain resource and an associated sample-domain1-cluster-1 cluster resource using a single YAML resource file which defines both resources. The domain resource and cluster resource tells the operator how to deploy a WebLogic domain. They do not replace the traditional WebLogic configuration files, but instead cooperate with those files to describe the Kubernetes artifacts of the corresponding domain.\nRun the following commands to generate resource files.\nExport Domain_Creation_Image_tag, which will be referred in create-domain-on-aks-generate-yaml.sh.\nexport Domain_Creation_Image_tag=${LOGIN_SERVER}/wdt-domain-image:WLS-v1 cd $BASE_DIR/sample-scripts/create-weblogic-domain-on-azure-kubernetes-service bash create-domain-on-aks-generate-yaml.sh After running above commands, you will get three files: domain-resource.yaml, admin-lb.yaml, cluster-lb.yaml.\nThe domain resource references the cluster resource, a WebLogic Server installation image, the secrets you defined, PV and PVC configuration details, and a sample domain creation image, which contains a traditional WebLogic configuration and a WebLogic application. For detailed information, see Domain and cluster resources.\nRun the following command to apply the two sample resources.\n$ kubectl apply -f domain-resource.yaml Create the load balancer services using the following commands:\n$ kubectl apply -f admin-lb.yaml The output will show something similar to the following:\nservice/domain1-admin-server-external-lb created $ kubectl apply -f cluster-lb.yaml The output will show something similar to the following:\nservice/domain1-cluster-1-external-lb created After a short time, you will see the Administration Server and Managed Servers running.\nUse the following command to check server pod status:\n$ kubectl get pods --watch It may take you up to 20 minutes to deploy all pods, please wait and make sure everything is ready.\nYou can tail the logs of the Administration Server with this command:\nkubectl logs -f domain1-admin-server The final example of pod output is as following:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE domain1-admin-server 1/1 Running 0 12m domain1-managed-server1 1/1 Running 0 10m domain1-managed-server2 1/1 Running 0 10m weblogic-operator-7796bc7b8-qmhzw 1/1 Running 0 48m weblogic-operator-webhook-b5b586bc5-ksfg9 1/1 Running 0 48m If Kubernetes advertises the WebLogic pod as Running you can be assured the WebLogic Server actually is running because the operator ensures that the Kubernetes health checks are actually polling the WebLogic health check mechanism.\nGet the addresses of the Administration Server and Managed Servers (please wait for the external IP addresses to be assigned):\n$ kubectl get svc --watch The final example of service output is as following:\n$ kubectl get svc --watch NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE domain1-admin-server ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 13m domain1-admin-server-external-lb LoadBalancer 10.0.30.252 4.157.147.131 7001:31878/TCP 37m domain1-cluster-1-lb LoadBalancer 10.0.26.96 4.157.147.212 8001:32318/TCP 37m domain1-cluster-cluster-1 ClusterIP 10.0.157.174 \u0026lt;none\u0026gt; 8001/TCP 10m domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 10m domain1-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 10m kubernetes ClusterIP 10.0.0.1 \u0026lt;none\u0026gt; 443/TCP 60m weblogic-operator-webhook-svc ClusterIP 10.0.41.121 \u0026lt;none\u0026gt; 8083/TCP,8084/TCP 49m In the example, the URL to access the Administration Server is: http://4.157.147.131:7001/console. The user name and password that you enter for the Administration Console must match the ones you specified for the domain1-weblogic-credentials secret in the Create secrets step.\nIf the WLS Administration Console is still not available, use kubectl get events --sort-by='.metadata.creationTimestamp' to troubleshoot.\n$ kubectl get events --sort-by=\u0026#39;.metadata.creationTimestamp\u0026#39; To access the sample application on WLS, skip to the section Access sample application. The next section includes a script that automates all of the preceding steps.\nAutomation If you want to automate the above steps of creating the AKS cluster and WLS domain, you can use the script ${BASE_DIR}/sample-scripts/create-weblogic-domain-on-azure-kubernetes-service/create-domain-on-aks.sh.\nThe sample script will create a WLS domain home on the AKS cluster, including:\nCreating a new Azure resource group, with a new Azure Storage Account and Azure File Share to allow WebLogic to persist its configuration and data separately from the Kubernetes pods that run WLS workloads. Creating WLS domain home. Generating the domain resource YAML files, which can be used to restart the Kubernetes artifacts of the corresponding domain. To customize the WLS domain, you can optionally edit ${BASE_DIR}/sample-scripts/create-weblogic-domain-on-azure-kubernetes-service/create-domain-on-aks-inputs.sh.\nYou can now run the script.\n$ cd ${BASE_DIR}/sample-scripts/create-weblogic-domain-on-azure-kubernetes-service $ ./create-domain-on-aks.sh The script will take some time to run. The script will print the Administration Server address after a successful deployment. To interact with the cluster using kubectl, use az aks get-credentials as shown in the script output.\nYou now have created an AKS cluster with Azure Files NFS share to contain the WLS domain configuration files. Using those artifacts, you have used the operator to create a WLS domain.\nAccess sample application Access the Administration Console using the admin load balancer IP address.\n$ ADMIN_SERVER_IP=$(kubectl get svc domain1-admin-server-external-lb -o=jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) $ echo \u0026#34;Administration Console Address: http://${ADMIN_SERVER_IP}:7001/console/\u0026#34; Access the sample application using the cluster load balancer IP address.\n$ CLUSTER_IP=$(kubectl get svc domain1-cluster-1-lb -o=jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) $ curl http://${CLUSTER_IP}:8001/myapp_war/index.jsp The test application will list the server host on the output, like the following:\n\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v1\u0026#39; of the sample JSP web-app. Welcome to WebLogic Server \u0026#39;managed-server1\u0026#39;! domain UID = \u0026#39;domain1\u0026#39; domain name = \u0026#39;domain1\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found min threads constraint runtime named \u0026#39;SampleMinThreads\u0026#39; with configured count: 1 Found max threads constraint runtime named \u0026#39;SampleMaxThreads\u0026#39; with configured count: 10 Found 0 local data sources: ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; Validate NFS volume There are several approaches to validate the NFS volume:\nUse Azure Storage browser. Make sure you have permission to access the NFS server, see Azure Storage firewalls and virtual networks document Mount the same NFS share in an existing virtual machine from Azure. Access files from the mounted path, see Mount Azure NFS file share to Linux. Use kubectl exec to enter the admin server pod to check file system status:\nkubectl exec -it domain1-admin-server -- df -h You will find output like the following, with filesystem ${AKS_PERS_STORAGE_ACCOUNT_NAME}.file.core.windows.net:/${AKS_PERS_STORAGE_ACCOUNT_NAME}/${AKS_PERS_SHARE_NAME}, size 100G, and mounted on /shared:\nFilesystem Size Used Avail Use% Mounted on ... wlsstorage1612795811.file.core.windows.net:/wlsstorage1612795811/wls-weblogic-1612795811 100G 76M 100G 1% /shared ... Clean up resources If you used the automation script, the output from the create-domain-on-aks.sh script includes a statement about the Azure resources created by the script. To delete the cluster and free all related resources, simply delete the resource groups. The output will list the resource groups, such as:\nThe following Azure Resources have been created: Resource groups: wlsresourcegroup6091605169, MC_wlsresourcegroup6091605169_wlsakscluster6091605169_eastus Given the above output, the following Azure CLI commands will delete the resource groups.\n$ az group delete --yes --no-wait --name wlsresourcegroup6091605169 If you created the AKS cluster step by step, run the following command to clean up resources.\n$ az group delete --yes --no-wait --name $AKS_PERS_RESOURCE_GROUP Troubleshooting For troubleshooting advice, see Troubleshooting.\nUseful links Domain on a PV sample "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/domain-on-pv/usage/",
	"title": "Usage",
	"tags": [],
	"description": "Instructions for using Domain on PV.",
	"content": "This document describes how to create and deploy a typical Domain home on persistent volume (Domain on PV).\nContents WebLogic Kubernetes Operator Configuration WebLogic base image Domain creation WDT models Optional WDT models ConfigMap Using WDT model encryption Volumes and VolumeMounts information Persistent Volume and Persistent Volume Claim PV and PVC requirements References Domain information Best practices Back up the JRF domain home directory and database Store the OPSS wallet in a Kubernetes Secret and update opss.walletFileSecret in the domain resource Recovering the domain when it\u0026rsquo;s corrupted or in other disaster scenarios Troubleshooting Cleanup Configuration example Basic configuration WebLogic Kubernetes Operator Deploy the operator and ensure that it is monitoring the desired namespace for your Domain on PV domain. See Manage operators and Quick Start.\nConfiguration Beginning with operator version 4.1.0, you can provide a section, domain.spec.configuration.initializeDomainOnPV, to initialize a WebLogic domain on a persistent volume when it is first deployed. This is a one time only initialization. After the domain is created, subsequent updates to this section in the domain resource YAML file will not recreate or update the WebLogic domain.\nTo use this feature, provide the following information:\nWebLogic base image - This is the Fusion Middleware Software to be used, for example, WebLogic Server or Fusion Middleware Infrastructure. Volumes and VolumeMounts information - This follows the standard Kubernetes pod requirements for mounting persistent volumes. PersistentVolume and PersistentVolumeClaim - This is environment specific and usually requires assistance from your administrator to provide the underlying details, such as storageClass or any permissions. Domain information - This describes the domain type and whether the operator should create the RCU schema. Domain WDT models - This is where the WDT Home, WDT model, WDT archive, and WDT variables files reside. Optional WDT models ConfigMap - Optional, WDT model, WDT variables files. Using WDT model encryption - Optional, using WDT model encryption. Domain resource YAML file - This is for deploying the domain in WebLogic Kubernetes Operator. For details about each field, see the initializeDomainOnPV section in the domain resource schema, or use the command kubectl explain domain.spec.configuration.initializeDomainOnPV.\nFor a basic configuration example, see Basic configuration.\nWebLogic base image Because the domain will be created on a persistent volume, the base image should contain only the FMW product binary and the JDK.\nspec: image: \u0026#34;container-registry.oracle.com/middleware/fmw-infrastructure_cpu:12.2.1.4-jdk8-ol8-221014\u0026#34; You can specify your own image, use a patched image from container-registry.oracle.com, or create and patch an image using the WebLogic Image Tool (WIT).\nDomain creation WDT models Specify an image that describes the domain topology, resources, and applications in the domain resource YAML file.\ndomain: domainCreationImages: - image: \u0026#39;myrepo/domain-images:v1\u0026#39; Field Notes Values Required domainCreationImages WDT domain images. An array of images. Y In this image or images, you must provide the required WDT installer, and also the WDT model files, WDT variables files, and WDT archive files. The operator will use them to create the initial domain.\nFor additional options in domainCreationImages, use the following command to obtain the details.\n$ kubectl explain domain.spec.configuration.initializeDomainOnPV.domain.domainCreationImages The image layout follows this directory structure:\n/auxiliary/weblogic-deploy - The directory where the WebLogic Deploy Tooling software is installed. /auxiliary/models - WDT model, WDT archive, and WDT variables files. You can create your own image using your familiar method or use the WebLogic Image Tool (WIT).\nFor example, because the file structure is the same as an Auxiliary Image that\u0026rsquo;s used in the Model in Image domain home source type, you can use the same WIT command createAuxImage.\n$ imagetool.sh createAuxImage --wdtArchive /home/acme/myapp/wdt/myapp.zip \\ --wdtVersion latest \\ --wdtModel /home/acme/myapp/wdt/model1.yaml \\ --wdtVariables /home/acme/myapp/wdt/model1.properties \\ --tag myrepo/domain-images:v1 Optional WDT models ConfigMap Optionally, you can provide a Kubernetes ConfigMap with additional WDT models and WDT variables files as supplements or overrides to those in domainCreationImages.\ndomain: ... domainCreationImages: ... domainCreationConfigMap: mymodel-domain-configmap Field Notes Values Required domainCreationConfigMap Optional WDT models and WDT variables files in ConfigMap. ConfigMap name. N The files inside this ConfigMap must have file extensions, .yaml, .properties, or .zip.\nUsing WDT model encryption Starting in WebLogic Kubernetes Operator version 4.2.16. If the provided WDT models are encrypted using the WDT encryptModel command. You can specify the encryption passphrase as a secret in the domain resource YAML. WDT will use the value in the secret to decrypt the models for domain creation.\ninitializeDomainOnPV: modelEncryptionPassphraseSecret: model-encryption-secret The secret must have a key passphrase containing the value of the WDT encryption passphrase used to encrypt the models.\nkubectl create secret generic model-encrypion-secret --from-literal=passphrase=\u0026lt;encryption passphrase value\u0026gt;\nVolumes and VolumeMounts information You must provide the volumes and volumeMounts information in domain.spec.serverPod. This allows the pod to mount the persistent storage at runtime. The mountPath needs to be part of the domain home and log home and persistentVolumeClaim.claimName needs to be a valid PVC name, whether it is a pre-existing PVC or one to be created by the operator. See Creating PVC by the operator.\nspec: domainHome: /share/domains/domain1 logHome: /share/logs/domain1 serverPod: volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: sample-domain1-pvc-rwm1 volumeMounts: - mountPath: /share name: weblogic-domain-storage-volume Persistent Volume and Persistent Volume Claim The Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC) is used by Kubernetes to access persistent storage in the Kubernetes environment. You can either use an existing PV/PVC or request that the operator create them for you.\nThe specifications of PersistentVolume and PersistentVolumeClaim are environment specific and often require information from your Kubernetes cluster administrator. See Persistent Storage in different environments.\nPV and PVC requirements Domain on PV requires that the PV and PVC are created with Filesystem volume mode; Block volume mode is not supported. If you request that the operator creates the PV and PVC, then it uses the default Filesystem volume mode. If you plan to use an existing PV and PVC, then ensure that it was created with Filesystem volume mode. You must use a storage provider that supports the ReadWriteMany option. The operator will automatically set the owner of all files in the domain home on the persistent volume to uid 1000 with gid 0. If you want to use a different user and group, then configure the desired runAsUser and runAsGroup in the security context under the spec.serverPod.podSecurityContext section of the Domain YAML file. The operator will use these values when setting the owner for files in the domain home directory. For example, if you provide the specification of the Persistent Volume and Persistent Volume Claim in the domain resource YAML file,\nthen the operator will create the PV and PVC, and mount the persistent volume to the /share directory.\nspec: domainHome: /share/domains/domain1 serverPod: volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: sample-domain1-pvc-rwm1 volumeMounts: - mountPath: /share name: weblogic-domain-storage-volume configuration: initializeDomainOnPV: waitForPvcToBind: true persistentVolume: metadata: name: sample-domain1-pv-rwm1 spec: storageClassName: my-storage-class capacity: storage: 20Gi persistentVolumeReclaimPolicy: Retain hostPath: path: \u0026#34;/share\u0026#34; persistentVolumeClaim: metadata: name: sample-domain1-pvc-rwm1 spec: storageClassName: my-storage-class resources: requests: storage: 10Gi Field Notes Values Required waitForPvcToBind Specify whether the operator will wait for the PVC to reach the bound state when it is creating it. In some cloud environments, the PVC is in the pending state until it is actually used in a running pod; set it to false if necessary. Boolean N (default true) persistentVolume Specification of the persistent volume if the operator is creating it. Specification of the persistent volume if you want the operator to create it. N (operator will not create any persistent volume) persistentVolumeClaim Specification of the persistent volume claim if the operator is creating it. Specification of the persistent volume claim if you want operator to create it. N (operator will not create any persistent volume claim) Not all the fields in the standard Kubernetes PV and PVC are supported. For the list of supported fields in persistentVolume and persistentVolumeClaim, see the initializeDomainOnPV.peristentVolume and initializeDomainOnPV.peristentVolumeClaim section in the domain resource schema, or use the commands:\n$ kubectl explain domain.spec.configuration.initializeDomainOnPV.persistentVolume $ kubectl explain domain.spec.configuration.initializeDomainOnPV.persistentVolumeClaim If the PV and PVC already exist in your environment, you do not need to specify any persistentVolume or persistentVolumeClaim under the intializedDomainOnPV section.\nReferences Oracle Kubernetes Engine Persistent Storage Azure Kubernetes Service Persistent Storage Amazon Kubernetes Service Persistent Storage OpenShift Persistent Storage Domain information For JRF-based domains, before proceeding, please be sure to read this document, JRF domains.\nThis is the section describing the WebLogic domain. For example:\nspec: domainHome: /share/domains/sample-domain1 domainHomeSourceType: PersistentVolume configuration: secrets: [ sample-domain1-rcu-access ] initializeDomainOnPV: ... domain: # Domain | DomainAndRCU createIfNotExists: DomainAndRCU domainCreationImages: - image: \u0026#39;myrepo/domain-images:v1\u0026#39; domainType: JRF domainCreationConfigMap: sample-domain1-wdt-config-map opss: walletPasswordSecret: sample-domain1-opss-wallet-password-secret Field Notes Values Required domainType Type of domain being created. JRF or WLS N (default WLS) createIfNotExists Specifies whether the operator should create the RCU schema first, before creating the domain. Domain or DomainAndRCU (drop existing RCU schema and create new RCU schema) N (default Domain) domainCreationImages WDT domain images. An array of images. Y domainCreationConfigMap Optional ConfigMap containing extra WDT models. Kubernetes ConfigMap name. N osss.walletPasswordSecret Password for extracting OPSS wallet for JRF domain. Kubernetes Secret name with the key walletPassword. Y osss.walletFileSecret Extracted OPSS wallet file. Kubernetes Secret name with the key walletFile. N (Only needed when recreating the domain during disaster recovery) After a JRF domain is successfully deployed: follow the next section, Best practices, to download and back up the OPSS wallet.\nBest practices Oracle recommends that you save the OPSS wallet file in a safe, backed-up location immediately after the initial JRF domain is created. In addition, you should make sure to store the wallet in a Kubernetes Secret in the same namespace. This will allow the secret to be available when the domain needs to be recovered in a disaster scenario or if the domain directory gets corrupted. There is no way to reuse the original RCU schema without this specific wallet key. Therefore, for disaster recovery, you should back up this OPSS wallet.\nBack up the JRF domain home directory and database Oracle recommends that you back up the domain home directory and database after the initial JRF domain is created, and then periodically, making sure all the latest changes are backed up.\nA JRF domain has a one-to-one relationship with the RCU schema. After a domain is created using a particular RCU schema, that schema cannot be reused by another domain and the same schema cannot be shared across different domains. Any attempts to create a new domain using the existing RCU schema will result in an error.\nIf the domain home is not properly backed up, you potentially can lose existing data if the domain home is corrupted or deleted. That\u0026rsquo;s because recreating the domain requires dropping the existing RCU schema and creating a new RCU schema. Therefore, backing up the existing domain home should be the highest priority in your Kubernetes environment.\nA Domain on PV domain configuration might get updated after its initial deployment. For example, using WLST or the console, you might have deployed new applications, added custom OPSS keystores, and added OWSM policies, and such. In that case, the original WDT model files used to create the initial domain will not match the current state of the domain (the WDT model files are not the source of truth). Therefore, if you recreate the domain using the original WDT model files, you will lose all the subsequent updates. In order to preserve the domain updates, you should restore the domain from the backup copy of the domain home directory and connect to the existing RCU schema from the database backup.\nStore the OPSS wallet in a Kubernetes Secret and update opss.walletFileSecret in the domain resource After the domain is created, the operator automatically exports the OPSS wallet and stores it in an introspector ConfigMap; the name of the ConfigMap follows the pattern \u0026lt;domain uid\u0026gt;-weblogic-domain-introspect-cm with the key ewallet.p12. Oracle recommends that you save the OPSS wallet file in a safe, backed-up location immediately after the initial JRF domain is created. In addition, you should make sure to store the wallet in a Kubernetes Secret in the same namespace. This will allow the secret to be available when the domain needs to be recovered in a disaster scenario or if the domain directory gets corrupted.\nThe following are the high-level steps for storing the OPSS wallet in a Kubernetes Secret.\nThe operator provides a utility script, OPSS wallet utility, for extracting the wallet file and storing it in a Kubernetes walletFileSecret. In addition, you should also save the wallet file in a safely backed-up location outside of Kubernetes. For example, the following command saves the OPSS wallet for the sample-domain1 domain in the sample-ns namespace to a file named ewallet.p12 in the /tmp directory and also stores it in the wallet secret named jrf-wallet-file-secret.\n$ opss-wallet.sh -n sample-ns -d sample-domain1 -s -r -wf /tmp/ewallet.p12 -ws jrf-wallet-file-secret Replace /tmp/ewallet.p12 in the previous command with the name of the file to be stored in a safe location and replace jrf-wallet-file-secret with the Secret name of your choice. For more information, see the OPSS wallet utility section in the README file.\nSave the extracted wallet file (/tmp/ewallet.p12 in the example) to a safe location, outside of Kubernetes.\nAdd the opss.walletFileSecret to the domain resource YAML file under configuration.initializeDomainOnPV.domain.\n... configuration: initializeDomainOnPV: ... domain: ... opss: walletFileSecret: jrf-wallet-file-secret ... You can use the following patch command to add it to the domain resource.\n$ kubectl -n sample-ns patch domain sample-domain1 --type=\u0026#39;JSON\u0026#39; -p=\u0026#39;[ { \u0026#34;op\u0026#34; : \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34; : \u0026#34;/spec/configuration/initializeDomainOnPV/domain/opss/walletFileSecret\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;jrf-wallet-file-secret\u0026#34; }]\u0026#39; Recovering the domain when it\u0026rsquo;s corrupted or in other disaster scenarios If the domain home directory is corrupted, and you have a recent backup of the domain home directory, then perform the following steps to recover the domain.\nRestore the domain home directory from the backup copy.\nUpdate the restartVersion of the domain resource to restart the domain. For example,\n$ kubectl -n sample-ns patch domain sample-domain1 --type=\u0026#39;JSON\u0026#39; -p=\u0026#39;[ { \u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34; : \u0026#34;/spec/restartVersion\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;15\u0026#34; }]\u0026#39; After the domain is restarted, check the WebLogic domain configuration to ensure that it has the latest changes. NOTE: If you made any changes that are persisted in the domain home directory after your last backup, you must reapply those changes to the domain home directory. However, because the operator will reconnect to the same RCU schema, the data stored in the OPSS, MDS, or OWSM tables will be current.\nReapply any domain configuration changes persisted to the domain home directory, such as data source connections, JMS destinations, or new application EAR deployments, after your last backup. To make these changes, use WLST, the WebLogic Server Administration Console, or Enterprise Manager.\nIn the rare scenario where the domain home directory is corrupted, and you do not have a recent backup of the domain home directory, or if the backup copy is also corrupted, then you can recreate the domain from the WDT model files without losing any RCU schema data.\nDelete the domain home directory from the persistent volume.\nAdd or replace the domain.spec.introspectVersion in the domain resource with a new value. The following is a sample patch command to update the introspectVersion for the sample domain.\n$ kubectl -n sample-ns patch domain sample-domain1 --type=\u0026#39;JSON\u0026#39; -p=\u0026#39;[ { \u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34; : \u0026#34;/spec/intropsectVersion\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;15\u0026#34; }]\u0026#39; The operator will then create a new domain from the existing WDT models and reuse the original RCU schema.\nNOTE: All the updates made to the domain after the initial deployment will not be available in the recovered domain. However, this allows you to access the original RCU schema database without losing all its data.\nApply all the domain configuration changes persisted to the domain home file system, such as data source connections, JMS destinations, or new application EAR deployments, that are not in the WDT model files. These are the changes you have made after the initial domain deployment.\nUpdate the restartVersion of the domain resource to restart the domain.\n$ kubectl -n sample-ns patch domain sample-domain1 --type=\u0026#39;JSON\u0026#39; -p=\u0026#39;[ { \u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34; : \u0026#34;/spec/restartVersion\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;15\u0026#34; }]\u0026#39; For more information, see Disaster Recovery.\nTroubleshooting Problem: An error in the introspector job.\nCheck the domain status of the domain.\n$ kubectl -n \u0026lt;domain namespace\u0026gt; get domain \u0026lt;domain uid\u0026gt; Check the domain log files in logHome.\nBy default, the operator persists the log files for the introspector job, RCU logs, and WebLogic server logs in domain.spec.logHome (default: /share/logs/\u0026lt;domain uid\u0026gt;).\nIf there are errors, the related output files can be found in the logHome directory. For example:\nadmin-server.log introspector_script.out createDomain.log wdt_output.log admin-server.out ... rculogdir/** Check the operator log. Additional error messages may be available in the operator log.\n$ kubectl -n \u0026lt;operator namespace\u0026gt; logs \u0026lt;operator pod name\u0026gt; Check the Kubernetes events.\n$ kubectl -n \u0026lt;domain namespace\u0026gt; get events Problem: Updated the WDT models but changes are not reflected in the domain.\nThis is the expected behavior. The WDT domain models specified in the domain image or ConfigMap is a one time only operation. They are used only for creating the initial domain. After the domain is created, they do not participate in the lifecycle operations. You should use the WebLogic console, WLST, or other means to update the domain. In order to use the updated models to recreate the domain, you must delete the domain home directory and also the applications directory for the JRF domain (applications/\u0026lt;domain uid\u0026gt; under the parent of the domain home directory) before trying to create the domain again.\nIf you see any other error, then consult Debugging\nCleanup If you need to delete the domain home and the application directory for a JRF domain, to recover from an error or if there is a need to recreate the domain home, the domain home is in the directory specified in domain.spec.domainHome, and the application directory for JRF domains (if not specified in your WDT model) is in \u0026lt;parent directory of domain home\u0026gt;/applications/\u0026lt;domain uid\u0026gt;, you can:\nIf the underlying storage volume is dynamically allocated, then delete the PVC with ReclaimPolcy: delete and recreate the PVC.\nAttach a pod to the shared volume and then access the pod to remove the contents. See the sample script, PV and PVC helper script.\nDelete the domain resource.\n$ kubectl -n \u0026lt;namespace\u0026gt; delete -f \u0026lt;domain resource YAML\u0026gt; Configuration example The following configuration example illustrates a basic configuration.\nBasic configuration spec: domainHome: /share/domains/sample-domain1 domainHomeSourceType: PersistentVolume image: \u0026#34;container-registry.oracle.com/middleware/fmw-infrastructure_cpu:12.2.1.4-jdk8-ol8-221014\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; imagePullSecrets: - name: ocir-credentials webLogicCredentialsSecret: name: sample-domain1-weblogic-credentials logHomeEnabled: true logHome: /share/logs/sample-domain1 serverPod: env: - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false -Dweblogic.security.SSL.ignoreHostnameVerification=true -Dweblogic.security.TrustKeyStore=DemoTrust -Dweblogic.debug.DebugManagementServicesResource=true\u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-XX:+UseContainerSupport -Djava.security.egd=file:/dev/./urandom \u0026#34; # set up pod to use persistent storage using pvc # domain home must be under the mountPath volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: sample-domain1-pvc-rwm1 volumeMounts: - mountPath: /share name: weblogic-domain-storage-volume clusters: - name: sample-domain1-cluster-1 replicas: 2 configuration: secrets: [ sample-domain1-rcu-access ] initializeDomainOnPV: persistentVolume: metadata: name: sample-domain1-pv-rwm1 spec: storageClassName: my-storage-class capacity: storage: 20Gi persistentVolumeReclaimPolicy: Retain hostPath: path: \u0026#34;/share\u0026#34; persistentVolumeClaim: metadata: name: sample-domain1-pvc-rwm1 spec: storageClassName: my-storage-class resources: requests: storage: 10Gi domain: # Domain | DomainAndRCU createIfNotExists: DomainAndRCU domainCreationImages: - image: \u0026#39;myrepo/domain-images:v1\u0026#39; domainType: JRF domainCreationConfigMap: sample-domain1-wdt-config-map opss: walletPasswordSecret: sample-domain1-opss-wallet-password-secret "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-operators/preparation/",
	"title": "Prepare for installation",
	"tags": [],
	"description": "Consult these preparation steps, strategy choices, and prequisites prior to installing an operator.",
	"content": "Introduction A single operator instance is capable of managing multiple domains in multiple namespaces, depending on how it is configured. A Kubernetes cluster can host multiple operators, but no more than one per namespace.\nBefore installing an operator, ensure that each of these prerequisite requirements is met:\nCheck environment Set up the operator Helm chart access Inspect the operator Helm chart Prepare an operator namespace and service account Prepare operator image Locating an operator image Default operator image Pulling operator image Customizing operator image name, pull secret, and private registry Determine the platform setting Choose a security strategy Any namespace with cluster role binding enabled Any namespace with cluster role binding disabled Local namespace only with cluster role binding disabled Choose a domain namespace selection strategy Choose a Helm release name Be aware of advanced operator configuration options Special use cases: How to download the Helm chart if Internet access is not available How to manually install the Domain and Cluster custom resource definitions (CRD) Check environment Review the Operator prerequisites to ensure that your Kubernetes cluster supports the operator.\nIt is important to keep in mind that some supported environments have additional help or samples that are specific to the operator, or are subject to limitations, special tuning requirements, special licensing requirements, or restrictions. See Supported environments for details.\nIf your environment doesn\u0026rsquo;t already have a Kubernetes setup, then see set up Kubernetes.\nIf it is not already installed, then install Helm. To install an operator, it is required to install Helm in your Kubernetes cluster. The operator uses Helm to create the necessary resources and then deploy the operator in a Kubernetes cluster.\nTo check if you already have Helm available, try the helm version command.\nFor detailed instructions on installing Helm, see the GitHub Helm Repository.\nOptionally, enable Istio.\nSet up the operator Helm chart access Before installing an operator, the operator Helm chart must be made available. The operator Helm chart includes:\nPre-configured default values for the configuration of the operator. Helm configuration value settings for fine-tuning operator behavior. Commands for deploying (installing) or undeploying the operator. You can set up access to the operator Helm chart using the chart repository.\nUse the operator Helm chart repository that is located at https://oracle.github.io/weblogic-kubernetes-operator/charts or in a custom repository that you control.\nTo set up your Helm installation so that it can access the https://oracle.github.io/weblogic-kubernetes-operator/charts repository and name the repository reference weblogic-operator, use the following command, helm repo add \u0026lt;helm-chart-repo-name\u0026gt; \u0026lt;helm-chart-repo-url\u0026gt;:\n$ helm repo add weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts --force-update To verify that a Helm chart repository was added correctly, or to list existing repositories:\n$ helm repo list NAME URL weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts For example, assuming you have named your repository weblogic-operator, simply use weblogic-operator/weblogic-operator in your Helm commands when specifying the chart location.\nTo list the versions of the operator that you can install from the Helm chart repository:\n$ helm search repo weblogic-operator/weblogic-operator --versions For a specified version of the Helm chart and operator, with the helm pull and helm install commands, use the --version \u0026lt;value\u0026gt; option to choose the version that you want, with the latest value being the default.\nInspect the operator Helm chart You can find out the configuration values that the operator Helm chart supports, as well as the default values, using the helm show command.\n$ helm show values weblogic-operator/weblogic-operator Alternatively, you can view most of the configuration values and their defaults in the operator source in the ./kubernetes/charts/weblogic-operator/values.yaml file.\nThe available configuration values are explained by category in the Operator Helm configuration values section of the operator Configuration Reference.\nHelm commands are explained in more detail here, see Useful Helm operations.\nPrepare an operator namespace and service account Each operator requires a namespace to run in and a Kubernetes service account within the namespace. The service account will be used to host the operator\u0026rsquo;s security permissions. Only one operator can run in a given namespace.\nTo simplifies management and monitoring of an operator, Oracle recommends:\nWhen possible, create an isolated namespace for each operator which hosts only the operator, does not host domains, and does not host Kubernetes resources that are unrelated to the operator. Sometimes this is not possible, in which case the operator can be configured to manage domains in its own namespace. For more information, see Choose a security strategy and Choose a domain namespace selection strategy. Creating a dedicated service account for each operator instead of relying on the default service account that Kubernetes creates when a new namespace is created. Directly creating a namespace and service account instead of relying on the operator Helm chart installation to create these resources for you. Here\u0026rsquo;s an example of each:\n$ kubectl create namespace sample-weblogic-operator-ns $ kubectl create serviceaccount -n sample-weblogic-operator-ns sample-weblogic-operator-sa In operator installation steps, you will specify the namespace using the --namespace MY-NAMESPACE operator Helm chart configuration setting on the Helm install command line. If not specified, then it defaults to default. If the namespace does not already exist, then Helm will automatically create it (and Kubernetes will create a default service account in the new namespace). If you later uninstall the operator, then Helm will not remove the specified namespace. These are standard Helm behaviors.\nSimilarly, you will specify the serviceAccount=MY-SERVICE-ACCOUNT operator Helm chart configuration setting on the Helm install command line to specify the service account in the operator\u0026rsquo;s namespace that the operator will use. If not specified, then it defaults to default. This service account will not be automatically removed when you uninstall an operator.\nPrepare operator image The operator image must be available to all nodes of your Kubernetes cluster.\nLocating an operator image Production-ready operator images for various supported versions of the operator are publicly located in the operator GitHub Container Registry. Operator GitHub container registry images can be directly referenced using an image name similar to ghcr.io/oracle/weblogic-kubernetes-operator:N.N.N where N.N.N refers to the operator version and ghcr.io is the DNS name of the GitHub container registry. You can also optionally build your own operator image, see the Developer Guide.\nDefault operator image Each Helm chart version defaults to using an operator image from the matching version. To find the default image name that will be used when installing the operator, see Inspect the operator Helm chart and look for the image value. The value will look something like this, ghcr.io/oracle/weblogic-kubernetes-operator:N.N.N.\nPulling operator image In most use cases, Kubernetes will automatically download (pull) the operator image, as needed, to the machines on its cluster nodes.\nIf you want to manually place an operator image in a particular machine\u0026rsquo;s container image pool, or test access to an image, then call docker pull. For example:\n$ docker pull ghcr.io/oracle/weblogic-kubernetes-operator:4.2.20 Note that if the image registry you are using is a private registry that requires an image pull credential, then you will need to call docker login my-registry-dns-name.com before calling docker pull.\nCustomizing operator image name, pull secret, and private registry Sometimes, you may want to specify a custom operator image name or an image pull secret. For example, you may want to deploy a different version than the default operator image name or you may want to place an operator image in your own private image registry.\nA private image registry requires using a custom image name for the operator where the first part of the name up to the first slash (/) character is the DNS location of the registry and the remaining part refers to the image location within the registry. A private image registry may also require an image pull registry secret to provide security credentials.\nTo reference a custom image name, specify the image= operator Helm chart configuration setting when installing the operator, for example --set \u0026quot;image=my-image-registry.io/my-operator-image:1.0\u0026quot;. To create an image pull registry secret, create a Kubernetes Secret of type docker-registry in the namespace where the operator is to be deployed, as described in Specifying imagePullSecrets on a Pod. To reference an image pull registry secret from an operator installation, there are two options: Use the imagePullSecrets operator Helm chart configuration setting when installing the operator. For examples, see imagePullSecrets. Or, add the pull secret name to the service account you will use for the operator. See Prepare an operator namespace and service account and Add image pull secret to service account. Determine the platform setting It is important to set the correct value for the kubernetesPlatform Helm chart configuration setting when installing the operator.\nIn particular, beginning with operator version 3.3.2, specify the operator kubernetesPlatform Helm chart setting with the value OpenShift when using the OpenShift Kubernetes platform, for example --set \u0026quot;kubernetesPlatform=OpenShift\u0026quot;. This accommodates OpenShift security requirements.\nFor more information, see kubernetesPlatform.\nChoose a security strategy There are three commonly used security strategies for deploying an operator:\nAny namespace with cluster role binding enabled Any namespace with cluster role binding disabled Local namespace only with cluster role binding disabled For a detailed description of the operator\u0026rsquo;s security-related resources, see the operator\u0026rsquo;s role-based access control (RBAC) requirements, which are documented here.\nAny namespace with cluster role binding enabled If you want to give the operator permission to access any namespace, then, for most use cases, set the enableClusterRoleBinding operator Helm chart configuration setting to true when installing the operator.\nFor example --set \u0026quot;enableClusterRoleBinding=true\u0026quot;. The default for this setting is true.\nThis is the most popular security strategy.\nAny namespace with cluster role binding disabled If your operator Helm enableClusterRoleBinding configuration value is false, then an operator is still capable of managing multiple namespaces but a running operator will not have privilege to manage a newly added namespace that matches its namespace selection criteria until you upgrade the operator\u0026rsquo;s Helm release. See Ensuring the operator has permission to manage a namespace.\nNOTE: You will need to manually install the Domain and Cluster CRDs because enableClusterRoleBinding is not set to true and installation of the CRD requires cluster role binding privileges. See How to manually install the Domain and Cluster custom resource definitions (CRD).\nLocal namespace only with cluster role binding disabled If you want to limit the operator so that it can access only resources in its local namespace, then:\nChoose the Dedicated namespace selection strategy. See Choose a domain namespace selection strategy. Install only the operator deployment and omit installing the webhook deployment using operatorOnly=true. This is because the webhook deployment must modify the Domain CRD to register the schema conversion webhook endpoint or register the validating webhook endpoint, both of which involve cluster-level resources. You will need to manually install the Domain and Cluster CRDs because enableClusterRoleBinding is not set to true and installing the CRD requires cluster role binding privileges. See How to manually install the Domain and Cluster custom resource definitions (CRD). This may be necessary in environments where the operator cannot have cluster-scoped privileges, such as may happen on OpenShift platforms or when running the operator with a Dedicated namespace strategy.\nMany customers do not have administrative privileges to their Kubernetes cluster because either a third-party or an infrastructure team is responsible for managing the cluster. In these cases, the customer, such as an applications team, will only have privilege in a single namespace. As described above, the CRD documents must be installed in advance if the operator will not have sufficient privilege at the Kubernetes cluster-level to manage the lifecycle of these CRD documents. Therefore, the third-party or infrastructure team must complete the kubectl create of the CRD documents prior to the application team\u0026rsquo;s installation of the operator.\nAt a minimum, the infrastructure team must install the CRD documents and create the namespace for the operator:\n$ kubectl create -f https://raw.githubusercontent.com/oracle/weblogic-kubernetes-operator/refs/heads/release/4.2/kubernetes/crd/domain-crd.yaml $ kubectl create -f https://raw.githubusercontent.com/oracle/weblogic-kubernetes-operator/refs/heads/release/4.2/kubernetes/crd/cluster-crd.yaml $ kubectl create ns weblogic-operator This would then allow the applications team to install a namespace-dedicated version of the operator without any webhooks or other cluster-level resources:\nhelm repo add weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts helm install weblogic-operator weblogic-operator/weblogic-operator --namespace weblogic-operator --set enableClusterRoleBinding=false --set domainNamespaceSelectionStrategy=Dedicated --set operatorOnly=true Since this combination of options omits installing the webhook deployment, customers must use the v9 schema version for Domain resources and manually upgrade any v8 resources from the 3.x version of the operator.\nChoose a domain namespace selection strategy Before installing your operator, choose the value for its domainNamespaceSelectionStrategy Helm chart configuration setting and its related setting (if any). See Choose a domain namespace section strategy.\nSee Choose a security strategy.\nFor a description of common namespace management issues, see Common mistakes and solutions. For reference, see WebLogic domain management.\nChoose a Helm release name The operator requires Helm for installation, and Helm requires that each installed operator be assigned a release name. Helm release names can be the same if they are deployed to different namespaces, but must be unique within a particular namespace.\nA typical Helm release name is simply weblogic-operator. The operator samples and documentation often use sample-weblogic-operator.\nBe aware of advanced operator configuration options Review the settings in the Configuration Reference for less commonly used advanced or fine tuning Helm chart configuration options that might apply to your particular use case. These include node selectors, node affinity, Elastic Stack integration, the operator REST API, setting operator pod labels, setting operator pod annotations, and Istio.\nSpecial use cases If applicable, please review the following special use cases.\nHow to download the Helm chart if Internet access is not available At a high level, you use helm pull to download a released version of the Helm chart and move it to the machine with no Internet access, so that then you can run helm install to install the operator.\nThe steps are:\nOn a machine with Internet access, to download the chart to the current directory, run: $ helm pull weblogic-operator --repo https://oracle.github.io/weblogic-kubernetes-operator/charts --destination . For a specified version of the Helm chart, with helm pull and helm install, use the --version \u0026lt;value\u0026gt; option to choose the version that you want, with the latest value being the default. To list the versions of the operator that you can install from the Helm chart repository, run:\n$ helm repo add weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts --force-update $ helm search repo weblogic-operator/weblogic-operator --versions Move the resulting weblogic-operator-\u0026lt;version\u0026gt;.tgz file to the machine without Internet access on which you want to install the WebLogic Kubernetes Operator. Run $ tar zxf weblogic-operator-\u0026lt;version\u0026gt;.tgz. This puts the Helm chart files in a local directory ./weblogic-operator. To create the namespace where the operator will be installed, run $ kubectl create namespace weblogic-operator. Creating a dedicated namespace for the operator is the most common approach, but is not always correct or sufficient. For details, see the prerequisite steps starting with Step 3. Inspect the operator Helm chart. Be sure to follow all the previously detailed prerequisite steps, ending at Step 10. Be aware of advanced operator configuration options.\nTo install the operator, run $ helm install weblogic-operator ./weblogic-operator --namespace weblogic-operator. How to manually install the Domain and Cluster custom resource definitions (CRD) The Domain and Cluster resource types are defined by Kubernetes CustomResourceDefinition (CRD) resources. The Domain and Cluster CRDs provide Kubernetes with the schema for WebLogic-related resources and these two CRDs must be installed in each Kubernetes cluster that hosts the operator. If you install multiple operators in the same Kubernetes cluster, then they all share the same CRDs.\nWhen do the Domain and Cluster CRDs need to be manually installed?\nTypically, the operator\u0026rsquo;s webhook deployment automatically installs the CRDs when it first starts. However, if the webhook lacks sufficient permission to install the CRDs, then you must choose to manually install the CRD documents in advance by using the provided YAML files. Manually installing the CRDs in advance allows you to run the operator without giving it privilege (through Kubernetes roles and bindings) to access or update the CRD documents or other cluster-scoped resources. This may be necessary in environments where the operator cannot have cluster-scoped security privileges, such when running the operator with a Dedicated namespace strategy. See Choose a security strategy.\nHow to manually install the Domain and Cluster CRDs.\nTo manually install the CRDs, perform the following steps:\n$ kubectl create -f https://raw.githubusercontent.com/oracle/weblogic-kubernetes-operator/refs/heads/release/4.2/kubernetes/crd/domain-crd.yaml $ kubectl create -f https://raw.githubusercontent.com/oracle/weblogic-kubernetes-operator/refs/heads/release/4.2/kubernetes/crd/cluster-crd.yaml How to check if a Domain and Cluster CRDs have been installed.\nYou can verify that the Domain CRD is installed correctly using:\n$ kubectl get crd domains.weblogic.oracle Or, by calling:\n$ kubectl explain domain.spec The kubectl explain call should succeed and list the domain resource\u0026rsquo;s domain.spec attributes that are defined in the Domain CRD.\nSimilarly, you can verify that the Cluster CRD is installed correctly using:\n$ kubectl get crd clusters.weblogic.oracle Or, by calling:\n$ kubectl explain cluster.spec The kubectl explain call should succeed and list the cluster resource\u0026rsquo;s cluster.spec attributes that are defined in the Cluster CRD.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/persistent-storage/oci-fss-pv/",
	"title": "Use OCI File Storage (FSS) for persistent volumes",
	"tags": [],
	"description": "If you are running your Kubernetes cluster on Oracle Container Engine for Kubernetes (OKE), and you use Oracle Cloud Infrastructure File Storage (FSS) for persistent volumes to store the WebLogic domain home, then the file system handling, as demonstrated in the operator persistent volume sample, will require an update to properly initialize the file ownership on the persistent volume when the domain is initially created.",
	"content": "Oracle recommends using Oracle Cloud Infrastructure File Storage (FSS) for persistent volumes to store the WebLogic domain home or log files when running the Kubernetes cluster on Oracle Container Engine for Kubernetes (OKE). When using the FSS with OKE for domain home or log files, the file system handling will require an update to properly initialize the file ownership on the persistent volume when the domain is initially created.\nFile permission handling on persistent volumes can differ between cloud providers and even with the underlying storage handling on Linux-based systems. The operator requires permission to create directories on the persistent volume under the shared mount path. The following instructions provide an option to update the file ownership and permissions.\nUpdating the permissions of shared directory on persistent storage The operator provides a utility script, pv-pvc-helper.sh, as part of the lifecycle scripts to change the ownership and permissions of the shared directory on the persistent storage.\nThis script launches a Pod and mounts the specified PVC in the Pod containers at the specified mount path. You can then exec in the Pod and manually change the permissions or ownership.\nSee the pv-pvc-helper.sh in \u0026ldquo;Examine, change permissions or delete PV contents\u0026rdquo; section in the README file for the script details.\nFor example, run the following command to create the Pod.\n$ pv-pvc-helper.sh -n sample-domain1-ns -r -c sample-domain1-weblogic-sample-pvc -m /shared The script will create a Pod with the following specifications.\napiVersion: v1 kind: Pod metadata: name: pvhelper namespace: sample-domain1-ns spec: containers: - args: - sleep - infinity image: ghcr.io/oracle/oraclelinux:8 name: pvhelper volumeMounts: - name: pv-volume mountPath: /shared volumes: - name: pv-volume persistentVolumeClaim: claimName: wko-domain-on-pv-pvc Run the following command to exec into the Pod.\n$ kubectl -n sample-domain1-ns exec -it pvhelper -- /bin/sh After you get a shell to the running Pod container, change the directory to /shared, and you can change the ownership or permissions using the appropriate chown or chmod commands. For example,\n$ chown 1000:0 /shared/. \u0026amp;\u0026amp; find /shared/. -maxdepth 1 ! -name \u0026#39;.snapshot\u0026#39; ! -name \u0026#39;.\u0026#39; -print0 | xargs -r -0 chown -R 1000:0 References Provisioning PVCs on the File Storage Service (FSS) in the OCI documentation. Setting up storage for Kubernetes clusters in the OCI documentation. "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/elastic-stack/weblogic-domain/",
	"title": "WebLogic domain",
	"tags": [],
	"description": "Sample for using Fluentd for WebLogic domain and operator&#39;s logs.",
	"content": "This document describes to how to configure a WebLogic domain to use Fluentd to send log information to Elasticsearch.\nHere\u0026rsquo;s the general mechanism for how this works:\nfluentd runs as a separate container in the Administration Server and Managed Server pods. The log files reside on a volume that is shared between the weblogic-server and fluentd containers. fluentd tails the domain logs files and exports them to Elasticsearch. A ConfigMap contains the filter and format rules for exporting log records. For information on how to use Fluent Bit as a DaemonSet to scrape WLS logs, see the blog, WLS For OKE, with Fluent bit and FSS.\nSample code The samples in this document assume that an existing domain is being edited. However, you can make all the changes to the domain YAML file before the domain is created.\nFor sample purposes, this document assumes that a domain with the following attributes is being configured:\nDomain name is bobs-bookstore Kubernetes Namespace is bob Kubernetes Secret is bobs-bookstore-weblogic-credentials The sample Elasticsearch configuration is:\nelasticsearchhost: elasticsearch.bobs-books.sample.com elasticsearchport: 443 elasticsearchuser: bob elasticsearchpassword: changeme Configure log files to use a volume The domain log files must be written to a volume that can be shared between the weblogic-server and fluentd containers. The following elements are required to accomplish this:\nlogHome must be a path that can be shared between containers. logHomeEnabled must be set to true so that the logs will be written outside the pod and persist across pod restarts. A volume must be defined on which the log files will reside. In the example, emptyDir is a volume that gets created empty when a pod is created. It will persist across pod restarts but deleting the pod would delete the emptyDir content. The volumeMounts mounts the named volume created with emptyDir and establishes the base path for accessing the volume. NOTE: For brevity, only the paths to the relevant configuration being added is shown. A complete example of a domain definition is at the end of this document.\nExample: $ kubectl edit domain bobs-bookstore -n bob and make the following edits:\nspec: logHome: /scratch/logs/bobs-bookstore logHomeEnabled: true serverPod: volumes: - emptyDir: {} name: weblogic-domain-storage-volume volumeMounts: - mountPath: /scratch name: weblogic-domain-storage-volume Create Elasticsearch secrets The fluentd container will be configured to look for Elasticsearch parameters in a Kubernetes secret. Create a secret with following keys:\nExample:\n$ kubectl -n bob create secret generic fluentd-credential --from-literal elasticsearchhost=quickstart-es-http.default --from-literal elasticsearchport=9200 --from-literal elasticsearchuser=elastic --from-literal elasticsearchpassword=xyz Specify fluentdSpecification in the domain resource spec: fluentdSpecification: elasticSearchCredentials: fluentd-credential watchIntrospectorLogs: false volumeMounts: - mountPath: /shared name: weblogic-domain-storage-volume The operator will:\nCreate a ConfigMap webogic-fluentd-configmap with a default fluentd configuration. See Fluentd configuration. Set up the fluentd container in each pod to use the Elasticsearch secrets. You can customize the fluentd configuration to fit your use case. See the following fluentdSpecification options:\nOption Description Notes elasticSearchCredentials Kubernetes secret name for the fluentd container to communicate with the Elasticsearch engine. watchIntrospectorLogs If set to true, the operator also will set up a fluentd container to watch the introspector job output. Default is false. The operator automatically added a volume mount referencing the ConfigMap volume containing the fluentd configuration. volumeMounts Additional list of volumeMounts for the fluentd container. It should contain at least the logHome shared volume. image fluentd container image name. Default: fluent/fluentd-kubernetes-daemonset:v1.14.5-debian-elasticsearch7-1.1 imagePullPolicy The ImagePull policy for the fluentd container. env Additional list of environment variables for the fluentd container. See Environment variables in the fluentd container. resources Resources for the fluentd container. fluentdConfiguration Text for the fluentd configuration instead of the operator\u0026rsquo;s defaults. See Fluentd configuration. Note: When you specify the configuration, you are responsible for the entire fluentd configuration, the Operator will not add or modify any part of it. For example:\nfluentdSpecification: elasticSearchCredentials: fluentd-credential watchIntrospectorLogs: true volumeMounts: - mountPath: /shared name: weblogic-domain-storage-volume fluentdConfiguration: |- \u0026lt;match fluent.**\u0026gt; @type null \u0026lt;/match\u0026gt; \u0026lt;source\u0026gt; @type tail path \u0026#34;#{ENV[\u0026#39;LOG_PATH\u0026#39;]}\u0026#34; pos_file /tmp/server.log.pos read_from_head true tag \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}\u0026#34; # multiline_flush_interval 20s \u0026lt;parse\u0026gt; @type multiline format_firstline /^####/ format1 /^####\u0026lt;(?\u0026lt;timestamp\u0026gt;(.*?))\u0026gt;/ format2 / \u0026lt;(?\u0026lt;level\u0026gt;(.*?))\u0026gt;/ format3 / \u0026lt;(?\u0026lt;subSystem\u0026gt;(.*?))\u0026gt;/ format4 / \u0026lt;(?\u0026lt;serverName\u0026gt;(.*?))\u0026gt;/ format5 / \u0026lt;(?\u0026lt;serverName2\u0026gt;(.*?))\u0026gt;/ format6 / \u0026lt;(?\u0026lt;threadName\u0026gt;(.*?))\u0026gt;/ format7 / \u0026lt;(?\u0026lt;info1\u0026gt;(.*?))\u0026gt;/ format8 / \u0026lt;(?\u0026lt;info2\u0026gt;(.*?))\u0026gt;/ format9 / \u0026lt;(?\u0026lt;info3\u0026gt;(.*?))\u0026gt;/ format10 / \u0026lt;(?\u0026lt;sequenceNumber\u0026gt;(.*?))\u0026gt;/ format11 / \u0026lt;(?\u0026lt;severity\u0026gt;(.*?))\u0026gt;/ format12 / \u0026lt;(?\u0026lt;messageID\u0026gt;(.*?))\u0026gt;/ format13 / \u0026lt;(?\u0026lt;message\u0026gt;(.*?))\u0026gt;.*/ # use the timestamp field in the message as the timestamp # instead of the time the message was actually read time_key timestamp keep_time_key true \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;source\u0026gt; @type tail path \u0026#34;#{ENV[\u0026#39;INTROSPECTOR_OUT_PATH\u0026#39;]}\u0026#34; pos_file /tmp/introspector.log.pos read_from_head true tag \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}-introspector\u0026#34; # multiline_flush_interval 20s \u0026lt;parse\u0026gt; @type multiline format_firstline /@\\[/ format1 /^@\\[(?\u0026lt;timestamp\u0026gt;.*)\\]\\[(?\u0026lt;filesource\u0026gt;.*?)\\]\\[(?\u0026lt;level\u0026gt;.*?)\\](?\u0026lt;message\u0026gt;.*)/ # use the timestamp field in the message as the timestamp # instead of the time the message was actually read time_key timestamp keep_time_key true \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;match **\u0026gt; @type elasticsearch host \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_HOST\u0026#39;]}\u0026#34; port \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PORT\u0026#39;]}\u0026#34; user \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_USER\u0026#39;]}\u0026#34; password \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PASSWORD\u0026#39;]}\u0026#34; index_name \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}\u0026#34; scheme https ssl_version TLSv1_2 ssl_verify false key_name timestamp types timestamp:time suppress_type_name true # inject the @timestamp special field (as type time) into the record # so you will be able to do time based queries. # not to be confused with timestamp which is of type string!!! include_timestamp true \u0026lt;/match\u0026gt; Note: When you set up watchIntrospectorLogs to watch the introspector job pod log. You may see briefly the status of the job pod transition from Running to NotReady and then Terminating, this is normal behavior.\nNAME READY STATUS RESTARTS AGE sample-domain1-introspector-kndk7 2/2 Running 0 57 sample-domain1-introspector-kndk7 1/2 NotReady 0 60s sample-domain1-introspector-kndk7 0/2 Terminating 0 63s Environment variables in the fluentd container The operator sets up the fluentd container with the following environment variables, they are referenced by the fluentd configuration:\nenv: - name: ELASTICSEARCH_HOST valueFrom: secretKeyRef: key: elasticsearchhost name: fluentd-credential optional: false - name: ELASTICSEARCH_PORT valueFrom: secretKeyRef: key: elasticsearchport name: fluentd-credential optional: false - name: ELASTICSEARCH_USER valueFrom: secretKeyRef: key: elasticsearchuser name: fluentd-credential optional: true - name: ELASTICSEARCH_PASSWORD valueFrom: secretKeyRef: key: elasticsearchpassword name: fluentd-credential optional: true - name: FLUENT_ELASTICSEARCH_SED_DISABLE value: \u0026#34;true\u0026#34; - name: FLUENTD_CONF value: fluentd.conf - name: DOMAIN_UID valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.labels[\u0026#39;weblogic.domainUID\u0026#39;] - name: SERVER_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.labels[\u0026#39;weblogic.serverName\u0026#39;] - name: LOG_PATH value: /shared/logs/sample-domain1/admin-server.log - name: INTROSPECTOR_OUT_PATH value: /shared/logs/sample-domain1/introspector_script.out Fluentd configuration The operator creates a ConfigMap named webogic-fluentd-configmap in the namespace of the domain. The ConfigMap contains the parsing rules and Elasticsearch configuration.\nHere\u0026rsquo;s an explanation of some elements defined in the ConfigMap:\nThe @type tail indicates that tail will be used to obtain updates to the log file. The path of the log file is obtained from the LOG_PATH environment variable that is defined in the fluentd container. The tag value of log records is obtained from the DOMAIN_UID environment variable that is defined in the fluentd container. The \u0026lt;parse\u0026gt; section defines how to interpret and tag each element of a log record. The \u0026lt;match **\u0026gt; section contains the configuration information for connecting to Elasticsearch and defines the index name of each record to be the domainUID. The following is the default fluentd configuration if fluentdConfiguration is not specified:\n\u0026lt;match fluent.**\u0026gt; @type null \u0026lt;/match\u0026gt; \u0026lt;source\u0026gt; @type tail path \u0026#34;#{ENV[\u0026#39;LOG_PATH\u0026#39;]}\u0026#34; pos_file /tmp/server.log.pos read_from_head true tag \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}\u0026#34; # multiline_flush_interval 20s \u0026lt;parse\u0026gt; @type multiline format_firstline /^####/ format1 /^####\u0026lt;(?\u0026lt;timestamp\u0026gt;(.*?))\u0026gt;/ format2 / \u0026lt;(?\u0026lt;level\u0026gt;(.*?))\u0026gt;/ format3 / \u0026lt;(?\u0026lt;subSystem\u0026gt;(.*?))\u0026gt;/ format4 / \u0026lt;(?\u0026lt;serverName\u0026gt;(.*?))\u0026gt;/ format5 / \u0026lt;(?\u0026lt;serverName2\u0026gt;(.*?))\u0026gt;/ format6 / \u0026lt;(?\u0026lt;threadName\u0026gt;(.*?))\u0026gt;/ format7 / \u0026lt;(?\u0026lt;info1\u0026gt;(.*?))\u0026gt;/ format8 / \u0026lt;(?\u0026lt;info2\u0026gt;(.*?))\u0026gt;/ format9 / \u0026lt;(?\u0026lt;info3\u0026gt;(.*?))\u0026gt;/ format10 / \u0026lt;(?\u0026lt;sequenceNumber\u0026gt;(.*?))\u0026gt;/ format11 / \u0026lt;(?\u0026lt;severity\u0026gt;(.*?))\u0026gt;/ format12 / \u0026lt;(?\u0026lt;messageID\u0026gt;(.*?))\u0026gt;/ format13 / \u0026lt;(?\u0026lt;message\u0026gt;(.*?))\u0026gt;/ # use the timestamp field in the message as the timestamp # instead of the time the message was actually read time_key timestamp keep_time_key true \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;source\u0026gt; @type tail path \u0026#34;#{ENV[\u0026#39;INTROSPECTOR_OUT_PATH\u0026#39;]}\u0026#34; pos_file /tmp/introspector.log.pos read_from_head true tag \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}-introspector\u0026#34; # multiline_flush_interval 20s \u0026lt;parse\u0026gt; @type multiline format_firstline /@\\[/ format1 /^@\\[(?\u0026lt;timestamp\u0026gt;.*)\\]\\[(?\u0026lt;filesource\u0026gt;.*?)\\]\\[(?\u0026lt;level\u0026gt;.*?)\\](?\u0026lt;message\u0026gt;.*)/ # use the timestamp field in the message as the timestamp # instead of the time the message was actually read time_key timestamp keep_time_key true \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;match \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}-introspector\u0026#34;\u0026gt; @type elasticsearch host \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_HOST\u0026#39;]}\u0026#34; port \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PORT\u0026#39;]}\u0026#34; user \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_USER\u0026#39;]}\u0026#34; password \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PASSWORD\u0026#39;]}\u0026#34; index_name \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}\u0026#34; suppress_type_name true type_name introspectord logstash_format true logstash_prefix introspectord # inject the @timestamp special field (as type time) into the record # so you will be able to do time based queries. # not to be confused with timestamp which is of type string!!! include_timestamp true \u0026lt;/match\u0026gt; \u0026lt;match \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}\u0026#34;\u0026gt; @type elasticsearch host \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_HOST\u0026#39;]}\u0026#34; port \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PORT\u0026#39;]}\u0026#34; user \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_USER\u0026#39;]}\u0026#34; password \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PASSWORD\u0026#39;]}\u0026#34; index_name \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}\u0026#34; suppress_type_name true type_name fluentd logstash_format true logstash_prefix fluentd # inject the @timestamp special field (as type time) into the record # so you will be able to do time based queries. # not to be confused with timestamp which is of type string!!! include_timestamp true \u0026lt;/match\u0026gt; Verify logs are exported to Elasticsearch After the Administration Server and Managed Server pods have started with all the changes described previously, the logs will be sent to Elasticsearch.\nYou can check if the fluentd container is successfully tailing the log by executing a command like $ kubectl logs -f bobs-bookstore-admin-server -n bob fluentd. The log output will look similar to this:\n2019-10-01 16:23:44 +0000 [info]: #0 starting fluentd worker pid=13 ppid=9 worker=0 2019-10-01 16:23:44 +0000 [warn]: #0 /scratch/logs/bobs-bookstore/managed-server1.log not found. Continuing without tailing it. 2019-10-01 16:23:44 +0000 [info]: #0 fluentd worker is now running worker=0 2019-10-01 16:24:01 +0000 [info]: #0 following tail of /scratch/logs/bobs-bookstore/managed-server1.log When you connect to Kibana, you will see an index created for the domainUID.\nExample Kibana log output:\ntimestamp:Oct 1, 2019 4:18:07,111 PM GMT level:Info subSystem:Management serverName:bobs-bookstore-admin-server serverName2: threadName:Thread-8 info1: info2: info3: sequenceNumber:1569946687111 severity:[severity-value: 64] [partition-id: 0] [partition-name: DOMAIN] messageID:BEA-141107 message:Version: WebLogic Server 12.2.1.3.0 Thu Aug 17 13:39:49 PDT 2017 1882952 _id:OQIeiG0BGd1zHsxmUrEJ _type:fluentd _index:bobs-bookstore _score:1 "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/cicd/why-layering-matters/",
	"title": "Why layering matters",
	"tags": [],
	"description": "Learn why container image layering affects CI/CD processes.",
	"content": "How does layering affect our CI/CD process? Now that we know more about layering, let’s talk about why it is important to our CI/CD process. Let\u0026rsquo;s consider the kinds of updates we might want to make to our domain:\nYou might want to update the domain by:\nInstalling a patch on the operating system or a library. Updating the version of the JDK you are using. Picking up a new version of WebLogic Server. Installing patches on WebLogic Server. Updating the domain configuration, for example: Adding or changing a resource like a data source or queue. Installing or updating applications. Changing various settings in the domain configuration. If we just want to update the domain configuration itself, that is the top layer, then it is pretty easy. We can make the necessary changes and save a new version of that layer, and then roll the domain. We could also choose to just build another layer on top of the existing top layer that contains our delta. If the change is small, then we will just end up with another small layer, and as we have seen, the small layers are no problem.\nBut consider a more complicated scenario - let\u0026rsquo;s take updating the JDK as an example to understand the impact of layers. Say we want to update from JDK 8u201 to 8u202 as shown in the previous example. If we took the \u0026ldquo;your first domain\u0026rdquo; image and updated the JDK, then we would end up with a new layer on top containing JDK 8u202. That other layer with JDK 8u201 is still there; even if we \u0026ldquo;delete\u0026rdquo; the directory, we don\u0026rsquo;t get that space back. So now our 1.5GB \u0026ldquo;image\u0026rdquo; has grown to 1.75GB. This is not ideal, and the more often we try to change lower layers, the worse it gets.\nYou might be asking, \u0026ldquo;Can\u0026rsquo;t we just swap out the JDK layer for a new one?\u0026rdquo; That is an excellent question, but the unfortunate reality today is that there is no reliable way to do that. There are various attempts to create a \u0026ldquo;rebasing\u0026rdquo; capability for Docker that would enable such an action, but some research will show you that they are mostly abandoned due to limited documentation of how the layering works at the level of detail needed to implement something like this.\nNext you might think, \u0026ldquo;Oh, that’s ok, we can just rebuild the layers above the JDK on top of this new layer.\u0026rdquo; That is very true, we can. But there is a big caveat here for Domain in Image domains. When you create a WebLogic domain, a domain encryption key is created. This key is stored in the security/SerializedSystemIni.dat file in your domain and it is used to encrypt several other things in your domain configuration, like passwords, for example. Today (in WebLogic Server 12.2.1.4.0) there is no way to conveniently \u0026ldquo;extract\u0026rdquo; or \u0026ldquo;reuse\u0026rdquo; this encryption key. So what does this mean in practice?\nIf you recreate a Domain in Image domain in your CI/CD process, even though you may end up with a domain that is for all intents and purposes identical to the previous domain, it will have a different encryption key.\nThis means that technically, it is a \u0026ldquo;different\u0026rdquo; domain for Domain in Image type domains. Does this matter? Maybe, maybe not. It depends. If you want to do a rolling restart of your domain, then yes, it matters. First of all, the \u0026ldquo;new\u0026rdquo; servers will fail to start because the operator will be trying to inject credentials to start the server which were encrypted with the \u0026ldquo;old\u0026rdquo; domain encryption key.\nBut even if this did not prevent Domain in Image pods from starting, there would still be a problem. You cannot have members of a domain with different encryption keys. If WebLogic saw a new member trying to join the domain with a different key, it would consider it to be an intruder and refuse to accept it into the domain. Client HTTP sessions would not work across the two different sets of servers, so clients could see errors and need to retry. Worse, if these two different sets of servers tried to access the same resources this could lead to data corruption.\nSo what can we do? Well, we could not roll the domain, but instead completely shut down the old version first, and then start up the new one. This way we avoid any issues with incompatibilities, but we do introduce a brief outage. This may be acceptable, or it may not.\nAnother option is to find a way to keep the \u0026ldquo;same\u0026rdquo; domain, that is, the same domain encryption key, so that we can still roll the domain and there will be no conflicts.\nMutating Domain in Image domain home configuration without losing encryption keys If we want to make a change in a lower layer in Domain in Image domains without losing our domain encryption keys, then we need to find a way to \u0026ldquo;save\u0026rdquo; the domain and then put it back into a new layer, later, on top of the other new (lower) layers, as depicted in the following image:\nThe process looks like this:\nFrom our existing image (left), we extract the domain into some kind of archive. Then we start with the new JDK image which was built on top of the same base image (or we build it ourselves, if needed). We build a new WebLogic layer (or grab the one that Oracle built for us) on top of this new JDK. Then we need to “restore” our domain from the archive into a new layer. "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/faq/cannot-pull-image/",
	"title": "Cannot pull image",
	"tags": [],
	"description": "My domain will not start and I see errors like `ImagePullBackoff` or `Cannot pull image`.",
	"content": " My domain will not start and I see errors like ImagePullBackoff or Cannot pull image\nWhen you see these kinds of errors, it means that Kubernetes cannot find your container image. The most common causes are:\nThe image value in your Domain is set incorrectly, meaning Kubernetes will be trying to pull the wrong image. The image requires authentication or permission to pull it and you have not configured Kubernetes with the necessary credentials, for example in an imagePullSecret. You built the image on a machine that is not where your kubelet is running and Kubernetes cannot see the image, meaning you need to copy the image to the worker nodes or put it in a container registry that is accessible the to all of the worker nodes. Let\u0026rsquo;s review what happens when Kubernetes starts a pod.\nThe definition of the pod contains a list of container specifications. Each container specification contains the name (and optionally, tag) of the image that should be used to run that container. In the previous example, there is a container called c1 which is configured to use the container image some.registry.com/owner/domain1:1.0. This image name is in the format registry address / owner / name : tag, so in this case the registry is some.registry.com, the owner is owner, the image name is domain and the tag is 1.0. Tags are a lot like version numbers, but they are not required to be numbers or to be in any particular sequence or format. If you omit the tag, it is assumed to be latest.\nThe tag latest is confusing - it does not actually mean the latest version of the image that was created or published in the registry; it just literally means whichever version the owner decided to call \u0026ldquo;latest\u0026rdquo;. Docker and Kubernetes make some assumptions about latest, and it is generally recommended to avoid using it and instead specify the actual version or tag that you really want.\nFirst, Kubernetes will check to see if the requested image is available in the local container image store on whichever worker node the pod was scheduled on. If it is there, then it will use that image to start the container. If it is not there, then Kubernetes will attempt to pull the image from a remote container registry.\nThere is another setting called imagePullPolicy that can be used to force Kubernetes to always pull the image, even if it is already present in the local container image store.\nIf the image is available in the remote registry and it is public, that is it does not require authentication, then Kubernetes will pull the image to the local container image store and start the container.\nImages that require authentication If the remote container registry requires authentication, then you will need to provide the authentication details in a Kubernetes docker-registry secret and tell Kubernetes to use that secret when pulling the image.\nTo create a secret, you can use the following command:\n$ kubectl create secret docker-registry \u0026lt;name of the secret\u0026gt; \\ --docker-server=\u0026lt;the registry host name\u0026gt; \\ --docker-username=\u0026lt;the user name\u0026gt; \\ --docker-password=\u0026lt;the actual password\u0026gt; \\ --docker-email=\u0026lt;the user email\u0026gt; \\ --namespace=\u0026lt;the selected namespace\u0026gt; where actual values should replace the strings in angle brackets. Note that the docker-server is set to the registry name, without the https:// prefix; the docker-username, docker-password and docker-email are set to match the credentials you use to authenticate to the remote container registry; and the namespace must be set to the same namespace where you intend to use the image.\nSome registries may need a suffix making the docker-server something like some.registry.com/v2 for example. You will need to check with your registry provider\u0026rsquo;s documentation to determine if this is needed.\nAfter the secret is created, you need to tell Kubernetes to use it. This is done by adding an imagePullSecret to your Kubernetes YAML file. In the case of a WebLogic domain, you add the secret name to the imagePullSecret in the domain custom resource YAML file.\nHere is an example of part of a domain custom resource file with the imagePullSecret specified:\napiVersion: \u0026#34;weblogic.oracle/v9\u0026#34; kind: Domain metadata: name: domain1 namespace: default labels: weblogic.domainUID: domain1 spec: domainHome: /u01/oracle/user_projects/domains/domain1 domainHomeSourceType: Image image: \u0026#34;some.registry.com/owner/domain1:1.0\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; imagePullSecrets: - name: secret1 Alternatively, you can associate the secret with the service account that will be used to run the pod. If you do this, then you will not need to add the imagePullSecret to the domain resource. This is useful if you are running multiple domains in the same namespace.\nTo add the secret shown previously to the default service account in the weblogic namespace, you would use a command like this:\n$ kubectl patch serviceaccount default \\ -n weblogic \\ -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;secret1\u0026#34;}]}\u0026#39; You can provide multiple imagePullSecrets if you need to pull container images from multiple remote container registries or if your images require different authentication credentials. For more information, see Container Image Protection.\nPushing the image to a repository If you have an image in your local repository that you would like to copy to a remote repository, then the Docker steps are:\nUse docker login to log in to the target repository\u0026rsquo;s registry. For example: $ docker login some.registry.com -u username -p password Use docker tag to mark the image with the target registry, owner, repository name, and tag. For example: $ docker tag domain1:1.0 some.registry.com/owner/domain1:1.0 Use docker push to push the image to the repository. For example: $ docker push some.registry.com/owner/domain1:1.0 Manually copying the image to your worker nodes If you are not able to use a remote container registry, for example if your Kubernetes cluster is in a secure network with no external access, then you can manually copy the container images to the cluster instead.\nOn the machine where you created the image, export it into a TAR file using this command:\n$ docker save domain1:1.0 \u0026gt; domain1.tar Then copy that TAR file to each worker node in your Kubernetes cluster and run this command on each node:\n$ docker load \u0026lt; domain1.tar Restart pods to clear the error After you have ensured that the images are accessible on all worker nodes, you may need to restart the pods so that Kubernetes will attempt to pull the images again. You can do this by deleting the pods themselves, or deleting the Domain and then recreating it.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/security/domain-security/",
	"title": "Domain security",
	"tags": [],
	"description": "WebLogic domain security and the operator.",
	"content": " Pod and container security Pod and container security.\nContainer image protection WebLogic domain in image protection.\nExternal network access security Remote access security.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/security/domain-security/image-protection/",
	"title": "Container image protection",
	"tags": [],
	"description": "WebLogic domain in image protection.",
	"content": " Oracle strongly recommends storing the container images that contain a WebLogic domain home as private in the container registry. In addition to any local registry, public container registries include GitHub Container Registry and the Oracle Cloud Infrastructure Registry (OCIR).\nThe WebLogic domain home that is part of a Domain in Image image contains sensitive information about the domain including keys and credentials that are used to access external resources (for example, the data source password). In addition, the image may be used to create a running server that further exposes the WebLogic domain outside of the Kubernetes cluster.\nNOTE: The Domain in Image domain home source type is deprecated in WebLogic Kubernetes Operator version 4.0. Oracle recommends that you choose either Domain on PV or Model in Image, depending on your needs.\nFor information about setting up Kubernetes to access a private registry, see Set up Kubernetes to access domain images.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/domain-lifecycle/restarting/",
	"title": "Restarting",
	"tags": [],
	"description": "This document describes when WebLogic Server instances should and will be restarted in the Kubernetes environment.",
	"content": "This document describes when WebLogic Server instances should and will be restarted in the Kubernetes environment.\nContents Overview Common restart scenarios Use cases Modifying the WebLogic domain configuration Domain in Image Model in Image Domain on PV Changing the domain configuration overrides Changing the WebLogic Server credentials Changing fields on the Domain that affect WebLogic Server instance Pods Applying WebLogic Server patches Updating deployed applications Rolling out an updated domain home in image or model in image Avoiding a rolling restart when changing the image field on a Domain Other considerations for restarting a domain Overview There are many situations where changes to the WebLogic or Kubernetes environment configuration require that all the servers in a domain or cluster be restarted, for example, when applying a WebLogic Server patch or when upgrading an application.\nOne of the operator\u0026rsquo;s most important jobs is to start and stop WebLogic Server instances by creating and deleting their corresponding Kubernetes pods. Sometimes, you need to make changes that make the pods obsolete, therefore the pods need to be deleted and recreated. Depending on the change, often the pods can be gradually recreated, without taking the domain or cluster out of service (for example, \u0026ldquo;rolling restarts\u0026rdquo;) and sometimes all the pods need to be deleted and then recreated as part of a downtime (for example, \u0026ldquo;full restarts\u0026rdquo;).\nThe following types of server restarts are supported by the operator:\nRolling restarts - a coordinated and controlled shut down of all of the servers in a domain or cluster while ensuring that service to the end user is not interrupted.\nOperator initiated - where the WebLogic Kubernetes Operator can detect some types of changes and will automatically initiate rolling restarts of pods in a domain or cluster.\nManually initiated - required when certain changes in the Oracle WebLogic Server in Kubernetes environment cannot be detected by the operator, so a rolling restart must be manually initiated.\nFull domain restarts - the Administration Server and all the Managed Servers in a domain are shutdown, impacting service availability to the end user, and then restarted. Unlike a rolling restart, the operator cannot detect and initiate a full domain restart; it must always be manually initiated.\nFor detailed information on how to restart servers using the operator, see Starting, stopping, and restarting servers.\nCommon restart scenarios This document describes what actions you need to take to properly restart your servers for a number of common scenarios:\nModifying the WebLogic domain configuration Changing the domain configuration overrides (also called situational configuration) for Domain on PV and Domain in Image domains Changing the model files for Model in Image domains Changing the WebLogic Server credentials (the user name and password) Changing fields on the Domain that affect WebLogic Server instance Pod generation (such as image, volumes, and env) Applying WebLogic Server patches Updating deployed applications for Domain in Image or Model in Image Use cases Modifying the WebLogic domain configuration Changes to the WebLogic domain configuration may require either a rolling or full domain restart depending on the domain home location and the type of configuration change.\nDomain in Image The Domain in Image domain home source type is deprecated in WebLogic Kubernetes Operator version 4.0. Oracle recommends that you choose either Domain on PV or Model in Image, depending on your needs.\nFor Domain in Image, you may only perform a rolling restart if both the WebLogic configuration changes between the present image and a new image are dynamic and you have followed the CI/CD guidelines to create an image with compatible encryption keys.\nOtherwise, use of a new image that does not have compatible encryption keys or any non-dynamic configuration changes require a full domain restart.\nIf you create a new image with a new name, then you must avoid a rolling restart, which can cause unexpected behavior for the running domain due to configuration inconsistencies as seen by the various servers, by following the steps in Avoiding a rolling restart when changing image field on a Domain. If you create a new image with the same name, then you must manually initiate a full domain restart. See Full domain restarts. Model in Image Any image that supplies configuration changes that are incompatible with the current running domain require a full shut down before changing the Domain image field, instead of a rolling restart. For changes that support a rolling restart, see Supported updates and Unsupported updates .\nIf you create a new image with a new name, and you want to avoid a rolling restart, see Avoiding a rolling restart when changing the image field on a Domain.\nIf you create a new image with the same name, then you must manually initiate either a full domain restart or rolling restart for pods to run with the new image. To initiate a full restart, see Full domain restarts. To initiate a rolling restart, change the value of your Domain restartVersion field. See Restarting servers and Rolling restarts.\nIf you are supplying updated models or Secrets for a running domain, then see Runtime updates.\nDomain on PV For Domain on PV, the type of restart needed depends on the nature of the WebLogic domain configuration change:\nDomain configuration changes that add new clusters (either configured or dynamic), member servers for these new clusters, or non-clustered servers can now be performed dynamically. This support requires that the new clusters or servers are added to the domain configuration and then that you initiate the operator\u0026rsquo;s introspection of that new configuration. Other changes to parts of the domain configuration that the operator introspects, require a full shutdown and restart, even if the changes are dynamic for WebLogic Server, such as: Adding or removing a network access point Adding a server to an existing cluster Changing a cluster, server, dynamic server, or network access point name Enabling or disabling the listen port, SSL port, or admin port Changing any port numbers Changing a network access point\u0026rsquo;s public address Other dynamic WebLogic configuration changes do not require a restart. For example, a change to a server\u0026rsquo;s connection timeout property is dynamic and does not require a restart. Other non-dynamic domain configuration changes require either a manually initiated rolling restart or a full domain shut down and restart, depending on the nature of the change. For example, a rolling restart is applicable when changing a WebLogic Server stuck thread timer interval property. See Restart all the servers in the domain. The preceding description of the operator\u0026rsquo;s life cycle of responding to WebLogic domain configuration changes applies to version 3.0.0 and later. Prior to operator version 3.0.0, while you could make changes to WebLogic domain configuration using the Administration Console or WLST, the operator would only detect and respond to those changes following a full domain shut down and restart.\nChanging the domain configuration overrides Beginning with operator version 3.0.0, many changes to domain configuration overrides can be applied dynamically or as part of a rolling restart. Previously, any changes to the configuration overrides required a full domain shutdown and restart. Changes to configuration overrides include:\nChanging the Domain YAML file\u0026rsquo;s configuration.overridesConfigMap to point to a different ConfigMap Changing the Domain YAML file\u0026rsquo;s configuration.secrets to point to a different list of Secrets Changing the contents of the ConfigMap referenced by configuration.overridesConfigMap Changing the contents to any of the Secrets referenced by configuration.secrets The changes to the previously listed fields or contents of related resources are not processed automatically. Instead, these fields are processed only when you initiate operator introspection. The operator then will apply the new configuration overrides dynamically or only apply the overrides when WebLogic Server instances restart, depending on the strategy that you select.\nChanges to configuration overrides distributed to running WebLogic Server instances can only take effect if the corresponding WebLogic configuration MBean attribute is \u0026ldquo;dynamic\u0026rdquo;. For instance, the Data Source \u0026ldquo;passwordEncrypted\u0026rdquo; attribute is dynamic while the \u0026ldquo;Url\u0026rdquo; attribute is non-dynamic.\nChanging the WebLogic Server credentials A change to the WebLogic Server credentials (the user name and password), contained in the Kubernetes Secret for the domain, requires a full domain restart. The Kubernetes Secret can be updated directly or a new Secret can be created and then referenced by the webLogicCredentialsSecret field in the Domain YAML file.\nChanging fields on the Domain that affect WebLogic Server instance Pods The operator will initiate a rolling restart of the domain when you modify any of the Domain YAML file fields that affect the WebLogic Server instance Pod generation, such as image, volumes, and env. For a complete list, see Fields that cause servers to be restarted.\nYou can modify these fields using the kubectl command-line tool\u0026rsquo;s edit and patch commands or through the Kubernetes REST API.\nFor example, to edit the Domain YAML file directly using the kubectl command-line tool:\n$ kubectl edit domain \u0026lt;domain name\u0026gt; -n \u0026lt;domain namespace\u0026gt; The edit command opens a text editor which lets you edit the Domain in place.\nTypically, it\u0026rsquo;s better to edit the Domain YAML file directly; otherwise, if you scaled the domain, and you edit only the original domain.yaml file and reapply it, you could go back to your old replicas count.\nApplying WebLogic Server patches Oracle provides different types of patches for WebLogic Server, such as Patch Set Updates, Security Patch Updates, and One-Off patches. Information on whether a patch is rolling-compatible or requires a manual full domain restart usually can be found in the patch\u0026rsquo;s documentation, such as the README file.\nWebLogic Server patches can be applied to either a domain home in image or a domain home on PV.\nWith rolling-compatible patches:\nIf you update the image property with a new image name, then the operator will initiate a rolling restart. If you keep the same image name, then you must manually initiate a rolling restart. See Restart all the servers in the domain. With patches that are not rolling-compatible:\nIf you keep the same image name, then you must manually initiate a full domain restart. See Full domain restarts. If you update the image property with a new image name, then you must avoid the rolling restart by following the steps in Avoiding a rolling restart when changing the image field on a Domain. Updating deployed applications Frequent updates of deployed applications using a continuous integration/continuous delivery (CI/CD) process is a very common use case. The process for applying an updated application is different for domain home in image and model in image than it is for domain home on PV. A rolling-compatible application update is where some servers are running the old version and some are running the new version of the application during the rolling restart process. On the other hand, an application update that is not rolling-compatible requires that all the servers in the domain be shut down and restarted.\nIf the application update is rolling-compatible:\nIf you update the image property with a new image name, then the operator will initiate a rolling restart. If you keep the same image name, then you must manually initiate a rolling restart. See Restart all the servers in the domain. If the application update is not rolling-compatible:\nIf you keep the same image name, then you must manually initiate a full domain restart. See Full domain restarts. If you update the image property with a new image name, then you must avoid the rolling restart by following the steps in Avoiding a rolling restart when changing the image field on a Domain. Rolling out an updated domain home in image or model in image Follow these steps to create new rolling-compatible image if you only need to patch your WebLogic Server domain or update application deployment files:\na. Select a different name for the new image.\nb. For Domain in Image, it is important to keep your original domain home in your new image.\nUsing the same domain home-in-image image as a base, create a new image by copying (COPY command in a Dockerfile) the updated application deployment files or WebLogic Server patches into the image during the image build.\nThe key here is to make sure that you do not re-run WLST or WDT to create a new domain home even though it will have the same configuration. Creating a new domain will change the domain encryption secret and you won\u0026rsquo;t be able to do a rolling restart.\nc. Deploy the new image to your container registry with the new name.\nd. Update the image field of the Domain YAML file, specifying the new image name.\nFor example:\n```yaml domain: spec: image: ghcr.io/oracle/weblogic-updated:4.2.20 ``` e. The operator will now initiate a rolling restart, which will apply the updated image, for all the servers in the domain.\nAvoiding a rolling restart when changing the image field on a Domain If you\u0026rsquo;ve created a new image that is not rolling-compatible, and you\u0026rsquo;ve changed the image name, then:\nBring the domain down (stopping all the server pods) by setting the serverStartPolicy to Never. See Shut down all the servers.\nUpdate the image property with a new image name.\nStart up the domain (starting all the server pods) by setting the serverStartPolicy to IfNeeded.\nOther considerations for restarting a domain Consider the order of changes:\nIf you need to make multiple changes to your domain at the same time, you\u0026rsquo;ll want to be careful about the order in which you do your changes, so that servers aren\u0026rsquo;t restarted prematurely or restarted needlessly. For example, if you want to change the readiness probe\u0026rsquo;s tuning parameters and the Java options (both of which are rolling-compatible), then you should update the Domain YAML file once, changing both values, so that the operator rolling restarts the servers once. Or, if you want to change the readiness probe\u0026rsquo;s tuning parameters (which is rolling-compatible) and change the domain customizations (which require a full restart), then you should do a full shutdown first, then make the changes, and then restart the servers.\nAlternatively, if you know that your set of changes are not rolling-compatible, then you must avoiding a rolling restart by:\nBringing the domain down (stopping all the server pods) by setting the serverStartPolicy to Never. See Shut down all the servers.\nMake all your changes to the Oracle WebLogic Server in Kubernetes environment.\nStarting up the domain (starting all the server pods) by setting the serverStartPolicy to IfNeeded.\nChanges that require domain knowledge.\nSometimes you need to make changes that require server restarts, yet the changes are not to the domain configuration, the image, or the Kubernetes resources that register your domain with the operator. For example, your servers are caching information from an external database and you\u0026rsquo;ve modified the contents of the database.\nIn these cases, you must manually initiate a restart.\nManaged Coherence Servers safe shut down.\nIf the domain is configured to use a Coherence cluster, then you will need to increase the Kubernetes graceful timeout value. When a server is shut down, Coherence needs time to recover partitions and rebalance the cluster before it is safe to shut down a second server. Using the Kubernetes graceful termination feature, the operator will automatically wait until the Coherence HAStatus MBean attribute indicates that it is safe to shut down the server. However, after the graceful termination timeout expires, the pod will be deleted regardless. Therefore, it is important to set the domain YAML timeoutSeconds to a large enough value to prevent the server from shutting down before Coherence is safe. Furthermore, if the operator is not able to access the Coherence MBean, then the server will not be shut down until the domain timeoutSeconds expires. To minimize any possibility of cache data loss, you should increase the timeoutSeconds value to a large number, for example, 15 minutes.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/introduction/design/",
	"title": "Design philosophy",
	"tags": [],
	"description": "Define the expected roles of an administrator, the operator, and domain and cluster resources.",
	"content": "The WebLogic Kubernetes Operator (the “operator”) is designed to fulfill a similar role to that which a human operator would fill in a traditional data center deployment. It contains a set of useful built-in knowledge about how to perform various lifecycle operations on a domain correctly.\nHuman operators are typically responsible for starting and stopping environments, performing scaling operations, performing manual tasks associated with disaster recovery and high availability needs and coordinating actions with other operators in other data centers. It is envisaged that the operator will have similar responsibilities in a Kubernetes environment.\nIt is important to note the distinction between an operator and an administrator. A WebLogic Server administrator typically has different responsibilities centered around managing the detailed configuration of the WebLogic domains. The operator has only limited interest in the domain configuration, with its main concern being the high-level topology of the domain; for example, how many clusters and servers, and information about network access points, such as channels.\nHuman operators may manage more than one domain, and the operator is also designed to be able to manage more than one domain. Like its human counterpart, the operator will only take actions against domains that it is told to manage, and will ignore any other domains that may be present in the same environment.\nLike a human operator, the operator is designed to be event-based. It waits for a significant event to occur, or for a scheduled time to perform some action, and then takes the appropriate action. Examples of significant events include being made aware of a new domain that needs to be managed, receiving a request to scale up a WebLogic cluster, or applying a WebLogic Server patch or an application while preserving cluster availability.\nThere are some operator tasks, such as initiating backups, that are presently not implemented by the WebLogic Kubernetes Operator. We welcome any feedback or requirements as this helps us to properly create our roadmap.\nThe operator is designed with security in mind from the outset. Some examples of the specific security practices we follow are:\nDuring the deployment of the operator, Kubernetes Roles are defined and assigned to the operator. These roles are designed to give the operator the minimum amount of privileges that it requires to perform its tasks. The code base is regularly scanned with security auditing tools and any issues that are identified are promptly resolved. All HTTP communications – between the operator and an external client, between the operator and WebLogic Server Administration Servers, and so on – are configured to require TLS 1.2. Unused code is pruned from the code base regularly. Dependencies are kept as up-to-date as possible and are regularly reviewed for security vulnerabilities. The operator is designed to avoid imposing any arbitrary restriction on how WebLogic Server may be configured or used in Kubernetes. Where there are restrictions, these are based on the availability of some specific feature in Kubernetes; for example, multicast support.\nThe operator learns of WebLogic domains and WebLogic clusters within domains through instances of domain Kubernetes resources and cluster Kubernetes resources. When the operator is installed, it creates a Kubernetes Custom Resource Definition for domain resources and another for cluster resources. These custom resource definitions define the Domain type and the Cluster type. After these types are defined, you can manage Domains and Clusters using kubectl just like any other resource type. For instance, kubectl get domain or kubectl edit domain domain1.\nThe schema for the Domain type is designed to be as sparse as possible. It includes the connection details for the Administration Server, but all of the other content is operational details about which servers should be started, environment variables, and details about what should be exposed outside the Kubernetes cluster. Similarly, the schema for the Cluster type is also sparse where often the only value set is the desired size of the corresponding WebLogic cluster. This way, the WebLogic domain\u0026rsquo;s configuration remains the normative configuration.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/accessing-the-domain/wlst/",
	"title": "Use WLST",
	"tags": [],
	"description": "Use the WebLogic Scripting Tool (WLST) with domains running in Kubernetes.",
	"content": "Contents Introduction Use kubectl exec Use a NodePort Use port forwarding Introduction You can use the WebLogic Scripting Tool (WLST) to manage a domain running in Kubernetes.\nTo give WLST access to a domain running in Kubernetes, you can:\nUse kubectl exec Use a NodePort Use port forwarding NOTE: If your domain home type is either Domain in Image or Model in Image, then do not use the WLST to make changes to the WebLogic domain configuration because these changes are ephemeral and will be lost when servers restart. See Choose a domain home source type.\nUse kubectl exec You can use the kubectl exec command to start an interactive WLST session within a pod or to remotely run a WLST script on a pod. Typically, this is the preferred method.\nNOTE: The WLST script uses the value of the environment variable USER_MEM_ARGS to control the heap settings of the JVM process. If you have set the environment variable USER_MEM_ARGS in the domain resource YAML, the WLST process will inherit the memory settings. For example, if you have USER_MEM_ARGS value set to -Xms2048m -Xmx2048m, the WebLogic server JAVA process will use this heap settings, and if you run the WLST script in the server pod, the WLST script JAVA process will also use this heap settings. This may cause unexpected behavior in the server pod due to additional memory usage.\nIn order to change the memory settings, you must do the following:\nUSER_MEM_ARGS=\u0026#34;\u0026#34; $ORACLE_HOME/oracle_common/common/bin/wlst.sh This will unset the USER_MEM_ARGS and let WLST use the default heap size, -Xms32m -Xmx1024m; this only affects the WLST script process.\nIf you want to use different memory settings, you can adjust it by\nUSER_MEM_ARGS=\u0026#34;-Xms128m -Xmx128m\u0026#34; $ORACLE_HOME/oracle_common/common/bin/wlst.sh For example, if a domainUID is sample-domain1, its Administration Server is named admin-server and is configured with default port 7001, and its pods are running in namespace sample-domain1-ns, then you can start an interactive WLST session this way:\n$ kubectl -n sample-domain1-ns exec -it sample-domain1-admin-server /bin/bash [oracle@sample-domain1-admin-server oracle]$ USER_MEM_ARGS=\u0026#34;\u0026#34; $ORACLE_HOME/oracle_common/common/bin/wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt; connect(\u0026#39;myusername\u0026#39;,\u0026#39;mypassword\u0026#39;,\u0026#39;t3://sample-domain1-admin-server:7001\u0026#39;) Connecting to t3://sample-domain1-admin-server:7001 with userid myusername ... Successfully connected to Admin Server \u0026#34;admin-server\u0026#34; that belongs to domain \u0026#34;base_domain\u0026#34;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/base_domain/serverConfig/\u0026gt; exit() Exiting WebLogic Scripting Tool. [oracle@sample-domain1-admin-server oracle]$ exit $ Use a NodePort If you are setting up WLST access through a NodePort and your external port is not going to be the same as the port number on the WebLogic Administration Server Pod, then see Enabling WLST access when local and remote ports do not match for an additional required setup step.\nA NodePort can expose a WebLogic T3 or administrative channel outside the Kubernetes cluster. For domain security considerations, see External network access security.\nYou can configure an Administration Server to expose an externally accessible NodePort using these two steps:\nConfigure a Network Access Point (custom channel) with the T3 protocol on the Administration Server. Expose this channel on a NodePort service using the domain.spec.adminServer.adminService.channels attribute. Here is an example snippet of a WebLogic domain config.xml file for T3 channel T3Channel defined for an Administration Server named admin-server:\n\u0026lt;server\u0026gt; \u0026lt;name\u0026gt;admin-server\u0026lt;/name\u0026gt; \u0026lt;listen-port\u0026gt;7001\u0026lt;/listen-port\u0026gt; \u0026lt;listen-address/\u0026gt; \u0026lt;network-access-point\u0026gt; \u0026lt;name\u0026gt;T3Channel\u0026lt;/name\u0026gt; \u0026lt;protocol\u0026gt;t3\u0026lt;/protocol\u0026gt; \u0026lt;public-address\u0026gt;kubernetes001\u0026lt;/public-address\u0026gt; \u0026lt;listen-port\u0026gt;30012\u0026lt;/listen-port\u0026gt; \u0026lt;public-port\u0026gt;30012\u0026lt;/public-port\u0026gt; \u0026lt;/network-access-point\u0026gt; \u0026lt;/server\u0026gt; Here is an example snippet of a domain resource that sets up a NodePort for the channel:\nspec: adminServer: adminService: channels: - channelName: T3Channel nodePort: 30012 If you set the nodePort: value to 0, then Kubernetes will choose an open port for you.\nFor more details on exposing the T3 channel using a NodePort service, run the kubectl explain domain.spec.adminServer.adminService.channels command or see the domain resource schema and documentation.\nFor example, if a domainUID is domain1, the Administration Server name is admin-server, and you have set up a NodePort service on external port 30012 using the domain.spec.adminServer.adminService.channels attribute, then the service would be called:\ndomain1-admin-server-ext This service will be in the same namespace as the domain, and its external port number can be obtained by checking its nodePort field:\n$ kubectl get service domain1-admin-server-ext -n mynamespace -o jsonpath=\u0026#39;{.spec.ports[0].nodePort}\u0026#39; 30012 If the Kubernetes node machine address is kubernetes001, then WLST can connect to the WebLogic Server Administration Server pod through the NodePort as follows:\n$ $ORACLE_HOME/oracle_common/common/bin/wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt; connect(\u0026#39;myusername\u0026#39;,\u0026#39;mypassword\u0026#39;,\u0026#39;t3://kubernetes001:30012\u0026#39;) Connecting to t3://kubernetes001:30012 with userid myusername ... Successfully connected to Admin Server \u0026#34;admin-server\u0026#34; that belongs to domain \u0026#34;base_domain\u0026#34;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/base_domain/serverConfig/\u0026gt; exit() Exiting WebLogic Scripting Tool. Use port forwarding One way to provide external access to WLST is to forward network traffic from a local port on your local machine to the administration port of an Administration Server Pod. See these instructions.\nPort forwarding can expose a WebLogic T3 or administrative channel outside the Kubernetes cluster. For domain security considerations, see External network access security.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/model-in-image/auxiliary-image-creation/",
	"title": "Auxiliary image creation",
	"tags": [],
	"description": "Create WebLogic images using the WebLogic Image Tool and WebLogic Deploy Tooling.",
	"content": "Contents Overview Auxiliary image creation Understand your first archive Stage a ZIP file of the archive Stage model files Create the image with WIT Before you begin: Complete the steps in Prerequisites.\nOverview Auxiliary image creation step uses the WebLogic Image Tool to create a Model in Image auxiliary image. This image contains:\nThe directory where the WebLogic Deploy Tooling software is installed (also known as WDT Home), expected in an image\u0026rsquo;s /auxiliary/weblogic-deploy directory, by default. WDT model YAML file (model), WDT variable (property), and WDT archive ZIP (archive) files, expected in directory /auxiliary/models, by default. Auxiliary image creation Use the steps in the following sections for creating the auxiliary image.\nUnderstand your first archive The sample includes a predefined archive directory in /tmp/sample/wdt-artifacts/archives/archive-v1 that you will use to create an archive ZIP file for the image.\nThe archive top directory, named wlsdeploy, contains a directory named applications, which includes an \u0026rsquo;exploded\u0026rsquo; sample JSP web application in the directory, myapp-v1. Three useful aspects to remember about WDT archives are:\nA model image can contain multiple WDT archives. WDT archives can contain multiple applications, libraries, and other components. WDT archives have a well defined directory structure, which always has wlsdeploy as the top directory. If you are interested in the web application source, click here to see the JSP code. \u0026lt;%-- Copyright (c) 2019, 2023, Oracle and/or its affiliates. --%\u0026gt; \u0026lt;%-- Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. --%\u0026gt; \u0026lt;%@ page import=\u0026#34;javax.naming.InitialContext\u0026#34; %\u0026gt; \u0026lt;%@ page import=\u0026#34;javax.management.*\u0026#34; %\u0026gt; \u0026lt;%@ page import=\u0026#34;java.io.*\u0026#34; %\u0026gt; \u0026lt;% InitialContext ic = null; try { ic = new InitialContext(); String srName=System.getProperty(\u0026#34;weblogic.Name\u0026#34;); String domainUID=System.getenv(\u0026#34;DOMAIN_UID\u0026#34;); String domainName=System.getenv(\u0026#34;CUSTOM_DOMAIN_NAME\u0026#34;); out.println(\u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt;\u0026#34;); out.println(\u0026#34;*****************************************************************\u0026#34;); out.println(); out.println(\u0026#34;Hello World! This is version \u0026#39;v1\u0026#39; of the mii-sample JSP web-app.\u0026#34;); out.println(); out.println(\u0026#34;Welcome to WebLogic Server \u0026#39;\u0026#34; + srName + \u0026#34;\u0026#39;!\u0026#34;); out.println(); out.println(\u0026#34; domain UID = \u0026#39;\u0026#34; + domainUID +\u0026#34;\u0026#39;\u0026#34;); out.println(\u0026#34; domain name = \u0026#39;\u0026#34; + domainName +\u0026#34;\u0026#39;\u0026#34;); out.println(); MBeanServer mbs = (MBeanServer)ic.lookup(\u0026#34;java:comp/env/jmx/runtime\u0026#34;); // display the current server\u0026#39;s cluster name Set\u0026lt;ObjectInstance\u0026gt; clusterRuntimes = mbs.queryMBeans(new ObjectName(\u0026#34;*:Type=ClusterRuntime,*\u0026#34;), null); out.println(\u0026#34;Found \u0026#34; + clusterRuntimes.size() + \u0026#34; local cluster runtime\u0026#34; + (String)((clusterRuntimes.size()!=1)?\u0026#34;s\u0026#34;:\u0026#34;\u0026#34;) + \u0026#34;:\u0026#34;); for (ObjectInstance clusterRuntime : clusterRuntimes) { String cName = (String)mbs.getAttribute(clusterRuntime.getObjectName(), \u0026#34;Name\u0026#34;); out.println(\u0026#34; Cluster \u0026#39;\u0026#34; + cName + \u0026#34;\u0026#39;\u0026#34;); } out.println(); // display the Work Manager configuration created by the sample Set\u0026lt;ObjectInstance\u0026gt; minTCRuntimes = mbs.queryMBeans(new ObjectName(\u0026#34;*:Type=MinThreadsConstraintRuntime,Name=SampleMinThreads,*\u0026#34;), null); for (ObjectInstance minTCRuntime : minTCRuntimes) { String cName = (String)mbs.getAttribute(minTCRuntime.getObjectName(), \u0026#34;Name\u0026#34;); int count = (int)mbs.getAttribute(minTCRuntime.getObjectName(), \u0026#34;ConfiguredCount\u0026#34;); out.println(\u0026#34;Found min threads constraint runtime named \u0026#39;\u0026#34; + cName + \u0026#34;\u0026#39; with configured count: \u0026#34; + count); } out.println(); Set\u0026lt;ObjectInstance\u0026gt; maxTCRuntimes = mbs.queryMBeans(new ObjectName(\u0026#34;*:Type=MaxThreadsConstraintRuntime,Name=SampleMaxThreads,*\u0026#34;), null); for (ObjectInstance maxTCRuntime : maxTCRuntimes) { String cName = (String)mbs.getAttribute(maxTCRuntime.getObjectName(), \u0026#34;Name\u0026#34;); int count = (int)mbs.getAttribute(maxTCRuntime.getObjectName(), \u0026#34;ConfiguredCount\u0026#34;); out.println(\u0026#34;Found max threads constraint runtime named \u0026#39;\u0026#34; + cName + \u0026#34;\u0026#39; with configured count: \u0026#34; + count); } out.println(); // display local data sources // - note that data source tests are expected to fail until the sample Update 4 use case updates the data source\u0026#39;s secret ObjectName jdbcRuntime = new ObjectName(\u0026#34;com.bea:ServerRuntime=\u0026#34; + srName + \u0026#34;,Name=\u0026#34; + srName + \u0026#34;,Type=JDBCServiceRuntime\u0026#34;); ObjectName[] dataSources = (ObjectName[])mbs.getAttribute(jdbcRuntime, \u0026#34;JDBCDataSourceRuntimeMBeans\u0026#34;); out.println(\u0026#34;Found \u0026#34; + dataSources.length + \u0026#34; local data source\u0026#34; + (String)((dataSources.length!=1)?\u0026#34;s\u0026#34;:\u0026#34;\u0026#34;) + \u0026#34;:\u0026#34;); for (ObjectName dataSource : dataSources) { String dsName = (String)mbs.getAttribute(dataSource, \u0026#34;Name\u0026#34;); String dsState = (String)mbs.getAttribute(dataSource, \u0026#34;State\u0026#34;); String dsTest = (String)mbs.invoke(dataSource, \u0026#34;testPool\u0026#34;, new Object[] {}, new String[] {}); out.println( \u0026#34; Datasource \u0026#39;\u0026#34; + dsName + \u0026#34;\u0026#39;: \u0026#34; + \u0026#34; State=\u0026#39;\u0026#34; + dsState + \u0026#34;\u0026#39;,\u0026#34; + \u0026#34; testPool=\u0026#39;\u0026#34; + (String)(dsTest==null ? \u0026#34;Passed\u0026#34; : \u0026#34;Failed\u0026#34;) + \u0026#34;\u0026#39;\u0026#34; ); if (dsTest != null) { out.println( \u0026#34; ---TestPool Failure Reason---\\n\u0026#34; + \u0026#34; NOTE: Ignore \u0026#39;mynewdatasource\u0026#39; failures until the MII sample\u0026#39;s Update 4 use case.\\n\u0026#34; + \u0026#34; ---\\n\u0026#34; + \u0026#34; \u0026#34; + dsTest.replaceAll(\u0026#34;\\n\u0026#34;,\u0026#34;\\n \u0026#34;).replaceAll(\u0026#34;\\n *\\n\u0026#34;,\u0026#34;\\n\u0026#34;) + \u0026#34;\\n\u0026#34; + \u0026#34; -----------------------------\u0026#34;); } } out.println(); out.println(\u0026#34;*****************************************************************\u0026#34;); } catch (Throwable t) { t.printStackTrace(new PrintStream(response.getOutputStream())); } finally { out.println(\u0026#34;\u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34;); if (ic != null) ic.close(); } %\u0026gt; The application displays important details about the WebLogic Server instance that it\u0026rsquo;s running on: namely its domain name, cluster name, and server name, as well as the names of any data sources that are targeted to the server. Also, you can see that application output reports that it\u0026rsquo;s at version v1; you will update this to v2 in a later use case that demonstrates upgrading the application.\nStage a ZIP file of the archive When you create the image, you will use the files in the staging directory, /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1. In preparation, you need it to contain a ZIP file of the WDT application archive.\nRun the following commands to create your application archive ZIP file and put it in the expected directory:\n# Delete existing archive.zip in case you have an old leftover version $ rm -f /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/archive.zip # Move to the directory which contains the source files for your archive $ cd /tmp/sample/wdt-artifacts/archives/archive-v1 Using the WDT archive helper tool, create the archive in the location that we will use later when we run the WebLogic Image Tool.\n$ /tmp/sample/wdt-artifacts/weblogic-deploy/bin/archiveHelper.sh add application -archive_file=/tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/archive.zip -source=wlsdeploy/applications/myapp-v1 Stage model files In this step, you explore the staged WDT model YAML file and properties in the /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1 directory. The model in this directory references the web application in your archive, configures a WebLogic Server Administration Server, and configures a WebLogic cluster. It consists of two files only, model.10.properties, a file with a single property, and, model.10.yaml, a YAML file with your WebLogic configuration model.10.yaml.\nCLUSTER_SIZE=5 Here is the WLS model.10.yaml:\ndomainInfo: AdminUserName: \u0026#39;@@SECRET:__weblogic-credentials__:username@@\u0026#39; AdminPassword: \u0026#39;@@SECRET:__weblogic-credentials__:password@@\u0026#39; ServerStartMode: \u0026#39;prod\u0026#39; topology: Name: \u0026#39;@@ENV:CUSTOM_DOMAIN_NAME@@\u0026#39; AdminServerName: \u0026#39;admin-server\u0026#39; Cluster: \u0026#39;cluster-1\u0026#39;: DynamicServers: ServerTemplate: \u0026#39;cluster-1-template\u0026#39; ServerNamePrefix: \u0026#39;managed-server\u0026#39; DynamicClusterSize: \u0026#39;@@PROP:CLUSTER_SIZE@@\u0026#39; MaxDynamicClusterSize: \u0026#39;@@PROP:CLUSTER_SIZE@@\u0026#39; MinDynamicClusterSize: \u0026#39;0\u0026#39; CalculatedListenPorts: false Server: \u0026#39;admin-server\u0026#39;: ListenPort: 7001 ServerTemplate: \u0026#39;cluster-1-template\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 appDeployments: Application: myapp: SourcePath: \u0026#39;wlsdeploy/applications/myapp-v1\u0026#39; ModuleType: ear Target: \u0026#39;cluster-1\u0026#39; resources: SelfTuning: MinThreadsConstraint: SampleMinThreads: Target: \u0026#39;cluster-1\u0026#39; Count: 1 MaxThreadsConstraint: SampleMaxThreads: Target: \u0026#39;cluster-1\u0026#39; Count: 10 WorkManager: SampleWM: Target: \u0026#39;cluster-1\u0026#39; MinThreadsConstraint: \u0026#39;SampleMinThreads\u0026#39; MaxThreadsConstraint: \u0026#39;SampleMaxThreads\u0026#39; The model files:\nDefine a WebLogic domain with:\nCluster cluster-1 Administration Server admin-server A cluster-1 targeted ear application that\u0026rsquo;s located in the WDT archive ZIP file at wlsdeploy/applications/myapp-v1 A Work Manager SampleWM configured with minimum threads constraint SampleMinThreads and maximum threads constraint SampleMaxThreads Use macros to inject external values:\nThe property file CLUSTER_SIZE property is referenced in the model YAML file DynamicClusterSize and MaxDynamicClusterSize fields using a PROP macro. The model file domain name is injected using a custom environment variable named CUSTOM_DOMAIN_NAME using an ENV macro. You set this environment variable later in this sample using an env field in its Domain. This conveniently provides a simple way to deploy multiple differently named domains using the same model image. The model file administrator user name and password are set using a weblogic-credentials secret macro reference to the WebLogic credential secret. This secret is in turn referenced using the webLogicCredentialsSecret field in the Domain. The weblogic-credentials is a reserved name that always dereferences to the owning Domain actual WebLogic credentials secret name. An image can contain multiple properties files, archive ZIP files, and YAML files but in this sample you use just one of each. For a complete description of WDT model file naming conventions, file loading order, and macro syntax, see Model files in the user documentation.\nCreate the image with WIT At this point, you have all of the files needed for image wdt-domain-image:WLS-v1 staged; they include:\n/tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/model.10.yaml /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/model.10.properties /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/archive.zip Now, you use the Image Tool to create an image named wdt-domain-image:WLS-v1. You\u0026rsquo;ve already set up this tool during the prerequisite steps.\nRun the following commands to create the image and verify that it worked:\n$ cd /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1 $ /tmp/sample/wdt-artifacts/imagetool/bin/imagetool.sh createAuxImage \\ --tag wdt-domain-image:WLS-v1 \\ --wdtModel ./model.10.yaml \\ --wdtVariables ./model.10.properties \\ --wdtArchive ./archive.zip If you don\u0026rsquo;t see the imagetool directory, then you missed a step in the prerequisites.\nThis command runs the WebLogic Image Tool to create the image and does the following:\nBuilds the final container image as a layer on a small busybox base image. Copies the WDT ZIP file that\u0026rsquo;s referenced in the WIT cache into the image. Note that you cached WDT in WIT using the keyword latest when you set up the cache during the sample prerequisites steps. This lets WIT implicitly assume it\u0026rsquo;s the desired WDT version and removes the need to pass a -wdtVersion flag. Copies the specified WDT model, properties, and application archives to image location /u01/wdt/models. When the command succeeds, it should end with output like the following:\n[INFO ] Build successful. Build time=36s. Image tag=wdt-domain-image:WLS-v1 Also, if you run the docker images command, then you will see an image named wdt-domain-image:WLS-v1.\nAfter the image is created, it should have the WDT executables in /auxiliary/weblogic-deploy, and WDT model, property, and archive files in /auxiliary/models. You can run ls in the Docker image to verify this:\n$ docker run -it --rm wdt-domain-image:WLS-v1 ls -l /auxiliary total 8 drwxr-xr-x 1 oracle root 4096 Jun 1 21:53 models drwxr-xr-x 1 oracle root 4096 May 26 22:29 weblogic-deploy $ docker run -it --rm wdt-domain-image:WLS-v1 ls -l /auxiliary/models total 16 -rw-rw-r-- 1 oracle root 5112 Jun 1 21:52 archive.zip -rw-rw-r-- 1 oracle root 173 Jun 1 21:59 model.10.properties -rw-rw-r-- 1 oracle root 1515 Jun 1 21:59 model.10.yaml $ docker run -it --rm wdt-domain-image:WLS-v1 ls -l /auxiliary/weblogic-deploy total 28 -rw-r----- 1 oracle root 4673 Oct 22 2019 LICENSE.txt -rw-r----- 1 oracle root 30 May 25 11:40 VERSION.txt drwxr-x--- 1 oracle root 4096 May 26 22:29 bin drwxr-x--- 1 oracle root 4096 May 25 11:40 etc drwxr-x--- 1 oracle root 4096 May 25 11:40 lib drwxr-x--- 1 oracle root 4096 Jan 22 2019 samples NOTE: If you have Kubernetes cluster worker nodes that are remote to your local machine, then you need to put the image in a location that these nodes can access. See Ensuring your Kubernetes cluster can access images.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/domain-home-on-pv/build-domain-creation-image/",
	"title": "Build domain creation image",
	"tags": [],
	"description": "Create WebLogic images using the WebLogic Image Tool and WebLogic Deploy Tooling.",
	"content": "Contents Overview Build the domain creation image Understand your first archive Stage the archive ZIP file Stage the model files Create the image with WIT Before you begin: Complete the steps in Prerequisites.\nOverview The image build process uses the WebLogic Image Tool to create a Domain on PV domain creation image. This image contains:\nThe directory where the WebLogic Deploy Tooling software is installed (also known as WDT Home), expected in an image\u0026rsquo;s /auxiliary/weblogic-deploy directory, by default. WDT model YAML (model), WDT variable (property), and WDT archive ZIP (archive) files, expected in directory /auxiliary/models, by default. Build the domain creation image Use the steps in the following sections to build the domain creation image.\nUnderstand your first archive The sample includes a predefined archive directory in /tmp/sample/wdt-artifacts/archives/archive-v1 that you will use to create an archive ZIP file for the image.\nThe archive top directory, named wlsdeploy, contains a directory named applications, which includes an \u0026rsquo;exploded\u0026rsquo; sample JSP web application in the directory, myapp-v1. Three useful aspects to remember about WDT archives are:\nA domain creation image can contain multiple WDT archives. WDT archives can contain multiple applications, libraries, and other components. WDT archives have a well defined directory structure, which always has wlsdeploy as the top directory. If you are interested in the web application source, click here to see the JSP code. \u0026lt;%-- Copyright (c) 2019, 2023, Oracle and/or its affiliates. --%\u0026gt; \u0026lt;%-- Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. --%\u0026gt; \u0026lt;%@ page import=\u0026#34;javax.naming.InitialContext\u0026#34; %\u0026gt; \u0026lt;%@ page import=\u0026#34;javax.management.*\u0026#34; %\u0026gt; \u0026lt;%@ page import=\u0026#34;java.io.*\u0026#34; %\u0026gt; \u0026lt;% InitialContext ic = null; try { ic = new InitialContext(); String srName=System.getProperty(\u0026#34;weblogic.Name\u0026#34;); String domainUID=System.getenv(\u0026#34;DOMAIN_UID\u0026#34;); String domainName=System.getenv(\u0026#34;CUSTOM_DOMAIN_NAME\u0026#34;); out.println(\u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt;\u0026#34;); out.println(\u0026#34;*****************************************************************\u0026#34;); out.println(); out.println(\u0026#34;Hello World! This is version \u0026#39;v1\u0026#39; of the mii-sample JSP web-app.\u0026#34;); out.println(); out.println(\u0026#34;Welcome to WebLogic Server \u0026#39;\u0026#34; + srName + \u0026#34;\u0026#39;!\u0026#34;); out.println(); out.println(\u0026#34; domain UID = \u0026#39;\u0026#34; + domainUID +\u0026#34;\u0026#39;\u0026#34;); out.println(\u0026#34; domain name = \u0026#39;\u0026#34; + domainName +\u0026#34;\u0026#39;\u0026#34;); out.println(); MBeanServer mbs = (MBeanServer)ic.lookup(\u0026#34;java:comp/env/jmx/runtime\u0026#34;); // display the current server\u0026#39;s cluster name Set\u0026lt;ObjectInstance\u0026gt; clusterRuntimes = mbs.queryMBeans(new ObjectName(\u0026#34;*:Type=ClusterRuntime,*\u0026#34;), null); out.println(\u0026#34;Found \u0026#34; + clusterRuntimes.size() + \u0026#34; local cluster runtime\u0026#34; + (String)((clusterRuntimes.size()!=1)?\u0026#34;s\u0026#34;:\u0026#34;\u0026#34;) + \u0026#34;:\u0026#34;); for (ObjectInstance clusterRuntime : clusterRuntimes) { String cName = (String)mbs.getAttribute(clusterRuntime.getObjectName(), \u0026#34;Name\u0026#34;); out.println(\u0026#34; Cluster \u0026#39;\u0026#34; + cName + \u0026#34;\u0026#39;\u0026#34;); } out.println(); // display the Work Manager configuration created by the sample Set\u0026lt;ObjectInstance\u0026gt; minTCRuntimes = mbs.queryMBeans(new ObjectName(\u0026#34;*:Type=MinThreadsConstraintRuntime,Name=SampleMinThreads,*\u0026#34;), null); for (ObjectInstance minTCRuntime : minTCRuntimes) { String cName = (String)mbs.getAttribute(minTCRuntime.getObjectName(), \u0026#34;Name\u0026#34;); int count = (int)mbs.getAttribute(minTCRuntime.getObjectName(), \u0026#34;ConfiguredCount\u0026#34;); out.println(\u0026#34;Found min threads constraint runtime named \u0026#39;\u0026#34; + cName + \u0026#34;\u0026#39; with configured count: \u0026#34; + count); } out.println(); Set\u0026lt;ObjectInstance\u0026gt; maxTCRuntimes = mbs.queryMBeans(new ObjectName(\u0026#34;*:Type=MaxThreadsConstraintRuntime,Name=SampleMaxThreads,*\u0026#34;), null); for (ObjectInstance maxTCRuntime : maxTCRuntimes) { String cName = (String)mbs.getAttribute(maxTCRuntime.getObjectName(), \u0026#34;Name\u0026#34;); int count = (int)mbs.getAttribute(maxTCRuntime.getObjectName(), \u0026#34;ConfiguredCount\u0026#34;); out.println(\u0026#34;Found max threads constraint runtime named \u0026#39;\u0026#34; + cName + \u0026#34;\u0026#39; with configured count: \u0026#34; + count); } out.println(); // display local data sources // - note that data source tests are expected to fail until the sample Update 4 use case updates the data source\u0026#39;s secret ObjectName jdbcRuntime = new ObjectName(\u0026#34;com.bea:ServerRuntime=\u0026#34; + srName + \u0026#34;,Name=\u0026#34; + srName + \u0026#34;,Type=JDBCServiceRuntime\u0026#34;); ObjectName[] dataSources = (ObjectName[])mbs.getAttribute(jdbcRuntime, \u0026#34;JDBCDataSourceRuntimeMBeans\u0026#34;); out.println(\u0026#34;Found \u0026#34; + dataSources.length + \u0026#34; local data source\u0026#34; + (String)((dataSources.length!=1)?\u0026#34;s\u0026#34;:\u0026#34;\u0026#34;) + \u0026#34;:\u0026#34;); for (ObjectName dataSource : dataSources) { String dsName = (String)mbs.getAttribute(dataSource, \u0026#34;Name\u0026#34;); String dsState = (String)mbs.getAttribute(dataSource, \u0026#34;State\u0026#34;); String dsTest = (String)mbs.invoke(dataSource, \u0026#34;testPool\u0026#34;, new Object[] {}, new String[] {}); out.println( \u0026#34; Datasource \u0026#39;\u0026#34; + dsName + \u0026#34;\u0026#39;: \u0026#34; + \u0026#34; State=\u0026#39;\u0026#34; + dsState + \u0026#34;\u0026#39;,\u0026#34; + \u0026#34; testPool=\u0026#39;\u0026#34; + (String)(dsTest==null ? \u0026#34;Passed\u0026#34; : \u0026#34;Failed\u0026#34;) + \u0026#34;\u0026#39;\u0026#34; ); if (dsTest != null) { out.println( \u0026#34; ---TestPool Failure Reason---\\n\u0026#34; + \u0026#34; NOTE: Ignore \u0026#39;mynewdatasource\u0026#39; failures until the MII sample\u0026#39;s Update 4 use case.\\n\u0026#34; + \u0026#34; ---\\n\u0026#34; + \u0026#34; \u0026#34; + dsTest.replaceAll(\u0026#34;\\n\u0026#34;,\u0026#34;\\n \u0026#34;).replaceAll(\u0026#34;\\n *\\n\u0026#34;,\u0026#34;\\n\u0026#34;) + \u0026#34;\\n\u0026#34; + \u0026#34; -----------------------------\u0026#34;); } } out.println(); out.println(\u0026#34;*****************************************************************\u0026#34;); } catch (Throwable t) { t.printStackTrace(new PrintStream(response.getOutputStream())); } finally { out.println(\u0026#34;\u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34;); if (ic != null) ic.close(); } %\u0026gt; The application displays important details about the WebLogic Server instance that it\u0026rsquo;s running on: namely its domain name, cluster name, and server name, as well as the names of any data sources that are targeted to the server. Also, you can see that application output reports that it\u0026rsquo;s at version v1.\nStage the archive ZIP file When you create the image, you will use the files in the staging directory, /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1. In preparation, you need it to contain a WDT application archive ZIP file.\nRun the following commands to create your application archive ZIP file and put it in the expected directory:\n# Delete existing archive.zip in case we have an old leftover version $ rm -f /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/archive.zip # Move to the directory which contains the source files for our archive $ cd /tmp/sample/wdt-artifacts/archives/archive-v1 Using the WDT archive helper tool, create the archive in the location that we will use later when we run the WebLogic Image Tool.\n$ /tmp/sample/wdt-artifacts/weblogic-deploy/bin/archiveHelper.sh add application -archive_file=/tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/archive.zip -source=wlsdeploy/applications/myapp-v1 Stage the model files In this step, you explore the staged WDT model YAML file and properties in the /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1 directory. The model in this directory references the web application in your archive, configures a WebLogic Administration Server, and configures a WebLogic cluster. It consists of two files only, model.10.properties, a file with a single property, and, model.10.yaml, a model YAML file with your WebLogic configuration model.10.yaml.\nCLUSTER_SIZE=5 Here is the WLS model.10.yaml:\ndomainInfo: AdminUserName: \u0026#39;@@SECRET:__weblogic-credentials__:username@@\u0026#39; AdminPassword: \u0026#39;@@SECRET:__weblogic-credentials__:password@@\u0026#39; ServerStartMode: \u0026#39;prod\u0026#39; topology: Name: \u0026#39;@@ENV:CUSTOM_DOMAIN_NAME@@\u0026#39; AdminServerName: \u0026#39;admin-server\u0026#39; Cluster: \u0026#39;cluster-1\u0026#39;: DynamicServers: ServerTemplate: \u0026#39;cluster-1-template\u0026#39; ServerNamePrefix: \u0026#39;managed-server\u0026#39; DynamicClusterSize: \u0026#39;@@PROP:CLUSTER_SIZE@@\u0026#39; MaxDynamicClusterSize: \u0026#39;@@PROP:CLUSTER_SIZE@@\u0026#39; MinDynamicClusterSize: \u0026#39;0\u0026#39; CalculatedListenPorts: false Server: \u0026#39;admin-server\u0026#39;: ListenPort: 7001 ServerTemplate: \u0026#39;cluster-1-template\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 appDeployments: Application: myapp: SourcePath: \u0026#39;wlsdeploy/applications/myapp-v1\u0026#39; ModuleType: ear Target: \u0026#39;cluster-1\u0026#39; resources: SelfTuning: MinThreadsConstraint: SampleMinThreads: Target: \u0026#39;cluster-1\u0026#39; Count: 1 MaxThreadsConstraint: SampleMaxThreads: Target: \u0026#39;cluster-1\u0026#39; Count: 10 WorkManager: SampleWM: Target: \u0026#39;cluster-1\u0026#39; MinThreadsConstraint: \u0026#39;SampleMinThreads\u0026#39; MaxThreadsConstraint: \u0026#39;SampleMaxThreads\u0026#39; Click here to view the JRF `model.10.yaml`, and note the `RCUDbInfo` stanza and its references to a `DOMAIN_UID-rcu-access` secret. domainInfo: AdminUserName: \u0026#39;@@SECRET:__weblogic-credentials__:username@@\u0026#39; AdminPassword: \u0026#39;@@SECRET:__weblogic-credentials__:password@@\u0026#39; ServerStartMode: \u0026#39;prod\u0026#39; RCUDbInfo: rcu_prefix: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_prefix@@\u0026#39; rcu_schema_password: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_schema_password@@\u0026#39; rcu_db_conn_string: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_db_conn_string@@\u0026#39; topology: AdminServerName: \u0026#39;admin-server\u0026#39; Name: \u0026#39;@@ENV:CUSTOM_DOMAIN_NAME@@\u0026#39; Cluster: \u0026#39;cluster-1\u0026#39;: Server: \u0026#39;admin-server\u0026#39;: ListenPort: 7001 \u0026#39;managed-server1-c1-\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 \u0026#39;managed-server2-c1-\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 \u0026#39;managed-server3-c1-\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 \u0026#39;managed-server4-c1-\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 appDeployments: Application: myapp: SourcePath: \u0026#39;wlsdeploy/applications/myapp-v1\u0026#39; ModuleType: ear Target: \u0026#39;cluster-1\u0026#39; resources: SelfTuning: MinThreadsConstraint: SampleMinThreads: Target: \u0026#39;cluster-1\u0026#39; Count: 1 MaxThreadsConstraint: SampleMaxThreads: Target: \u0026#39;cluster-1\u0026#39; Count: 10 WorkManager: SampleWM: Target: \u0026#39;cluster-1\u0026#39; MinThreadsConstraint: \u0026#39;SampleMinThreads\u0026#39; MaxThreadsConstraint: \u0026#39;SampleMaxThreads\u0026#39; The model files:\nDefine a WebLogic domain with:\nCluster cluster-1 Administration Server admin-server A cluster-1 targeted ear application that\u0026rsquo;s located in the WDT archive ZIP file at wlsdeploy/applications/myapp-v1 A Work Manager SampleWM configured with minimum threads constraint SampleMinThreads and maximum threads constraint SampleMaxThreads Use macros to inject external values:\nThe property file CLUSTER_SIZE property is referenced in the model YAML file DynamicClusterSize and MaxDynamicClusterSize fields using a PROP macro. The model file domain name is injected using a custom environment variable named CUSTOM_DOMAIN_NAME using an ENV macro. You set this environment variable later in this sample using an env field in its Domain. This conveniently provides a simple way to deploy multiple differently named domains using the same domain creation image. The model file administrator user name and password are set using a weblogic-credentials secret macro reference to the WebLogic credential secret. This secret is in turn referenced using the webLogicCredentialsSecret field in the Domain. The weblogic-credentials is a reserved name that always dereferences to the owning Domain actual WebLogic credentials secret name. An image can contain multiple properties files, archive ZIP files, and model YAML files but in this sample you use just one of each. For a complete description of WDT model file naming conventions, file loading order, and macro syntax, see Model files in the user documentation.\nCreate the image with WIT NOTE: If you are using JRF in this sample, substitute JRF for each occurrence of WLS in the following imagetool command line.\nAt this point, you have all of the files needed for image wdt-domain-image:WLS-v1 staged; they include:\n/tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/model.10.yaml /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/model.10.properties /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/archive.zip Now, you use the Image Tool to create an image named wdt-domain-image:WLS-v1. You\u0026rsquo;ve already set up this tool during the prerequisite steps.\nRun the following commands to create the image and verify that it worked:\n$ cd /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1 $ /tmp/sample/wdt-artifacts/imagetool/bin/imagetool.sh createAuxImage \\ --tag wdt-domain-image:WLS-v1 \\ --wdtModel ./model.10.yaml \\ --wdtVariables ./model.10.properties \\ --wdtArchive ./archive.zip If you don\u0026rsquo;t see the imagetool directory, then you missed a step in the prerequisites.\nThis command runs the WebLogic Image Tool to create the domain creation image and does the following:\nBuilds the final container image as a layer on a small busybox base image. Copies the WDT ZIP file that\u0026rsquo;s referenced in the WIT cache into the image. Note that you cached WDT in WIT using the keyword latest when you set up the cache during the sample prerequisites steps. This lets WIT implicitly assume it\u0026rsquo;s the desired WDT version and removes the need to pass a -wdtVersion flag. Copies the specified WDT model, properties, and application archives to image location /auxiliary/models. When the command succeeds, it should end with output like the following:\n[INFO ] Build successful. Build time=36s. Image tag=wdt-domain-image:WLS-v1 Also, if you run the docker images command, then you will see an image named wdt-domain-image:WLS-v1.\nAfter the image is created, it should have the WDT executables in /auxiliary/weblogic-deploy, and WDT model, property, and archive files in /auxiliary/models. You can run ls in the Docker image to verify this:\n$ docker run -it --rm wdt-domain-image:WLS-v1 ls -l /auxiliary total 8 drwxr-xr-x 1 oracle root 4096 Jun 1 21:53 models drwxr-xr-x 1 oracle root 4096 May 26 22:29 weblogic-deploy $ docker run -it --rm wdt-domain-image:WLS-v1 ls -l /auxiliary/models total 16 -rw-rw-r-- 1 oracle root 5112 Jun 1 21:52 archive.zip -rw-rw-r-- 1 oracle root 173 Jun 1 21:59 model.10.properties -rw-rw-r-- 1 oracle root 1515 Jun 1 21:59 model.10.yaml $ docker run -it --rm wdt-domain-image:WLS-v1 ls -l /auxiliary/weblogic-deploy total 28 -rw-r----- 1 oracle root 4673 Oct 22 2019 LICENSE.txt -rw-r----- 1 oracle root 30 May 25 11:40 VERSION.txt drwxr-x--- 1 oracle root 4096 May 26 22:29 bin drwxr-x--- 1 oracle root 4096 May 25 11:40 etc drwxr-x--- 1 oracle root 4096 May 25 11:40 lib drwxr-x--- 1 oracle root 4096 Jan 22 2019 samples NOTE: If you have Kubernetes cluster worker nodes that are remote to your local machine, then you need to put the image in a location that these nodes can access. See Ensuring your Kubernetes cluster can access images.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/model-in-image/initial/",
	"title": "Initial use case",
	"tags": [],
	"description": "",
	"content": "Contents Overview Auxiliary image Deploy resources - Introduction Secrets Domain resource Verify the domain Verify the pods Invoke the web application Before you begin: Perform the steps in Prerequisites and then create a Model in Image auxiliary image by completing the steps in Auxiliary image creation.\nOverview In this use case, you set up an initial WebLogic domain. This involves:\nUsing the auxiliary image that you previously created. Creating secrets for the domain. Creating a Domain YAML file for the domain that references your Secrets, auxiliary image, and a WebLogic image. After the Domain is deployed, the operator starts an \u0026lsquo;introspector job\u0026rsquo; that converts your models into a WebLogic configuration, and then passes this configuration to each WebLogic Server in the domain.\nAuxiliary image The sample uses an auxiliary image with the name wdt-domain-image:WLS-v1 that you created in the Auxiliary image creation step. The WDT model files in this auxiliary image define the WebLogic domain configuration. The image contains:\nThe directory where the WebLogic Deploy Tooling software is installed (also known as WDT Home), expected in an image\u0026rsquo;s /auxiliary/weblogic-deploy directory, by default. WDT model YAML, property, and archive files (expected in the directory /auxiliary/models by default). Deploy resources - Introduction In this section, you will deploy the domain resource with the new auxiliary image to namespace sample-domain1-ns, including the following steps:\nCreate a Secret containing your WebLogic administrator user name and password. Create a Secret containing your Model in Image runtime encryption password: All Model in Image domains must supply a runtime encryption Secret with a password value. It is used to encrypt configuration that is passed around internally by the operator. The value must be kept private but can be arbitrary; you can optionally supply a different secret value every time you restart the domain. Deploy a Domain YAML file that references the new image. Wait for the domain\u0026rsquo;s Pods to start and reach their ready state. Secrets First, create the secrets needed by the domain. You have to create the WebLogic credentials secret and any other secrets that are referenced from the macros in the WDT model file. For more details about using macros in the WDT model files, see Working with the WDT model files.\nRun the following kubectl commands to deploy the required secrets:\nNOTE: Substitute a password of your choice for MY_WEBLOGIC_ADMIN_PASSWORD. This password should contain at least seven letters plus one digit.\nNOTE: Substitute a password of your choice for MY_RUNTIME_PASSWORD. It should be unique and different than the admin password, but this is not required.\n$ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-weblogic-credentials \\ --from-literal=username=weblogic --from-literal=password=MY_WEBLOGIC_ADMIN_PASSWORD $ kubectl -n sample-domain1-ns label secret \\ sample-domain1-weblogic-credentials \\ weblogic.domainUID=sample-domain1 $ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-runtime-encryption-secret \\ --from-literal=password=MY_RUNTIME_PASSWORD $ kubectl -n sample-domain1-ns label secret \\ sample-domain1-runtime-encryption-secret \\ weblogic.domainUID=sample-domain1 Some important details about these secrets:\nThe WebLogic credentials secret is required and must contain username and password fields. You reference it in spec.webLogicCredentialsSecret field of Domain YAML and macros in the domainInfo.AdminUserName and domainInfo.AdminPassWord fields your model YAML file. The Model WDT runtime secret is a special secret required by Model in Image. It must contain a password field and must be referenced using the spec.model.runtimeEncryptionSecret field in its Domain. It is used to encrypt data as it\u0026rsquo;s internally passed using log files from the domain\u0026rsquo;s introspector job and on to its WebLogic Server pods. It must remain the same for as long as the domain is deployed to Kubernetes but can be changed between deployments. Delete a secret before creating it, otherwise the create command will fail if the secret already exists. Name and label the secrets using their associated domain UID to clarify which secrets belong to which domains and make it easier to clean up a domain. Some important details about these secrets: Domain resource Now, you create a Domain YAML file. A Domain is the key resource that tells the operator how to deploy a WebLogic domain.\nCopy the contents of the domain resource YAML file file to a file called /tmp/sample/mii-initial-domain.yaml or similar. Alternatively, you can use the file /tmp/sample/domain-resources/WLS/mii-initial-d1-WLS-v1.yaml that is included in the sample source. This file contains both the domain resource and the referenced cluster resource definition. See Domain and Cluster resources.\nClick here to view the Domain YAML file.\nNOTE: Before you deploy the domain custom resource, ensure all nodes in your Kubernetes cluster can access auxiliary-image and other images.\nRun the following command to create the domain custom resource:\n$ kubectl apply -f /tmp/sample/mii-initial-domain.yaml The domain resource references the cluster resource, a WebLogic Server installation image, the secrets you defined, and a sample auxiliary image, which contains a traditional WebLogic configuration and a WebLogic application. For detailed information, see Domain and cluster resources.\nVerify the domain Run the following kubectl describe domain command to check the status and events for the created domain.\n$ kubectl describe domain sample-domain1 -n sample-domain1-ns Verify the pods If you run kubectl get pods -n sample-domain1-ns --watch, then you will see the introspector job run and your WebLogic Server pods start. The output will look something like this:\nClick here to expand. $ kubectl get pods -n sample-domain1-ns --watch ``` ``` NAME READY STATUS RESTARTS AGE sample-domain1-introspector-lqqj9 0/1 Pending 0 0s sample-domain1-introspector-lqqj9 0/1 ContainerCreating 0 0s sample-domain1-introspector-lqqj9 1/1 Running 0 1s sample-domain1-introspector-lqqj9 0/1 Completed 0 65s sample-domain1-introspector-lqqj9 0/1 Terminating 0 65s sample-domain1-admin-server 0/1 Pending 0 0s sample-domain1-admin-server 0/1 ContainerCreating 0 0s sample-domain1-admin-server 0/1 Running 0 1s sample-domain1-admin-server 1/1 Running 0 32s sample-domain1-managed-server1 0/1 Pending 0 0s sample-domain1-managed-server2 0/1 Pending 0 0s sample-domain1-managed-server1 0/1 ContainerCreating 0 0s sample-domain1-managed-server2 0/1 ContainerCreating 0 0s sample-domain1-managed-server1 0/1 Running 0 2s sample-domain1-managed-server2 0/1 Running 0 2s sample-domain1-managed-server1 1/1 Running 0 43s sample-domain1-managed-server2 1/1 Running 0 42s For a more detailed view of this activity, you can use the waitForDomain.sh sample lifecycle script. This script provides useful information about a domain\u0026rsquo;s pods and optionally waits for its Completed status condition to become True. A Completed domain indicates that all of its expected pods have reached a ready state plus their target restartVersion, introspectVersion, and image. For example:\n$ cd /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/domain-lifecycle $ ./waitForDomain.sh -n sample-domain1-ns -d sample-domain1 -p Completed If you see an error, then consult Debugging.\nInvoke the web application Now that all the initial use case resources have been deployed, you can invoke the sample web application through the Traefik ingress controller\u0026rsquo;s NodePort.\nNOTE: The web application will display a list of any data sources it finds, but at this point, we don\u0026rsquo;t expect it to find any because the model doesn\u0026rsquo;t contain any.\nSend a web application request to the load balancer for the application, as shown in the following example.\nRequest from a local machine Request from a remote machine $ curl -s -S -m 10 -H 'host: sample-domain1-cluster-cluster-1.sample.org' http://localhost:30305/myapp_war/index.jsp $ K8S_CLUSTER_ADDRESS=$(kubectl cluster-info | grep DNS | sed 's/^.*https:\\/\\///g' | sed 's/:.*$//g') $ curl -s -S -m 10 -H 'host: sample-domain1-cluster-cluster-1.sample.org' http://${K8S_CLUSTER_ADDRESS}:30305/myapp_war/index.jsp You will see output like the following:\n\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v1\u0026#39; of the sample JSP web-app. Welcome to WebLogic Server \u0026#39;managed-server2\u0026#39;! domain UID = \u0026#39;sample-domain1\u0026#39; domain name = \u0026#39;domain1\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found min threads constraint runtime named \u0026#39;SampleMinThreads\u0026#39; with configured count: 1 Found max threads constraint runtime named \u0026#39;SampleMaxThreads\u0026#39; with configured count: 10 Found 0 local data sources: ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; If you want to continue to the Update 1 use case, then leave your domain running.\nTo remove the resources you have created in this sample, see Cleanup.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/base-images/custom-images/",
	"title": "Create custom images",
	"tags": [],
	"description": "Create custom WebLogic images using the WebLogic Image Tool (WIT) with specified patches and/or JDK version.",
	"content": "Contents Use the WebLogic Image Tool to create custom images Create a custom image with patches applied Create a custom base image Create a custom image with your domain inside the image Create a custom image with your model inside the image Use the WebLogic Image Tool to create custom images You can use the WebLogic Image Tool (WIT) to build your own WebLogic Server or Fusion Middleware Infrastructure images (with the latest Oracle Linux images, Java updates, and WebLogic Server patches), apply one-off patches to existing OCR images, or overlay your own files and applications on top of an OCR image.\nDownload and install the WebLogic Image Tool (WIT) following the WIT Setup instructions. Also, refer to the WIT Quick Start Guide. The samples in this document assume that you have installed WIT in /tmp/imagetool; you can choose to install it in any location.\nThe WebLogic Image Tool create, update, or rebase commands supply three different ways to generate a custom WebLogic Server installation image from a base OS image (optionally, with WebLogic patches). In addition, the WIT createAuxImage command supports creating auxiliary images which do not contain a WebLogic Server installation, and instead, solely contain the WebLogic Deploy Tooling (WDT) installation and model files; this option is designed for the Model in Image domain home source type.\nFinally, you can use the WIT inspect command to inspect images.\nIn detail:\nWIT create command:\nCreates a new WebLogic image from a base OS image. Can be used for all domain home source types (Domain in Image, Model in Image, and Domain on PV). Optionally, includes a WebLogic Deploy Tooling (WDT) installation and model files in the image (for Model in Image domains). See also, Create a custom image with your model inside the image. Optionally, generates a domain home in the image using WLST or WDT (for Domain in Image domains). See also, Create a custom image with your domain inside the image. Important: The create command is not suitable for updating an existing domain home in existing Domain in Image images when the update is intended for a running domain. Use rebase instead or shut down the running domain entirely before applying the new image. For more information, see Create a custom image with your domain inside the image. WIT rebase command:\nThe rebase command is used for Domain in Image domains.\nCreates a new WebLogic image and copies an existing WebLogic domain home from an older image to the new image. The created image is suitable for deploying to an already running domain with an older version of the domain home. The new image can be created in one of two ways:\nAs a layer on an existing WebLogic image in the repository that doesn\u0026rsquo;t already have a domain home, such as an updated CPU image from OCR. Or, as a new WebLogic image from a base OS image. NOTE: Oracle strongly recommends rebasing your images with the latest security patches by applying the --recommendedPatches option. For more information, see Apply patched images to a running domain. WIT update command:\nCreates a new WebLogic image layered on an existing WebLogic image (specified in the WIT --fromImage parameter). Note that if you specify the --pull parameter for WIT, and the --fromImage parameter refers to an image in a repository, and the repository image is newer than the locally cached version of the image, then the command will download the repository image to the local Docker cache and use it instead of using the outdated local image. Optionally, generates a domain home in the new image using WDT or WLST (for Domain in Image domains). Optionally, includes a WDT installation and model files in the image (for Model in Image domains). Important: NOTE: Patching an Oracle Home using the WIT update command results in a larger WebLogic Server image due to the immutable layering in container images. For small updates, such as a one-off patch, the increase in the image size may be negligible. However, for larger updates, the increase in size will be significant. Consider using rebase or create to reduce the size impact of applying patches. The WIT update command is not suitable for updating an existing domain home in an existing Domain in Image image when the update is intended for a running domain. Use the WIT rebase command instead or shut down the running domain entirely before applying the new image. For more information, see Apply patched images to a running domain. Optionally, includes a WDT installation and model files in the image (for Model in Image domains). WIT createAuxImage command:\nSupports creating auxiliary images for Model in Image domains only. The auxiliary images solely contain WebLogic Deploy Tooling files for the Model in Image use case and are used in addition to the domain resource image that contains your WebLogic and Java installations. For more information, see Auxiliary images. WIT inspect command:\nInspects images created with the WebLogic Image Tool. See Inspect images. Create a custom image with patches applied All domain home source types require a base image which contains JDK and WebLogic Server binaries. This base image is usually obtained directly from the Oracle Container Registry, but, as needed, you can also create your own custom base image.\nIf you are using the Domain in Image domain home source type, then you will additionally need to use the base image to create a custom image with your domain inside the image.\nOr, if you are using the Model in Image domain home source type without auxiliary images, then you will additionally need to use the base image to create a custom image with your model inside the image.\nCreate a custom base image This section describes using the WebLogic Image Tool (WIT) create command to build a custom base WebLogic Server image. This is sometimes necessary to build an image with a specific patch, and such, but most use cases can instead, obtain pre-built patched images directly from the Oracle Container Registry. See Obtain images from the Oracle Container Registry.\nHere\u0026rsquo;s an example of using the WIT create command to create a base WebLogic Server image from a base Oracle Linux image, a WebLogic installer download, and a JRE installer download:\nFirst, install the WebLogic Image Tool. This sample assumes that you have installed WIT in /tmp/imagetool; you can choose to install it in any location.\nDownload your desired JRE installer from the Oracle Technology Network Java downloads page or from the Oracle Software Delivery Cloud (OSDC).\nDownload your desired WebLogic Server installer from the Oracle Technology Network WebLogic Server installers page or from the Oracle Software Delivery Cloud (OSDC).\nNOTE: The WebLogic Server installers will not be fully patched. In a subsequent step, you\u0026rsquo;ll use the WIT --patches or --recommendedPatches options to apply one-off and recommended Oracle patches.\nAdd the installers to your WIT cache using the cache command. For example, assuming you downloaded the installers to the /home/acmeuser/wls-installers directory:\n$ /tmp/imagetool/bin/imagetool.sh cache addInstaller \\ --type=jdk \\ --version=8u291 \\ --path=/home/acmeuser/wls-installers/jre-8u291-linux-x64.tar.gz $ /tmp/imagetool/bin/imagetool.sh cache addInstaller \\ --type=wls \\ --version=12.2.1.4.0 \\ --path=/home/acmeuser/wls-installers/fmw_12.2.1.4.0_wls_Disk1_1of1.zip For details, see the WIT Quick Start guide.\nUse the create command to build the image using a default Oracle Linux image as its base, and download and apply the patches.\nFor example, use the following command to create a WebLogic Server image named latest_weblogic:12.2.1.4 with:\nThe WebLogic Server 12.2.1.4.0 generic installer JDK 8u291 The latest version of the Oracle Linux 7 slim container image The latest quarterly Patch Set Update (PSU), which include security fixes, or with one-off patches $ /tmp/imagetool/bin/imagetool.sh create \\ --tag latest_weblogic:12.2.1.4 \\ --pull \\ --jdkVersion=8u291 \\ --type=wls \\ --version=12.2.1.4.0 \\ --recommendedPatches \\ --user myusername@mycompany.com \\ --passwordEnv=MYPWD As of June, 2023, Oracle WebLogic Server 12.2.1.3 is no longer supported. The last Critical Patch Updates (CPU) images for WebLogic Server 12.2.1.3 were published in April, 2023. As of December, 2022, Fusion Middleware 12.2.1.3 is no longer supported. The last CPU images for FMW Infrastructure 12.2.1.3 were published in October, 2022.\nNOTES:\nTo enable WIT to download patches, you must supply your My Oracle Support (Oracle Single Sign-On) credentials using the --user and --passwordEnv parameters (or one of the other password CLA options). This example assumes that you have set the MYPWD shell environment variable so that it contains your password. The --type parameter designates the Oracle product installation, such as WebLogic Server, Fusion Middleware (FMW) Infrastructure, and such, to include in the generated image. For example, the wls type corresponds to the WebLogic Server (WLS) generic installation, the wlsslim type to the WLS slim installation, and the wlsdev type to the WLS developer installation. For a description of each installation type, see WebLogic distribution installer type. The --recommendedPatches parameter finds and applies the latest PatchSet Update (PSU) and recommended patches for each of the products included in the installer. For example, for WebLogic Server, the recommended patches for Coherence and TopLink are included. These sample commands use a default base image, which is an Oracle Linux OS image, and downloads (pulls) this image only if it is not already cached locally. You can use docker images to view your image cache. The --pull parameter for WIT is passed to the container build engine which forces a check to the remote repository, if applicable, prior to the build execution of the new image to update any image used during the build (updates dependencies). For details about each parameter, see the WebLogic Image Tool User Guide. After the tool creates the image, verify that the image is in your local repository:\n$ docker images You can also inspect the contents of the image.\nCreate a custom image with your domain inside the image Oracle strongly recommends storing Domain in Image images in a private registry. A container image that contains a WebLogic domain home has sensitive information including credentials that are used to access external resources (for example, a data source password), and decryption keys (for example, the DOMAIN_HOME/security/SerializedSystemIni.dat domain secret file). For more information, see Container image protection.\nThe sample scripts in this section reference base image container-registry.oracle.com/middleware/weblogic:12.2.1.4. This is an OCR General Availability (GA) image which does not include the latest security patches for WebLogic Server. GA images are intended for single desktop demonstration and development purposes only. For all other purposes, Oracle strongly recommends using only images with the latest set of recommended patches applied, such as OCR Critical Patch Updates (CPU) images or custom generated images. See Ensure you are using recently patched images.\nThis section provides guidance for creating a new Domain in Image image. This type of image cannot be used in pods that must join the pods in an already running domain. If you need to create a Domain in Image image that is meant for updating an already running domain, then see Apply patched images to a running domain.\nNOTE: The Domain in Image domain home source type is deprecated in WebLogic Kubernetes Operator version 4.0. Oracle recommends that you choose either Domain on PV or Model in Image, depending on your needs.\nFor Domain in Image domains, you must create an image with the WebLogic domain inside the image. Samples are provided that demonstrate how to create the image using either WLST to define the domain or WebLogic Deploy Tooling models to define the domain. In these samples, you will see a reference to a \u0026ldquo;base\u0026rdquo; or --fromImage image. You should use an image with the recommended security patches installed as this base image, where this image could be an OCR image or a custom image. See Obtain images from the Oracle Container Registry and Create a custom image with patches applied.\nThe samples perform multiple steps for you using a single provided script and are not intended for production use. To help understand the individual steps, use the following step-by-step guidance for using WLST or WDT to create the domain home in Domain in Image.\nDomain in Image using WDT:\nHere we explore a step-by-step approach for Domain in Image using WebLogic Deploy Tooling models to create the domain home. These steps stage files to /tmp/dii-wdt-stage, assume the operator source is in /tmp/weblogic-kubernetes-operator, assume you have installed WIT in /tmp/imagetool, and generate a Domain in Image image named my-dii-wdt:v1.\nClick here to view the script. #!/bin/bash set -eux # Define paths for: # - the operator source (assumed to already be downloaded) # - the WIT installation (assumed to already be installed) # - a staging directory for the image build srcDir=/tmp/weblogic-kubernetes-operator imageToolBin=/tmp/imagetool/bin stageDir=/tmp/dii-wdt-stage # Define location of the domain home within the image domainHome=/u01/oracle/user_projects/domains/dii-wdt # Define base image and final image fromImage=container-registry.oracle.com/middleware/weblogic:12.2.1.4 finalImage=my-dii-wdt:v1 # Init staging directory [ -e \u0026#34;$stageDir\u0026#34; ] \u0026amp;\u0026amp; echo \u0026#34;Error: stage dir \u0026#39;$stagedir\u0026#39; already exists.\u0026#34; \u0026amp;\u0026amp; exit 1 mkdir -p $stageDir # Copy sample model file to the stage directory cp $srcDir/kubernetes/samples/scripts/create-weblogic-domain/domain-home-in-image/wdt/wdt_model_dynamic.yaml $stageDir # Create a set of properties that are referenced by the model file cat \u0026lt;\u0026lt; EOF \u0026gt; $stageDir/domain.properties DOMAIN_NAME=dii-wdt SSL_ENABLED=false ADMIN_PORT=7001 ADMIN_SERVER_SSL_PORT=7002 ADMIN_NAME=admin-server ADMIN_HOST=wlsadmin ADMIN_USER_NAME=weblogic1 ADMIN_USER_PASS=mypassword1 MANAGED_SERVER_PORT=8001 MANAGED_SERVER_SSL_PORT=8002 MANAGED_SERVER_NAME_BASE=managed-server CONFIGURED_MANAGED_SERVER_COUNT=5 CLUSTER_NAME=cluster-1 DEBUG_PORT=8453 DB_PORT=1527 DEBUG_FLAG=true PRODUCTION_MODE_ENABLED=true CLUSTER_TYPE=DYNAMIC JAVA_OPTIONS=-Dweblogic.StdoutDebugEnabled=false T3_CHANNEL_PORT=30012 T3_PUBLIC_ADDRESS= EOF # Download WDT and add a reference in the WIT cache curl -m 120 \\ -fL https://github.com/oracle/weblogic-deploy-tooling/releases/latest/download/weblogic-deploy.zip \\ -o $stageDir/weblogic-deploy.zip $imageToolBin/imagetool.sh \\ cache deleteEntry --key wdt_latest $imageToolBin/imagetool.sh \\ cache addInstaller \\ --type wdt \\ --version latest \\ --path $stageDir/weblogic-deploy.zip # Create the image # (this will run the latest version of the WIT tool during image creation # to create the domain home from the provided model files) $imageToolBin/imagetool.sh update \\ --fromImage \u0026#34;$fromImage\u0026#34; \\ --tag \u0026#34;$finalImage\u0026#34; \\ --wdtModel \u0026#34;$stageDir/wdt_model_dynamic.yaml\u0026#34; \\ --wdtVariables \u0026#34;$stageDir/domain.properties\u0026#34; \\ --wdtOperation CREATE \\ --wdtVersion LATEST \\ --wdtDomainHome \u0026#34;$domainHome\u0026#34; \\ --chown=oracle:root Domain in Image using WLST:\nHere is a step-by-step approach for Domain in Image images using WLST. These steps stage files to dii-wlst-stage, put the domain home inside the image at /u01/oracle/user_projects/domains/dii-wlst, assume the operator source is in /tmp/weblogic-kubernetes-operator, assume you have installed WIT in /tmp/imagetool, and name the final image my-dii-wlst:v1.\nClick here to view the script. #!/bin/bash set -eux # Define paths for: # - the operator source (assumed to already be downloaded) # - the WIT installation (assumed to already be installed) # - a staging directory for the image build srcDir=/tmp/weblogic-kubernetes-operator imageToolBin=/tmp/imagetool/bin stageDir=/tmp/dii-wlst-stage # Define location of the domain home within the image domainHome=/u01/oracle/user_projects/domains/dii-wlst # Define base image and final image fromImage=container-registry.oracle.com/middleware/weblogic:12.2.1.4 finalImage=my-dii-wlst:v1 # Copy sample WLST, Docker commands, and setup scripts # to the staging directory and modify to point to the domain home [ -e \u0026#34;$stageDir\u0026#34; ] \u0026amp;\u0026amp; echo \u0026#34;Error: stage dir \u0026#39;$stagedir\u0026#39; already exists.\u0026#34; \u0026amp;\u0026amp; exit 1 mkdir -p $stageDir sampleDir=$srcDir/kubernetes/samples/scripts/create-weblogic-domain/domain-home-in-image cp $sampleDir/wlst/additional-build-commands-template $stageDir/additional-build-commands sed -i -e \u0026#34;s:%DOMAIN_HOME%:$domainHome:g\u0026#34; $stageDir/additional-build-commands cp $sampleDir/wlst/createWLSDomain.sh $stageDir cp $sampleDir/wlst/create-wls-domain.py $stageDir # Create a set of properties to pass to the create-wls-domain.py WLST script cat \u0026lt;\u0026lt; EOF \u0026gt; $stageDir/domain.properties DOMAIN_NAME=dii-wlst DOMAIN_HOME=$domainHome SSL_ENABLED=false ADMIN_PORT=7001 ADMIN_SERVER_SSL_PORT=7002 ADMIN_NAME=admin-server ADMIN_HOST=wlsadmin ADMIN_USER_NAME=weblogic1 ADMIN_USER_PASS=mypassword1 MANAGED_SERVER_PORT=8001 MANAGED_SERVER_SSL_PORT=8002 MANAGED_SERVER_NAME_BASE=managed-server CONFIGURED_MANAGED_SERVER_COUNT=5 CLUSTER_NAME=cluster-1 DEBUG_PORT=8453 DB_PORT=1527 PRODUCTION_MODE_ENABLED=true CLUSTER_TYPE=DYNAMIC JAVA_OPTIONS=-Dweblogic.StdoutDebugEnabled=false T3_CHANNEL_PORT=30012 T3_PUBLIC_ADDRESS= EOF # Create the image # Notes: # - This will run the provided WLST during image creation to create the domain home. # - The wdt parameters are required, but ignored. $imageToolBin/imagetool.sh update \\ --fromImage \u0026#34;$fromImage\u0026#34; \\ --tag \u0026#34;$finalImage\u0026#34; \\ --wdtOperation CREATE \\ --wdtVersion LATEST \\ --wdtDomainHome \u0026#34;$domainHome\u0026#34; \\ --additionalBuildCommands $stageDir/additional-build-commands \\ --additionalBuildFiles \u0026#34;$stageDir/createWLSDomain.sh,$stageDir/create-wls-domain.py,$stageDir/domain.properties\u0026#34; \\ --chown=oracle:root NOTES:\nThe sample script and its domain.properties file include a sample WebLogic administration password. These files must be protected and the sample password must be changed. The sample scripts, sample properties, the files provided in --additionalBuildCommands and --additionalBuildFiles parameters for the WLST approach, or the sample WDT model files provided in the WDT approach, are not intended for production use. These files can all change substantially in new versions of the operator and must all be copied, preserved, and customized to suite your particular use case. Create a custom image with your model inside the image NOTE: Model in Image without auxiliary images (the WDT model and installation files are included in the same image with the WebLogic Server installation) is deprecated in WebLogic Kubernetes Operator version 4.0.7. Oracle recommends that you use Model in Image with auxiliary images. See Auxiliary images.\nThe example in this section references a base image, container-registry.oracle.com/middleware/weblogic:12.2.1.4. This is an OCR General Availability (GA) image which does not include the latest security patches for WebLogic Server. GA images are intended for single desktop demonstration and development purposes only. For all other purposes, Oracle strongly recommends using only images with the latest set of recommended patches applied, such as OCR Critical Patch Updates (CPU) images or custom generated images. See Ensure you are using recently patched images.\nExample steps for creating a custom WebLogic image with a Model in Image file layer (using files from the Model in Image sample):\nTo gain an overall understanding of Model in Image domains, read the Model in Image User Guide and the Model in Image Sample. Note that the sample uses the recommended best approach, auxiliary images, instead of the alternative approach, which is used in this example.\nFollow the prerequisite steps in the Model in Image Sample that describe how to:\nDownload the operator source and its Model in Image sample (including copying the sample to the suggested location, /tmp/sample). Download the latest WebLogic Deploy Tooling (WDT) and WebLogic Image Tool (WIT) installer ZIP files to your /tmp/sample/wdt-artifacts directory. Both WDT and WIT are required to create your Model in Image container images. Install (unzip) the WebLogic Image Tool and configure its cache to reference your WebLogic Deploy Tooling download. Locate or create a base WebLogic image.\nSee Obtain images from the Oracle Container Registry or Create a custom image with patches applied.\nIn the following step, you will use the container-registry.oracle.com/middleware/weblogic:12.2.1.4 GA image.\nBuild the final image using WIT while specifying the base image, target image tag, WDT installation location, and WDT model file locations. For example:\nFirst, create a model ZIP file application archive and place it in the same directory where the sample model YAML file and model properties files are already staged:\n$ rm -f /tmp/sample/wdt-artifacts/wdt-model-files/WLS-LEGACY-v1/archive.zip $ cd /tmp/sample/wdt-artifacts/archives/archive-v1 $ zip -r /tmp/sample/wdt-artifacts/wdt-model-files/WLS-LEGACY-v1/archive.zip wlsdeploy (The rm -f command is included in case there\u0026rsquo;s an old version of the archive ZIP file from a previous run of this sample.)\nSecond, run the following WIT command:\n$ cd /tmp/sample/wdt-artifacts/wdt-model-files/WLS-LEGACY-v1 $ ./imagetool/bin/imagetool.sh update \\ --tag wdt-domain-image:WLS-LEGACY-v1 \\ --fromImage container-registry.oracle.com/middleware/weblogic:12.2.1.4 \\ --wdtModel ./model.10.yaml \\ --wdtVariables ./model.10.properties \\ --wdtArchive ./archive.zip \\ --wdtModelOnly \\ --wdtDomainType WLS \\ --chown oracle:root NOTE that JRF support in Model in Image domains is deprecated in operator version 4.1.0; For JRF domains, use the Domain on PV domain home source type instead.\nFor an example Domain YAML file that sets up Model in Image to reference the image, see /tmp/sample/domain-resources/WLS-LEGACY/mii-initial-d1-WLS-LEGACY-v1.yaml\nNOTES:\nThe default values for domain.spec.configuration.model.wdtInstallHome and .modelHome reference the location of the WDT installation and model files that WIT copied into the image.\nThe domain type specified in domain.spec.configuration.model.domainType must correspond with the --wdtDomainType specified on the WIT command line when creating the image.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/choosing-a-model/",
	"title": "Choose a domain home source type",
	"tags": [],
	"description": "Choose among these domain home types depending on your requirements and create images that are appropriate for your type.",
	"content": "This document describes the domain home source types for deploying a domain, and creating images that are appropriate for each type.\nContents Overview of domain home source types Use or create WebLogic images depending on domain home source type Overview of domain home source types When using the operator to start WebLogic Server instances from a domain, you have the choice of the following WebLogic domain home source types:\nThe Domain in Image domain home source type is deprecated in WebLogic Kubernetes Operator version 4.0. Oracle recommends that you choose either Domain on PV or Model in Image, depending on your needs.\nModel in Image:\nSet the domain resource domain.spec.domainHomeSourceType attribute to FromModel. Supply a WebLogic installation in an image and supply a WebLogic configuration in one of three ways: As WDT model YAML file supplied in separate auxiliary images. As WebLogic Deployment Tool (WDT) model YAML file layered on the WebLogic installation image. NOTE: Model in Image without auxiliary images (the WDT model and installation files are included in the same image with the WebLogic Server installation) is deprecated in WebLogic Kubernetes Operator version 4.0.7. Oracle recommends that you use Model in Image with auxiliary images. See Auxiliary images. As WDT model YAML file in a Kubernetes ConfigMap. Supply WebLogic applications in one of two ways: In auxiliary images. Layered on the installation image. NOTE: Model in Image without auxiliary images (the WDT model and installation files are included in the same image with the WebLogic Server installation) is deprecated in WebLogic Kubernetes Operator version 4.0.7. Oracle recommends that you use Model in Image with Auxiliary images. See Auxiliary images. Mutate the WebLogic configuration by supplying a new image and rolling, or model updates supplied in a Kubernetes ConfigMap. Domain in Image: NOTE: The Domain in Image domain home source type is deprecated in WebLogic Kubernetes Operator version 4.0. Oracle recommends that you choose either Domain on PV or Model in Image, depending on your needs.\nSet the domain resource domain.spec.domainHomeSourceType attribute to Image. Supply a WebLogic installation in an image and supply a WebLogic configuration as a domain home layered on this image. Supply WebLogic applications layered on the installation image. Mutate the WebLogic configuration by supplying a new image and rolling, or by configuration overrides supplied in a Kubernetes ConfigMap. Domain on PV:\nSet the domain resource domain.spec.domainHomeSourceType attribute to PersistentVolume. Supply a WebLogic installation in an image and supply a WebLogic configuration as a domain home in a persistent volume. Optionally supply the domain resource domain.spec.configuration.initialDomainOnPV section to provide information for the Operator to create the initial domain home. Supply WebLogic applications in the persistent volume. Update the WebLogic configuration using WLST, or the WebLogic Server Administration Console. Optionally use configuration overrides supplied in a Kubernetes ConfigMap. Use this only if WLST, or the WebLogic Server Administration Console does not fit your deployment strategy. Note that you can use different domain home types for different domains; there\u0026rsquo;s no restriction on having domains with different domain home types in the same Kubernetes cluster or namespace.\nThere are advantages for each domain home source type where Model in Image is the most popular choice, but sometimes there are also technical limitations of various cloud providers that may make one type better suited to your needs. The following table compares the types:\nDomain on PV Domain in Image Model in Image Lets you use the same standard WebLogic Server image for every server in every domain. Requires a different image for each domain, but all servers in that domain use the same image. Different domains can use the same image, but require different domainUID and may have different configuration. No state is kept in images making the containers created from these images completely throw away (cattle not pets). Runtime state should not be kept in the images, but applications and configuration are. Runtime state should not be kept in the images. Application and configuration may be. You can deploy new applications using the Administration Console or WLST. If you want to deploy application updates, then you must create a new image. If you want to deploy application updates, then you must create a new image, which optionally can be an auxiliary image that doesn\u0026rsquo;t include a WebLogic installation. You can use configuration overrides to mutate the domain configuration before it is deployed, but there are limitations. Same as Domain on PV. You can deploy model files to a ConfigMap to mutate the domain before it is deployed. The model file syntax is far simpler and less error prone than the configuration override syntax, and, unlike configuration overrides, allows you to directly add data sources and JMS modules. You can change WebLogic domain configuration at runtime using the Administration Console or WLST. You can also change configuration overrides and distribute the new overrides to running servers; however, non-dynamic configuration attributes can be changed only when servers are starting and some changes may require a full domain restart. You also can change configuration overrides and distribute the new overrides to running servers; however, non-dynamic configuration attributes can be changed only when servers are starting and some changes may require a full domain restart. You should not use the Administration Console or WLST for these domains as changes are ephemeral and will be lost when servers restart. You can change configuration at runtime using model YAML file snippets supplied in runtime updates (which are substantially easier to specify than configuration overrides); however, non-dynamic configuration attributes will change only when servers are restarted (rolled) and some changes may require a full domain restart. You should not use the Administration Console or WLST for these domains as changes are ephemeral and will be lost when servers restart. Logs are automatically placed on persistent storage and sent to the pod\u0026rsquo;s stdout. Logs are kept in the containers and sent to the pod\u0026rsquo;s log (stdout) by default. To change the log location, you can set the Domain logHomeEnabled to true and configure the desired directory using logHome. Same as Domain in Image. Patches can be applied by simply changing the image and rolling the domain. To apply patches, you must update the domain-specific image and then restart or roll the domain depending on the nature of the patch. Same as Domain on PV when using dedicated auxiliary images to supply model artifacts; same as Domain in Image otherwise. Many cloud providers do not provide persistent volumes that are shared across availability zones, so you may not be able to use a single persistent volume. You may need to use some kind of volume replication technology or a clustered file system. Provided you do not store and state in containers, you do not have to worry about volume replication across availability zones because each pod has its own copy of the domain. WebLogic replication will handle propagation of any online configuration changes. Same as Domain in Image. CI/CD pipelines may be more complicated because you would need to run WLST against the live domain directory to effect changes. CI/CD pipelines are simpler because you can create the whole domain in the image and don\u0026rsquo;t have to worry about a persistent copy of the domain. CI/CD pipelines are even simpler because you don\u0026rsquo;t need to generate a domain home. The operator will create a domain home for you based on the model that you supply. There are fewer images to manage and store, which could provide significant storage and network savings. There are more images to manage and store in this approach. Same as Domain in Image unless you use the auxiliary images approach. With auxiliary images, you can use a single image to distribute your WebLogic installation (similar to Domain on PV), plus one or more specific dedicated images that contain your WebLogic configuration and applications. You may be able to use standard Oracle-provided images or, at least, a very small number of self-built images, for example, with patches installed. You may need to do more work to set up processes to build and maintain your images. Same as Domain in Image. Use or create WebLogic images depending on domain home source type For information relevant to your chosen domain home source type, refer to the following documentation.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/database/",
	"title": "Run a database",
	"tags": [],
	"description": "Run an ephemeral database in Kubernetes that is suitable for sample or basic testing purposes.",
	"content": "Contents Overview Oracle database in Kubernetes MySQL database in Kubernetes Overview This section describes how to run an ephemeral Oracle database or MySQL database in your Kubernetes cluster using approaches suitable for sample or basic testing purposes.\nNOTES:\nThe databases are configured with ephemeral storage, which means all information will be lost on any shutdown or pod failure.\nThe Oracle Database images are supported for non-production use only. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker Doc ID 2216342.1.\nOracle database in Kubernetes The following example shows how to set up an ephemeral Oracle database with the following attributes:\nAttribute Value Kubernetes namespace default Kubernetes pod oracle-db Kubernetes service name oracle-db Kubernetes service port 1521 Kubernetes node port 30011 Image container-registry.oracle.com/database/enterprise:12.2.0.1-slim DBA user (with full privileges) sys as sysdba DBA password \u0026lt;password placeholder\u0026gt; Database Domain (not the same as a WebLogic Domain) k8s Database PDB devpdb Database URL inside Kubernetes cluster (from any namespace) oracle-db.default.svc.cluster.local:1521/devpdb.k8s Database URL outside Kubernetes cluster dns-name-that-resolves-to-node-location:30011/devpdb.k8s Get the operator source and put it in /tmp/weblogic-kubernetes-operator.\nFor example:\n$ cd /tmp $ git clone --branch v4.2.20 https://github.com/oracle/weblogic-kubernetes-operator.git NOTE: We will refer to the top directory of the operator source tree as /tmp/weblogic-kubernetes-operator; however, you can use a different location.\nFor additional information about obtaining the operator source, see the Developer Guide Requirements.\nEnsure that you have access to the database image:\nUse a browser to log in to https://container-registry.oracle.com, select Database -\u0026gt; enterprise, and accept the license agreement.\nGet the database image:\nIn the local shell, docker login container-registry.oracle.com. In the local shell, docker pull container-registry.oracle.com/database/enterprise:12.2.0.1-slim. If your Kubernetes cluster nodes do not all have access to the database image in a local cache, then:\nDeploy a Kubernetes docker secret to the default namespace with login credentials for container-registry.oracle.com: kubectl create secret docker-registry docker-secret \\ --docker-server=container-registry.oracle.com \\ --docker-username=your.email@some.com \\ --docker-password=your-password \\ --docker-email=your.email@some.com \\ -n default Pass the name of this secret as a parameter to the start-db-service.sh in the following step using -s your-image-pull-secret. Alternatively, copy the database image to each local Docker cache in the cluster. For more information, see the FAQ, Cannot pull image. WARNING: The Oracle Database images are supported only for non-production use. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker Doc ID 2216342.1.\nCreate a secret named oracle-db-secret in the default namespace with your desired Oracle SYS DBA password in its password key.\nFor example: $ kubectl -n default create secret generic oracle-db-secret \\ --from-literal=\u0026#39;password=\u0026lt;password placeholder\u0026gt;\u0026#39; (Replace \u0026lt;password placeholder\u0026gt; with your desired password.) Oracle Database passwords can contain upper case, lower case, digits, and special characters. Use only \u0026ldquo;_\u0026rdquo; and \u0026ldquo;#\u0026rdquo; as special characters to eliminate potential parsing errors for Oracle Database connection strings. Create a deployment using the database image:\nUse the sample script in /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oracle-db-service to create an Oracle database running in the deployment, oracle-db.\n$ cd /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oracle-db-service $ start-db-service.sh Notes:\nCall start-db-service.sh -h to see how to customize the namespace, node port, secret name, etc. Call stop-db-service.sh to shutdown and cleanup the oracle-db deployment. To troubleshoot, use the kubectl describe pod DB_POD_NAME and kubectl logs DB_POD_NAME commands on the database pod. MySQL database in Kubernetes The following example shows how to set up an ephemeral MySQL database with the following attributes:\nAttribute Value Kubernetes namespace default Kubernetes pod mysql-db Kubernetes service name mysql-db Kubernetes service port 3306 Image mysql:5.6 Root user (with full privileges) \u0026lt;user name placeholder\u0026gt; Root password \u0026lt;password placeholder\u0026gt; Database URL inside Kubernetes cluster (from any namespace) jdbc:mysql://mysql-db.default.svc.cluster.local:3306/mysql Copy the following YAML into a file named mysql.yaml:\napiVersion: v1 kind: Pod metadata: name: mysql-db namespace: default labels: app: mysql-db spec: terminationGracePeriodSeconds: 5 containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_USER valueFrom: secretKeyRef: name: mysql-secret key: root-user - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: root-password ports: - containerPort: 3306 name: mysql --- apiVersion: v1 kind: Service metadata: name: mysql-db namespace: default spec: ports: - port: 3306 selector: app: mysql-db clusterIP: None --- apiVersion: v1 kind: Secret metadata: name: mysql-secret namespace: default data: root-user: \u0026lt;user name placeholder\u0026gt; root-password: \u0026lt;password placeholder\u0026gt; In file mysql.yaml, replace \u0026lt;user name placeholder\u0026gt; and \u0026lt;password placeholder\u0026gt;, respectively, with the output from piping the root user name and password through base64:\necho -n \u0026lt;user name placeholder\u0026gt; | base64 echo -n \u0026lt;password placeholder\u0026gt; | base64 Deploy MySQL using the command kubectl create -f mysql.yaml.\nTo shut down and clean up the resources, use kubectl delete -f mysql.yaml.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/quickstart/prepare/",
	"title": "Prepare for a domain",
	"tags": [],
	"description": "",
	"content": " Create and label a namespace that can host one or more domains.\n$ kubectl create namespace sample-domain1-ns $ kubectl label ns sample-domain1-ns weblogic-operator=enabled Configure Traefik to manage ingresses created in this namespace.\n$ helm upgrade traefik-operator traefik/traefik \\ --namespace traefik \\ --reuse-values \\ --set \u0026#34;kubernetes.namespaces={traefik,sample-domain1-ns}\u0026#34; If you have reached this point while following the prerequisites in the domain samples, stop here and return to the Model in Image sample instructions or Domain on PV sample instructions.\nAccept the license agreement for WebLogic Server images.\na. In a browser, go to the Oracle Container Registry (OCR) and log in using the Oracle Single Sign-On (SSO) authentication service. If you do not already have SSO credentials, then at the top, right side of the page, click Sign In to create them.\nb. Search for weblogic, then select weblogic in the Search Results.\nc. From the drop-down menu, select your language and click Continue.\nd. Then read and accept the license agreement.\nCreate a docker-registry secret to enable pulling the example WebLogic Server image from the registry.\n$ kubectl create secret docker-registry weblogic-repo-credentials \\ --docker-server=container-registry.oracle.com \\ --docker-username=YOUR_REGISTRY_USERNAME \\ --docker-password=YOUR_REGISTRY_PASSWORD \\ --docker-email=YOUR_REGISTRY_EMAIL \\ -n sample-domain1-ns Replace YOUR_REGISTRY_USERNAME, YOUR_REGISTRY_PASSWORD, and YOUR_REGISTRY_EMAIL with the values you use to access the registry.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "Gain an overall understanding of the operator and learn where you can get help.",
	"content": "Gain an overall understanding of the operator and learn where you can get help.\nImportant terms Define important terms used throughout this documentation.\nDesign philosophy Define the expected roles of an administrator, the operator, and domain and cluster resources.\nArchitecture An architectural overview of the operator runtime and related resources.\nGet help Where to get help, submit suggestions, or submit issues.\nOperator prerequisites Review the prerequisites for the current release of the operator.\nSupported environments The supported environments, pricing and licensing, and support details for the operator.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/azure-kubernetes-service/model-in-image/",
	"title": "Model in Image",
	"tags": [],
	"description": "Sample for creating a WebLogic cluster on the Azure Kubernetes Service with model in image domain home source type.",
	"content": "This sample demonstrates how to use the WebLogic Kubernetes Operator (hereafter \u0026ldquo;the operator\u0026rdquo;) to set up a WebLogic Server (WLS) cluster on the Azure Kubernetes Service (AKS) using the model in image domain home source type. After going through the steps, your WLS domain runs on an AKS cluster instance and you can manage your WLS domain by interacting with the operator.\nContents Prerequisites Prepare parameters Oracle Container Registry Sign in with Azure CLI Download the WebLogic Kubernetes Operator sample Create Resource Group Create the AKS cluster Install WebLogic Kubernetes Operator Create Docker image Image creation prerequisites Image creation - Introduction Understanding your first archive Staging a ZIP file of the archive Staging model files Creating the image with WIT Pushing the image to Azure Container Registry Create WebLogic domain Namespace Kubernetes Secrets for WebLogic image Kubernetes Secrets for WebLogic Domain resource Invoke the web application Create Azure load balancer Access the application Rolling updates Database connection Clean up resources Troubleshooting Useful links Prerequisites This sample assumes the following prerequisite environment.\nIf you don\u0026rsquo;t have an Azure subscription, create a free account before you begin. It\u0026rsquo;s recommended that the Azure identity you use to sign in and complete this article has either the Owner role in the current subscription or the Contributor and User Access Administrator roles in the current subscription. If your identity has very limited role assignments, ensure you have Contributor role and User Access Administrator role in the resource group that runs the AKS cluster. This requires asking a privileged user to assign the roles before creating resources in the resource group. Operating System: GNU/Linux, macOS (Intel only, Apple Silicon not supported), Windows Subsystem for Linux (WSL). Git; use git --version to test if git works. This document was tested with version 2.25.1. Azure CLI; use az --version to test if az works. This document was tested with version 2.58.0. Docker for Desktop. This document was tested with Docker version 20.10.7. kubectl; use kubectl version to test if kubectl works. This document was tested with version v1.21.2. Helm, version 3.1 and later; use helm version to check the helm version. This document was tested with version v3.6.2. A JDK, version 8 or 11. Azure recommends Microsoft Build of OpenJDK. Ensure that your JAVA_HOME environment variable is set correctly in the shells in which you run the commands. Ensure that you have the zip/unzip utility installed; use zip/unzip -v to test if zip/unzip works. You will need an Oracle account. Prepare parameters Set required parameters by running the following commands.\n# Change these parameters as needed for your own environment export ORACLE_SSO_EMAIL=\u0026lt;replace with your oracle account email\u0026gt; export ORACLE_SSO_PASSWORD=\u0026#34;\u0026lt;replace with your oracle password.\u0026gt;\u0026#34; export BASE_DIR=~ export NAME_PREFIX=wls # Used to generate resource names. export TIMESTAMP=`date +%s` export ACR_NAME=\u0026#34;acr${TIMESTAMP}\u0026#34; export AKS_CLUSTER_NAME=\u0026#34;aks${TIMESTAMP}\u0026#34; export AKS_PERS_RESOURCE_GROUP=\u0026#34;resourcegroup${TIMESTAMP}\u0026#34; export AKS_PERS_LOCATION=eastus export SECRET_NAME_DOCKER=\u0026#34;${NAME_PREFIX}regcred\u0026#34; export WEBLOGIC_USERNAME=weblogic export WEBLOGIC_PASSWORD=Secret123456 export WEBLOGIC_WDT_PASSWORD=Secret123456 Oracle Container Registry The following steps will direct you to accept the license agreement for WebLogic Server. Make note of your Oracle Account password and email. This sample pertains to 12.2.1.4, but other versions may work as well.\nIn a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them. The Oracle Container Registry provides a WebLogic 12.2.1.4 General Availability (GA) installation image that is used in this sample. In the Oracle Container Registry, navigate to Middleware, then weblogic. On the left, choose a language and accept the license agreement. You will then see a message such as: \u0026ldquo;You last accepted the Oracle Standard Terms and Restrictions on 08/10/2020 at 06:12 AM Coordinated Universal Time (UTC).\u0026rdquo; NOTE: General Availability (GA) images are suitable for demonstration and development purposes only where the environments are not available from the public Internet; they are not acceptable for production use. In production, you should always use CPU (patched) images from the OCR or create your images using the WebLogic Image Tool (WIT) with the --recommendedPatches option. For more guidance, see Apply the Latest Patches and Updates in Securing a Production Environment for Oracle WebLogic Server. Ensure that Docker is running. Find and pull the WebLogic 12.2.1.4 installation image: $ docker login container-registry.oracle.com -u ${ORACLE_SSO_EMAIL} -p ${ORACLE_SSO_PASSWORD} $ docker pull container-registry.oracle.com/middleware/weblogic:12.2.1.4 If you have problems accessing the Oracle Container Registry, you can build your own images from the Oracle GitHub repository.\nSign in with Azure CLI The steps in this section show you how to sign in to the Azure CLI.\nOpen a Bash shell.\nSign out and delete some authentication files to remove any lingering credentials.\n$ az logout $ rm ~/.azure/accessTokens.json $ rm ~/.azure/azureProfile.json Sign in to your Azure CLI.\n$ az login Set the subscription ID. Be sure to replace the placeholder with the appropriate value.\n$ export SUBSCRIPTION_ID=\u0026#34;\u0026lt;your-sub-id\u0026gt;\u0026#34; $ az account set -s $SUBSCRIPTION_ID Download the WebLogic Kubernetes Operator sample Download the WebLogic Kubernetes Operator sample ZIP file. We will use several scripts in this zip file to create a WebLogic domain. This sample was tested with v4.2.8, but should work with the latest release.\n$ cd $BASE_DIR $ mkdir sample-scripts $ curl -m 120 -fL https://github.com/oracle/weblogic-kubernetes-operator/releases/download/v4.2.8/sample-scripts.zip \\ -o ${BASE_DIR}/sample-scripts/sample-scripts.zip $ unzip ${BASE_DIR}/sample-scripts/sample-scripts.zip -d ${BASE_DIR}/sample-scripts Create Resource Group Create the resource group by issuing the following commands.\n$ az extension add --name resource-graph $ az group create --name $AKS_PERS_RESOURCE_GROUP --location $AKS_PERS_LOCATION Create the AKS cluster This sample doesn\u0026rsquo;t enable application routing. If you want to enable application routing, follow Managed nginx Ingress with the application routing add-on in AKS.\nRun the following command to create the AKS cluster.\n$ az aks create \\ --resource-group $AKS_PERS_RESOURCE_GROUP \\ --name $AKS_CLUSTER_NAME \\ --node-count 2 \\ --generate-ssh-keys \\ --nodepool-name nodepool1 \\ --node-vm-size Standard_DS2_v2 \\ --location $AKS_PERS_LOCATION \\ --enable-managed-identity Successful output will be a JSON object with the entry \u0026quot;type\u0026quot;: \u0026quot;Microsoft.ContainerService/ManagedClusters\u0026quot;.\nAfter the deployment finishes, run the following command to connect to the AKS cluster. This command updates your local ~/.kube/config so that subsequent kubectl commands interact with the named AKS cluster.\n$ az aks get-credentials --resource-group $AKS_PERS_RESOURCE_GROUP --name $AKS_CLUSTER_NAME Successful output will look similar to:\nMerged \u0026#34;wlsaks1596087429\u0026#34; as current context in /home/username/.kube/config After your Kubernetes cluster is up and running, run the following commands to make sure kubectl can access the Kubernetes cluster:\n$ kubectl get nodes -o wide Successful output will look like the following.\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME aks-nodepool1-15679926-vmss000000 Ready agent 118s v1.25.6 10.224.0.4 \u0026lt;none\u0026gt; Ubuntu 22.04.2 LTS 5.15.0-1041-azure containerd://1.7.1+azure-1 aks-nodepool1-15679926-vmss000001 Ready agent 2m8s v1.25.6 10.224.0.5 \u0026lt;none\u0026gt; Ubuntu 22.04.2 LTS 5.15.0-1041-azure containerd://1.7.1+azure-1 NOTE: If you run into VM size failure, see Troubleshooting - Virtual Machine size is not supported.\nInstall WebLogic Kubernetes Operator The WebLogic Kubernetes Operator is an adapter to integrate WebLogic Server and Kubernetes, allowing Kubernetes to serve as container infrastructure hosting WLS instances. The operator runs as a Kubernetes Pod and stands ready to perform actions related to running WLS on Kubernetes.\nCreate a namespace and service account for the operator.\n$ kubectl create namespace sample-weblogic-operator-ns The output will show something similar to the following:\nnamespace/sample-weblogic-operator-ns created $ kubectl create serviceaccount -n sample-weblogic-operator-ns sample-weblogic-operator-sa The output will show something similar to the following:\nserviceaccount/sample-weblogic-operator-sa created Validate the service account was created with this command.\n$ kubectl -n sample-weblogic-operator-ns get serviceaccount The output will show something similar to the following:\nNAME SECRETS AGE default 1 9m24s sample-weblogic-operator-sa 1 9m5s Install the operator. The operator’s Helm chart is located in the kubernetes/charts/weblogic-operator directory. This sample installs the operator using Helm charts from GitHub. It may take you several minutes to install the operator.\n$ helm repo add weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts --force-update Update the repo to get the latest Helm charts. It is a best practice to do this every time before installing a new operator version. In this example, we are using a pinned version, but you may also find success if you use the latest version. In this case, you can omit the --version argument. Be warned that these instructions have only been tested with the exact version shown.\n$ helm repo update $ helm install weblogic-operator weblogic-operator/weblogic-operator \\ --namespace sample-weblogic-operator-ns \\ --version 4.2.8 \\ --set serviceAccount=sample-weblogic-operator-sa \\ --wait The output will show something similar to the following:\nNAME: weblogic-operator LAST DEPLOYED: Fri Aug 12 14:28:47 2022 NAMESPACE: sample-weblogic-operator-ns STATUS: deployed REVISION: 1 TEST SUITE: None If you wish to use a more recent version of the operator, replace the 4.2.8 in the preceding command with the other version number. To see the list of versions, visit the GitHub releases page.\nVerify the operator with the following commands; the status will be Running.\n$ helm list -A The output will show something similar to the following:\nNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION weblogic-operator sample-weblogic-operator-ns 1 2023-05-15 10:31:05.1890341 +0800 CST deployeweblogic-operator-4.2.8 4.2.8 $ kubectl get pods -n sample-weblogic-operator-ns The output will show something similar to the following:\nNAME READY STATUS RESTARTS AGE weblogic-operator-54b5c8df46-g4rcm 1/1 Running 0 86s weblogic-operator-webhook-6c5885f69f-pd8qw 1/1 Running 0 86s You can specify the operator image by changing value of --set image. If you run into failures, see Troubleshooting - WebLogic Kubernetes Operator installation failure.\nIf you have an image built with domain models following Model in Image, you can go to Create WebLogic domain directly.\nCreate Docker image Image creation prerequisites The JAVA_HOME environment variable must be set and must reference a valid JDK 8 or 11 installation.\nCopy the sample to a new directory; for example, use the directory /tmp/mii-sample. In the directory name, mii is short for \u0026ldquo;model in image\u0026rdquo;. Model in image is one of three domain home source types supported by the operator. To learn more, see Choose a domain home source type.\n$ rm /tmp/mii-sample -f -r $ mkdir /tmp/mii-sample $ cp -r $BASE_DIR/sample-scripts/create-weblogic-domain/wdt-artifacts/* /tmp/mii-sample Save the model file directory.\n$ export WDT_MODEL_FILES_PATH=/tmp/mii-sample/wdt-model-files NOTE: We will refer to this working copy of the sample as /tmp/mii-sample; however, you can use a different location.\nDownload the latest WebLogic Deploying Tooling (WDT) and WebLogic Image Tool (WIT) installer ZIP files to your ${WDT_MODEL_FILES_PATH} directory. Both WDT and WIT are required to create your Model in Image images.\n$ curl -m 120 -fL https://github.com/oracle/weblogic-deploy-tooling/releases/latest/download/weblogic-deploy.zip \\ -o ${WDT_MODEL_FILES_PATH}/weblogic-deploy.zip $ curl -m 120 -fL https://github.com/oracle/weblogic-image-tool/releases/latest/download/imagetool.zip \\ -o ${WDT_MODEL_FILES_PATH}/imagetool.zip Set up the WebLogic Image Tool, run the following commands:\n$ unzip ${WDT_MODEL_FILES_PATH}/imagetool.zip -d ${WDT_MODEL_FILES_PATH} $ ${WDT_MODEL_FILES_PATH}/imagetool/bin/imagetool.sh cache deleteEntry --key wdt_latest $ ${WDT_MODEL_FILES_PATH}/imagetool/bin/imagetool.sh cache addInstaller \\ --type wdt \\ --version latest \\ --path ${WDT_MODEL_FILES_PATH}/weblogic-deploy.zip These steps will install WIT to the ${WDT_MODEL_FILES_PATH}/imagetool directory, plus put a wdt_latest entry in the tool’s cache which points to the WDT ZIP file installer. You will use WIT later in the sample for creating model images.\nImage creation - Introduction The goal of image creation is to demonstrate using the WebLogic Image Tool to create an image tagged as wdt-domain-image:WLS-v1 from files that you will stage to ${WDT_MODEL_FILES_PATH}/WLS-v1/.\nThe directory where the WebLogic Deploy Tooling software is installed (also known as WDT Home) is expected in an image’s /auxiliary/weblogic-deploy directory. WDT model YAML (model), WDT variable (property), and WDT archive ZIP (archive) files are expected in directory /auxiliary/models. Understanding your first archive See Understanding your first archive.\nStaging a ZIP file of the archive When you create the image, you will use the files in the staging directory, ${WDT_MODEL_FILES_PATH}/WLS-v1. In preparation, you need it to contain a ZIP file of the WDT application archive.\nRun the following commands to create your application archive ZIP file and put it in the expected directory:\n# Delete existing archive.zip in case we have an old leftover version $ rm -f ${WDT_MODEL_FILES_PATH}/WLS-v1/archive.zip Create a ZIP file of the archive in the location that we will use when we run the WebLogic Image Tool.\n$ cd /tmp/mii-sample/archives/archive-v1 $ zip -r ${WDT_MODEL_FILES_PATH}/WLS-v1/archive.zip wlsdeploy Staging model files In this step, you explore the staged WDT model YAML file and properties in the ${WDT_MODEL_FILES_PATH}/WLS-v1 directory. The model in this directory references the web application in your archive, configures a WebLogic Server Administration Server, and configures a WebLogic cluster. It consists of only two files, model.10.properties, a file with a single property, and, model.10.yaml, a YAML file with your WebLogic configuration.\nHere is the WLS model.10.properties:\nCLUSTER_SIZE=5 Here is the WLS model.10.yaml:\ndomainInfo: AdminUserName: \u0026#39;@@SECRET:__weblogic-credentials__:username@@\u0026#39; AdminPassword: \u0026#39;@@SECRET:__weblogic-credentials__:password@@\u0026#39; ServerStartMode: \u0026#39;prod\u0026#39; topology: Name: \u0026#39;@@ENV:CUSTOM_DOMAIN_NAME@@\u0026#39; AdminServerName: \u0026#39;admin-server\u0026#39; Cluster: \u0026#39;cluster-1\u0026#39;: DynamicServers: ServerTemplate: \u0026#39;cluster-1-template\u0026#39; ServerNamePrefix: \u0026#39;managed-server\u0026#39; DynamicClusterSize: \u0026#39;@@PROP:CLUSTER_SIZE@@\u0026#39; MaxDynamicClusterSize: \u0026#39;@@PROP:CLUSTER_SIZE@@\u0026#39; MinDynamicClusterSize: \u0026#39;0\u0026#39; CalculatedListenPorts: false Server: \u0026#39;admin-server\u0026#39;: ListenPort: 7001 ServerTemplate: \u0026#39;cluster-1-template\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 appDeployments: Application: myapp: SourcePath: \u0026#39;wlsdeploy/applications/myapp-v1\u0026#39; ModuleType: ear Target: \u0026#39;cluster-1\u0026#39; The model file:\nDefines a WebLogic domain with:\nCluster cluster-1 Administration Server admin-server An EAR application, targeted to cluster-1, located in the WDT archive ZIP file at wlsdeploy/applications/myapp-v1 Leverages macros to inject external values:\nThe property file CLUSTER_SIZE property is referenced in the model YAML file DynamicClusterSize and MaxDynamicClusterSize fields using a PROP macro. The model file domain name is injected using a custom environment variable named CUSTOM_DOMAIN_NAME using an ENV macro. You set this environment variable later in this sample using an env field in its Domain. This conveniently provides a simple way to deploy multiple differently named domains using the same model image. The model file administrator user name and password are set using a weblogic-credentials secret macro reference to the WebLogic credential secret. This secret is in turn referenced using the webLogicCredentialsSecret field in the Domain. The weblogic-credentials is a reserved name that always dereferences to the owning Domain actual WebLogic credentials secret name. A Model in Image image can contain multiple properties files, archive ZIP files, and YAML files but in this sample you use just one of each. For a complete description of Model in Images model file naming conventions, file loading order, and macro syntax, see Model files in the Model in Image user documentation.\nCreating the image with WIT At this point, you have all of the files needed for image wdt-domain-image:WLS-v1 staged; they include:\n/tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/model.10.yaml /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/model.10.properties /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/archive.zip Now, you use the Image Tool to create an image named wdt-domain-image:WLS-v1. You’ve already set up this tool during the prerequisite steps.\nRun the following command to create the image and verify that it worked.\n$ ${WDT_MODEL_FILES_PATH}/imagetool/bin/imagetool.sh createAuxImage \\ --tag wdt-domain-image:WLS-v1 \\ --wdtModel ${WDT_MODEL_FILES_PATH}/WLS-v1/model.10.yaml \\ --wdtVariables ${WDT_MODEL_FILES_PATH}/WLS-v1/model.10.properties \\ --wdtArchive ${WDT_MODEL_FILES_PATH}/WLS-v1/archive.zip This command runs the WebLogic Image Tool to create the domain creation image and does the following:\nBuilds the final container image as a layer on a small busybox base image. Copies the WDT ZIP file that\u0026rsquo;s referenced in the WIT cache into the image. Note that you cached WDT in WIT using the keyword latest when you set up the cache during the sample prerequisites steps. This lets WIT implicitly assume it\u0026rsquo;s the desired WDT version and removes the need to pass a -wdtVersion flag. Copies the specified WDT model, properties, and application archives to image location /auxiliary/models. When the command succeeds, it should end with output like the following:\n[INFO ] Build successful. Build time=70s. Image tag=wdt-domain-image:WLS-v1 Verify the image is available in the local Docker server with the following command.\n$ docker images | grep WLS-v1 The output will show something similar to the following:\nwdt-domain-image WLS-v1 012d3bfa3536 5 days ago 1.13GB The imagetool.sh is not supported on macOS with Apple Silicon. See Troubleshooting - exec format error.\nYou may run into a Dockerfile parsing error if your Docker buildkit is enabled, see Troubleshooting - WebLogic Image Tool failure.\nPushing the image to Azure Container Registry AKS can pull images from any container registry, but the easiest integration is to use Azure Container Registry (ACR). In addition to simplicity, using ACR simplifies high availability and disaster recovery with features such as geo-replication. For more information, see Geo-replication in Azure Container Registry. In this section, we will create a new Azure Container Registry, connect it to our pre-existing AKS cluster and push the image built in the preceding section to it. For complete details, see Azure Container Registry documentation.\nLet\u0026rsquo;s create an instance of ACR in the same resource group we used for AKS. We will use the environment variables used during the steps above. For simplicity, we use the resource group name as the name of the ACR instance.\n$ az acr create --resource-group $AKS_PERS_RESOURCE_GROUP --name $ACR_NAME --sku Basic --admin-enabled true Closely examine the JSON output from this command. Save the value of the loginServer property aside. It will look something like the following.\n\u0026#34;loginServer\u0026#34;: \u0026#34;contosoresourcegroup1610068510.azurecr.io\u0026#34;, Use this value to sign in to the ACR instance. Note that because you are signing in with the az CLI, you do not need a password because your identity is already conveyed by having done az login previously.\n$ export LOGIN_SERVER=$(az acr show -n $ACR_NAME --resource-group $AKS_PERS_RESOURCE_GROUP --query \u0026#34;loginServer\u0026#34; -o tsv) $ az acr login --name $LOGIN_SERVER Successful output will include Login Succeeded.\nEnsure Docker is running on your local machine. Run the following commands to tag and push the image to your ACR.\n$ docker tag wdt-domain-image:WLS-v1 $LOGIN_SERVER/mii-aks-auxiliary-image:1.0 $ docker push $LOGIN_SERVER/mii-aks-auxiliary-image:1.0 The output will show something similar to the following:\nThe push refers to repository [contosorgresourcegroup1610068510.azurecr.io/mii-aks-auxiliary-image] 1.0: digest: sha256:208217afe336053e4c524caeea1a415ccc9cc73b206ee58175d0acc5a3eeddd9 size: 2415 Finally, connect the AKS cluster to the ACR. For more details on connecting ACR to an existing AKS, see Configure ACR integration for existing AKS clusters.\n$ export ACR_ID=$(az acr show -n $ACR_NAME --resource-group $AKS_PERS_RESOURCE_GROUP --query \u0026#34;id\u0026#34; -o tsv) $ az aks update --name $AKS_CLUSTER_NAME --resource-group $AKS_PERS_RESOURCE_GROUP --attach-acr $ACR_ID Successful output will be a JSON object with the entry \u0026quot;type\u0026quot;: \u0026quot;Microsoft.ContainerService/ManagedClusters\u0026quot;.\nIf you see an error that seems related to you not being an Owner on this subscription, please refer to the troubleshooting section Cannot attach ACR due to not being Owner of subscription.\nCreate WebLogic domain In this section, you will deploy the new image to the namespace sample-domain1-ns, including the following steps:\nCreate a namespace for the WebLogic domain. Upgrade the operator to manage the WebLogic domain namespace. Create a secret containing your WebLogic administrator user name and password. Create a secret containing your Model in Image runtime encryption password: All Model in Image domains must supply a runtime encryption Secret with a password value. The runtime encryption password is used to encrypt configuration that is passed around internally by the operator. The value must be kept private but can be arbitrary; you can optionally supply a different secret value every time you restart the domain. Deploy a domain YAML file that references the new image. Wait for the domain’s pods to start and reach their ready state. Namespace Create a namespace that can host one or more domains:\n$ kubectl create namespace sample-domain1-ns Label the domain namespace so that the operator can autodetect and create WebLogic Server pods. Without this step, the operator cannot see the namespace.\n$ kubectl label namespace sample-domain1-ns weblogic-operator=enabled Kubernetes Secrets for WebLogic image You will use the kubernetes/samples/scripts/create-kubernetes-secrets/create-docker-credentials-secret.sh script to create the Docker credentials as a Kubernetes secret to pull image from OCR. Please run:\n$ $BASE_DIR/sample-scripts/create-kubernetes-secrets/create-docker-credentials-secret.sh \\ -n sample-domain1-ns \\ -s ${SECRET_NAME_DOCKER} \\ -e ${ORACLE_SSO_EMAIL} \\ -p ${ORACLE_SSO_PASSWORD} \\ -u ${ORACLE_SSO_EMAIL} The output will show something similar to the following:\nsecret/wlsregcred created The secret wlsregcred has been successfully created in the sample-domain1-ns namespace. Kubernetes Secrets for WebLogic First, create the secrets needed by the WLS type model domain. For more on secrets in the context of running domains, see Prepare to run a domain. In this case, you have two secrets.\nRun the following kubectl commands to deploy the required secrets:\n$ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-weblogic-credentials \\ --from-literal=username=\u0026#34;${WEBLOGIC_USERNAME}\u0026#34; \\ --from-literal=password=\u0026#34;${WEBLOGIC_PASSWORD}\u0026#34; $ kubectl -n sample-domain1-ns label secret \\ sample-domain1-weblogic-credentials \\ weblogic.domainUID=sample-domain1 $ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-runtime-encryption-secret \\ --from-literal=password=\u0026#34;${WEBLOGIC_WDT_PASSWORD}\u0026#34; $ kubectl -n sample-domain1-ns label secret \\ sample-domain1-runtime-encryption-secret \\ weblogic.domainUID=sample-domain1 Some important details about these secrets:\nMake sure to enclose your values in double quotes and perform the necessary escaping to prevent the shell from modifying the values before the secret values are set.\nChoosing passwords and user names:\nSet the variables WEBLOGIC_USERNAME and WEBLOGIC_PASSWORD with a user name and password of your choice. The password should be at least eight characters long and include at least one digit. Remember what you specified. These credentials may be needed again later. Set the variable WEBLOGIC_WDT_PASSWORD with a password of your choice. The WebLogic credentials secret:\nIt is required and must contain username and password fields. It must be referenced by the spec.webLogicCredentialsSecret field in your Domain resource YAML file. For complete details about the Domain resource, see the Domain resource reference. It also must be referenced by macros in the domainInfo.AdminUserName and domainInfo.AdminPassWord fields in your model.10.yaml file. The Model WDT runtime encryption secret:\nThis is a special secret required by Model in Image. It must contain a password field. It must be referenced using the spec.model.runtimeEncryptionSecret field in your Domain resource YAML file. It must remain the same for as long as the domain is deployed to Kubernetes but can be changed between deployments. It is used to encrypt data as it\u0026rsquo;s internally passed using log files from the domain\u0026rsquo;s introspector job and on to its WebLogic Server pods. Deleting and recreating the secrets:\nYou must delete a secret before creating it, otherwise the create command will fail if the secret already exists. This allows you to change the secret when using the kubectl create secret command. You name and label secrets using their associated domainUID for two reasons:\nTo make it obvious which secrets belong to which domains. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all resources associated with a domain. Now, you can verify the secrets with command:\nkubectl get secrets -n sample-domain1-ns The output looks similar to the following content.\nNAME TYPE DATA AGE sample-domain1-runtime-encryption-secret Opaque 1 19s sample-domain1-weblogic-credentials Opaque 2 28s wlsregcred kubernetes.io/dockerconfigjson 1 47s Domain resource Now, you create a domain YAML file. Think of the domain YAML file as the way to configure some aspects of your WebLogic domain using Kubernetes. The operator uses the Kubernetes \u0026ldquo;custom resource\u0026rdquo; feature to define a Kubernetes resource type called Domain. For more on the Domain Kubernetes resource, see Domain Resource. For more on custom resources see the Kubernetes documentation.\nWe provide a script at $BASE_DIR/sample-scripts/create-weblogic-domain-on-azure-kubernetes-service/create-domain-on-aks-mii-generate-yaml.sh to generate a domain resource description.\nRun the following command to generate resource files.\nexport Domain_Creation_Image_tag=\u0026#34;$LOGIN_SERVER/mii-aks-auxiliary-image:1.0\u0026#34; $ cd $BASE_DIR $ bash $BASE_DIR/sample-scripts/create-weblogic-domain-on-azure-kubernetes-service/create-domain-on-aks-mii-generate-yaml.sh After running above commands, you will get three files: mii-initial.yaml, admin-lb.yaml and cluster-lb.yaml.\nRun the following command to create the domain custom resource:\n$ kubectl apply -f mii-initial.yaml Successful output will look like:\ndomain.weblogic.oracle/sample-domain1 created cluster.weblogic.oracle/sample-domain1-cluster-1 created Verify the WebLogic Server pods are all running:\n$ kubectl get pods -n sample-domain1-ns --watch Output will look similar to the following.\nNAME READY STATUS RESTARTS AGE sample-domain1-introspector-xwpbn 0/1 ContainerCreating 0 0s sample-domain1-introspector-xwpbn 1/1 Running 0 1s sample-domain1-introspector-xwpbn 0/1 Completed 0 66s sample-domain1-introspector-xwpbn 0/1 Terminating 0 67s sample-domain1-introspector-xwpbn 0/1 Terminating 0 67s sample-domain1-admin-server 0/1 Pending 0 0s sample-domain1-admin-server 0/1 Pending 0 0s sample-domain1-admin-server 0/1 ContainerCreating 0 0s sample-domain1-admin-server 0/1 Running 0 2s sample-domain1-admin-server 1/1 Running 0 42s sample-domain1-managed-server1 0/1 Pending 0 0s sample-domain1-managed-server1 0/1 Pending 0 0s sample-domain1-managed-server1 0/1 ContainerCreating 0 0s sample-domain1-managed-server2 0/1 Pending 0 0s sample-domain1-managed-server2 0/1 Pending 0 0s sample-domain1-managed-server2 0/1 ContainerCreating 0 0s sample-domain1-managed-server2 0/1 Running 0 3s sample-domain1-managed-server2 1/1 Running 0 40s sample-domain1-managed-server1 0/1 Running 0 53s sample-domain1-managed-server1 1/1 Running 0 93s When the system stabilizes with the following state, it is safe to proceed.\nNAME READY STATUS RESTARTS AGE sample-domain1-admin-server 1/1 Running 0 2m sample-domain1-managed-server1 1/1 Running 0 83s sample-domain1-managed-server2 1/1 Running 0 83s It may take you up to 10 minutes to deploy all pods, please wait and make sure everything is ready.\nIf the system does not reach this state, troubleshoot and resolve the problem before continuing. See Troubleshooting for hints.\nInvoke the web application Create Azure load balancer Create an Azure public standard load balancer to access the WebLogic Server Administration Console and applications deployed to the cluster.\nUse the file admin-lb.yaml to create a load balancer service for the Administration Server. If you are choosing not to use the predefined YAML file and instead created a new one with customized values, then substitute the following content with your domain values.\nClick here to view YAML content. apiVersion: v1 kind: Service metadata: name: sample-domain1-admin-server-external-lb namespace: sample-domain1-ns spec: ports: - name: default port: 7001 protocol: TCP targetPort: 7001 selector: weblogic.domainUID: sample-domain1 weblogic.serverName: admin-server sessionAffinity: None type: LoadBalancer Use the file cluster-lb.yaml to create a load balancer service for the managed servers. If you are choosing not to use the predefined YAML file and instead created new one with customized values, then substitute the following content with your domain values.\nClick here to view YAML content. apiVersion: v1 kind: Service metadata: name: sample-domain1-cluster-1-lb namespace: sample-domain1-ns spec: ports: - name: default port: 8001 protocol: TCP targetPort: 8001 selector: weblogic.domainUID: sample-domain1 weblogic.clusterName: cluster-1 sessionAffinity: None type: LoadBalancer Create the load balancer services using the following commands:\n$ kubectl apply -f admin-lb.yaml Successful output will look like:\nservice/sample-domain1-admin-server-external-lb created $ kubectl apply -f cluster-lb.yaml Successful output will look like:\nservice/sample-domain1-cluster-1-external-lb created Get the external IP addresses of the Administration Server and cluster load balancers (please wait for the external IP addresses to be assigned):\n$ kubectl get svc -n sample-domain1-ns --watch Successful output will look like:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE sample-domain1-admin-server ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 8m33s sample-domain1-admin-server-external-lb LoadBalancer 10.0.184.118 52.191.234.149 7001:30655/TCP 2m30s sample-domain1-cluster-1-lb LoadBalancer 10.0.76.7 52.191.235.71 8001:30439/TCP 2m25s sample-domain1-cluster-cluster-1 ClusterIP 10.0.118.225 \u0026lt;none\u0026gt; 8001/TCP 7m53s sample-domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 7m53s sample-domain1-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 7m52s In the example, the URL to access the Administration Server is: http://52.191.234.149:7001/console. The expected username and password must match the values that you chose during the Kubernetes Secrets for WebLogic step.\nIMPORTANT: You must ensure that any Network Security Group rules that govern access to the console allow inbound traffic on port 7001.\nIf the WLS Administration Console is still not available, use kubectl describe domain to check domain status.\n$ kubectl describe domain domain1 Make sure the status of cluster-1 is ServersReady and Available.\nClick here to view example domain status. Name: sample-domain1 Namespace: sample-domain1-ns Labels: weblogic.domainUID=sample-domain1 Annotations: \u0026lt;none\u0026gt; API Version: weblogic.oracle/v9 Kind: Domain Metadata: Creation Timestamp: 2020-11-30T05:40:11Z Generation: 1 Resource Version: 9346 Self Link: /apis/weblogic.oracle/v9/namespaces/sample-domain1-ns/domains/sample-domain1 UID: 9f10a602-714a-46c5-8dcb-815616b587af Spec: Admin Server: Server Start State: RUNNING Clusters: Cluster Name: cluster-1 Replicas: 2 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Start State: RUNNING Configuration: Model: Domain Type: WLS Runtime Encryption Secret: sample-domain1-runtime-encryption-secret Domain Home: /u01/domains/sample-domain1 Domain Home Source Type: FromModel Image: docker.io/sleepycat2/wls-on-aks:model-in-image Image Pull Policy: IfNotPresent Image Pull Secrets: Name: regsecret Include Server Out In Pod Log: true Replicas: 1 Restart Version: 1 Server Pod: Env: Name: CUSTOM_DOMAIN_NAME Value: domain1 Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx512m Resources: Requests: Cpu: 250m Memory: 768Mi Server Start Policy: IfNeeded Web Logic Credentials Secret: Name: sample-domain1-weblogic-credentials Status: Clusters: Cluster Name: cluster-1 Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 2 Replicas: 2 Replicas Goal: 2 Conditions: Last Transition Time: 2020-11-30T05:45:15.493Z Reason: ServersReady Status: True Type: Available Introspect Job Failure Count: 0 Replicas: 2 Servers: Desired State: RUNNING Health: Activation Time: 2020-11-30T05:44:15.652Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: aks-pool1model-71528953-vmss000001 Server Name: admin-server State: RUNNING Cluster Name: cluster-1 Desired State: RUNNING Health: Activation Time: 2020-11-30T05:44:54.699Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: aks-pool1model-71528953-vmss000000 Server Name: managed-server1 State: RUNNING Cluster Name: cluster-1 Desired State: RUNNING Health: Activation Time: 2020-11-30T05:45:07.211Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: aks-pool1model-71528953-vmss000001 Server Name: managed-server2 State: RUNNING Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server3 Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server4 Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server5 Start Time: 2020-11-30T05:40:11.709Z Events: \u0026lt;none\u0026gt; Access the application Access the Administration Console using the admin load balancer IP address.\n$ ADMIN_SERVER_IP=$(kubectl -n sample-domain1-ns get svc sample-domain1-admin-server-external-lb -o=jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) $ echo \u0026#34;Administration Console Address: http://${ADMIN_SERVER_IP}:7001/console/\u0026#34; Access the sample application using the cluster load balancer IP address.\n## Access the sample application using the cluster load balancer IP. $ CLUSTER_IP=$(kubectl -n sample-domain1-ns get svc sample-domain1-cluster-1-lb -o=jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) $ curl http://${CLUSTER_IP}:8001/myapp_war/index.jsp Successful output will look like:\n\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v1\u0026#39; of the sample JSP web-app. Welcome to WebLogic Server \u0026#39;managed-server1\u0026#39;! domain UID = \u0026#39;sample-domain1\u0026#39; domain name = \u0026#39;domain1\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found min threads constraint runtime named \u0026#39;SampleMinThreads\u0026#39; with configured count: 1 Found max threads constraint runtime named \u0026#39;SampleMaxThreads\u0026#39; with configured count: 10 Found 0 local data sources: ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; Rolling updates Naturally, you will want to deploy newer versions of the EAR application, located in the WDT archive ZIP file at wlsdeploy/applications/myapp-v1. To learn how to do this, follow the steps in Update 3.\nDatabase connection For guidance on how to connect a database to your AKS with WebLogic Server application, see Deploy a Java application with WebLogic Server on an Azure Kubernetes Service (AKS) cluster.\nClean up resources Run the following commands to clean up resources.\n$ az group delete --yes --no-wait --name $AKS_PERS_RESOURCE_GROUP Troubleshooting For troubleshooting advice, see Troubleshooting.\nUseful links Model in Image user documentation Model in Image sample Deploy a Java application with WebLogic Server on an Azure Kubernetes Service (AKS) cluster "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/azure-kubernetes-service/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "Troubleshooting.",
	"content": " Access Administration Console: Possible causes for Administration Console inaccessibility Domain debugging Pod Error: How to get details of the pod error WebLogic Image Tool failure WebLogic Kubernetes Operator installation failure System pods are pending WebLogic Kubernetes Operator ErrImagePull WSL2 bad timestamp Cannot attach ACR due to not being Owner of subscription Virtual Machine size is not supported Get pod error details You may get the following message while creating the WebLogic domain: \u0026quot;the job status is not Completed!\u0026quot;\nstatus on iteration 20 of 20 pod domain1-create-weblogic-sample-domain-job-nj7wl status is Init:0/1 The create domain job is not showing status completed after waiting 300 seconds. Check the log output for errors. Error from server (BadRequest): container \u0026#34;create-weblogic-sample-domain-job\u0026#34; in pod \u0026#34;domain1-create-weblogic-sample-domain-job-nj7wl\u0026#34; is waiting to start: PodInitializing [ERROR] Exiting due to failure - the job status is not Completed! You can get further error details by running kubectl describe pod, as shown here:\n$ kubectl describe pod \u0026lt;your-pod-name\u0026gt; This is an output example:\n$ kubectl describe pod domain1-create-weblogic-sample-domain-job-nj7wl Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 4m2s default-scheduler Successfully assigned default/domain1-create-weblogic-sample-domain-job-qqv6k to aks-nodepool1-58449474-vmss000001 Warning FailedMount 119s kubelet, aks-nodepool1-58449474-vmss000001 Unable to mount volumes for pod \u0026#34;domain1-create-weblogic-sample-domain-job-qqv6k_default(15706980-73cb-11ea-b804-b2c91b494b00)\u0026#34;: timeout expired waiting for volumes to attach or mount for pod \u0026#34;default\u0026#34;/\u0026#34;domain1-create-weblogic-sample-domain-job-qqv6k\u0026#34;. list of unmounted volumes=[weblogic-sample-domain-storage-volume]. list of unattached volumes=[create-weblogic-sample-domain-job-cm-volume weblogic-sample-domain-storage-volume weblogic-credentials-volume default-token-zr7bq] Warning FailedMount 114s (x9 over 4m2s) kubelet, aks-nodepool1-58449474-vmss000001 MountVolume.SetUp failed for volume \u0026#34;wls-azurefile\u0026#34; : Couldn\u0026#39;t get secret default/azure-secrea Fail to access Administration Console Here are some common reasons for this failure, along with some tips to help you investigate.\nCreate WebLogic domain job fails\nCheck the deploy log and find the failure details with kubectl describe pod podname. Please go Getting pod error details.\nProcess of starting the servers is still running\nCheck with kubectl get svc and if domainUID-admin-server, domainUID-managed-server1, and domainUID-managed-server2 are not listed, we need to wait some more for the Administration Server to start.\nThe following output is an example of when the Administration Server has started.\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE domain1-admin-server ClusterIP None \u0026lt;none\u0026gt; 30012/TCP,7001/TCP 7m3s domain1-admin-server-ext NodePort 10.0.78.211 \u0026lt;none\u0026gt; 7001:30701/TCP 7m3s domain1-admin-server-external-lb LoadBalancer 10.0.6.144 40.71.233.81 7001:32758/TCP 7m32s domain1-cluster-1-lb LoadBalancer 10.0.29.231 52.142.39.152 8001:31022/TCP 7m30s domain1-cluster-cluster-1 ClusterIP 10.0.80.134 \u0026lt;none\u0026gt; 8001/TCP 1s domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 1s domain1-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 1s internal-weblogic-operator-svc ClusterIP 10.0.1.23 \u0026lt;none\u0026gt; 8082/TCP 9m59s kubernetes ClusterIP 10.0.0.1 \u0026lt;none\u0026gt; 443/TCP 16m If services are up but the WLS Administration Console is still not available, use kubectl describe domain to check domain status.\n$ kubectl describe domain domain1 Make sure the status of cluster-1 is ServersReady and Available. The status of admin-server, managed-server1, and managed-server2 should be RUNNING. Otherwise, the cluster is likely still in the process of becoming fully ready.\nClick here to view the example status. Status: Clusters: Cluster Name: cluster-1 Maximum Replicas: 5 Minimum Replicas: 1 Ready Replicas: 2 Replicas: 2 Replicas Goal: 2 Conditions: Last Transition Time: 2020-07-06T05:39:32.539Z Reason: ServersReady Status: True Type: Available Replicas: 2 Servers: Desired State: RUNNING Node Name: aks-nodepool1-11471722-vmss000001 Server Name: admin-server State: RUNNING Cluster Name: cluster-1 Desired State: RUNNING Node Name: aks-nodepool1-11471722-vmss000001 Server Name: managed-server1 State: RUNNING Cluster Name: cluster-1 Desired State: RUNNING Node Name: aks-nodepool1-11471722-vmss000001 Server Name: managed-server2 State: RUNNING Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server3 Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server4 Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server5 Domain debugging For some suggestions for debugging problems with Model in Image after your Domain YAML file is deployed, see Debugging.\nWSL2 bad timestamp If you are running with WSL2, you may run into the bad timestamp issue, which blocks Azure CLI. You may see the following error:\n$ kubectl get pod Unable to connect to the server: x509: certificate has expired or is not yet valid: current time 2020-11-25T15:58:10+08:00 is before 2020-11-27T04:25:04Z You can run the following command to update WSL2 system time:\n# Fix the outdated systime time $ sudo hwclock -s # Check systime time $ data Fri Nov 27 13:07:14 CST 2020 Timeout for the operator installation You may run into a timeout while installing the operator and get the following error:\n$ helm install weblogic-operator weblogic-operator/weblogic-operator \\ --namespace sample-weblogic-operator-ns \\ --set serviceAccount=sample-weblogic-operator-sa \\ --wait Error: timed out waiting for the condition Make sure you are working with the main branch. Remove the operator and install again.\n$ helm uninstall weblogic-operator -n sample-weblogic-operator-ns release \u0026#34;weblogic-operator\u0026#34; uninstalled Check out main and install the operator.\n$ cd weblogic-kubernetes-operator $ git checkout main $ helm install weblogic-operator weblogic-operator/weblogic-operator \\ --namespace sample-weblogic-operator-ns \\ --set serviceAccount=sample-weblogic-operator-sa \\ --wait WebLogic Image Tool failure If your version of WIT is older than 1.9.8, you will get an error running ./imagetool/bin/imagetool.sh if the Docker buildkit is enabled.\nHere is the warning message shown:\nfailed to solve with frontend dockerfile.v0: failed to create LLB definition: failed to parse stage name \u0026#34;WDT_BUILD\u0026#34;: invalid reference format: repository name must be lowercase To resolve the error, either upgrade to a newer version of WIT or disable the Docker buildkit with the following commands and run the imagetool command again.\n$ export DOCKER_BUILDKIT=0 $ export COMPOSE_DOCKER_CLI_BUILD=0 WebLogic Kubernetes Operator installation failure Currently, we meet two cases that block the operator installation:\nThe system pods in the AKS cluster are pending. The operator image is unavailable. Follow these steps to dig into the error.\nThe AKS cluster system pods are pending If system pods in the AKS cluster are pending, it will block the operator installation.\nThis is an error example with warning message no nodes available to schedule pods.\n$ kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE default weblogic-operator-c5c78b8b5-ssvqk 0/1 Pending 0 13m kube-system coredns-79766dfd68-wcmkd 0/1 Pending 0 3h22m kube-system coredns-autoscaler-66c578cddb-tc946 0/1 Pending 0 3h22m kube-system dashboard-metrics-scraper-6f5fb5c4f-9f5mb 0/1 Pending 0 3h22m kube-system kubernetes-dashboard-849d5c99ff-xzknj 0/1 Pending 0 3h22m kube-system metrics-server-7f5b4f6d8c-bqzrn 0/1 Pending 0 3h22m kube-system tunnelfront-765bf6df59-msj27 0/1 Pending 0 3h22m sample-weblogic-operator-ns weblogic-operator-f86b879fd-v2xrz 0/1 Pending 0 35m $ kubectl describe pod weblogic-operator-f86b879fd-v2xrz -n sample-weblogic-operator-ns Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 71s (x25 over 36m) default-scheduler no nodes available to schedule pods If you run into this error, remove the AKS cluster and create a new one.\nRun the kubectl get pod -A to make sure all the system pods are running.\n$ kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-79766dfd68-ch5b9 1/1 Running 0 3h44m kube-system coredns-79766dfd68-sxk4g 1/1 Running 0 3h43m kube-system coredns-autoscaler-66c578cddb-s5qm5 1/1 Running 0 3h44m kube-system dashboard-metrics-scraper-6f5fb5c4f-wtckh 1/1 Running 0 3h44m kube-system kube-proxy-fwll6 1/1 Running 0 3h42m kube-system kube-proxy-kq6wj 1/1 Running 0 3h43m kube-system kube-proxy-t2vbb 1/1 Running 0 3h43m kube-system kubernetes-dashboard-849d5c99ff-hrz2w 1/1 Running 0 3h44m kube-system metrics-server-7f5b4f6d8c-snnbt 1/1 Running 0 3h44m kube-system omsagent-8tf4j 1/1 Running 0 3h43m kube-system omsagent-n9b7k 1/1 Running 0 3h42m kube-system omsagent-rcmgr 1/1 Running 0 3h43m kube-system omsagent-rs-787ff54d9d-w7tp5 1/1 Running 0 3h44m kube-system tunnelfront-794845c84b-v9f98 1/1 Running 0 3h44m WebLogic Kubernetes Operator ErrImagePull If you got an error of ErrImagePull from pod status, use docker pull to check the operator image. If an error occurs, you can switch to a version that is greater than 3.1.1.\n$ docker pull ghcr.io/oracle/weblogic-kubernetes-operator:\u0026lt;version\u0026gt; # Example: pull 3.1.1. $ docker pull ghcr.io/oracle/weblogic-kubernetes-operator:3.1.1 3.1.1: Pulling from oracle/weblogic-kubernetes-operator 980316e41237: Pull complete c980371d97ea: Pull complete db19c8ff0d12: Pull complete 550f44317ae5: Pull complete be2e701f5ee0: Pull complete 1cb891615559: Pull complete 4f4fb700ef54: Pull complete Digest: sha256:6b060ec1989fcb26e1acb0d0b906d81fce6b8ec2e0a30fa2b9d9099290eb6416 Status: Downloaded newer image for ghcr.io/oracle/weblogic-kubernetes-operator:3.1.1 ghcr.io/oracle/weblogic-kubernetes-operator:3.1.1 Cannot attach ACR due to not being Owner of subscription If you\u0026rsquo;re unable to create an ACR and you\u0026rsquo;re using a service principal, you can use manual Role Assignments to grant access to the ACR as described in Azure Container Registry authentication with service principals.\nFirst, find the objectId of the service principal used when the AKS cluster was created. You will need the output from az ad sp create-for-rbac, which you were directed to save to a file. Within the output, you need the value of the name property. It will start with http. Get the objectId with this command.\n$ az ad sp show --id http://\u0026lt;your-name-from-the-saved-output\u0026gt; | grep objectId \u0026#34;objectId\u0026#34;: \u0026#34;nror4p30-qnoq-4129-o89r-p60n71805npp\u0026#34;, Next, assign the acrpull role to that service principal with this command.\n$ az role assignment create --assignee-object-id \u0026lt;your-objectId-from-above\u0026gt; --scope $AKS_PERS_RESOURCE_GROUP --role acrpull { \u0026#34;canDelegate\u0026#34;: null, \u0026#34;condition\u0026#34;: null, ... \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Authorization/roleAssignments\u0026#34; } After you do this, re-try the command that gave the error.\nVirtual Machine size is not supported If you run into the following error when creating the AKS cluster, please use an available VM size in the region.\n$ az aks create \\ --resource-group $AKS_PERS_RESOURCE_GROUP \\ --name $AKS_CLUSTER_NAME \\ --node-count 2 \\ --generate-ssh-keys \\ --nodepool-name nodepool1 \\ --node-vm-size Standard_DS2_v2 \\ --location $AKS_PERS_LOCATION \\ --service-principal $SP_APP_ID \\ --client-secret $SP_CLIENT_SECRET BadRequestError: Operation failed with status: \u0026#39;Bad Request\u0026#39;. Details: Virtual Machine size: \u0026#39;Standard_DS2_v2\u0026#39; is not supported for subscription subscription-id in location \u0026#39;eastus\u0026#39;. The available VM sizes are \u0026#39;basic_a0,basic_a1,basic_a2,basic_a3,basic_a4,standard_a2\u0026#39;. Please refer to aka.ms/aks-vm-sizes for the details. ResourceNotFoundError: The Resource \u0026#39;Microsoft.ContainerService/managedClusters/wlsaks1613726008\u0026#39; under resource group \u0026#39;wlsresourcegroup1613726008\u0026#39; was not found. For more details please go to https://aka.ms/ARMResourceNotFoundFix As shown in the example, you can use standard_a2; pay attention to the CPU and memory of that size; make sure it meets your memory requirements.\nexec /weblogic-operator/scripts/introspectDomain.sh: exec format error This error happens if you run the imagetool.sh command to build the Docker image under macOS with Apple Silicon. This is not supported. See this issue in the WebLogic Image Tool issue tracker.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/faq/boot-identity-not-valid/",
	"title": "Boot identity not valid",
	"tags": [],
	"description": "One or more WebLogic Server instances in my domain will not start and I see errors in the server log like this: `Boot identity not valid`.",
	"content": "One or more WebLogic Server instances in my domain will not start and I see errors in the server log like this:\n***\u0026lt;Feb 6, 2020 12:05:35,550 AM GMT\u0026gt; \u0026lt;Critical\u0026gt; \u0026lt;Security\u0026gt; \u0026lt;BEA-090402\u0026gt; \u0026lt;Authentication denied: Boot identity not valid. The user name or password or both from the boot identity file (boot.properties) is not valid. The boot identity may have been changed since the boot identity file was created. Please edit and update the boot identity file with the proper values of username and password. The first time the updated boot identity file is used to start the server, these new values are encrypted.\u0026gt;*** When you see these kinds of errors, it typically means that the user name and password provided in the weblogicCredentialsSecret are incorrect. Prior to operator version 2.5.0, this error could have also indicated that the WebLogic domain directory\u0026rsquo;s security configuration files have changed in an incompatible way between when the operator scanned the domain directory, which occurs during the \u0026ldquo;introspection\u0026rdquo; phase, and when the server instance attempted to start. There is now a separate validation for that condition described in the Domain secret mismatch FAQ entry.\nCheck that the user name and password credentials stored in the Kubernetes Secret, referenced by weblogicCredentialsSecret, contain the expected values for an account with administrative privilege for the WebLogic domain. Then stop all WebLogic Server instances in the domain before restarting so that the operator will repeat its introspection and generate the corrected boot.properties files.\nIf the Boot identity not valid error is still not resolved, and the error is logged on a WebLogic Managed Server, then the server may be failing to contact the Administration Server. Check carefully for any errors or exceptions logged before the Boot identity not valid error in the Managed Server log file and look for indications of communication failure.\nFor example:\nIf you have enabled SSL for the Administration Server default channel or have configured an administration port on the Administration Server, and the SSL port is using the demo identity certificates, then a Managed Server may fail to establish an SSL connection to the Administration Server due to a hostname verification exception (such as the SSLKeyException: Hostname verification failed). For non-production environments, you can turn off the hostname verification by setting the -Dweblogic.security.SSL.ignoreHostnameVerification=true property in the Java options for starting the WebLogic Server.\nNOTE: Turning off hostname verification leaves WebLogic Server vulnerable to man-in-the-middle attacks. Oracle recommends leaving hostname verification on in production environments.\nFor other SSL-related errors, make sure the keystores and passwords are specified correctly in the Java options for starting the WebLogic Server. See Configuring SSL for details on configuring SSL in the Oracle WebLogic Server environment.\nIf the domain has Istio enabled and Istio is setup incorrectly, then you may see Info, Alert, Warning, or Error messages such as \u0026lt;BEA-141298\u0026gt; \u0026lt;Could not register with the Administration Server: java.rmi.RemoteException: [Deployer:149150]An IOException occurred while reading the input.; nested exception is: java.io.EOFException: Response had end of stream after 0 bytes\u0026gt; or \u0026lt;BEA-141151\u0026gt; \u0026lt;The Administration Server could not be reached at http://my-domain-admin-server:7011.\u0026gt;. Make sure the domain is configured correctly for your Istio version. In particular, make sure spec.configuration.istio.localHostBindingsEnabled is set correctly. See Istio.\nIf the DNS hostname of the Administration Server can\u0026rsquo;t be resolved during Managed Server startup with errors such as java.net.UnknownHostException: domain1-admin-server, then check the DNS server pod logs for any errors and take corrective actions to resolve those errors.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/persistent-storage/volumes/",
	"title": "Provide access to a PersistentVolumeClaim",
	"tags": [],
	"description": "Provide an instance with access to a PersistentVolumeClaim.",
	"content": "Some applications need access to a file, either to read data or to provide additional logging beyond what is built into the operator. One common way of doing that within Kubernetes is to create a PersistentVolumeClaim (PVC) and map it to a file. The domain configuration can then be used to provide access to the claim across the domain, within a single cluster, or for a single server. In each case, the access is configured within the serverPod element of the configuration of the desired scope.\nFor example, here is a read-only PersistentVolumeClaim specification. Note that its name is myclaim.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: accessModes: - ReadOnlyMany volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: slow To provide access to this claim to all Managed Servers in the cluster-1 cluster, specify the following in your Domain:\nclusters: - clusterName: cluster-1 serverPod: volumes: - name: my-volume-1 persistentVolumeClaim: claimName: myclaim volumeMounts: - name: my-volume-1 mountPath: /weblogic-operator/my/volume1 Note the use of the claim name in the claimName field of the volume entry. Both a volume and a volumeMount entry are required, and must have the same name. The volume entry associates that name with the claim, while the volumeMount entry defines the path to it that the application can use to access the file.\nNOTE: If the PVC is mapped either across the domain or to a cluster, its access mode must be either ReadOnlyMany or ReadWriteMany.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/cicd/choose-an-approach/",
	"title": "Choose an approach",
	"tags": [],
	"description": "How to choose an approach.",
	"content": "Let\u0026rsquo;s review what we have discussed and talk about when we might want to use various approaches. We can start by asking ourselves questions like these:\nCan you make the desired change with a configuration override or Model in Image ConfigMap?\nWhen your domain home source type is Domain on PV or Domain in Image, the operator allows you to inject a number of configuration overrides into your pods before starting any servers in the domain.\nWhen your domain home source type is Model in Image, you can inject model updates before starting any servers in the domain for any type of update, or even while your domain is running for most types of updates. Model in Image runtime model updates are propagated by restarting (rolling) the running WebLogic Servers.\nA good example of a change would be changing the settings for a data source. For example, you may wish to have a larger connection pool in your production environment than you do in your development/test environments. You probably also want to have different credentials. You may want to change the service name, and so on. All of these kinds of updates can be made with configuration overrides for Domain on PV and Domain in Image, and with model updates for Model in Image. These are placed in a Kubernetes ConfigMap, meaning that they are outside of the image, so they do not require rebuilding the image. If all of your changes fit into this category, it is probably much better to just use configuration overrides for Domain on PV and Domain in Image, and use model updates for Model in Image.\nAre you only changing the WebLogic configuration, for example, deploying or updating an application, changing a resource configuration in a way that is not supported by configuration overrides, Model in Image model updates, and such?\nIf your changes fit into this category, and you have used the \u0026ldquo;domain-in-image\u0026rdquo; approach and the image layering model, then you only need to update the top layer of your image. This is relatively easy compared to making changes in lower layers. You could create a new layer with the changes, or you could rebuild/replace the existing top layer with a new one. Which approach you choose depends mainly on whether you need to maintain the same domain encryption keys or not.\nDo you need to be able to do a rolling restart?\nIf you need to do a rolling restart with Domain in Image, for example, to maintain the availability of your applications, then you need to make sure that a new domain layer has the same domain encryption keys. You cannot perform a rolling restart of a domain if the new members have a different encryption key.\nIf you need to do a rolling restart with Model in Image or Domain on PV, the domain encryption keys are not a concern. In Model in Image, the keys are generated by the operator at runtime before the first WebLogic Server pod is started. In Domain on PV, the keys are generated once, the first time the domain is started, and remain in the domain home within the PV.\nDo you need to mutate something in a lower layer, for example, patch WebLogic, the JDK, or Linux?\nIf you need to make an update in a lower layer, then you will need to rebuild that layer and all of the layers above it. This means that you will need to rebuild the domain layer. In the case of Domain in Image, you will need to determine if you need to keep the same domain encryption keys.\nThe following diagram summarizes these concerns in a decision tree for the “domain-in-image” case:\nIf you are using the \u0026ldquo;domain-on-PV\u0026rdquo; or \u0026ldquo;model-in-image\u0026rdquo; approach, many of these concerns become moot because you have an effective separation between your domain and the image. There is still the possibility that an update in the image could affect your domain; for example, if you updated the JDK, you may need to update some of your domain scripts to reflect the new JDK path.\nHowever, in this scenario, your environment is much closer to what you are probably used to in a traditional (non-Kubernetes) environment, and you will probably find that all of the practices you used from that pre-Kubernetes environment are directly applicable here too, with just some small modifications. For example, applying a WebLogic patch would now involve building a new image.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/security/domain-security/weblogic-channels/",
	"title": "External network access security",
	"tags": [],
	"description": "Remote access security.",
	"content": "WebLogic T3 and administrative channels Oracle recommends not exposing any administrative, RMI, or T3 channels outside the Kubernetes cluster unless absolutely necessary.\nIf exposing an administrative, RMI, EJB, JMS, or T3 capable channel using a load balancer, port forwarding, NodePorts, or similar, then limit access by using a custom dedicated WebLogic Server port that you have configured with the T3 or administration protocol (a network access point) instead of relaying the traffic to a default port, leverage two-way SSL, use controls like security lists, and/or set up a Bastion to provide access. A custom channel is preferred over a default channel because a default port supports multiple protocols.\nWhen accessing T3 or RMI based channels for administrative purposes, such as running WLST, the preferred approach is to kubectl exec into the Kubernetes Pod and then run wlst.sh, or set up Bastion access and then run java weblogic.WLST or $ORACLE_HOME/oracle_common/common/bin/wlst.sh from the Bastion host to connect to the Kubernetes cluster (some cloud environments use the term Jump Host or Jump Server instead of Bastion).\nAlso, if you need to use cross-domain T3 access between clouds, data centers, and such, consider a private VPN.\nWebLogic HTTP channels When providing remote access to HTTP using a load balancer, port forwarding, NodePorts, or similar, Oracle recommends relaying the traffic to a dedicated WebLogic Server port that you have configured using a custom HTTP channel (network access point) instead of relaying the traffic to a default port. This helps ensure that external traffic is limited to the HTTP protocol. A custom HTTP channel is preferred over a default port because a default port supports multiple protocols.\nDo not enable tunneling on an HTTP channel that is exposed for remote access unless you specifically intend to allow it to handle T3 traffic (tunneling allows T3 to tunnel through the channel using HTTP) and you perform the additional steps that may be necessary to further secure access, as described in WebLogic T3 and administrative channels.\nLimit use of Kubernetes NodePorts Although Kubernetes NodePorts are good for use in demos and getting-started guides, they are typically not suited for production systems for multiple reasons, including:\nWith some cloud providers, a NodePort may implicitly expose a port to the public Internet. They bypass almost all network security in Kubernetes. They allow all protocols (load balancers can limit to the HTTP protocol). They cannot expose standard, low-numbered ports like 80 and 443 (or even 8080 and 8443). Some Kubernetes cloud environments cannot expose usable NodePorts because their Kubernetes clusters run on a private network that cannot be reached by external clients. General advice Set up administration ports: Configure an administration port on WebLogic, or an administrative channel, to prevent all other channels from accepting administration-privileged traffic (this includes preventing administration-privileged traffic from a WebLogic console over HTTP).\nBe aware of anonymous defaults: If an externally available port supports a protocol suitable for WebLogic JNDI, EJB/RMI, or JMS clients, then note that by default:\nWebLogic enables anonymous users to access such a port. JNDI entries, EJB/RMI applications, and JMS are open to anonymous users. Configure SSL: You can configure two-way SSL to help prevent external access by unwanted applications (often SSL is setup between the caller and the load balancer, and plain-text traffic flows internally from the load balancer to WebLogic).\nSee also External WebLogic clients Remote Console, Administration Console, WLST, and Port Forwarding access "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/introduction/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "An architectural overview of the operator runtime and related resources.",
	"content": "Contents Overall architecture Domain UID Domain architecture Network name predictability Domain state stored outside container images Overall architecture The operator consists of the following parts:\nThe operator runtime, which is a process that: Runs in a container deployed into a Kubernetes Pod and that monitors one or more Kubernetes namespaces. Performs the actual management tasks for domain and cluster resources deployed to these namespaces. A Helm chart for installing the operator runtime and its related resources. Kubernetes custom resource definitions (CRD) for domains and clusters that, when installed, enable the Kubernetes API server and the operator to monitor and manage domain and cluster resource instances. Domain resources that reference WebLogic domain configuration, a WebLogic install, and anything else necessary to run the domain. Cluster resources that are referenced by domain resources and that reference WebLogic clusters to scale the cluster and to allow other cluster-specific configurations. The operator is packaged in a container image which you can access using the following docker pull commands:\n$ docker pull ghcr.io/oracle/weblogic-kubernetes-operator:4.2.20 For more details on acquiring the operator image and prerequisites for installing the operator, consult the Quick Start guide.\nThe operator registers two Kubernetes custom resource definitions. The first is for the Domain resource and is named domain.weblogic.oracle (shortname domain, plural domains). The second is for the Cluster resource and is named cluster.weblogic.oracle (shortname cluster, plural clusters). More details about the types defined by these CRDs, including their schema, are available here.\nThe following diagram shows the general layout of high-level components, including optional components, in a Kubernetes cluster that is hosting WebLogic domains and the operator:\nThe Kubernetes cluster has several namespaces. Components may be deployed into namespaces as follows:\nOne or more operators, each deployed into its own namespace. There can be more than one operator in a Kubernetes cluster but only a single operator per namespace. Each operator is configured with the specific namespaces that it is responsible for. The operator will not take any action on any domain that is not in one of the namespaces the operator is configured to manage. Multiple operators cannot manage the same namespace. The operator can be configured to monitor its own namespace. There is no limit on the number of domains or namespaces that an operator can manage. If the Elastic Stack integration option is configured to monitor the operator, then a Logstash container will also be created in the operator’s pod. WebLogic domain and cluster resources deployed into various namespaces. There can be more than one domain in a namespace, if desired. Every domain resource must be configured with a domain unique identifier. Cluster resources are only active when they are referenced by a domain resource. Each cluster resource must be referenced by at most one domain resource. While a cluster resource configures a particular WebLogic cluster, it is not required that every WebLogic cluster be associated with a cluster resource. If there is no cluster resource then the WebLogic cluster will inherit the default configuration from the domain resource. Customers are responsible for load balancer configuration, which will typically be in the same namespace with domains or in a shared namespace. Customers are responsible for Elasticsearch and Kibana deployments that may be used to monitor WebLogic server and pod logs. Domain UID Every domain resource must be configured with a domain unique identifier which is a string and may also be called a Domain UID, domainUID, or DOMAIN_UID depending on the context. This value is distinct and need not match the domain name from the WebLogic domain configuration. The operator will use this as a name prefix for the domain related resources that it creates for you (such as services and pods).\nA Domain UID is set on a domain resource using spec.domainUID, and defaults to the value of metadata.name. The spec.domainUID domain resource field is usually left unset to take advantage of this default.\nIt is recommended that a Domain UID be configured to be unique across all Kubernetes namespaces and even across different Kubernetes clusters to assist in future work to identify related domains in active-passive scenarios across data centers; however, it is only required that this value be unique within a namespace, similarly to the names of Kubernetes resources.\nAs a convention, any resource that is associated with a particular Domain UID is given a Kubernetes label named weblogic.domainUID that is assigned to that UID, and the resource name is prefixed with that UID followed by a dash (-). If the operator creates a resource for you on behalf of a particular domain, it will follow this convention. For example, to see all pods created with the weblogic.domainUID label in a Kubernetes cluster try: kubectl get pods -l weblogic.domainUID --all-namespaces=true --show-labels=true.\nA Domain UID may be up to 45 characters long. For more details about Domain UID name requirements, see Meet Kubernetes resource name restrictions.\nDomain architecture The following diagram shows how the various parts of a WebLogic domain are manifest in Kubernetes by the operator.\nThis diagram shows the following details:\nAn optional, persistent volume is created by the customer using one of the available providers. If the persistent volume is shared across the domain or members of a cluster, then the chosen provider must support “Read Write Many” access mode. The shared state on the persistent volume may include the domain directory, the applications directory, a directory for storing logs, and a directory for any file-based persistence stores. A pod is created for the WebLogic Server Administration Server. This pod is named DOMAIN_UID-wlservername and is labeled with weblogic.domainUID, weblogic.serverName, and weblogic.domainName. One container runs in this pod. WebLogic Node Manager and Administration Server processes are run inside this container. The Node Manager process is used as an internal implementation detail for the liveness probe which we will describe in more detail later, for patching, and to provide monitoring and control capabilities to the Administration Console. It is not intended to be used for other purposes, and it may be removed in some future release. A ClusterIP type service is created for the Administration Server pod. This service provides a stable, well-known network (DNS) name for the Administration Server. This name is derived from the domainUID and the Administration Server name as described here, and it is known before starting up any pod. The Administration Server ListenAddress is set to this well-known name. ClusterIP type services are only visible inside the Kubernetes cluster. They are used to provide the well-known names that all of the servers in a domain use to communicate with each other. This service is labeled with weblogic.domainUID and weblogic.domainName. A NodePort type service is optionally created for the Administration Server pod. This service provides HTTP access to the Administration Server to clients that are outside the Kubernetes cluster. This service is intended to be used to access the WebLogic Server Administration Console or for the T3 protocol for WLST connections. This service is labeled with weblogic.domainUID and weblogic.domainName. A pod is created for each WebLogic Server Managed Server. These pods are named DOMAIN_UID-wlservername and are labeled with weblogic.domainUID, weblogic.serverName, and weblogic.domainName. One container runs in each pod. WebLogic Node Manager and Managed Server processes are run inside each of these containers. The Node Manager process is used as an internal implementation detail for the liveness probe which we will describe in more detail later. It is not intended to be used for other purposes, and it may be removed in some future release. A ClusterIP type service is created for each Managed Server pod as described here. These services are intended to be used to access applications running on the Managed Servers. These services are labeled with weblogic.domainUID and weblogic.domainName. A ClusterIP type service is also created for each WebLogic cluster as described here. Customers can expose these services using a load balancer or NodePort type service to expose these endpoints outside the Kubernetes cluster. A PodDisruptionBudget is created for each WebLogic cluster. These pod disruption budgets are labeled with weblogic.domainUID, weblogic.clusterName and weblogic.domainName. An Ingress may optionally be created by the customer for each WebLogic cluster. An Ingress provides load balanced HTTP access to all Managed Servers in that WebLogic cluster. The load balancer updates its routing table for an Ingress every time a Managed Server in the WebLogic cluster becomes “ready” or ceases to be able to service requests, such that the Ingress always points to just those Managed Servers that are able to handle user requests. Kubernetes requires that the names of some resource types follow the DNS label standard as defined in DNS Label Names and RFC 1123. Therefore, the operator enforces that the names of the Kubernetes resources do not exceed Kubernetes limits (see Meet Kubernetes resource name restrictions.\nThe following diagram shows the components inside the containers running WebLogic Server instances:\nThe Domain specifies a container image, defaulting to container-registry.oracle.com/middleware/weblogic:12.2.1.4. All containers running WebLogic Server use this same image. Depending on the use case, this image could contain the WebLogic Server product binaries or also include the domain directory. For detailed information about domain images, see WebLogic images.\nThe default image is a General Availability image. GA images are suitable for demonstration and development purposes only where the environments are not available from the public Internet; they are not acceptable for production use. In production, you should always use CPU (patched) images from OCR or create your images using the WebLogic Image Tool (WIT) with the --recommendedPatches option. For more guidance, see Apply the Latest Patches and Updates in Securing a Production Environment for Oracle WebLogic Server.\nDuring a rolling event caused by a change to the Domain\u0026rsquo;s image field, containers will be using a mix of the updated value of the image field and its previous value.\nWithin the container, the following aspects are configured by the operator:\nThe ENTRYPOINT is configured by a script that starts up a Node Manager process, and then uses WLST to request that Node Manager start the server. Node Manager is used to start servers so that the socket connection to the server will be available to obtain server status even when the server is unresponsive. This is used by the liveness probe. The liveness probe is configured to check that a server is alive by querying the Node Manager process. By default, the liveness probe is configured to check liveness every 45 seconds and to timeout after 5 seconds. If a pod fails the liveness probe, Kubernetes will restart that container. For details about liveness probe customization, see Liveness probe customization. The readiness probe is configured to use the WebLogic Server ReadyApp framework. The readiness probe determines if a server is ready to accept user requests. The readiness probe is used to determine when a server should be included in a load balancer\u0026rsquo;s endpoints, in the case of a rolling restart, when a restarted server is fully started, and for various other purposes. For details about readiness probe customization, see Readiness probe customization. A shutdown hook is configured that will execute a script that performs a graceful shutdown of the server. This ensures that servers have an opportunity to shut down cleanly before they are killed. Network name predictability The operator deploys services with predictable well-defined DNS names for each WebLogic server and cluster in your WebLogic configuration. The name of a WebLogic server service is DOMAIN_UID-wlservername and the name of a WebLogic server cluster is DOMAIN_UID-cluster-wlclustername, all in lowercase, with underscores _ converted to hyphens -.\nThe operator also automatically overrides the ListenAddress fields in each running WebLogic Server to match its service name in order to ensure that the servers will always be able to find each other.\nFor details, see Meet Kubernetes resource name restrictions.\nDomain state stored outside container images The operator expects (and requires) that all state that is expected to outlive the life of a pod be stored outside of the images that are used to run the domain. This means either in a persistent file system, or in a database. The WebLogic configuration, that is, the domain directory and the applications directory may come from the image or a persistent volume. However, other state, such as file-based persistent stores, and such, must be stored on a persistent volume or in a database. All of the containers that are participating in the WebLogic domain use the same image, and take on their personality; that is, which server they execute, at startup time. Each Pod mounts storage, according to the Domain, and has access to the state information that it needs to fulfill its role in the domain.\nIt is worth providing some background information on why this approach was adopted, in addition to the fact that this separation is consistent with other existing operators (for other products) and the Kubernetes “cattle, not pets” philosophy when it comes to containers.\nThe external state approach allows the operator to treat the images as essentially immutable, read-only, binary images. This means that the image needs to be pulled only once, and that many domains can share the same image. This helps to minimize the amount of bandwidth and storage needed for WebLogic Server images.\nThis approach also eliminates the need to manage any state created in a running container, because all of the state that needs to be preserved is written into either the persistent volume or a database backend. The containers and pods are completely throwaway and can be replaced with new containers and pods, as necessary. This makes handling failures and rolling restarts much simpler because there is no need to preserve any state inside a running container.\nWhen users wish to apply a binary patch to WebLogic Server, it is necessary to create only a single new, patched image. If desired, any domains that are running may be updated to this new patched image with a rolling restart. See the WebLogic Server images and Domain life cycle documentation.\nIt is envisaged that in some future release of the operator, it will be desirable to be able to “move” or “copy” domains to support scenarios like Kubernetes federation, high availability, and disaster recovery. Separating the state from the running containers is seen as a way to greatly simplify this feature, and to minimize the amount of data that would need to be moved over the network, because the configuration is generally much smaller than the size of WebLogic Server images.\nThe team developing the operator felt that these considerations provided adequate justification for adopting the external state approach.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/accessing-the-domain/port-forward/",
	"title": "Use port forwarding",
	"tags": [],
	"description": "Use port forwarding to access WebLogic Server administration consoles and WLST.",
	"content": "Contents Overview Set up Administration Server network channels for port forward access Port forward to an Administration Server Pod Port forward example Port forward notes and warnings Enabling WLST access when local and remote ports do not match Terminating port forwarding Overview Beginning with WebLogic Kubernetes Operator version 3.3.2, or earlier if you are using an Istio-enabled domain, you can use the kubectl port-forward command to set up external access for the WebLogic Server Administration Console, the Remote Console, and WLST. This approach is particularly useful for managing WebLogic from a private local network without exposing WebLogic\u0026rsquo;s ports to a public network.\nHere are the steps:\nSet up Administration Server network channels for port forward access. If you are setting up WLST access and your port forwarding local port is not going to be the same as the port number on the WebLogic Administration Server Pod, then see enabling WLST access when local and remote ports do not match for an additional required setup step. Be sure to review the port forward notes and warnings first, and then run a port forwarding command. Use the WebLogic Server Administration Console, the Remote Console, or WLST with the port forwarding command\u0026rsquo;s local address. Finally, terminate port forwarding. Externally exposing administrative, RMI, or T3 capable WebLogic channels using a Kubernetes NodePort, load balancer, port forwarding, or a similar method can create an insecure configuration. For more information, see External network access security.\nSet up Administration Server network channels for port forward access To enable a kubectl port-forward command to communicate with a WebLogic Administration Server Pod, the operator must modify the Administration Server configuration to add network channels (Network Access Points) with a localhost address for each existing administration protocol capable port. This behavior depends on your version and domain resource configuration:\nIf Istio is not enabled on the domain, then, for operator versions 3.3.2 and later, or for Istio enabled domains running Istio 1.10 and later, this behavior is configurable on the domain resource using the domain.spec.adminServer.adminChannelPortForwardingEnabled domain resource attribute.\nThis attribute is enabled by default in operator versions 4.0 and later, and is disabled by default in versions prior to 4.0.\nFor details about this attribute, run the kubectl explain domain.spec.adminServer.adminChannelPortForwardingEnabled command or see the domain resource schema.\nIf WLST access is required for Istio-enabled domains running Istio versions prior to 1.10, you must add an additional network channel to the WebLogic Administration Server configured with the following attributes:\nProtocol defined as t3. Listen address defined with localhost. (Note: Setting the address to localhost is solely for self-documenting purposes. The address can be set to any value, and the operator will override it to the required value.) Listen port. Note: Choose a port value that does not conflict with any ports defined in any of the additional network channels created for use with Istio versions prior to v1.10. For more details, see Added network channels for Istio versions prior to v1.10. Enable HTTP protocol for this network channel. Do NOT set an external listen address or external listen port. For Istio-enabled domains running Istio versions prior to 1.10, if console only access is required, then it is not necessary to add an additional network channel to the WebLogic Administration Server.\nFor example, here is a snippet of a WebLogic domain config.xml file for channel PortForward for the Administration Server.\n\u0026lt;server\u0026gt; \u0026lt;name\u0026gt;admin-server\u0026lt;/name\u0026gt; \u0026lt;network-access-point\u0026gt; \u0026lt;name\u0026gt;PortForward\u0026lt;/name\u0026gt; \u0026lt;protocol\u0026gt;t3\u0026lt;/protocol\u0026gt; \u0026lt;listen-address\u0026gt;localhost\u0026lt;/listen-address\u0026gt; \u0026lt;listen-port\u0026gt;7890\u0026lt;/listen-port\u0026gt; \u0026lt;http-enabled-for-this-protocol\u0026gt;true\u0026lt;/http-enabled-for-this-protocol\u0026gt; \u0026lt;/network-access-point\u0026gt; \u0026lt;/server\u0026gt; For Model in Image (MII) and Domain in Image (DII), here is a snippet model configuration for channel PortForward for the Administration Server.\ntopology: ... Server: \u0026#39;admin-server\u0026#39;: ListenPort: 7001 NetworkAccessPoint: PortForward: Protocol: \u0026#39;t3\u0026#39; ListenAddress: \u0026#39;localhost\u0026#39; ListenPort: \u0026#39;7890\u0026#39; HttpEnabledForThisProtocol: true If your domain is already running, and you have made configuration changes, then you will need to rerun its introspector job and ensure that the admin pod restarts for the configuration changes to take effect.\nIf Istio is not enabled on the domain or for Istio enabled domains running Istio 1.10 and later, when administration channel port forwarding is enabled, the operator automatically adds the following network channels (also known as Network Access Points) to the WebLogic Administration Server Pod:\nDescription Channel Name Listen Address Port Protocol When there\u0026rsquo;s no administration port or administration channels configured, and a non-SSL default channel exists internal-t3 localhost Server listening port t3 When there\u0026rsquo;s no administration port or administration channels configured, and an SSL default channel exists internal-t3s localhost Server SSL listening port t3s When an administration port is enabled internal-admin localhost WebLogic administration port admin When one or more custom administration channels are configured internal-admin${index} (where ${index} is a number that starts with 1 and increases by 1 for each custom administration channel) localhost Custom administration port admin Port forward to an Administration Server Pod If you have set up Administration Server network channels for port forward access, then you can run a kubectl port-forward command to access such a channel. The command:\nBy default, opens a port of your choosing on localhost on the machine where you run the command. Forwards traffic from this location to an address and port on the pod. For administrative access, you need to forward to a port on the Administration Server Pod that accepts administration traffic. Port forwarding occurs as long as the remote pod and the command are running. If you exit the command, then the forwarding stops and the local port and address is freed. If the pod cycles or shuts down, then the forwarding also stops and the command must be rerun after the pod recovers in order for the forwarding to restart.\nThe kubectl port-forward command has the following syntax:\nkubectl port-forward K8S_RESOURCE_TYPE/K8S_RESOURCE_NAME [options] LOCAL_PORT:REMOTE_PORT_ON_RESOURCE For detailed usage, see port-forward in the Kubernetes reference documentation and run kubectl port-forward -h.\nFor examples, notes, and warnings, see the Port forward example and Port forward notes and warnings.\nPort forward example For example, if you have a WebLogic domain with:\nDomain UID domain1 Namespace mynamespace An Administration Server named admin-server listening on non-SSL listen port 7001 No administration port or administration channels configured on the Administration Server And, you have set up Administration Server network channels for port forward access for this domain, then you can run either of the following commands to forward local port 32015 to the Administration Server Pod:\nkubectl port-forward pods/domain1-admin-server -n mynamespace 32015:7001 or\nkubectl port-forward service/domain1-admin-server -n mynamespace 32015:7001 The command output will be similar to the following:\nForwarding from 127.0.0.1:32015 -\u0026gt; 7001 In this example:\nYou can access the WebLogic Server Administration Console at the http://localhost:32015/console URL by using a browser on the machine where you run the kubectl port-forward command.\nYou can use WLST on the machine where you run the kubectl port-forward command to connect to t3://localhost:32015 as shown:\n$ $ORACLE_HOME/oracle_common/common/bin/wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt; connect(\u0026#39;myadminuser\u0026#39;,\u0026#39;myadminpassword\u0026#39;,\u0026#39;t3://localhost:32015\u0026#39;) Connecting to t3://localhost:32015 with userid myadminuser ... Successfully connected to Admin Server \u0026#34;admin-server\u0026#34; that belongs to domain \u0026#34;base_domain\u0026#34;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/base_domain/serverConfig/\u0026gt; exit() Port forward notes and warnings Security warning: A port-forward connection can expose a WebLogic T3 or administrative channel outside the Kubernetes cluster. For domain security considerations, see External network access security.\nWorking with administration or SSL ports: If a WebLogic administration port is configured and enabled on the Administration Server, then you will need to forward to this port instead of its pod\u0026rsquo;s other ports. In this case, the Administration Console access requires using the secure https protocol and WLST access requires using t3s protocol. Similarly, if forwarding to an SSL port, then this requires using the https and t3s protocols for Administration Console and WLST access respectively.\nRecovering from pod failures and restarts: A port-forward connection terminates after the pod instance fails or restarts. You can rerun the same command to establish a new forwarding session and resume forwarding.\nUsing WLST when the local port differs from the remote port: If you are setting up WLST access and your port forwarding local port is not going to be the same as the port number on the WebLogic Administration Server Pod, then see Enabling WLST access when local and remote ports do not match for an additional required setup step.\nSpecifying a custom local IP address for port forwarding:\nSpecifying a custom local IP address for port forwarding allows you to run WLST or your browser on a different machine than the port forward command.\nYou can use the --address option of the kubectl port-forward command to listen on specific IP addresses instead of, or in addition to, localhost.\nThe --address option only accepts numeric IP addresses or localhost (comma-separated) as a value.\nFor example, to enable Administration Console access at http://my-ip-address:32015/console, the command:\nkubectl port-forward --address my-ip-address pods/domain1-admin-server -n mynamespace 32015:7001 See kubectl port-forward -h for help and examples.\nOptionally let kubectl port-forward choose the local port:\nIf you don\u0026rsquo;t specify a local port in your port forward command, for example:\nkubectl port-forward pods/domain1-admin-server -n mynamespace :7001 Then the command finds a local port number that is not in use, and its output is similar to:\nForwarding from 127.0.0.1:63753 -\u0026gt; 7001 And the Administration Console is accessible using the http://localhost:63753/console URL.\nEnabling WLST access when local and remote ports do not match If a local (forwarded) port number is not the same as the administration port number, then WLST access will not work by default and you may see a BEA-000572 RJVM error in the Administration Server logs:\n\u0026lt;Aug 30, 2021 9:33:24,753 PM GMT\u0026gt; \u0026lt;Error\u0026gt; \u0026lt;RJVM\u0026gt; \u0026lt;BEA-000572\u0026gt; \u0026lt;The server rejected a connection attempt JVMMessage from: \u0026#39;-2661445766084484528C:xx.xx.xx.xxR:-5905806878036317188S:domain1-admin-server:domain1:admin-server\u0026#39; to: \u0026#39;0B:xx.xx.xx.xx:[-1,-1,32015,-1,-1,-1,-1]\u0026#39; cmd: \u0026#39;CMD_IDENTIFY_REQUEST\u0026#39;, QOS: \u0026#39;102\u0026#39;, responseId: \u0026#39;-1\u0026#39;, invokableId: \u0026#39;-1\u0026#39;, flags: \u0026#39;JVMIDs Sent, TX Context Not Sent, 0x1\u0026#39;, abbrev offset: \u0026#39;114\u0026#39; probably due to an incorrect firewall configuration or administrative command.\u0026gt;\u0026lt; To work around the problem, configure the JAVA_OPTIONS environment variable for your Administration Server with the -Dweblogic.rjvm.enableprotocolswitch=true system property in your domain resource YAML. For more information on this switch, refer to MOS \u0026lsquo;Doc 860340.1\u0026rsquo;.\nTerminating port forwarding A port-forward connection is active only while the kubectl port-forward command is running.\nYou can terminate the port-forward connection by pressing CTRL+C in the terminal where the port forward command is running.\nIf you run the command in the background, then you can kill the process with the kill -9 \u0026lt;pid\u0026gt; command. For example:\n$ ps -ef | grep port-forward oracle 27072 25312 1 21:45 pts/3 00:00:00 kubectl -n mynamespace port-forward pods/domain1-admin-server 32015:7001 oracle 27944 11417 0 21:45 pts/1 00:00:00 grep --color=auto port-forward $ kill -9 27072 "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/security/encryption/",
	"title": "Encryption",
	"tags": [],
	"description": "WebLogic domain encryption and the operator.",
	"content": "Contents Introspector encryption Encryption of Kubernetes Secrets Additional reading Introspector encryption The operator has an introspection job that handles WebLogic domain encryption. The introspection job also addresses the use of Kubernetes Secrets with configuration overrides. For additional information on the configuration handling, see Configuration overrides.\nThe introspection job also creates a boot.properties file that is made available to the pods in the WebLogic domain. The credential used for the WebLogic domain is kept in a Kubernetes Secret which follows the naming pattern \u0026lt;domainUID\u0026gt;-weblogic-credentials, where \u0026lt;domainUID\u0026gt; is the unique identifier of the domain, for example, mydomain-weblogic-credentials.\nFor more information about the WebLogic credentials secret, see Secrets.\nEncryption of Kubernetes Secrets To better protect your credentials and private keys, the Kubernetes cluster should be set up with encryption. Please see the Kubernetes documentation about encryption at rest for secret data and using a KMS provider for data encryption.\nAdditional reading Encryption of values for WebLogic configuration overrides "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/",
	"title": "Domains",
	"tags": [],
	"description": "These samples show various choices for working with domains.",
	"content": "These samples show various choices for working with domains.\nDomain on PV Sample for creating and deploying a WebLogic domain on a persistent volume (PV).\nModel in image Sample for supplying a WebLogic Deploy Tooling (WDT) model that the operator expands into a full domain home during runtime.\nDelete resources associated with the domain Delete the Kubernetes resources associated with the domain created while executing the samples.\nDomain lifecycle operations Start and stop Managed Servers, clusters, and domains.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/domain-home-on-pv/sample/",
	"title": "Sample",
	"tags": [],
	"description": "Sample for creating a WebLogic domain home on a PV for deploying the generated WebLogic domain.",
	"content": "Contents Overview Domain creation image Deploy resources - Introduction Secrets Domain resource Verify the PV, PVC, and domain Verify the persistent volume Verify the persistent volume claim Verify the domain Verify the pods Verify the services Invoke the web application Clean up resources and remove the generated domain home Remove the PVC and PV Delete the domain namespace. Before you begin: Perform the steps in Prerequisites and then build a Domain on PV domain creation image by completing the steps in Build the domain creation image. If you are taking the JRF path through the sample, then substitute JRF for WLS in your image names and directory paths. Also note that the JRF-v1 model YAML file differs from the WLS-v1 YAML file (it contains an additional domainInfo -\u0026gt; RCUDbInfo stanza).\nOverview The sample demonstrates setting up a WebLogic domain with a domain home on a Kubernetes PersistentVolume (PV) (Domain on PV). This involves:\nUsing the domain creation image that you previously built. Creating secrets for the domain. Creating a Domain resource YAML file for the domain that: References your Secrets and a WebLogic image. References the domain creation image in the spec.configuration.initializeDomainOnPV section for the initial Domain on PV configuration defined using WDT model YAML. Defines PV and PVC metadata and specifications in the spec.configuration.initializeDomainOnPV section to create a PV and PVC (optional). PV and PVC Notes:\nThe specifications of PersistentVolume and PersistentVolumeClaim defined in the spec.configuration.initializeDomainOnPV section of the Domain resource YAML file are environment specific and often require information from your Kubernetes cluster administrator to provide the information. See Persistent volume and Persistent Volume Claim in the user documentation for more details. You must use a storage provider that supports the ReadWriteMany option. This sample will automatically set the owner of all files in the domain home on the persistent volume to uid 1000. If you want to use a different user, configure the desired runAsUser and runAsGroup in the security context under the spec.serverPod.podSecurityContext section of the Domain YAML file. The operator will use these values when setting the owner for files in the domain home directory. After the Domain is deployed, the operator creates the PV and PVC (if they are configured and do not already exist) and starts an \u0026lsquo;introspector job\u0026rsquo; that converts your models included in the domain creation image and config map into a WebLogic configuration to initialize the Domain on PV.\nDomain creation image The sample uses a domain creation image with the name wdt-domain-image:WLS-v1 that you created in the Build the domain creation image step. The WDT model files in this image define the initial WebLogic Domain on PV configuration. The image contains:\nThe directory where the WebLogic Deploy Tooling software is installed (also known as WDT Home), expected in an image\u0026rsquo;s /auxiliary/weblogic-deploy directory, by default. WDT model YAML, property, and archive files, expected in the directory /auxiliary/models, by default. Deploy resources - Introduction In this section, you will define the PV and PVC configuration and reference the domain creation image created earlier in the domain resource YAML file. You will then deploy the domain resource YAML file to the namespace sample-domain1-ns, including the following steps:\nCreate a Secret containing your WebLogic administrator user name and password. If your domain type is JRF, create secrets containing your RCU access URL, credentials, and prefix. Deploy a Domain YAML file that references the PV/PVC configuration and a domain creation image under the spec.configuration.initializeDomainOnPV section. Wait for the PV and PVC to be created if they do not already exist. Wait for domain\u0026rsquo;s Pods to start and reach their ready state. Secrets First, create the secrets needed by both WLS and JRF type model domains. You have to create the \u0026ldquo;WebLogic credentials secret\u0026rdquo; and any other secrets that are referenced from the macros in the WDT model file. For more details about using macros in the WDT model files, see Working with the WDT model files.\nRun the following kubectl commands to deploy the required secrets:\nNOTE: Substitute a password of your choice for MY_WEBLOGIC_ADMIN_PASSWORD. This password should contain at least seven letters plus one digit.\n$ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-weblogic-credentials \\ --from-literal=username=weblogic --from-literal=password=MY_WEBLOGIC_ADMIN_PASSWORD $ kubectl -n sample-domain1-ns label secret \\ sample-domain1-weblogic-credentials \\ weblogic.domainUID=sample-domain1 Some important details about these secrets:\nThe WebLogic credentials secret is required and must contain username and password fields. You reference it in spec.webLogicCredentialsSecret field of Domain YAML file and macros in the domainInfo.AdminUserName and domainInfo.AdminPassWord fields your model YAML file. Delete a secret before creating it, otherwise the create command will fail if the secret already exists.. Name and label the secrets using their associated domain UID to clarify which secrets belong to which domains and make it easier to clean up a domain. If you\u0026rsquo;re following the JRF path through the sample, then you also need to deploy the additional secret referenced by macros in the JRF model RCUDbInfo clause, plus an OPSS wallet password secret. For details about the uses of these secrets, see the Domain on PV user documentation.\nClick here for the commands for deploying additional secrets for JRF. NOTE: Replace MY_RCU_SCHEMA_PASSWORD with the RCU schema password that you chose in the prequisite steps when setting up JRF.\n$ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-rcu-access \\ --from-literal=rcu_prefix=FMW1 \\ --from-literal=rcu_schema_password=MY_RCU_SCHEMA_PASSWORD \\ --from-literal=rcu_db_conn_string=oracle-db.default.svc.cluster.local:1521/devpdb.k8s $ kubectl -n sample-domain1-ns label secret \\ sample-domain1-rcu-access \\ weblogic.domainUID=sample-domain1 NOTES:\nReplace MY_OPSS_WALLET_PASSWORD with a password of your choice. The password can contain letters and digits. The domain\u0026rsquo;s JRF RCU schema will be automatically initialized plus generate a JRF OPSS wallet file upon first use. If you plan to save and reuse this wallet file, as is necessary for reusing RCU schema data after a migration or restart, then it will also be necessary to use this same password again. $ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-opss-wallet-password-secret \\ --from-literal=walletPassword=MY_OPSS_WALLET_PASSWORD $ kubectl -n sample-domain1-ns label secret \\ sample-domain1-opss-wallet-password-secret \\ weblogic.domainUID=sample-domain1 Domain resource Now, you deploy a sample-domain1 domain resource and an associated sample-domain1-cluster-1 cluster resource using a single YAML resource file which defines both resources. The domain resource and cluster resource tells the operator how to deploy a WebLogic domain. They do not replace the traditional WebLogic configuration files, but instead cooperate with those files to describe the Kubernetes artifacts of the corresponding domain.\nCopy the contents of the WLS domain resource YAML file file that is included in the sample source to a file called /tmp/sample/domain-resource.yaml or similar.\nClick here to view the WLS Domain YAML file.\nClick here to view the JRF Domain YAML file.\nModify the PV and PVC specifications defined in the spec.configuration.initializeDomainOnPV section of the Domain resource YAML file based on your environment. These specifications often require your Kubernetes cluster administrator to provide the information. See Persistent volume and Persistent Volume Claim in the user documentation for more details.\nBy default, this sample creates a Persistent Volume (PV) of type hostPath. This works only for a single node Kubernetes cluster for testing or proof of concept activities. In a multinode Kubernetes cluster, consider using a Kubernetes StorageClass, or PV of nfs type. If you use Oracle Container Engine for Kubernetes (OKE) and plan to use Oracle Cloud Infrastructure File Storage (FSS) for PV, then Oracle recommends creating a StorageClass and specifying the name of the StorageClass in your PersistentVolumeClaim (PVC) configuration in the initializeDomainOnPV section. See Provisioning PVCs on the File Storage Service (FSS) in the OCI documentation for more details.\nNOTE: Before you deploy the domain custom resource, ensure all nodes in your Kubernetes cluster can access domain-creation-image and other images.\nRun the following command to apply the two sample resources.\n$ kubectl apply -f /tmp/sample/domain-resource.yaml The domain resource references the cluster resource, a WebLogic Server installation image, the secrets you defined, PV and PVC configuration details, and a sample domain creation image, which contains a traditional WebLogic configuration and a WebLogic application. For detailed information, see Domain and cluster resources.\nVerify the PV, PVC, and domain To confirm that the PV, PVC, and domain were created, use the following instructions.\nVerify the persistent volume If the spec.configuration.initializeDomainOnPV.persistentVolume is configured for the operator to create the PV, then verify that a PV with the given name is created and is in Bound status. If the PV already exists, then ensure that the existing PV is in Bound status.\n$ kubectl get pv Here is an example output of this command:\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE sample-domain1-weblogic-sample-pv 5Gi RWX Retain Bound sample-domain1-ns/sample-domain1-weblogic-sample-pvc manual 14m Verify the persistent volume claim If the spec.configuration.initializeDomainOnPV.persistentVolumeClaim is configured for the operator to create the PVC, then verify that the PVC with the given name is created and is in Bound status. If the PVC already exists, then ensure that the existing PVC is in Bound status.\n$ kubectl get pvc -n sample-domain1-ns Here is an example output of this command:\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE sample-domain1-weblogic-sample-pvc Bound sample-domain1-weblogic-sample-pv 5Gi RWX manual 11m Verify the domain Run the following kubectl describe domain command to check the status and events for the created domain.\n$ kubectl describe domain sample-domain1 -n sample-domain1-ns Click here to see an example of the output of this command. Name: sample-domain1 Namespace: sample-domain1-ns Labels: weblogic.domainUID=sample-domain1 Annotations: \u0026lt;none\u0026gt; API Version: weblogic.oracle/v9 Kind: Domain Metadata: Creation Timestamp: 2023-04-28T20:31:09Z Generation: 1 Resource Version: 345297 UID: 69b69019-c430-4f01-b96c-4364bf1f39c1 Spec: Clusters: Name: sample-domain1-cluster-1 Configuration: Initialize Domain On PV: Domain: Create If Not Exists: domain Domain Creation Images: Image: phx.ocir.io/weblogick8s/domain-on-pv-image:WLS-v1 Domain Type: WLS Persistent Volume: Metadata: Name: sample-domain1-weblogic-sample-pv Spec: Capacity: Storage: 5Gi Host Path: Path: /shared Storage Class Name: manual Persistent Volume Claim: Metadata: Name: sample-domain1-weblogic-sample-pvc Spec: Resources: Requests: Storage: 1Gi Storage Class Name: manual Volume Name: sample-domain1-weblogic-sample-pv Override Distribution Strategy: Dynamic Domain Home: /shared/domains/sample-domain1 Domain Home Source Type: PersistentVolume Failure Retry Interval Seconds: 120 Failure Retry Limit Minutes: 1440 Http Access Log In Log Home: true Image: container-registry.oracle.com/middleware/weblogic:12.2.1.4 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Introspect Version: 1 Log Home Layout: ByServers Max Cluster Concurrent Shutdown: 1 Max Cluster Concurrent Startup: 0 Max Cluster Unavailable: 1 Replicas: 1 Restart Version: 1 Server Pod: Env: Name: CUSTOM_DOMAIN_NAME Value: domain1 Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx512m Resources: Requests: Cpu: 250m Memory: 768Mi Volume Mounts: Mount Path: /shared Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: sample-domain1-weblogic-sample-pvc Server Start Policy: IfNeeded Web Logic Credentials Secret: Name: sample-domain1-weblogic-credentials Status: Clusters: Cluster Name: cluster-1 Conditions: Last Transition Time: 2023-04-28T20:33:57.983431Z Status: True Type: Available Last Transition Time: 2023-04-28T20:33:57.983495Z Status: True Type: Completed Label Selector: weblogic.domainUID=sample-domain1,weblogic.clusterName=cluster-1 Maximum Replicas: 5 Minimum Replicas: 0 Observed Generation: 1 Ready Replicas: 2 Replicas: 2 Replicas Goal: 2 Conditions: Last Transition Time: 2023-04-28T20:33:53.450209Z Status: True Type: Available Last Transition Time: 2023-04-28T20:33:58.076050Z Status: True Type: Completed Observed Generation: 1 Replicas: 2 Servers: Health: Activation Time: 2023-04-28T20:33:01.803000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: phx32822d1 Pod Phase: Running Pod Ready: True Server Name: admin-server State: RUNNING State Goal: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2023-04-28T20:33:40.165000Z Overall Health: ok Subsystems: Node Name: phx32822d1 Pod Phase: Running Pod Ready: True Server Name: managed-server1 State: RUNNING State Goal: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2023-04-28T20:33:44.225000Z Overall Health: ok Subsystems: Node Name: phx32822d1 Pod Phase: Running Pod Ready: True Server Name: managed-server2 State: RUNNING State Goal: RUNNING Cluster Name: cluster-1 Server Name: managed-server3 State: SHUTDOWN State Goal: SHUTDOWN Cluster Name: cluster-1 Server Name: managed-server4 State: SHUTDOWN State Goal: SHUTDOWN Cluster Name: cluster-1 Server Name: managed-server5 State: SHUTDOWN State Goal: SHUTDOWN Start Time: 2023-04-28T20:32:02.339082Z Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Failed 4m7s weblogic.operator Domain sample-domain1 failed due to \u0026#39;Persistent volume claim unbound\u0026#39;: PersistentVolumeClaim \u0026#39;sample-domain1-weblogic-sample-pvc\u0026#39; is not bound; the status phase is \u0026#39;Pending\u0026#39;.. Operator is waiting for the persistent volume claim to be bound, it may be a temporary condition. If this condition persists, then ensure that the PVC has a correct volume name or storage class name and is in bound status.. Normal PersistentVolumeClaimBound 3m57s weblogic.operator The persistent volume claim is bound and ready. Normal Available 2m16s weblogic.operator Domain sample-domain1 is available: a sufficient number of its servers have reached the ready state. Normal Completed 2m11s weblogic.operator Domain sample-domain1 is complete because all of the following are true: there is no failure detected, there are no pending server shutdowns, and all servers expected to be running are ready and at their target image, auxiliary images, restart version, and introspect version. In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued very soon after the domain resource is applied, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. The operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods If you run kubectl get pods -n sample-domain1-ns --watch, then you will see the introspector job run and your WebLogic Server pods start. The output will look something like this: Click here to expand. $ kubectl get pods -n sample-domain1-ns --watch NAME READY STATUS RESTARTS AGE sample-domain1-introspector-lqqj9 0/1 Pending 0 0s sample-domain1-introspector-lqqj9 0/1 ContainerCreating 0 0s sample-domain1-introspector-lqqj9 1/1 Running 0 1s sample-domain1-introspector-lqqj9 0/1 Completed 0 65s sample-domain1-introspector-lqqj9 0/1 Terminating 0 65s sample-domain1-admin-server 0/1 Pending 0 0s sample-domain1-admin-server 0/1 ContainerCreating 0 0s sample-domain1-admin-server 0/1 Running 0 1s sample-domain1-admin-server 1/1 Running 0 32s sample-domain1-managed-server1 0/1 Pending 0 0s sample-domain1-managed-server2 0/1 Pending 0 0s sample-domain1-managed-server1 0/1 ContainerCreating 0 0s sample-domain1-managed-server2 0/1 ContainerCreating 0 0s sample-domain1-managed-server1 0/1 Running 0 2s sample-domain1-managed-server2 0/1 Running 0 2s sample-domain1-managed-server1 1/1 Running 0 43s sample-domain1-managed-server2 1/1 Running 0 42s For a more detailed view of this activity, you can use the waitForDomain.sh sample lifecycle script. This script provides useful information about a domain\u0026rsquo;s pods and optionally waits for its Completed status condition to become True. A Completed domain indicates that all of its expected pods have reached a ready state plus their target restartVersion, introspectVersion, and image. For example:\n$ cd /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/domain-lifecycle $ ./waitForDomain.sh -n sample-domain1-ns -d sample-domain1 -p Completed If you see an error, then consult Debugging.\nVerify the services Use the following command to see the services for the domain:\n$ kubectl get services -n sample-domain1-ns Here is an example output of this command:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE sample-domain1-admin-server ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 10m sample-domain1-cluster-cluster-1 ClusterIP 10.107.178.255 \u0026lt;none\u0026gt; 8001/TCP 9m49s sample-domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 9m49s sample-domain1-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 9m43s Invoke the web application Now that all the sample resources have been deployed, you can invoke the sample web application through the Traefik ingress controller\u0026rsquo;s NodePort.\nSend a web application request to the load balancer URL for the application, as shown in the following example.\nRequest from a local machine Request from a remote machine $ curl -s -S -m 10 -H 'host: sample-domain1-cluster-cluster-1.sample.org' http://localhost:30305/myapp_war/index.jsp $ K8S_CLUSTER_ADDRESS=$(kubectl cluster-info | grep DNS | sed 's/^.*https:\\/\\///g' | sed 's/:.*$//g') $ curl -s -S -m 10 -H 'host: sample-domain1-cluster-cluster-1.sample.org' http://${K8S_CLUSTER_ADDRESS}:30305/myapp_war/index.jsp You will see output like the following:\n\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v1\u0026#39; of the sample JSP web-app. Welcome to WebLogic Server \u0026#39;managed-server2\u0026#39;! domain UID = \u0026#39;sample-domain1\u0026#39; domain name = \u0026#39;domain1\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found min threads constraint runtime named \u0026#39;SampleMinThreads\u0026#39; with configured count: 1 Found max threads constraint runtime named \u0026#39;SampleMaxThreads\u0026#39; with configured count: 10 Found 0 local data sources: ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; Clean up resources and remove the generated domain home Follow the cleanup instructions here to remove the domain, cluster and other associated resources.\nSometimes in production, but most likely in testing environments, you might also want to remove the Domain on PV contents that are generated using this sample. You can use the pv-pvc-helper.sh helper script in the domain lifecycle directory for this. The script launches a Kubernetes pod named pvhelper using the provided persistent volume claim name and the mount path. You can run kubectl exec to get a shell to the running pod container and run commands to examine or clean up the contents of shared directories on the persistent volume. For example:\n$ cd /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/domain-lifecycle $ ./pv-pvc-helper.sh -n sample-domain1-ns -r -c sample-domain1-weblogic-sample-pvc -m /shared Click here to see the output. [2023-04-28T21:02:42.851294126Z][INFO] Creating pod \u0026#39;pvhelper\u0026#39; using image \u0026#39;ghcr.io/oracle/oraclelinux:8-slim\u0026#39;, persistent volume claim \u0026#39;sample-domain1-weblogic-sample-pvc\u0026#39; and mount path \u0026#39;/shared\u0026#39;. pod/pvhelper created [pvhelper] already initialized .. Checking Pod READY column for State [1/1] Pod [pvhelper] Status is NotReady Iter [1/120] Pod [pvhelper] Status is Ready Iter [2/120] NAME READY STATUS RESTARTS AGE pvhelper 1/1 Running 0 10s [2023-04-28T21:02:58.972826241Z][INFO] Executing \u0026#39;kubectl -n sample-domain1-ns exec -i pvhelper -- ls -l /shared\u0026#39; command to print the contents of the mount path in the persistent volume. [2023-04-28T21:02:59.115119523Z][INFO] =============== Command output ==================== total 0 drwxr-xr-x 4 1000 root 43 Apr 26 19:41 domains drwxr-xr-x 4 1000 root 43 Apr 26 19:41 logs [2023-04-28T21:02:59.118355423Z][INFO] =================================================== [2023-04-28T21:02:59.121482356Z][INFO] Use command \u0026#39;kubectl -n sample-domain1-ns exec -it pvhelper -- /bin/sh\u0026#39; and cd to \u0026#39;/shared\u0026#39; directory to view or delete the contents on the persistent volume. [2023-04-28T21:02:59.124658180Z][INFO] Use command \u0026#39;kubectl -n sample-domain1-ns delete pod pvhelper\u0026#39; to delete the pod created by the script. $ kubectl -n sample-domain1-ns exec -it pvhelper -- /bin/sh sh-4.4$ cd /shared sh-4.4$ ls domains applications After you get a shell to the running pod container, you can recursively delete the contents of the domain home and applications directories using rm -rf /shared/domains/sample-domain1 and rm -rf /shared/applications/sample-domain1 commands. Since these commands will actually delete files on the persistent storage, we recommend that you understand and execute these commands carefully.\nRemove the PVC and PV If the PVC and PV were created by the operator and you don\u0026rsquo;t want to preserve them, then run following command to delete PVC and PV.\n$ kubectl delete pvc sample-domain1-weblogic-sample-pvc -n sample-domain1-ns $ kubectl delete pv sample-domain1-weblogic-sample-pv Delete the domain namespace. $ kubectl delete namespace sample-domain1-ns "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/model-in-image/update1/",
	"title": "Update 1",
	"tags": [],
	"description": "",
	"content": "This use case demonstrates dynamically adding a data source to your running domain by updating your model and rolling your domain. It demonstrates several features of WDT and Model in Image:\nThe syntax used for updating a model is the same syntax you use for creating the original model. A domain\u0026rsquo;s model can be updated dynamically by supplying a model update in a file in a Kubernetes ConfigMap. Model updates can be as simple as changing the value of a single attribute, or more complex, such as adding a JMS Server. For a detailed description of model updates, see Runtime Updates.\nThe operator does not support all possible dynamic model updates. For model update limitations, consult Runtime Updates in the Model in Image user docs, and carefully test any model update before attempting a dynamic update in production.\nHere are the steps:\nEnsure that you have a running domain.\nMake sure you have deployed the domain from the Initial use case.\nCreate a data source model YAML file.\nCreate a WDT model snippet for a data source (or use the example provided). Make sure that its target is set to cluster-1, and that its initial capacity is set to 0.\nThe reason for the latter is to prevent the data source from causing a WebLogic Server startup failure if it can\u0026rsquo;t find the database, which would be likely to happen because you haven\u0026rsquo;t deployed one.\nHere\u0026rsquo;s an example data source model configuration that meets these criteria:\nresources: JDBCSystemResource: mynewdatasource: Target: \u0026#39;cluster-1\u0026#39; JdbcResource: JDBCDataSourceParams: JNDIName: [ jdbc/mydatasource1, jdbc/mydatasource2 ] GlobalTransactionsProtocol: TwoPhaseCommit JDBCDriverParams: DriverName: oracle.jdbc.xa.client.OracleXADataSource URL: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-datasource-secret:url@@\u0026#39; PasswordEncrypted: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-datasource-secret:password@@\u0026#39; Properties: user: Value: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-datasource-secret:user@@\u0026#39; oracle.net.CONNECT_TIMEOUT: Value: 5000 oracle.jdbc.ReadTimeout: Value: 30000 JDBCConnectionPoolParams: InitialCapacity: 0 MaxCapacity: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-datasource-secret:max-capacity@@\u0026#39; TestTableName: SQL ISVALID TestConnectionsOnReserve: true Place the previous model snippet in a file named /tmp/sample/mydatasource.yaml and then use it in the later step where you deploy the model ConfigMap, or alternatively, use the same data source that\u0026rsquo;s provided in /tmp/sample/model-configmaps/datasource/model.20.datasource.yaml.\nCreate the data source secret.\nThe data source references a new secret that needs to be created. Run the following commands to create the secret:\n$ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-datasource-secret \\ --from-literal=\u0026#39;user=sys as sysdba\u0026#39; \\ --from-literal=\u0026#39;password=incorrect_password\u0026#39; \\ --from-literal=\u0026#39;max-capacity=1\u0026#39; \\ --from-literal=\u0026#39;url=jdbc:oracle:thin:@oracle-db.default.svc.cluster.local:1521/devpdb.k8s\u0026#39; $ kubectl -n sample-domain1-ns label secret \\ sample-domain1-datasource-secret \\ weblogic.domainUID=sample-domain1 We deliberately specify an incorrect password and a low maximum pool capacity because we will demonstrate dynamically correcting the data source attributes in the Update 4 use case without requiring rolling the domain.\nYou name and label secrets using their associated domain UID for two reasons:\nTo make it obvious which secret belongs to which domains. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all the resources associated with a domain. Create a ConfigMap with the WDT model that contains the data source definition.\nRun the following commands:\n$ kubectl -n sample-domain1-ns create configmap sample-domain1-wdt-config-map \\ --from-file=/tmp/sample/model-configmaps/datasource $ kubectl -n sample-domain1-ns label configmap sample-domain1-wdt-config-map \\ weblogic.domainUID=sample-domain1 If you\u0026rsquo;ve created your own data source file, then substitute the file name in the --from-file= parameter (we suggested /tmp/sample/mydatasource.yaml earlier). Note that the -from-file= parameter can reference a single file, in which case it puts the designated file in the ConfigMap, or it can reference a directory, in which case it populates the ConfigMap with all of the files in the designated directory. You name and label the ConfigMap using its associated domain UID for two reasons:\nTo make it obvious which ConfigMap belong to which domains. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all resources associated with a domain. Update your Domain YAML file to refer to the ConfigMap and Secret.\nOption 1: Update a copy of your Domain YAML file from the Initial use case.\nIn the Initial use case, we suggested creating a Domain YAML file named /tmp/sample/mii-initial-domain.yaml or using the domain resource file that is supplied with the sample.\nWe suggest copying the original Domain YAML file and naming the copy /tmp/sample/mii-update1.yaml before making any changes.\nWorking on a copy is not strictly necessary, but it helps keep track of your work for the different use cases in this sample and provides you a backup of your previous work.\nAdd the secret to its spec.configuration.secrets stanza:\nspec: ... configuration: ... secrets: - sample-domain1-datasource-secret (Leave any existing secrets in place.)\nChange its spec.configuration.model.configMap to look like the following:\nspec: ... configuration: ... model: ... configMap: sample-domain1-wdt-config-map Apply your changed Domain YAML file:\nNOTE: Before you deploy the domain custom resource, ensure all nodes in your Kubernetes cluster can access auxliary-image and other images.\n$ kubectl apply -f /tmp/sample/mii-update1.yaml Option 2: Use the updated Domain YAML file that is supplied with the sample:\nNOTE: Before you deploy the domain custom resource, ensure all nodes in your Kubernetes cluster can access auxliary-image and other images.\n$ kubectl apply -f /tmp/sample/domain-resources/WLS/mii-update1-d1-WLS-v1-ds.yaml Restart (\u0026lsquo;roll\u0026rsquo;) the domain.\nNow that the data source is deployed in a ConfigMap and its secret is also deployed, and you have applied an updated Domain YAML file with its spec.configuration.model.configMap and spec.configuration.secrets referencing the ConfigMap and secret, tell the operator to roll the domain.\nWhen a model domain restarts, it will rerun its introspector job to regenerate its configuration, and it will also pass the configuration changes found by the introspector to each restarted server. One way to cause a running domain to restart is to change the domain\u0026rsquo;s spec.restartVersion. To do this:\nOption 1: Edit your domain custom resource.\nCall kubectl -n sample-domain1-ns edit domain sample-domain1. Edit the value of the spec.restartVersion field and save. The field is a string; typically, you use a number in this field and increment it with each restart. Option 2: Dynamically change your domain using kubectl patch.\nTo get the current restartVersion call:\n$ kubectl -n sample-domain1-ns get domain sample-domain1 \u0026#39;-o=jsonpath={.spec.restartVersion}\u0026#39; Choose a new restart version that\u0026rsquo;s different from the current restart version.\nThe field is a string; typically, you use a number in this field and increment it with each restart. Use kubectl patch to set the new value. For example, assuming the new restart version is 2:\n$ kubectl -n sample-domain1-ns patch domain sample-domain1 --type=json \u0026#39;-p=[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/restartVersion\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2\u0026#34; }]\u0026#39; Option 3: Use the sample helper script.\nCall /tmp/sample/utils/patch-restart-version.sh -n sample-domain1-ns -d sample-domain1. This will perform the same kubectl get and kubectl patch commands as Option 2. Wait for the roll to complete.\nNow that you\u0026rsquo;ve started a domain roll, you\u0026rsquo;ll need to wait for it to complete if you want to verify that the data source was deployed.\nOne way to do this is to call kubectl get pods -n sample-domain1-ns --watch and wait for the pods to cycle back to their ready state.\nFor a more detailed view of this activity, you can use the waitForDomain.sh sample lifecycle script. This script provides useful information about a domain\u0026rsquo;s pods and optionally waits for its Completed status condition to become True. A Completed domain indicates that all of its expected pods have reached a ready state plus their target restartVersion, introspectVersion, and image. For example:\n$ cd /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/domain-lifecycle $ ./waitForDomain.sh -n sample-domain1-ns -d sample-domain1 -p Completed After your domain is running, you can call the sample web application to determine if the data source was deployed.\nSend a web application request to the ingress controller:\n$ curl -s -S -m 10 -H \u0026#39;host: sample-domain1-cluster-cluster-1.sample.org\u0026#39; \\ http://localhost:30305/myapp_war/index.jsp Or, if Traefik is unavailable and your Administration Server pod is running, you can run kubectl exec:\n$ kubectl exec -n sample-domain1-ns sample-domain1-admin-server -- bash -c \\ \u0026#34;curl -s -S -m 10 http://sample-domain1-cluster-cluster-1:8001/myapp_war/index.jsp\u0026#34; You will see something like the following:\n\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v1\u0026#39; of the sample JSP web-app. Welcome to WebLogic Server \u0026#39;managed-server1\u0026#39;! domain UID = \u0026#39;sample-domain1\u0026#39; domain name = \u0026#39;domain1\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found min threads constraint runtime named \u0026#39;SampleMinThreads\u0026#39; with configured count: 1 Found max threads constraint runtime named \u0026#39;SampleMaxThreads\u0026#39; with configured count: 10 Found 1 local data source: Datasource \u0026#39;mynewdatasource\u0026#39;: State=\u0026#39;Running\u0026#39;, testPool=\u0026#39;Failed\u0026#39; ---TestPool Failure Reason--- NOTE: Ignore \u0026#39;mynewdatasource\u0026#39; failures until the sample\u0026#39;s Update 4 use case. --- ... ... invalid host/username/password ... ----------------------------- ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; A TestPool Failure is expected because we will demonstrate dynamically correcting the data source attributes in Update 4.\nIf you see an error other than the expected TestPool Failure, then consult Debugging.\nIf you plan to run the Update 3 or Update 4 use case, then leave your domain running.\nTo remove the resources you have created in the samples, see Cleanup.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/developerguide/building/",
	"title": "Building",
	"tags": [],
	"description": "Learn about the operator build process.",
	"content": "The operator is built using Apache Maven. The build machine will also need to have Docker installed.\nTo build the operator, issue the following command in the project directory:\n$ mvn clean install This will compile the source files, build JAR files containing the compiled classes and libraries needed to run the operator, and will also execute all of the unit tests.\nContributions must conform to coding and formatting standards.\nBuilding and pushing the operator container image These commands should be executed in the project root directory:\n$ docker build -t weblogic-kubernetes-operator:some-tag . Replace \u0026lt;version\u0026gt; with the version of the project found in the pom.xml file in the project root directory.\nWe recommend that you use a tag other than latest, to make it easy to distinguish your image. In the previous example, the tag could be the GitHub ID of the developer.\nRunning the operator from an IDE The operator can be run from an IDE, which is useful for debugging. To do so, the machine running the IDE must be configured with a Kubernetes configuration file in ~/.kube/config or in a location pointed to by the KUBECONFIG environment variable.\nConfigure the IDE to run the class oracle.kubernetes.operator.Main.\nYou may need to create a directory called /operator on your machine. Please be aware that the operator code is targeted to Linux, and although it will run fine on macOS, it will probably not run on other operating systems. If you develop on another operating system, you should deploy the operator to a Kubernetes cluster and use remote debugging instead.\nRunning the operator in a Kubernetes cluster If you\u0026rsquo;re not running Kubernetes on your development machine, you\u0026rsquo;ll need to make the container image available to a registry visible to your Kubernetes cluster. Either docker push the image to a private registry or upload your image to a machine running Docker and Kubernetes as follows:\nOn your build machine:\n$ docker save weblogic-kubernetes-operator:some-tag \u0026gt; operator.tar $ scp operator.tar YOUR_USER@YOUR_SERVER:/some/path/operator.tar On the Kubernetes server:\n$ docker load \u0026lt; /some/path/operator.tar Use the Helm charts to install the operator.\nIf the operator\u0026rsquo;s behavior or pod log is insufficient to diagnose and resolve failures, then you can connect a Java debugger to the operator using the debugging options.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/domain-lifecycle/scaling/",
	"title": "Scaling",
	"tags": [],
	"description": "The operator provides several ways to initiate scaling of WebLogic clusters.",
	"content": "This document describes approaches for scaling WebLogic clusters in a Kubernetes environment.\nContents Overview kubectl CLI commands kubectl scale command kubectl patch command On-demand, updating the Cluster or Domain directly Updating the Cluster directly Updating the Domain directly Using Domain lifecycle sample scripts Calling the operator\u0026rsquo;s REST scale API Operator REST endpoints What does the operator do in response to a scaling request? Supported autoscaling controllers Kubernetes Horizontal Pod Autoscaler (HPA) Using a WLDF policy rule and script action to call the operator\u0026rsquo;s REST scale API Configure automatic scaling of WebLogic clusters in Kubernetes with WLDF Create ClusterRoleBindings to allow a namespace user to query WLS Kubernetes cluster information Horizontal Pod Autoscaler (HPA) using WebLogic Exporter Metrics Using a Prometheus alert action to call the operator\u0026rsquo;s REST scale API Helpful tips Debugging scalingAction.sh Example on accessing the external REST endpoint Overview WebLogic Server supports two types of clustering configurations, configured and dynamic. Configured clusters are created by defining each individual Managed Server instance. In dynamic clusters, the Managed Server configurations are generated from a single, shared template. With dynamic clusters, when additional server capacity is needed, new server instances can be added to the cluster without having to configure them individually. Also, unlike configured clusters, scaling up of dynamic clusters is not restricted to the set of servers defined in the cluster but can be increased based on runtime demands. For more information on how to create, configure, and use dynamic clusters in WebLogic Server, see Dynamic Clusters.\nWebLogic Kubernetes Operator 4.0 introduces a new custom resource, Cluster. A Cluster resource references an individual WebLogic cluster and its configuration. It also controls how many member servers are running, along with potentially any additional Kubernetes resources that are specific to that WebLogic cluster. Because the Cluster resource enables the Kubernetes Scale subresource, the kubectl scale command and the Kubernetes Horizontal Pod Autoscaler (HPA) are fully supported for scaling individual WebLogic clusters.\nThe operator provides several ways to initiate scaling of WebLogic clusters, including:\nkubectl CLI commands On-demand, updating the Cluster or Domain directly (using kubectl) Using Domain lifecycle sample scripts Calling the operator\u0026rsquo;s REST scale API, for example, from curl Kubernetes Horizontal Pod Autoscaler (HPA) Using a WLDF policy rule and script action to call the operator\u0026rsquo;s REST scale API Using a Prometheus alert action to call the operator\u0026rsquo;s REST scale API kubectl CLI commands Use the Kubernetes command-line tool, kubectl, to manually scale WebLogic clusters with the following commands:\nscale - Set a new size for a resource. patch - Update a field or fields of a resource using a strategic merge patch. kubectl scale command You can scale an individual WebLogic cluster directly using the kubectl scale command. For example, the following command sets .spec.replicas of the Cluster resource cluster-1 to 5:\n$ kubectl scale --replicas=5 clusters/cluster-1 clusters \u0026#34;cluster-1\u0026#34; scaled $ kubectl get clusters cluster-1 -o jsonpath=\u0026#39;{.spec.replicas}\u0026#39; 5 kubectl patch command Also, you can scale an individual WebLogic cluster directly using the kubectl patch command with your patch object inline. For example, the following command sets .spec.replicas of the Cluster resource cluster-1 to 3:\n$ kubectl patch cluster cluster-1 --type=merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;replicas\u0026#34;:3}}\u0026#39; cluster.weblogic.oracle/cluster-1 patched $ kubectl get clusters cluster-1 -o jsonpath=\u0026#39;{.spec.replicas}\u0026#39; 3 On-demand, updating the Cluster or Domain directly You can use on-demand scaling (scaling a cluster manually) by directly updating the .spec.replicas field of the Cluster or Domain resources.\nUpdating the Cluster directly To scale an individual WebLogic cluster directly, simply edit the replicas field of its associated Cluster resource; you can do this using kubectl. More specifically, you can modify the Cluster directly by using the kubectl edit command. For example:\n$ kubectl edit cluster cluster-1 -n [namespace] In this command, you are editing a Cluster named cluster-1. The kubectl edit command opens the Cluster definition in an editor and lets you modify the replicas value directly. Once committed, the operator will be notified of the change and will immediately attempt to scale the corresponding cluster by reconciling the number of running pods/Managed Server instances with the replicas value specification.\nspec: ... clusterName: cluster-1 replicas: 1 ... Updating the Domain directly To scale every WebLogic cluster in a domain that does not have a corresponding Cluster resource, or that has a Cluster resource which does not specify a cluster.spec.replicas field, modify the domain.spec.replicas field using the kubectl edit command:\n$ kubectl edit domain domain1 -n [namespace] In this command, you are editing a Domain named domain1. The kubectl edit command opens the Domain definition in an editor and lets you modify the replicas value directly. Once committed, the operator will be notified of the change and will immediately attempt to scale the corresponding cluster or clusters by reconciling the number of running pods/Managed Server instances with the replicas value specification.\nspec: ... replicas: 1 ... Using Domain lifecycle sample scripts Beginning in version 3.1.0, the operator provides sample lifecycle scripts. See the helper scripts in the Domain lifecycle sample scripts section, which you can use for scaling WebLogic clusters.\nCalling the operator\u0026rsquo;s REST scale API Scaling up or scaling down a WebLogic cluster provides increased reliability of customer applications as well as optimization of resource usage. In Kubernetes environments, scaling WebLogic clusters involves scaling the number of corresponding Pods in which Managed Server instances are running. Because the operator manages the life cycle of a WebLogic domain, the operator exposes a REST API that allows an authorized actor to request scaling of a WebLogic cluster.\nThe following URL format is used for describing the resources for scaling (up and down) a WebLogic cluster:\nhttp(s)://${OPERATOR_ENDPOINT}/operator/\u0026lt;version\u0026gt;/domains/\u0026lt;domainUID\u0026gt;/clusters/\u0026lt;clusterName\u0026gt;/scale For example:\nhttp(s)://${OPERATOR_ENDPOINT}/operator/v1/domains/domain1/clusters/cluster-1/scale In this URL format:\nOPERATOR_ENDPOINT is the host and port of the operator REST endpoint (internal or external). \u0026lt;version\u0026gt; denotes the version of the REST resource. \u0026lt;domainUID\u0026gt; is the unique identifier of the WebLogic domain. \u0026lt;clusterName\u0026gt; is the name of the WebLogic cluster to be scaled. The /scale REST endpoint accepts an HTTP POST request and the request body supports the JSON \u0026quot;application/json\u0026quot; media type. The request body is the same as a Kubernetes autoscaling ScaleSpec item; for example:\n{ \u0026#34;spec\u0026#34;: { \u0026#34;replicas\u0026#34;: 3 } } The replicas value designates the number of Managed Server instances to scale to. On a successful scaling request, the REST interface will return an HTTP response code of 204 (“No Content”).\nWhen you POST to the /scale REST endpoint, you must send the following headers:\nX-Requested-By request value. The value is an arbitrary name such as MyClient. Authorization: Bearer request value. The value of the Bearer token is the WebLogic domain service account token. For example, when using curl:\n$ curl -v -k -H X-Requested-By:MyClient -H Content-Type:application/json -H Accept:application/json -H \u0026#34;Authorization:Bearer ...\u0026#34; -d \u0026#39;{ \u0026#34;spec\u0026#34;: {\u0026#34;replicas\u0026#34;: 3 } }\u0026#39; https://.../scaling If you omit the header, you\u0026rsquo;ll get a 400 (bad request) response. If you omit the Bearer Authentication header, then you\u0026rsquo;ll get a 401 (Unauthorized) response. If the service account or user associated with the Bearer token does not have permission to patch the WebLogic domain resource, then you\u0026rsquo;ll get a 403 (Forbidden) response.\nTo resolve a 403 (Forbidden) response, when calling the operator\u0026rsquo;s REST scaling API, you may need to add the patch request verb to the cluster role associated with the WebLogic domains resource. The following example ClusterRole definition grants get, list, patch and update access to the WebLogic domains resource\nkind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: weblogic-domain-cluster-role rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;services/status\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;weblogic.oracle\u0026#34;] resources: [\u0026#34;domains\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;update\u0026#34;] --- Operator REST endpoints The WebLogic Kubernetes Operator can expose both an internal and external REST HTTPS endpoint. The internal REST endpoint is only accessible from within the Kubernetes cluster. The external REST endpoint is accessible from outside the Kubernetes cluster. The internal REST endpoint is enabled by default and thus always available, whereas the external REST endpoint is disabled by default and only exposed if explicitly configured. Detailed instructions for configuring the external REST endpoint are available here.\nRegardless of which endpoint is being invoked, the URL format for scaling is the same.\nWhat does the operator do in response to a scaling request? When the operator receives a scaling request, it will:\nPerform an authentication and authorization check to verify that the specified user is allowed to perform the specified operation on the specified resource. Validate that the specified domain, identified by domainUID, exists. Validate that the WebLogic cluster, identified by clusterName, exists. Verify that the specified WebLogic cluster has a sufficient number of configured servers or sufficient dynamic cluster size to satisfy the scaling request. Verify that the specified cluster has a corresponding Cluster resource defined or else creates one if necessary. Initiate scaling by setting the replicas field within the corresponding Cluster resource. In response to a change in the replicas field in the Cluster resource, the operator will increase or decrease the number of Managed Server instance Pods to match the desired replica count.\nSupported autoscaling controllers While continuing to support automatic scaling of WebLogic clusters with the WebLogic Diagnostic Framework (WLDF) and Prometheus, Operator 4.0 now supports the Kubernetes Horizontal Pod Autoscaler (HPA).\nKubernetes Horizontal Pod Autoscaler (HPA) Automatic scaling of an individual WebLogic cluster, by the Kubernetes Horizontal Pod Autoscaler, is now supported since the Cluster custom resource has enabled the Kubernetes /scale subresource. Autoscaling based on resource metrics requires the installation of the Kubernetes Metrics Server or another implementation of the resource metrics API. If using Prometheus for monitoring WebLogic Server metrics, then you can use the Prometheus Adapter in place of the Kubernetes Metrics Server.\nThe following step-by-step example illustrates how to configure and run an HPA to scale a WebLogic cluster, cluster-1, based on the cpu utilization resource metric.\nTo illustrate scaling of a WebLogic cluster based on CPU utilization, deploy the Kubernetes Metrics Server to the Kubernetes cluster. $ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml Confirm that the Kubernetes Metric Server is running by listing the pods in the kube-system namespace. $ kubectl get po -n kube-system If the Kubernetes Metric Server does not reach the READY state (for example, READY 0/1 ) due to Readiness probe failed: HTTP probe failed with statuscode: 500, then you may need to install a valid cluster certificate. For testing purposes, you can resolve this issue by downloading the components.yaml file and adding the argument --kubelet-insecure-tls to the Metrics Server container.\nAssuming a WebLogic domain running in the default namespace, use the following command to create an HPA resource targeted at the Cluster resource (sample-domain1-cluster-1) that will autoscale WebLogic Server instances from a minimum of 2 cluster members up to 5 cluster members, and the scale up or down will occur when the average CPU is consistently over 50%. $ kubectl autoscale cluster sample-domain1-cluster-1 --cpu-percent=50 --min=2 --max=5 horizontalpodautoscaler.autoscaling/sample-domain1-cluster-1 autoscaled Beginning with Operator 4.0, the allowReplicasBelowMinDynClusterSize field has been removed from the Domain resource schema. When scaling down, the minimum allowed replica count must now be configured on the selected autoscaling controller.\nVerify the status of the autoscaler and its behavior by inspecting the HPA resource. $ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE sample-domain1-cluster-1 Cluster/sample-domain1-cluster-1 8%/50% 2 5 2 3m27s To see the HPA scale up the WebLogic cluster sample-domain1-cluster-1, generate a loaded CPU by getting a shell to a running container in one of the cluster member pods and run the following command. $ kubectl exec --stdin --tty sample-domain1-managed-server1 -- /bin/bash [oracle@sample-domain1-managed-server1 oracle]$ dd if=/dev/zero of=/dev/null By listing the Managed Server pods, you will see the autoscaler increase the replicas on the Cluster resource and the operator respond by starting additional cluster member servers. Conversely, after stopping the load and when the CPU utilization average is consistently under 50%, the autoscaler will scale down the WebLogic cluster by decreasing the replicas value on the Cluster resource. For more in-depth information on the Kubernetes Horizontal Pod Autoscaler, see Horizontal Pod Autoscaling.\nUsing a WLDF policy rule and script action to call the operator\u0026rsquo;s REST scale API The WebLogic Diagnostics Framework (WLDF) is a suite of services and APIs that collect and surface metrics that provide visibility into server and application performance. To support automatic scaling of WebLogic clusters in Kubernetes, WLDF provides the Policies and Actions component, which lets you write policy expressions for automatically executing scaling operations on a cluster. These policies monitor one or more types of WebLogic Server metrics, such as memory, idle threads, and CPU load. When the configured threshold in a policy is met, the policy is triggered, and the corresponding scaling action is executed. The WebLogic Kubernetes Operator project provides a shell script, scalingAction.sh, for use as a Script Action, which illustrates how to issue a request to the operator’s REST endpoint.\nBeginning with operator version 4.0.5, the operator\u0026rsquo;s REST endpoint is disabled by default. Install the operator with the Helm install option --set \u0026quot;enableRest=true\u0026quot; to enable the REST endpoint.\nConfigure automatic scaling of WebLogic clusters in Kubernetes with WLDF The following steps are provided as a guideline on how to configure a WLDF Policy and Script Action component for issuing scaling requests to the operator\u0026rsquo;s REST endpoint:\nCopy the scalingAction.sh script to $DOMAIN_HOME/bin/scripts so that it\u0026rsquo;s accessible within the Administration Server pod. For more information, see Configuring Script Actions in Configuring and Using the Diagnostics Framework for Oracle WebLogic Server.\nConfigure a WLDF policy and action as part of a diagnostic module targeted to the Administration Server. For information about configuring the WLDF Policies and Actions component, see Configuring Policies and Actions in Configuring and Using the Diagnostics Framework for Oracle WebLogic Server.\na. Configure a WLDF policy with a rule expression for monitoring WebLogic Server metrics, such as memory, idle threads, and CPU load for example.\nb. Configure a WLDF script action and associate the scalingAction.sh script.\nImportant notes about the configuration properties for the Script Action:\nThe scalingAction.sh script requires access to the SSL certificate of the operator’s endpoint and this is provided through the environment variable INTERNAL_OPERATOR_CERT.\nThe operator’s SSL certificate can be found in the internalOperatorCert entry of the operator’s ConfigMap weblogic-operator-cm:\nFor example:\n#\u0026gt; kubectl describe configmap weblogic-operator-cm -n weblogic-operator ... Data ==== internalOperatorCert: ---- LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR3akNDQXFxZ0F3SUJBZ0lFRzhYT1N6QU... ... The scalingAction.sh script accepts a number of customizable parameters:\naction - scaleUp or scaleDown (Required)\ndomain_uid - WebLogic domain unique identifier (Required)\ncluster_name - WebLogic cluster name (Required)\nkubernetes_master - Kubernetes master URL, default=https://kubernetes\nSet this to https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT} when invoking scalingAction.sh from the Administration Server pod.\naccess_token - Service Account Bearer token for authentication and authorization for access to REST Resources\nwls_domain_namespace - Kubernetes Namespace in which the WebLogic domain is defined, default=default\noperator_service_name - WebLogic Kubernetes Operator Service name of the REST endpoint, default=internal-weblogic-operator-service\noperator_service_account - Kubernetes Service Account name for the operator, default=weblogic-operator\noperator_namespace – Namespace in which the operator is deployed, default=weblogic-operator\nscaling_size – Incremental number of Managed Server instances by which to scale up or down, default=1\nYou can use any of the following tools to configure policies for diagnostic system modules:\nWebLogic Server Administration Console WLST REST JMX application A more in-depth description and example on using WLDF\u0026rsquo;s Policies and Actions component for initiating scaling requests through the operator\u0026rsquo;s REST endpoint can be found in the blogs:\nAutomatic Scaling of WebLogic Clusters on Kubernetes WebLogic Dynamic Clusters on Kubernetes Create ClusterRoleBindings to allow a namespace user to query WLS Kubernetes cluster information The script scalingAction.sh, specified in the WLDF script action, needs the appropriate RBAC permissions granted for the service account user (in the namespace in which the WebLogic domain is deployed) to query the Kubernetes API server for both configuration and runtime information of the Domain. The following is an example YAML file for creating the appropriate Kubernetes ClusterRole bindings:\nIn the following example ClusterRoleBinding definition, the WebLogic domain is deployed to a namespace weblogic-domain. Replace the namespace value with the name of the namespace in which the WebLogic domain is deployed in your Kubernetes environment.\nkind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: weblogic-domain-cluster-role rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;services/status\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;weblogic.oracle\u0026#34;] resources: [\u0026#34;domains\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;update\u0026#34;] --- # # creating role-bindings for cluster role # kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: domain-cluster-rolebinding subjects: - kind: ServiceAccount name: default namespace: weblogic-domain apiGroup: \u0026#34;\u0026#34; roleRef: kind: ClusterRole name: weblogic-domain-cluster-role apiGroup: \u0026#34;rbac.authorization.k8s.io\u0026#34; --- # # creating role-bindings # kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: weblogic-domain-operator-rolebinding namespace: weblogic-operator subjects: - kind: ServiceAccount name: default namespace: weblogic-domain apiGroup: \u0026#34;\u0026#34; roleRef: kind: ClusterRole name: cluster-admin apiGroup: \u0026#34;rbac.authorization.k8s.io\u0026#34; --- Horizontal Pod Autoscaler (HPA) using WebLogic Exporter Metrics Please read this blog post to learn how to scale a WebLogic cluster, based on WebLogic metrics provided by the Monitoring Exporter, using the Kubernetes Horizontal Pod Autoscaler (HPA). We will use the Prometheus Adapter to gather the names of the available metrics from Prometheus at regular intervals. A custom configuration of the adapter will expose only metrics that follow specific formats. Horizontal Pod Autoscaler (HPA) using WebLogic Exporter Metrics. See this corresponding video for a demonstration of the blog post in action. WebLogic Kubernetes Operator support for Kubernetes Horizontal Pod Autoscaling.\nUsing a Prometheus alert action to call the operator\u0026rsquo;s REST scale API In addition to using the WebLogic Diagnostic Framework for automatic scaling of a dynamic cluster, you can use a third-party monitoring application like Prometheus. Please read the following blog for details about Using Prometheus to Automatically Scale WebLogic Clusters on Kubernetes.\nHelpful tips Debugging scalingAction.sh The scalingAction.sh script was designed to be executed within a container of the Administration Server Pod because the associated diagnostic module is targeted to the Administration Server.\nThe easiest way to verify and debug the scalingAction.sh script is to open a shell on the running Administration Server pod and execute the script on the command line.\nThe following example illustrates how to open a bash shell on a running Administration Server pod named domain1-admin-server and execute the scriptAction.sh script. It assumes that:\nThe domain home is in /u01/oracle/user-projects/domains/domain1 (that is, the domain home is inside an image). The Dockerfile copied scalingAction.sh to /u01/oracle/user-projects/domains/domain1/bin/scripts/scalingAction.sh. $ kubectl exec -it domain1-admin-server /bin/bash $ cd /u01/oracle/user-projects/domains/domain1/bin/scripts \u0026amp;\u0026amp; \\ ./scalingAction.sh A log, scalingAction.log, will be generated in the same directory in which the script was executed and can be examined for errors.\nExample on accessing the external REST endpoint The easiest way to test scaling using the external REST endpoint is to use a command-line tool like curl. Using curl to issue an HTTPS scale request requires these mandatory header properties:\nBearer Authorization token SSL certificate for the operator\u0026rsquo;s external REST endpoint X-Requested-By header value The following shell script is an example of how to issue a scaling request, with the necessary HTTP request header values, using curl. This example assumes the operator and Domain YAML file are configured with the following fields in Kubernetes:\nOperator properties: externalRestEnabled: true externalRestHttpsPort: 31001 operator\u0026rsquo;s namespace: weblogic-operator operator\u0026rsquo;s hostname is the same as the host shell script is executed on. Domain fields: WebLogic cluster name: ApplicationCluster Domain UID: domain1 #!/bin/sh # Setup properties ophost=`uname -n` opport=31001 #externalRestHttpsPort cluster=cluster-1 size=3 #New cluster size ns=weblogic-operator # Operator NameSpace sa=weblogic-operator # Operator ServiceAccount domainuid=domain1 # Retrieve service account name for given namespace sec=`kubectl get serviceaccount ${sa} -n ${ns} -o jsonpath=\u0026#39;{.secrets[0].name}\u0026#39;` #echo \u0026#34;Secret [${sec}]\u0026#34; # Retrieve base64 encoded secret for the given service account enc_token=`kubectl get secret ${sec} -n ${ns} -o jsonpath=\u0026#39;{.data.token}\u0026#39;` #echo \u0026#34;enc_token [${enc_token}]\u0026#34; # Decode the base64 encoded token token=`echo ${enc_token} | base64 --decode` #echo \u0026#34;token [${token}]\u0026#34; # clean up any temporary files rm -rf operator.rest.response.body operator.rest.stderr operator.cert.pem # Retrieve SSL certificate from the Operator\u0026#39;s external REST endpoint `openssl s_client -showcerts -connect ${ophost}:${opport} \u0026lt;/dev/null 2\u0026gt;/dev/null | openssl x509 -outform PEM \u0026gt; operator.cert.pem` echo \u0026#34;Rest EndPoint url https://${ophost}:${opport}/operator/v1/domains/${domainuid}/clusters/${cluster}/scale\u0026#34; # Issue \u0026#39;curl\u0026#39; request to external REST endpoint curl --noproxy \u0026#39;*\u0026#39; -v --cacert operator.cert.pem \\ -H \u0026#34;Authorization: Bearer ${token}\u0026#34; \\ -H Accept:application/json \\ -H \u0026#34;Content-Type:application/json\u0026#34; \\ -H \u0026#34;X-Requested-By:WLDF\u0026#34; \\ -d \u0026#34;{\\\u0026#34;spec\\\u0026#34;: {\\\u0026#34;replicas\\\u0026#34;: $size} }\u0026#34; \\ -X POST https://${ophost}:${opport}/operator/v1/domains/${domainuid}/clusters/${cluster}/scale \\ -o operator.rest.response.body \\ --stderr operator.rest.stderr "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-operators/installation/",
	"title": "Installation and upgrade",
	"tags": [],
	"description": "How to install, upgrade, and uninstall the operator.",
	"content": "Contents Introduction Install the operator Install the WebLogic domain resource conversion webhook Set up domain namespaces Update a running operator Upgrade the operator Uninstall the operator Installation sample Introduction This installation guide describes how to configure, install (deploy), update, upgrade, and uninstall an instance of the WebLogic Kubernetes Operator. A single instance is capable of managing multiple domains in multiple namespaces, depending on how it is configured. A Kubernetes cluster can host multiple operators, but no more than one per namespace.\nInstall the operator Before installing the operator, ensure that each of its prerequisite requirements is met. See Prepare for installation.\nBy default, installing the operator also configures a deployment and supporting resources for the conversion webhook and deploys the conversion webhook. The conversion webhook deployment is required for operator version 4.x. When a conversion webhook is already installed, skip the conversion webhook installation by setting the Helm configuration value operatorOnly to true in the helm install command. For more details, see install the conversion webhook.\nAfter meeting the prerequisite requirements, install the operator using the helm install command with the operator Helm chart according to the following instructions.\nMinimally you should specify:\nA Helm \u0026ldquo;release\u0026rdquo; name for the operator The Helm chart location The operator namespace The platform (if required) The namespace selection settings A typical Helm release name is weblogic-operator. The operator samples and documentation often use sample-weblogic-operator.\nYou can override default configuration values in the Helm chart by doing one of the following:\nCreating a custom YAML file containing the values to be overridden, and specifying the --value option on the Helm command line. Overriding individual values directly on the Helm command line, using the --set option. You supply the --namespace argument from the helm install command line to specify the namespace in which the operator will be installed. If not specified, then it defaults to default. If the namespace does not already exist, then Helm will automatically create it (and Kubernetes will create a default service account in the new namespace), but note that Helm will not remove the namespace or service account when the release is uninstalled. If the namespace already exists, then Helm will use it. These are standard Helm behaviors.\nSimilarly, you may override the default serviceAccount configuration value to specify a service account in the operator\u0026rsquo;s namespace that the operator will use. For common use cases, the namespace default service account is sufficient. If you want to use a different service account (recommended), then you must create the operator\u0026rsquo;s namespace and the service account before installing the operator Helm chart (for instructions, see Prepare for installation).\nFor example, using Helm 3.x, with the following settings:\nSetting Value and Notes Helm release name sample-weblogic-operator (you may choose any name) Helm chart repo URL https://oracle.github.io/weblogic-kubernetes-operator/charts Helm chart repo name weblogic-operator namespace sample-weblogic-operator-ns $ kubectl create namespace sample-weblogic-operator-ns Access the operator Helm chart using this format: helm repo add \u0026lt;helm-chart-repo-name\u0026gt; \u0026lt;helm-chart-repo-url\u0026gt;. Each version of the Helm chart defaults to using an operator image from the matching version.\n$ helm repo add weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts --force-update To get information about the operator Helm chart, use the helm show command with this format: helm show \u0026lt;helm-chart-repo-name\u0026gt;/weblogic-operator. For example, with an operator Helm chart where the repository is named weblogic-operator:\n$ helm show chart weblogic-operator/weblogic-operator $ helm show values weblogic-operator/weblogic-operator To list the versions of the operator that you can install from the Helm chart repository:\n$ helm search repo weblogic-operator/weblogic-operator --versions For a specified version of the Helm chart and operator, use the --version \u0026lt;value\u0026gt; option with helm install to choose the version that you want, with the latest value being the default.\nInstall the operator using this format: helm install \u0026lt;helm-release-name\u0026gt; \u0026lt;helm-chart-repo-name\u0026gt;/weblogic-operator ...\n$ helm install sample-weblogic-operator \\ weblogic-operator/weblogic-operator \\ --namespace sample-weblogic-operator-ns \\ --wait This creates a Helm release named sample-weblogic-operator in the sample-weblogic-operator-ns namespace, configures a deployment and supporting resources for the operator, and deploys the operator.\nYou can verify the operator installation by examining the output from the helm install command.\nTo check if the operator is deployed and running, see Troubleshooting.\nNOTES:\nIn this example, you have not set the kubernetesPlatform, but this may be required for your environment. See Determine the platform setting. For more information on specifying the registry credentials when the operator image is stored in a private registry, see Customizing operator image name, pull secret, and private registry. Do not include a backslash (\\) before the equals sign (=) in a domain namespace label selector when specifying the selector in a YAML file. A backslash (\\) is only required when specifying the selector on the command line using --set, as shown in the previous example. Install the WebLogic domain resource conversion webhook By default, the WebLogic domain resource conversion webhook is automatically installed the first time an operator is installed in a cluster and removed the first time an operator is uninstalled.\nNOTE: If you are using multiple operators, or want to be able to create or alter domains even when no operators are running, then you will need to fine tune this life cycle. For conversion webhook installation details, see Install the conversion webhook.\nSet up domain namespaces To configure or alter the namespaces that an operator will check for domain resources, see Namespace management.\nUpdate a running operator You can update the settings on a running operator by using the helm upgrade command.\nIn most use cases, you should specify --reuse-values on the helm upgrade command line to ensure that the operator continues to use the values that you have already specified (otherwise the operator will revert to using the default for all values).\nExample updates:\nChange the image of a running operator; see Upgrade the operator. Change the logging level; see Operator logging level. Change the managed namespaces; see Namespace management. Upgrade the operator You can upgrade a 3.x operator while the operator\u0026rsquo;s domain resources are deployed and running. The following instructions will be applicable to upgrade operators as additional versions are released.\nWhen upgrading the operator:\nUse helm repo add to supply a new version of the Helm chart. Use the helm upgrade command with the --reuse-values parameter. Supply a new image value. The rationale for supplying a new image value is because, even with a new version of the Helm chart, --reuse-values will retain the previous image value from when it was installed. To upgrade, you must override the image value to use the new operator image version.\nNOTE: When upgrading a 3.x operator to 4.x, note that the default value of domainNamespaceSelectionStrategy changed from List to LabelSelector, so you need to label the namespaces that the operator is supposed to watch, rather than just providing the list of namespaces. For detailed information, see Namespace management.\nFor example:\n$ helm repo add weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts --force-update $ helm upgrade sample-weblogic-operator \\ weblogic-operator/weblogic-operator \\ --reuse-values \\ --set image=ghcr.io/oracle/weblogic-kubernetes-operator:4.2.20 \\ --namespace sample-weblogic-operator-ns \\ --wait Upgrading a 3.x operator will not automatically roll any running WebLogic Server instances created by the original operator. It is not necessary and such instances will continue to run without interruption during the upgrade.\nWhen you upgrade a 3.x operator to 4.0, it will also create a WebLogic Domain resource conversion webhook deployment and its associated resources in the same namespace. If the conversion webhook deployment already exists in some other namespace, then a new conversion webhook deployment is not created. The webhook automatically and transparently upgrades the existing Domains from the 3.x schema to the 4.0 schema. For more information, see WebLogic Domain resource conversion webhook.\nUninstall the operator If you uninstall an operator, then any domains that it is managing will continue running; however, any changes to a domain resource that was managed by the operator will not be detected or automatically handled, and, if you want to clean up such a domain, then you will need to manually delete all of the domain\u0026rsquo;s resources (domain, pods, services, and such).\nThe helm uninstall command is used to remove an operator release and its associated resources from the Kubernetes cluster. The Helm release name and namespace used with the helm uninstall command must be the same release name used with the helm install command (see Install the operator).\nFor example, assuming the Helm release name is sample-weblogic-operator and the operator namespace is sample-weblogic-operator-ns:\n$ helm uninstall sample-weblogic-operator -n sample-weblogic-operator-ns If the operator\u0026rsquo;s namespace or service account did not exist before the Helm chart was installed, then Helm will create them during helm install; however, helm uninstall will not remove them.\nAfter removing the operator deployment, you should also remove the Domain and Cluster custom resource definitions (CRD) if they are no longer needed:\n$ kubectl delete customresourcedefinition domains.weblogic.oracle $ kubectl delete customresourcedefinition clusters.weblogic.oracle Note that the custom resource definitions are shared. Do not delete them if there are other operators in the same cluster or you have running domain resources.\nBeginning with operator version 4.0, uninstalling an operator also removes the conversion webhook deployment and its associated resources by default. Therefore, if you have multiple operators running, then, by default, an uninstall of one operator will affect the other operators. The uninstall will not delete the conversion definition in the domain CRD so you will be unable to create domains using weblogic.oracle/v8 schema. If you want to prevent the uninstall of an operator\nfrom having these side effects, then use one of the following two options:\nInstall the conversion webhook in a separate namespace using webhookOnly=true Helm configuration value. Use the preserveWebhook=true Helm configuration value during operator installation with the helm install command. For more information, see uninstall the conversion webhook for the conversion webhook uninstallation details.\nInstallation sample For an example of installing the operator, setting the namespace that it monitors, deploying a domain resource to its monitored namespace, and uninstalling the operator, see the Quick Start.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/base-images/patch-images/",
	"title": "Patch running domains",
	"tags": [],
	"description": "Dynamically update or upgrade WebLogic images in a running domain.",
	"content": "Apply patched images to a running domain When updating the WebLogic binaries of a running domain in Kubernetes with a patched container image, the operator applies the update in a zero downtime fashion. The procedure for the operator to update the running domain differs depending on the domain home source type. See the following corresponding sections:\nDomain on PV Model in Image with auxiliary images Model in Image without auxiliary images Domain in Image For a broader description of managing the evolution and mutation of container images to run WebLogic Server in Kubernetes, see CI/CD.\nDomain on PV Oracle strongly recommends strictly limiting access to Domain on PV domain home files. A WebLogic domain home has sensitive information including credentials that are used to access external resources (for example, a data source password), and decryption keys (for example, the DOMAIN_HOME/security/SerializedSystemIni.dat domain secret file).\nFor Domain on PV domains, the container image contains only the JDK and WebLogic Server binaries, and its domain home is located on a Persistent Volume (PV) where the domain home is generated by the user.\nFor this domain home source type, you can create your own patched images using the steps in Create a custom image with patches applied or you can obtain patched images from the Oracle Container Registry, see Obtain images from the Oracle Container Registry.\nTo apply the patched image, edit the Domain Resource image reference with the new image name/tag (for example, oracle/weblogic:12.2.1.4-patched). Then, the operator automatically performs a rolling restart of the WebLogic domain to update the Oracle Home of the servers. For more information on server restarts, see Restarting.\nModel in Image with auxiliary images For Model in Image domains when using auxiliary images:\nThe container image contains only the JDK and WebLogic Server binaries. The WebLogic Deployment Tooling (WDT) installation and model files are located in a separate auxiliary image. The domain home is generated by the operator during runtime. To create and apply patched WebLogic Server images to a running domain of this type, first follow the steps in Obtain images from the Oracle Container Registry or Create a custom image with patches applied to obtain or create the container image, and then edit the Domain Resource image field with the new image name (for example, oracle/weblogic:12.2.1.4-patched).\nTo apply patched images to a running domain of this type, follow the same steps that you used to create your original auxiliary image and alter your domain resource to reference the new image (see Auxiliary images). The operator will then perform a rolling restart of the WebLogic domain to update the Oracle Home of the servers.\nModel in Image without auxiliary images NOTE: Model in Image without auxiliary images (the WDT model and installation files are included in the same image with the WebLogic Server installation) is deprecated in WebLogic Kubernetes Operator version 4.0.7. Oracle recommends that you use Model in Image with auxiliary images. See Auxiliary images.\nFor Model in Image domains without using auxiliary images:\nThe container image contains the JDK, WebLogic Server binaries, a WebLogic Deployment Tooling (WDT) installation and model files. The domain home is generated by the operator during runtime. If you need to update the image for a running Model in Image domain, then simply follow the same steps that you used to create the original image as described in Create a custom image with patches applied, and edit the domain resource\u0026rsquo;s domain.spec.image attribute with the new image\u0026rsquo;s name/tag (mydomain:v2). The operator will then perform a rolling restart of the WebLogic domain to update the Oracle Home of the servers.\nDomain in Image NOTE: The Domain in Image domain home source type is deprecated in WebLogic Kubernetes Operator version 4.0. Oracle recommends that you choose either Domain on PV or Model in Image, depending on your needs.\nIf you need to update the image for a running Domain in Image domain, then use the WIT rebase command to update the Oracle Home for an existing domain image using the patched Oracle Home from a patched container image. For Domain in Image domains:\nThe container image contains the JDK, WebLogic Server binaries, and domain home.\nThe domain home is generated during image creation using either WLST or WDT, usually with the assistance of the WebLogic Image Tool (WIT).\nThe rebase command does the following:\nMinimizes the image size. The alternative update command does not remove old WebLogic installations in the image but instead, layers new WebLogic installations on top of the original installation, thereby greatly increasing the image size; we strongly recommend against using the update command in this situation.\nCreates a new WebLogic image by copying an existing WebLogic domain home from an existing image to a new image. It finds the domain home location within the original image using the image\u0026rsquo;s internal DOMAIN_HOME environment variable.\nMaintains the same security configuration as the original image because the domain home is copied (for example, the DOMAIN_HOME/security/SerializedSystemIni.dat file). This ensures that pods that are based on the new image are capable of joining an already running domain with pods on an older version of the image with same security configuration.\nUsing rebase, the new image can be created in one of two ways:\nAs a new WebLogic image from a base OS image (similar to the create command; recommended).\nNOTE: Oracle strongly recommends rebasing your images with the latest security patches by applying the --recommendedPatches option.\nTo activate:\nSet --tag to the name of the final new image. Set --sourceImage to the WebLogic image that contains the WebLogic configuration. Set additional fields (such as the WebLogic and JDK locations), similar to those used by create. See Create a custom base image. Do not set --targetImage. (When you don\u0026rsquo;t specify a --targetImage, rebase will use the same options and defaults as create.) Or, as a base image, use WebLogic Server CPU images from OCR that do not already have a domain home.\nUsage: Set --tag to the name of the final new image. Set --sourceImage to the WebLogic image that contains the WebLogic configuration. Set --targetImage to the image that you will you use as a base for the new layer. Example: First, generate the new image: $ /tmp/imagetool/bin/imagetool.sh rebase \\ --tag mydomain:v2 \\ --sourceImage mydomain:v1 \\ --targetImage container-registry.oracle.com/middleware/weblogic_cpu:12.2.1.4-generic-jdk8-ol8 Second, edit the domain resource domain.spec.image attribute with the new image\u0026rsquo;s name mydomain:v2. Then, the operator automatically performs a rolling upgrade on the domain. In summary, the rebase command preserves the original domain home\u0026rsquo;s security configuration files in a Domain in Image image so that, when they are both deployed to the same running domain, your updated images and original images can interoperate without a domain secret mismatch.\nNOTES:\nYou cannot use the rebase command alone to update the domain home configuration. If you need to update the domain home configuration, then use the rebase command first, followed by the update command.\nAn Oracle Home and the JDK must be installed in the same directories on each image.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/domain-on-pv/domain-creation-images/",
	"title": "Domain creation images",
	"tags": [],
	"description": "Domain creation images supply the WDT model for Domain on PV.",
	"content": "Contents Introduction Configuration References Source locations Multiple images Configuration examples Example 1: Basic configuration Example 2: Source locations Example 3: Multiple images Introduction Domain creation images are used for supplying WebLogic Deploy Tooling (WDT) model files, WDT variables files, WDT application archive files (collectively known as WDT model files), and the directory where the WebLogic Deploy Tooling software is installed (known as the WDT Home) when deploying a domain using a Domain on PV model. You distribute WDT model files and the WDT executable using these images, then the operator uses them to manage the domain.\nNOTE: These images are only used for creating the domain and will not be used to update the domain.\nConfiguration You can configure one or more domain creation images in a domain resource.\nOptionally, you can set the imagePullPolicy, which defaults to Always if the image ends in :latest and IfNotPresent, otherwise. If image pull secrets are required for pulling the images, then the secrets must be referenced using domain.spec.imagePullSecrets.\nAlso, optionally, you can configure the source locations of WDT model files and WDT Home using the sourceModelHome and sourceWDTInstallHome fields, as described in this section.\nFor details about each field, see the References.\nFor a basic configuration example, see Configuration example 1.\nReferences Run the kubectl explain domain.spec.configuration.initializeDomainOnPV.domain.domainCreationImages command, or\nSee the initializeDomainOnPV.domain.domainCreationImages section in the domain resource schema.\nSource locations Use the optional attributes, sourceModelHome and sourceWdtInstallHome, to specify non-default locations for the WDT model files and WDT Home in your domain creation image(s). Allowed values for sourceModelHome and sourceWdtInstallHome:\nUnset - Defaults to /auxiliary/models and /auxiliary/weblogic-deploy, respectively. Set to a path - Must point to an existing location containing the WDT model files and WDT Home, respectively. None - Indicates that the image has no WDT model files or WDT Home, respectively. If you set the sourceModelHome or sourceWDTInstallHome to None or, the source attributes are left unset and there are no files at the default locations, then the operator will ignore the source directories. Otherwise, note that if you set a source directory attribute to a specific value and there are no files in the specified directory in the domain creation image, then the domain deployment will fail.\nThe files in sourceModelHome and sourceWDTInstallHome directories will be made available in /aux/models and /aux/weblogic-deploy directories of the WebLogic Server container in all pods, respectively.\nFor example source locations, see Configuration example 2.\nMultiple images If specifying multiple images with WDT model files in their respective sourceModelHome directories, then WDT model files are merged. Files from later images take precedence over files from earlier images.\nWhen specifying multiple images, ensure that only one of the images supplies a WDT Home using sourceWDTInstallHome. If you provide more than one WDT Home among multiple images, then the domain deployment will fail. Set sourceWDTInstallHome to None, or make sure there are no files in /auxiliary/weblogic-deploy, for all but one of your specified domain creation images.\nFor an example of configuring multiple images, see Configuration example 3.\nConfiguration examples The following configuration examples illustrate each of the previously described sections.\nExample 1: Basic configuration This example specifies the image location; all other fields are at default values.\nspec: configuration: initializeDomainOnPV: domainCreationImages: - image: wdt-model-image:v1 Example 2: Source locations This example is the same as Example 1, except that it specifies the source locations for the WDT model files and WDT Home.\nspec: configuration: configuration: initializeDomainOnPV: domainCreationImages: - image: wdt-model-image:v1 sourceModelHome: /foo/models sourceWDTInstallHome: /bar/weblogic-deploy Example 3: Multiple images This example is the same as Example 1, except it configures multiple images and sets the sourceWDTInstallHome for the second image to None. In this case, the source location of the WDT installation from the second image wdt-model-image2:v1 will be ignored.\nspec: configuration: initializeDomainOnPV: domainCreationImages: - image: wdt-model-image:v1 - image: wdt-model-image2:v1 sourceWDTInstallHome: None "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/domain-resource/",
	"title": "Domain and Cluster resources",
	"tags": [],
	"description": "Create your own Domain and Cluster resources.",
	"content": "This document describes how to create your own Domain and Cluster resources.\nContents Overview Prerequisites Deploying domain and cluster resource YAML files Domain and cluster resource attribute references Using kubectl explain Domain and cluster spec elements Domain spec elements Cluster spec elements JVM memory and Java option environment variables Node Manager environment variables Pod generation Overview Domain resources reference WebLogic domain configuration, a WebLogic install, images, and anything else necessary to run the domain. Beginning with operator 4.0, WebLogic clusters that are within a WebLogic domain configuration may optionally be associated with a Cluster resource in addition to a Domain resource. This Cluster resource makes it simpler to scale the number of running member servers using kubectl scale, the Kubernetes built-in Horizontal Pod Autoscaling, or similar tools. These Cluster resources are only active if referenced by a Domain resource. NOTE: This Cluster resource is a new custom resource different from the Domain resource. In earlier operator versions, you specified the life cycle attributes of WebLogic clusters in the Domain resource cluster section. Beginning with operator 4.0, the Domain resource cluster section references only the name of the Cluster resources; all the life cycle attributes are specified in the new Cluster resource objects.\nMany of the samples accompanying the operator project include scripts to generate initial Domain and Cluster resources from a set of simplified inputs; however, these resources are the actual source of truth for how the operator will manage each WebLogic Server domain. You are encouraged to either start with the YAML files generated by the various samples, or create Domain and Cluster resources manually or by using other tools, based on the schema referenced here or in this documentation.\nPrerequisites The following prerequisites must be fulfilled before proceeding with the creation of domain and cluster resources:\nCreate a Kubernetes Namespace unless your intention is to use the default namespace. Make sure the WebLogic Kubernetes Operator is running and is configured to monitor the namespace. Depending on the configuration, you may need to update the operator by running helm upgrade to be able to monitor a new namespace. For more information, see Namespace management. Choose a domain home source type. Make sure any resources that will be referenced are deployed to the same namespace. For example, all domain resources have a spec.webLogicCredentialsSecret field that references a Kubernetes Secret containing the username and password of the WebLogic Server administrative account. Make sure any domain and cluster resources and the corresponding WebLogic configuration meet Kubernetes resource name restrictions. Deploying domain and cluster resource YAML files Domains and clusters are defined using YAML files. For each WebLogic Server domain you want to run, you should create one YAML file defining a Domain resource and, optionally, one Cluster resource for each WebLogic cluster, and apply it. In the following referenced example, the sample scripts generate a YAML file that you can use as a basis. Copy the file and override the default settings so that it matches all the WebLogic Server domain parameters that define your domain.\nSee the WebLogic Server samples, Domain home on a PV, Domain home in Image, and Model in Image.\nAfter you have written your YAML files, you use them to create your domain artifacts using the kubectl apply -f command.\n$ kubectl apply -f domain-resource.yaml $ kubectl apply -f cluster-resource.yaml To confirm that the Domain and any Clusters were created, use this command:\n$ kubectl get weblogic -n \u0026lt;namespace\u0026gt; To view all of the attributes of a running domain, including the domain\u0026rsquo;s status, use this command:\n$ kubectl describe domain \u0026lt;domain-resource-name\u0026gt; -n \u0026lt;namespace\u0026gt; Or this command:\n$ kubectl get domain \u0026lt;domain-resource-name\u0026gt; -n \u0026lt;namespace\u0026gt; -o yaml To view all of the attributes of a cluster, use this command:\n$ kubectl describe cluster \u0026lt;cluster-resource-name\u0026gt; -n \u0026lt;namespace\u0026gt; Domain and cluster resource attribute references The domain resource metadata section names the Domain and its namespace. The name of the Domain is the default value for the domainUID which is used by the operator to distinguish domains running in the Kubernetes cluster that may have the same domain name. The Domain name must be unique in the namespace and the domainUID should be unique across the cluster. The domainUID, Domain resource name, and domain name (from the WebLogic domain configuration) may all be different. Similarly, the cluster resource metadata section names the Cluster and its namespace. The metadata.name of a cluster resource must be referenced from the spec.clusters field of the domain resource. This name does not need to be the same as the name of the WebLogic cluster. The spec.clusterName field of the cluster resource must name the WebLogic cluster from the domain configuration.\nThe domain resource spec section describes the intended running state of the domain, including intended runtime state of WebLogic Server instances, and details about Kubernetes Pod or Service generation, such as resource constraints, scheduling requirements, or volume mounts. The cluster resource spec section describes the intended number of member servers of that cluster to run and can set or override the other settings inherited from the domain resource.\nThe operator automatically updates the status section of a deployed domain or cluster resource to describe the actual running state, including WebLogic Server instance runtime states and current health.\nHere are some references you can use for the fields in these sections:\nSee Domain and cluster spec elements, Pod Generation, and JVM memory and Java option environment variables in this doc. See the Domain Resource reference document. See the Cluster Resource reference document. Use kubectl explain from the command line. Using kubectl explain You can access the description of any field using kubectl explain. For instance, the following command displays the description of the domainUID field:\n$ kubectl explain domain.spec.domainUID KIND: Domain VERSION: weblogic.oracle/v9 FIELD: domainUID \u0026lt;string\u0026gt; DESCRIPTION: Domain unique identifier. It is recommended that this value be unique to assist in future work to identify related domains in active-passive scenarios across data centers; however, it is only required that this value be unique within the namespace, similarly to the names of Kubernetes resources. This value is distinct and need not match the domain name from the WebLogic domain configuration. Defaults to the value of `metadata.name`. NOTE: The VERSION field\u0026rsquo;s value may be different, depending on the operator version you are using.\nDomain and cluster spec elements The Domain spec section contains elements for configuring the domain operation and sub-sections specific to the Administration Server, specific clusters, or specific Managed Servers. The Cluster spec section contains elements for configuring the lifecycle options for all of the Managed Server members of a WebLogic cluster, including Java options, environment variables, additional Pod content, and the ability to explicitly start, stop, or restart cluster members.\nDomain spec elements Elements related to domain identification, container image, and domain home:\ndomainUID: Domain unique identifier. This identifier is required to be no more than 45 characters (for more details, see Meet Kubernetes resource name restrictions. It is recommended that this value be unique to assist in future work to identify related domains in active-passive scenarios across data centers; however, it is only required that this value be unique within the namespace, similarly to the names of Kubernetes resources. This value is distinct and need not match the domain name from the WebLogic domain configuration. Defaults to the value of metadata.name. image: The WebLogic container image; required when domainHomeSourceType is Image or FromModel (when not specifying an auxiliary image); otherwise, defaults to container-registry.oracle.com/middleware/weblogic:12.2.1.4. WARNING: The default image is unpatched and therefore, should always be specified to use an image with the latest patches installed (for example, container-registry.oracle.com/middleware/weblogic_cpu:12.2.1.4-slim-jdk8-ol8). For more information, see Understand Oracle Container Registry images. imagePullPolicy: The image pull policy for the WebLogic container image. Legal values are Always, Never, and IfNotPresent. Defaults to Always if the image ends in :latest; IfNotPresent, otherwise. To ensure that all nodes have the same image, any images that are updated without changing the tag should be set to Always. imagePullSecrets: A list of image pull Secrets for the WebLogic container image. domainHome: The directory containing the WebLogic domain configuration inside the container. Defaults to /shared/domains/domains/ if domainHomeSourceType is PersistentVolume. Defaults to /u01/oracle/user_projects/domains/ if domainHomeSourceType is Image. Defaults to /u01/domains/ if domainHomeSourceType is FromModel. domainHomeSourceType: Domain home file system source type: Legal values: Image, PersistentVolume, FromModel. Image indicates that the domain home file system is present in the container image specified by the image field. PersistentVolume indicates that the domain home file system is located on a persistent volume. FromModel indicates that the domain home file system will be created and managed by the operator based on a WDT domain model. dataHome: An optional directory in a server\u0026rsquo;s container for data storage of default and custom file stores. If dataHome is not specified or its value is either not set or empty, then the data storage directories are determined from the WebLogic domain configuration. Elements related to logging:\nincludeServerOutInPodLog: Specifies whether the server .out file will be included in the Pod\u0026rsquo;s log. Defaults to true. logHome: The directory in a server\u0026rsquo;s container in which to store the domain, Node Manager, server logs, server *.out, introspector .out, and optionally HTTP access log files if httpAccessLogInLogHome is true. Ignored if logHomeEnabled is false. See also logHomeLayout. logHomeEnabled: Specifies whether the log home folder is enabled. Defaults to true if domainHomeSourceType is PersistentVolume; false, otherwise. logHomeLayout: Specifies how log files are organized under the logHome directory when logHomeEnabled is set to true. Defaults to ByServers, where domain logs and introspector.out are placed at root level and all other log files are organized under logHome/servers/\u0026lt;server-name\u0026gt;/logs. If Flat is specified, then all files are placed at root level. httpAccessLogInLogHome: Specifies whether the server HTTP access log files will be written to the same directory specified in logHome. Otherwise, server HTTP access log files will be written to the directory configured in the WebLogic domain configuration. Defaults to true. fluentdSpecification: Automatic fluentd sidecar injection. If specified, the operator will deploy a sidecar container alongside each WebLogic Server instance that runs the fluentd, Elements related to failure retry:\nfailureRetryIntervalSeconds: The wait time in seconds before the start of the next retry after a Severe failure. Defaults to 120. failureRetryLimitMinutes: The time in minutes before the operator will stop retrying Severe failures. Defaults to 1440. Elements related to security:\nwebLogicCredentialsSecret: Reference to a Kubernetes Secret that contains the user name and password needed to boot a WebLogic Server under the username and password fields. See also the following elements under configuration. Elements related to domain startup and shutdown:\nserverStartPolicy: The strategy for deciding whether to start a WebLogic Server instance. Legal values are AdminOnly, Never, or IfNeeded. Defaults to IfNeeded. restartVersion: Changes to this field cause the operator to restart WebLogic Server instances. replicas: The default number of cluster member Managed Server instances to start for each WebLogic cluster in the domain configuration, unless replicas is specified for that cluster in its Cluster resource. For each cluster, first the operator will sort cluster member Managed Server names from the WebLogic domain configuration by normalizing any numbers in the Managed Server name and then sorting alphabetically. This is done so that server names such as \u0026ldquo;managed-server10\u0026rdquo; come after \u0026ldquo;managed-server9\u0026rdquo;. Then, the operator will start Managed Servers from the sorted list, up to the replicas count, unless specific Managed Servers are specified as starting in their entry under the managedServers field. In that case, the specified Managed Servers will be started and then additional cluster members will be started, up to the replicas count, by finding further cluster members in the sorted list that are not already started. Note that if cluster members are started because of their entries under managedServers, then a cluster may have more cluster members running than its replicas count. Defaults to 1. maxClusterConcurrentStartup: The maximum number of cluster member Managed Server instances that the operator will start in parallel for a given cluster, if maxConcurrentStartup is not specified for a specific cluster under the clusters field. A value of 0 means there is no configured limit. Defaults to 0. maxClusterConcurrentShutdown: The default maximum number of WebLogic Server instances that a cluster will shut down in parallel when it is being partially shut down by lowering its replica count. introspectVersion: Changes to this field cause the operator to repeat its introspection of the WebLogic domain configuration (see Initiating introspection). Repeating introspection is required for the operator to recognize changes to the domain configuration, such as adding a new WebLogic cluster or Managed Server instance, to regenerate configuration overrides, or to regenerate the WebLogic domain home when the domainHomeSourceType is FromModel. Introspection occurs automatically, without requiring change to this field, when servers are first started or restarted after a full domain shut down. For the FromModel domainHomeSourceType, introspection also occurs when a running server must be restarted because of changes to any of the fields listed here. See also overrideDistributionStrategy. Elements related to specifying and overriding WebLogic domain configuration:\nThese elements are under configuration.\noverridesConfigMap: The name of the ConfigMap for WebLogic configuration overrides. overrideDistributionStrategy: Determines how updated configuration overrides are distributed to already running WebLogic Server instances following introspection when the domainHomeSourceType is PersistentVolume or Image. Configuration overrides are generated during introspection from Secrets, the overridesConfigMap field, and WebLogic domain topology. Legal values are Dynamic, which means that the operator will distribute updated configuration overrides dynamically to running servers, and OnRestart, which means that servers will use updated configuration overrides only after the server\u0026rsquo;s next restart. The selection of OnRestart will not cause servers to restart when there are updated configuration overrides available. See also introspectVersion. Defaults to Dynamic. secrets: A list of names of the Secrets for WebLogic configuration overrides or model. introspectorJobActiveDeadlineSeconds: The introspector job timeout value in seconds. If this field is specified, then the operator\u0026rsquo;s ConfigMap data.introspectorJobActiveDeadlineSeconds value is ignored. Defaults to 120 seconds. These elements are under configuration.model, only apply if the domainHomeSourceType is FromModel, and are discussed in Model in Image.\nconfigMap: (Optional) Name of a ConfigMap containing WebLogic Deploy Tooling model YAML files or .properties files. domainType: WebLogic Deploy Tooling domain type. Legal values: WLS, RestrictedJRF, JRF. Defaults to WLS. runtimeEncryptionSecret: The name of the Secret containing the runtime encryption password, which must be in a field named password. Required when domainHomeSourceType is set to FromModel. modelHome: Location of the WebLogic Deploy Tooling model home directory, which can include model YAML files, .properties variable files, and application .zip archives. Defaults to /u01/wdt/models. onlineUpdate.*: Settings related to the online update option for Model In Image dynamic updates. onlineUpdate.enabled: Enable online update for model changes to a running domain. Default is false. onlineUpdate.onNonDynamicChanges: Controls behavior when non-dynamic WebLogic configuration changes are detected during an online update. Non-dynamic changes are changes that require a domain restart to take effect. Valid values are CommitUpdateOnly (default) and CommitUpdateAndRoll. For more information, see Online update handling of non-dynamic WebLogic configuration changes in the Runtime Updates section of the Model in Image user guide. onlineUpdate.wdtTimeouts.*: Rarely needed timeout settings for online update calls to the WebLogic domain from WebLogic Deploy Tooling within the introspector job. All timeouts are specified in milliseconds and default to two or three minutes. For a full list of timeouts, call kubectl explain domain.spec.configuration.model.onlineUpdate.wdtTimeouts. These elements are under configuration.opss, and only apply if the domainHomeSourceType is FromModel and the domainType is JRF.\nwalletPasswordSecret: Name of a Secret containing the OPSS key passphrase, which must be in a field named walletPassword. Used to encrypt and decrypt the wallet that is used for accessing the domain\u0026rsquo;s entries in its RCU database. walletFileSecret: Name of a Secret containing the OPSS key wallet file, which must be in a field named walletFile. Use this to allow a JRF domain to reuse its entries in the RCU database. This allows you to specify a wallet file that was obtained from the domain home after the domain was booted for the first time. Elements related to Kubernetes Pod and Service generation:\nserverPod: Customization affecting the generation of Pods for WebLogic Server instances. serverService: Customization affecting the generation of ClusterIP Services for WebLogic Server instances. Sub-sections related to the Administration Server, specific clusters, or specific Managed Servers:\nadminServer: Lifecycle options for the Administration Server, including Java options, environment variables, additional Pod content, and which channels or network access points should be exposed using a NodePort Service. clusters: Beginning with operator 4.0 and the weblogic.oracle/v9 API version of the domain resource, this field lists the referenced Cluster resources. The Cluster resource now has lifecycle options for all the Managed Server members of a WebLogic cluster, including Java options, environment variables, additional Pod content, and the ability to explicitly start, stop, or restart cluster members. The spec.clusterName field of each Cluster resource must match a cluster that already exists in the WebLogic domain configuration. managedServers: Lifecycle options for individual Managed Servers, including Java options, environment variables, additional Pod content, and the ability to explicitly start, stop, or restart a named server instance. The serverName field of each entry must match a Managed Server that already exists in the WebLogic domain configuration or that matches a dynamic cluster member based on the server template. The elements serverStartPolicy, serverPod and serverService are repeated under adminServer and managedServers and as part of each Cluster resource. The values directly under spec, set the defaults for the entire domain. The values that are part of a Cluster resource set the defaults for cluster members of that cluster. The values under adminServer or an entry under managedServers, set the values for that specific server. Values from the domain scope and values from the cluster (for cluster members) are merged with or overridden by the setting for the specific server depending on the element. See Startup and shutdown for details about serverStartPolicy combinations.\nElements related to the customization of liveness and readiness probes:\nSee Liveness probe customization for details about the elements related to liveness probe customization and Readiness probe customization for details about the elements related to readiness probe customization. Cluster spec elements For a complete list of the Cluster spec elements, see Cluster Spec.\nAdditional information for the clusterService.sessionAffinity element:\nsessionAffinity: This is an advanced setting that is applicable only when the kube-proxy is running in non-default proxy modes, such as User space (legacy) proxy mode and IPVS proxy mode. It is used to enable session affinity based on the client\u0026rsquo;s IP addresses. For more information, see the Virtual IPs and service proxies. Must be ClientIP or None. Defaults to None.\nNOTE: This setting is not applicable when the kube-proxy is running in the default iptables proxy mode.\nFor additional domain and cluster resource attribute reference material, see Domain and cluster resource attribute references.\nJVM memory and Java option environment variables You can use the following environment variables to specify JVM memory and JVM option arguments to WebLogic Server Managed Server and Node Manager instances:\nJAVA_OPTIONS: Java options for starting WebLogic Server. USER_MEM_ARGS: JVM memory arguments for starting WebLogic Server. NODEMGR_JAVA_OPTIONS: Java options for starting a Node Manager instance. NODEMGR_MEM_ARGS: JVM memory arguments for starting a Node Manager instance; this will take precedence over JAVA_OPTIONS and USER_MEM_ARGS. WLST_PROPERTIES: System properties for WLST commands in introspector jobs or WebLogic Server instance containers. WLST_EXTRA_PROPERTIES: System properties appended to WLST_PROPERTIES for WLST commands in introspector jobs or WebLogic Server instance containers. WLSDEPLOY_PROPERTIES: System properties for WebLogic Deploy Tooling commands during Model in Image introspector jobs or WebLogic Server instance containers. PRE_CLASSPATH: Path(s) that are prepended to the WebLogic Server system classpath; delimit multiple paths with a colon :. CLASSPATH: Path(s) that are appended to the WebLogic Server system classpath; delimit multiple paths with a colon :. Node Manager environment variables You can use the following environment variables to specify the logging files limit.\nNODEMGR_LOG_FILE_MAX: Maximum size of the Node Manager Log specified as an integer. When this limit is reached, a new log file is started. Default: 0, no limit. NODEMGR_LOG_LEVEL: Severity level of logging used for the Node Manager log. Node Manager uses the standard logging levels from the java.util.logging.level package. Default: FINEST. NODEMGR_LOG_COUNT: Maximum number of log files to create when LogLimit is exceeded. Default: 1. NOTES:\nThe following behavior occurs depending on whether or not NODEMGR_JAVA_OPTIONS and NODEMGR_MEM_ARGS are defined: If NODEMGR_JAVA_OPTIONS is not defined and JAVA_OPTIONS is defined, then the JAVA_OPTIONS value will be applied to the Node Manager instance. If NODEMGR_MEM_ARGS is not defined and USER_MEM_ARGS is defined, then the USER_MEM_ARGS value will be applied to the Node Manager instance. If nothing else is specified, then the default Java property values (-Xms64m -Xmx100m -Djava.security.egd=file:/dev/./urandom) will be applied to the Node Manager instance. The USER_MEM_ARGS and WLST_EXTRA_PROPERTIES environment variables both default to -Djava.security.egd=file:/dev/./urandom in all WebLogic Server pods and the WebLogic introspection job. They can be explicitly set to another value in your Domain or Cluster YAML file using the env attribute under the serverPod configuration. Notice that the NODEMGR_MEM_ARGS, USER_MEM_ARGS, and WLST_EXTRA_PROPERTIES environment variables all include -Djava.security.egd=file:/dev/./urandom by default. This helps to speed up the Node Manager and WebLogic Server startup on systems with low entropy, plus similarly helps to speed up introspection job usage of the WLST encrypt command. For a detailed description of Java and pod memory tuning see the Pod memory and CPU resources FAQ. You can use JAVA_OPTIONS and WLSDEPLOY_PROPERTIES to disable Fast Application Notifications (FAN); see the Disable Fast Application Notifications FAQ for details. This example snippet illustrates how to add some of the previously mentioned environment variables using the env attribute under the serverPod configuration in your Domain or Cluster YAML file.\n# Copyright (c) 2017, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # apiVersion: \u0026#34;weblogic.oracle/v9\u0026#34; kind: Domain metadata: name: domain1 namespace: domains23 labels: weblogic.domainUID: domain1 spec: serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false \u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-Djava.security.egd=file:/dev/./urandom \u0026#34; - name: NODEMGR_JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false \u0026#34; - name: NODEMGR_MEM_ARGS value: \u0026#34;-Xms64m -Xmx100m -Djava.security.egd=file:/dev/./urandom \u0026#34; - name: PRE_CLASSPATH value: \u0026#34;/sample/path/prepended/to/weblogic/system/classpath:/another/prepended/path\u0026#34; - name: CLASSPATH value: \u0026#34;/sample/path/appended/to/weblogic/system/classpath:/another/appended/path\u0026#34; Pod generation The operator creates a Pod for each running WebLogic Server instance. This Pod will have a container, named weblogic-server, based on the container image specified by the image field. Additional Pod or container content can be specified using the elements under serverPod. This includes Kubernetes sidecar and init containers, labels, annotations, volumes, volume mounts, scheduling constraints, including anti-affinity, resource requirements, or security context.\nCustomer provided labels and annotations may not begin with \u0026ldquo;weblogic\u0026rdquo; and the operator will generate the following labels:\nweblogic.createdByOperator: \u0026quot;true\u0026quot; weblogic.domainName: \u0026lt;domain-name\u0026gt;, where \u0026lt;domain-name\u0026gt; is the name of the WebLogic domain weblogic.domainUID: \u0026lt;uid\u0026gt;, where \u0026lt;uid\u0026gt; is the domain UID from the Domain resource weblogic.serverName: \u0026lt;server-name\u0026gt;, where \u0026lt;server-name\u0026gt; is the name of the WebLogic Server instance weblogic.clusterName: \u0026lt;cluster-name\u0026gt;, where \u0026lt;cluster-name\u0026gt; is the name of the cluster of which this instance is a member, if any weblogic.domainRestartVersion: \u0026lt;restart-version\u0026gt;, matches domain.spec.restartVersion after the pod is up to date with this version weblogic.introspectVersion: \u0026lt;introspect-version\u0026gt;, matches domain.spec.introspectVersion after the pod is up to date with this version Prior to creating a Pod, the operator replaces variable references allowing the Pod content to be templates. The format of these variable references is $(VARIABLE_NAME) where VARIABLE_NAME is one of the variable names available in the container for the WebLogic Server instance. The default set of environment variables includes:\nDOMAIN_NAME: The WebLogic Server domain name. DOMAIN_UID: The domain unique identifier. DOMAIN_HOME: The domain home location as a file system path within the container. SERVER_NAME: The WebLogic Server instance name. CLUSTER_NAME: The WebLogic cluster name, if this is a cluster member. LOG_HOME: If the `domain.spec.logHomeEnabled\u0026rsquo; attribute is set to true, then this contains the WebLogic log location as a file system path within the container This example Domain and Cluster YAML file specifies that Pods for WebLogic Server instances in the cluster-1 cluster will have a per-Managed Server volume and volume mount (similar to a Kubernetes StatefulSet), an init container to initialize some files in that volume, and anti-affinity scheduling so that the server instances are scheduled, as much as possible, on different Nodes:\n# Copyright (c) 2019, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # apiVersion: \u0026#34;weblogic.oracle/v9\u0026#34; kind: Domain metadata: name: domain1 namespace: domains23 labels: weblogic.domainUID: domain1 spec: domainHome: /u01/oracle/user_projects/domains/domain1 domainHomeSourceType: Image image: \u0026#34;phx.ocir.io/weblogick8s/my-domain-home-in-image:12.2.1.4\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; imagePullSecrets: - name: ocirsecret webLogicCredentialsSecret: name: domain1-weblogic-credentials includeServerOutInPodLog: true serverStartPolicy: IfNeeded serverPod: env: - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false\u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-Djava.security.egd=file:/dev/./urandom \u0026#34; clusters: - domain1-cluster-1 --- apiVersion: \u0026#34;weblogic.oracle/v1\u0026#34; kind: Cluster metadata: name: domain1-cluster-1 namespace: domains23 labels: weblogic.domainUID: domain1 spec: - clusterName: cluster-1 serverPod: volumes: - name: $(SERVER_NAME)-volume emptyDir: {} volumeMounts: - mountPath: /server-volume name: $(SERVER_NAME)-volume initContainers: - name: volumeinit image: \u0026#34;oraclelinux:7-slim\u0026#34; imagePullPolicy: IfNotPresent command: [\u0026#34;/usr/bin/sh\u0026#34;] args: [\u0026#34;echo\u0026#34;, \u0026#34;Replace with command to initialize files in /init-volume\u0026#34;] volumeMounts: - mountPath: /init-volume name: $(SERVER_NAME)-volume replicas: 2 The operator uses an \u0026ldquo;introspection\u0026rdquo; job to discover details about the WebLogic domain configuration, such as the list of clusters and network access points. The Job Pod for the introspector is generated using the serverPod entries for the Administration Server. Because the Administration Server name is not known until the introspection step is complete, the value of the $(SERVER_NAME) variable for the introspection job will be \u0026ldquo;introspector\u0026rdquo;.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-operators/",
	"title": "Manage operators",
	"tags": [],
	"description": "Install, configure, and manage the operator.",
	"content": "The following sections guide you through installing, configuring, and managing the operator.\nOverview An introduction to the operator runtime.\nPrepare for installation Consult these preparation steps, strategy choices, and prequisites prior to installing an operator.\nInstallation and upgrade How to install, upgrade, and uninstall the operator.\nUpgrade operator from version 3.x to 4.x Conversion webhook for upgrading the domain resource schema.\nConfiguration reference An operator runtime is installed and configured using Helm. Here are useful Helm operations and operator configuration values.\nNamespace management Configure or dynamically change the namespaces that a running operator manages.\nService accounts Kubernetes ServiceAccounts for the operator.\nRBAC Operator role-based authorization.\nREST services Use the operator\u0026#39;s REST services.\nSet up Kubernetes Get help for setting up a Kubernetes environment\nCommon mistakes and solutions Help for common installing and upgrading mistakes.\nTroubleshooting General advice for debugging and monitoring the operator.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/reference/domain-resource/",
	"title": "Domain resource",
	"tags": [],
	"description": "Use this document to set up and configure your own YAML file containing Domains and Clusters.",
	"content": "View the Domain reference document here.\nView the Cluster reference document here.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/quickstart/create-domain/",
	"title": "Create a domain",
	"tags": [],
	"description": "",
	"content": "Create the domain using a domain resource. Select a user name and password for the WebLogic domain administrator credentials and use them to create a Kubernetes Secret for the domain.\n$ kubectl create secret generic sample-domain1-weblogic-credentials \\ --from-literal=username=ADMIN_USERNAME --from-literal=password=ADMIN_PASSWORD \\ -n sample-domain1-ns Replace ADMIN_USERNAME and ADMIN_PASSWORD with your choice of user name and password. Note that the password must be at least 8 characters long and must contain at least one non-alphabetical character.\nCreate a domain runtime encryption secret.\n$ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-runtime-encryption-secret \\ --from-literal=password=my_runtime_password These two commands create secrets named sample-domain1-weblogic-credentials and sample-domain1-runtime-encryption-secret used in the sample domain YAML file. If you want to use different secret names, then you will need to update the sample domain YAML file accordingly in the next step.\nCreate the sample-domain1 domain resource and an associated sample-domain1-cluster-1 cluster resource using a single YAML resource file which defines both resources. The domain resource and cluster resource do not replace the traditional WebLogic configuration files, but instead cooperates with those files to describe the Kubernetes artifacts of the corresponding domain.\nUse the following command to apply the two sample resources.\n$ kubectl apply -f https://raw.githubusercontent.com/oracle/weblogic-kubernetes-operator/release/4.2/kubernetes/samples/quick-start/domain-resource.yaml NOTE: If you want to view or need to modify it, you can download the sample domain resource to a file called /tmp/quickstart/domain-resource.yaml or similar. Then apply the file using kubectl apply -f /tmp/quickstart/domain-resource.yaml.\nThe domain resource references the cluster resource, a WebLogic Server installation image, the secrets you defined, and a sample \u0026ldquo;auxiliary image\u0026rdquo;, which contains traditional WebLogic configuration and a WebLogic application.\nTo examine the domain resource, click here. For detailed information, see Domain resource. The Quick Start guide\u0026rsquo;s sample domain resource references a WebLogic Server version 12.2.1.4 General Availability (GA) image. GA images are suitable for demonstration and development purposes only where the environments are not available from the public Internet; they are not acceptable for production use. In production, you should always use CPU (patched) images from OCR or create your images using the WebLogic Image Tool (WIT) with the --recommendedPatches option. For more guidance, see Apply the Latest Patches and Updates in Securing a Production Environment for Oracle WebLogic Server.\nConfirm that the operator started the servers for the domain.\na. Use kubectl to show that the Domain was created.\n$ kubectl describe domain sample-domain1 -n sample-domain1-ns b. Get the domain status using the following command. If you don\u0026rsquo;t have the jq executable installed, then run the second command to get the domain status.\n$ kubectl get domain sample-domain1 -n sample-domain1-ns -o json | jq .status OR\n$ kubectl get domain sample-domain1 -n sample-domain1-ns -o jsonpath=\u0026#39;{.status}\u0026#39; c. After a short time, you will see the Administration Server and Managed Servers running.\n$ kubectl get pods -n sample-domain1-ns d. You should also see all the Kubernetes Services for the domain.\n$ kubectl get services -n sample-domain1-ns If the operator didn\u0026rsquo;t start the servers for the domain, see Domain debugging.\nCreate an ingress route for the domain. Create an ingress route for the domain, in the domain namespace, by using the following YAML file.\na. Download the ingress route YAML to a file called /tmp/quickstart/ingress-route.yaml or similar.\nb. Then apply the file using the following command.\n$ kubectl apply -f /tmp/quickstart/ingress-route.yaml \\ --namespace sample-domain1-ns To confirm that the ingress controller noticed the new ingress route and is successfully routing to the domain\u0026rsquo;s server pods, send a request to the URL for the \u0026ldquo;quick start app\u0026rdquo;, as shown in the following example, which will return an HTTP 200 status code.\nSingle Node Cluster OKE Cluster $ curl -i http://localhost:30305/quickstart/ HTTP/1.1 200 OK Content-Length: 274 Content-Type: text/html; charset=UTF-8 Date: Wed, 15 Jun 2022 14:20:59 GMT Set-Cookie: JSESSIONID=JONnvS__IkBUN9nqZG4SfuUU3QdEj_4bissfck1GPbY6YJxgjXpS!1733001435; path=/; HttpOnly X-Oracle-Dms-Ecid: be865b9d-cc96-4dca-ab80-f0b6c5b05326-00000015 X-Oracle-Dms-Rid: 0 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to the WebLogic on Kubernetes Quick Start Sample\u0026lt;/font\u0026gt;\u0026lt;/h1\u0026gt;\u0026lt;br\u0026gt; \u0026lt;b\u0026gt;WebLogic Server Name:\u0026lt;/b\u0026gt; managed-server1\u0026lt;br\u0026gt;\u0026lt;b\u0026gt;Pod Name:\u0026lt;/b\u0026gt; sample-domain1-managed-server1\u0026lt;br\u0026gt;\u0026lt;b\u0026gt;Current time:\u0026lt;/b\u0026gt; 14:21:00\u0026lt;br\u0026gt;\u0026lt;p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ LOADBALANCER_INGRESS_IP=$(kubectl get svc traefik-operator -n traefik -o jsonpath='{.status.loadBalancer.ingress[].ip}{\u0026quot;\\n\u0026quot;}') $ curl -i http://${LOADBALANCER_INGRESS_IP}/quickstart/ HTTP/1.1 200 OK Content-Length: 274 Content-Type: text/html; charset=UTF-8 Date: Wed, 15 Jun 2022 14:20:59 GMT Set-Cookie: JSESSIONID=JONnvS__IkBUN9nqZG4SfuUU3QdEj_4bissfck1GPbY6YJxgjXpS!1733001435; path=/; HttpOnly X-Oracle-Dms-Ecid: be865b9d-cc96-4dca-ab80-f0b6c5b05326-00000015 X-Oracle-Dms-Rid: 0 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to the WebLogic on Kubernetes Quick Start Sample\u0026lt;/font\u0026gt;\u0026lt;/h1\u0026gt;\u0026lt;br\u0026gt; \u0026lt;b\u0026gt;WebLogic Server Name:\u0026lt;/b\u0026gt; managed-server1\u0026lt;br\u0026gt;\u0026lt;b\u0026gt;Pod Name:\u0026lt;/b\u0026gt; sample-domain1-managed-server1\u0026lt;br\u0026gt;\u0026lt;b\u0026gt;Current time:\u0026lt;/b\u0026gt; 14:21:00\u0026lt;br\u0026gt;\u0026lt;p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Depending on where your Kubernetes cluster is running, you may need to open firewall ports or update security lists to allow ingress to this port.\nTo access the WebLogic Server Administration Console: Single Node Cluster OKE Cluster Open a browser to http://localhost:30305/console. a. Get the load balancer ingress IP address using the following command: $ LOADBALANCER_INGRESS_IP=$(kubectl get svc traefik-operator -n traefik -o jsonpath='{.status.loadBalancer.ingress[].ip}{\u0026quot;\\n\u0026quot;}') b. Open a browser to http://${LOADBALANCER_INGRESS_IP}/console. Do not use the WebLogic Server Administration Console to start or stop servers, or for scaling clusters. See Starting and stopping servers and Scaling.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/domain-on-pv/model-files/",
	"title": "Working with WDT model files",
	"tags": [],
	"description": "Learn about model file requirements, macros, and loading order.",
	"content": "Contents Sample WDT model file with macros Important notes about WDT model files WDT models source location and loading order Model file macros Using secrets in model files Using environment variables in model files Combining secrets and environment variables in model files This document describes working with WebLogic Deploy Tooling (WDT) model files in the operator. For additional information, see the WebLogic Deploy Tooling documentation.\nThe WDT Discover Domain Tool is particularly useful for generating WDT model files from an existing domain home.\nSample WDT model file with macros Here\u0026rsquo;s an example of a WDT model YAML file describing a domain. For detailed information, see Metadata model.\ndomainInfo: AdminUserName: \u0026#39;@@SECRET:__weblogic-credentials__:username@@\u0026#39; AdminPassword: \u0026#39;@@SECRET:__weblogic-credentials__:password@@\u0026#39; RCUDbInfo: rcu_prefix: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_prefix@@\u0026#39; rcu_schema_password: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_schema_password@@\u0026#39; rcu_db_conn_string: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_db_conn_string@@\u0026#39; topology: Name: \u0026#39;@@ENV:DOMAIN_UID@@\u0026#39; AdminServerName: \u0026#34;admin-server\u0026#34; Cluster: \u0026#34;cluster-1\u0026#34;: ... resources: ... appDeployments: Application: myear: SourcePath: wlsdeploy/applications/sample_app.ear ModuleType: ear Target: \u0026#39;cluster-1\u0026#39; This sample model file has four sections:\nSection Purpose domainInfo Describes the domain level information. topology Describes the topology of the domain. resources Describes the J2EE resources used in the domain. appDeployments Describes the applications and libraries used in the domain. Notice this value pattern: @@...@@. These are macros that will be resolved at runtime by WDT in the operator environment. For a description of model file macro references to secrets and environment variables, see Model file macros.\nImportant notes about WDT model files Using model file macros:\nYou can use model macros to reference arbitrary secrets from model files. This is recommended for handling mutable values such as database user names, passwords, and URLs. See Using secrets in model files.\nAll password fields in a model should use a secret macro. Passwords should not be directly included in property or model files because the files may appear in logs or debugging.\nModel files encrypted with the WDT Encrypt Model Tool are not supported. Use secrets instead.\nYou can use model macros to reference arbitrary environment variables from model files. This is useful for handling plain text mutable values that you can define using an env stanza in your Domain YAML file, and is also useful for accessing the built in DOMAIN_UID environment variable. See Using environment variables in model files.\nFor most models, it\u0026rsquo;s useful to minimize or eliminate the usage of model variable files (also known as property files) and use secrets or environment variables instead.\nYou can control the order that WDT uses to load your model files, see WDT models location and loading order.\nWDT models source location and loading order Refer to this section if you need to control the order in which your model files are loaded. The order is important when two or more model files refer to the same configuration, because the last model that\u0026rsquo;s loaded has the highest precedence.\nDuring domain home creation, model and property files are first loaded from the models image and then from the optional WDT ConfigMap.\nDomain deployment model Models image source specification Optional WDT ConfigMap specification Model in Image domain.spec.image or domain.spec.configuration.model.auxiliaryImages domain.spec.configuration.model.configMap Domain on PV domain.spec.configuration.initializeDomainOnPV.domain.domainCreationImages domain.spec.configuration.initializeDomainOnPV.domain.domainCreationConfigMap The loading order within each of these locations is first determined using the convention filename.##.yaml and filename.##.properties, where ## are digits that specify the desired order when sorted numerically. Additional details:\nEmbedding a .##. in a file name is optional. When present, it must be placed just before the properties or yaml extension in order for it to take precedence over alphabetical precedence. The precedence of file names that include more than one .##. is undefined. The number can be any integer greater than or equal to zero. File names that don\u0026rsquo;t include .##. sort before other files as if they implicitly have the lowest possible .##. If two files share the same number, the loading order is determined alphabetically as a tie-breaker. After all the models are sorted, the operator will create a comma-separated list and this is passed to the WDT create domain command:\n/u01/wdt/models/model1.yaml,/u01/wdt/models/model2.yaml,/weblogic-operator/wdt-config-map/modela.yaml,/weblogic-operator/wdt-config-maap/mdoelb.yaml Internally, the WDT create command will first merge all the model files into a single model file and resolve all the macros before processing the model to create the domain. The final merged model must be valid, both syntactically and semantically. If the deployment model is Model in Image, then the merged model is saved internally.\nNOTE: If the WDT model files in the image source are supplied by combining multiple images , then the files in this directory are populated according to their Merge order before the loading order is determined.\nFor example, if you have these files in the model home directory:\njdbc.20.yaml main-model.10.yaml my-model.10.yaml y.yaml And, you have these files in the ConfigMap:\njdbc-dev-urlprops.10.yaml z.yaml Then the combined model files list is passed to WebLogic Deploy Tooling as:\ny.yaml,main-model.10.yaml,my-model.10.yaml,jdbc.20.yaml,z.yaml,jdbc-dev-urlprops.10.yaml Property files (ending in .properties) use the same sorting algorithm, but they are appended together into a single file prior to passing them to the WebLogic Deploy Tooling.\nModel file macros WDT models can have macros that reference secrets or environment variables.\nUsing secrets in model files You can use WDT model @@SECRET macros to reference the WebLogic administrator username and password keys that are stored in a Kubernetes Secret and to optionally reference additional secrets. Here is the macro pattern for accessing these secrets:\nDomain Resource Attribute Corresponding WDT Model @@SECRET Macro webLogicCredentialsSecret @@SECRET:__weblogic-credentials__:username@@ and @@SECRET:__weblogic-credentials__:password@@ configuration.secrets @@SECRET:mysecret:mykey@@ For example, you can reference the WebLogic credential user name using @@SECRET:__weblogic-credentials__:username@@, and you can reference a custom secret mysecret with key mykey using @@SECRET:mysecret:mykey@@.\nAny secrets that are referenced by an @@SECRET macro must be deployed to the same namespace as your Domain, and must be referenced in your Domain YAML file using the weblogicCredentialsSecret and configuration.secrets fields.\nHere\u0026rsquo;s a sample snippet from a Domain YAML file that sets a webLogicCredentialsSecret and two custom secrets my-custom-secret1 and my-custom-secret2.\nspec: webLogicCredentialsSecret: name: my-weblogic-credentials-secret configuration: secrets: [ my-custom-secret1,my-custom-secret2 ] ... Using environment variables in model files You can reference operator environment variables in model files. This includes any that you define yourself in your Domain YAML file using domain.spec.serverPod.env or domain.spec.adminServer.serverPod.env, or the built-in DOMAIN_UID environment variable.\nFor example, the @@ENV:DOMAIN_UID@@ macro resolves to the current domain\u0026rsquo;s domain UID.\nCombining secrets and environment variables in model files You can embed an environment variable macro in a secret macro. This is useful for referencing secrets that you\u0026rsquo;ve named based on your domain\u0026rsquo;s domainUID.\nFor example, if your domainUID is domain1, then the macro @@SECRET:@@ENV:DOMAIN_UID@@-super-double-secret:mykey@@ resolves to the value stored in mykey for secret domain1-super-double-secret.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/faq/domain-secret-mismatch/",
	"title": "Domain secret mismatch",
	"tags": [],
	"description": "One or more WebLogic Server instances in my domain will not start and the domain resource `status` or the pod log reports errors like this: Domain secret mismatch.",
	"content": " One or more WebLogic Server instances in my domain will not start and the Domain status or the pod log reports errors like this:\nDomain secret mismatch. The domain secret in DOMAIN_HOME/security/SerializedSystemIni.dat where DOMAIN_HOME=$DOMAIN_HOME does not match the domain secret found by the introspector job. WebLogic requires that all WebLogic Servers in the same domain share the same domain secret.\nWhen you see these kinds of errors, it means that the WebLogic domain directory\u0026rsquo;s security configuration files have changed in an incompatible way between when the operator scanned the domain directory, which occurs during the \u0026ldquo;introspection\u0026rdquo; phase, and when the server instance attempted to start.\nTo understand the \u0026ldquo;incompatible domain security configuration\u0026rdquo; type of failure, it\u0026rsquo;s important to review the contents of the WebLogic domain directory. Each WebLogic domain directory contains a security subdirectory that contains a file called SerializedSystemIni.dat. This file contains security data to bootstrap the WebLogic domain, including a domain-specific encryption key.\nDuring introspection, the operator generates a Kubernetes Job that runs a pod in the domain\u0026rsquo;s Kubernetes Namespace and with the same Kubernetes ServiceAccount that will be used later to run the Administration Server. This pod has access to the Kubernetes secret referenced by weblogicCredentialsSecret and encrypts these values with the domain-specific encryption key so that the secured value can be injected in to the boot.properties files when starting server instances.\nWhen the domain directory is changed such that the domain-specific encryption key is different, the boot.properties entries generated during introspection will now be invalid.\nThis can happen in a variety of ways, depending on the domain home source type.\nDomain in Image Rolling to an image containing new or unrelated domain directory The error occurs while rolling pods have containers based on a new container image that contains an entirely new or unrelated domain directory.\nThe problem is that WebLogic cannot support server instances being part of the same WebLogic domain if the server instances do not all share the same domain-specific encryption key. Additionally, operator introspection currently happens only when starting servers following a total shutdown. Therefore, the boot.properties files generated from introspecting the image containing the original domain directory will be invalid when used with a container started with the updated container image containing the new or unrelated domain directory.\nThe solution is to follow either the recommended CI/CD guidelines so that the original and new container images contain domain directories with consistent domain-specific encryption keys and bootstrapping security details, or to perform a total shutdown of the domain so that introspection reoccurs as servers are restarted.\nFull domain shutdown and restart The error occurs while starting servers after a full domain shutdown.\nIf your development model generates new container images with new and unrelated domain directories and then tags those images with the same tag, then different Kubernetes worker nodes may have different images under the same tag in their individual, local container repositories.\nThe simplest solution is to set imagePullPolicy to Always; however, the better solution would be to design your development pipeline to generate new container image tags on every build and to never reuse an existing tag.\nDomain on PV Completely replacing the domain directory The error occurs while starting servers when the domain directory change was made while other servers were still running.\nIf completely replacing the domain directory, then you must stop all running servers.\nBecause all servers will already be stopped, there is no requirement that the new contents of the domain directory be related to the previous contents of the domain directory. When starting servers again, the operator will perform its introspection of the domain directory. However, you may want to preserve the domain directory security configuration including the domain-specific encryption key and, in that case, you should follow a similar pattern as is described in the CI/CD guidelines for the domain in a container image model to preserve the original security-related domain directory files.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/persistent-storage/configmaps/",
	"title": "Provide access to a ConfigMap",
	"tags": [],
	"description": "Provide an instance with access to a ConfigMap.",
	"content": "Configuration files can be supplied to Kubernetes Pods and Jobs by a ConfigMap, which consists of a set of key-value pairs. Each entry may be accessed by one or more operator-managed nodes as a read-only text file. Access can be provided across the domain, within a single cluster, or for a single server. In each case, the access is configured within the serverPod element of the desired scope.\nFor example, given a ConfigMap named my-map with entries key-1 and key-2, you can provide access to both values as separate files in the same directory within the cluster-1 cluster with the following in your Domain:\nclusters: - clusterName: cluster-1 serverPod: volumes: - name: my-volume-1 configMap: name: my-map items: - key: key-1 path: first - key: key-2 path: second volumeMounts: - name: my-volume-1 mountPath: /weblogic-operator/my This provides access to two files, found at paths /weblogic-operator/my/first and /weblogic-operator/my/second. Both a volume and a volumeMount entry are required, and must have the same name. The name of the ConfigMap is specified in the name field under the configMap entry. The items entry is an array, in which each entry maps a ConfigMap key to a file name under the directory specified as mountPath under a volumeMount.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/cicd/mutate-the-domain-layer/",
	"title": "Mutate the domain layer",
	"tags": [],
	"description": "How to mutate the domain layer.",
	"content": "If you need to mutate the domain layer, and keep the same domain encryption keys, then there are some choices about how to implement that, as alluded to previously. Let\u0026rsquo;s explore those in some more detail now.\nThe first option is to implement each mutation as a delta to the previous state. This is conceptually similar to how immutable objects (like Java Strings) are implemented, a \u0026ldquo;copy on write\u0026rdquo; approach applied to the domain configuration as a unit. This does have the advantage that it is simple to implement, but the disadvantage that your builds would depend on the previous good build and this is somewhat contrary to typical CI/CD practices. You also have to work out what to do with the bad builds, or \u0026ldquo;holes\u0026rdquo; in the sequence.\nAn alternative is to capture a \u0026ldquo;primordial state\u0026rdquo; of the domain before starting the sequence. In practical terms, this might mean creating a very simple domain with no applications or resources in it, and \u0026ldquo;saving\u0026rdquo; it before ever starting any servers. This primordial domain (let’s call it t=0) would then be used to build each mutation. So each state is built from t=0, plus all of the changes up to that point.\nSaid another way, each build would start with t=0 as the base image and extend it. This eliminates the need to keep each intermediate state, and would also likely have benefits when you remove things from the domain, because you would not have \u0026ldquo;lost\u0026rdquo; (\u0026ldquo;whited out\u0026rdquo; is the image layer term) space in the intermediate layers. Although, these layers tend to be relatively small, so this is possibly not a big issue.\nThis approach is probably an improvement. It does get interesting though when you update a lower layer, for example when you patch WebLogic or update the JDK. When this happens, you need to create another base image, shown in the diagram as v2 t-0. All of the mutations in this new chain are based on this new base image. So that still leaves us with the problem of how to take the domain from the first series (v1 t=0 to t=3) and \u0026ldquo;copy\u0026rdquo; it across to the second series (v2).\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/accessing-the-domain/ingress/",
	"title": "Ingress",
	"tags": [],
	"description": "Configure load balancers with ingresses.",
	"content": "Ingresses are one approach provided by Kubernetes to configure load balancers. Depending on the version of Kubernetes you are using, and your cloud provider, you may need to use Ingresses. For more information about Ingresses, see the Kubernetes Ingress documentation.\nWebLogic clusters as backends of an Ingress In an Ingress object, a list of backends are provided for each target that will be load balanced. Each backend is typically a Kubernetes Service, more specifically, a combination of a serviceName and a servicePort.\nWhen the operator creates a WebLogic domain, it also creates a service for each WebLogic cluster in the domain. The operator defines the service such that its selector will match all WebLogic Server pods within the WebLogic cluster which are in the \u0026ldquo;ready\u0026rdquo; state.\nThe name of the service created for a WebLogic cluster follows the pattern \u0026lt;domainUID\u0026gt;-cluster-\u0026lt;clusterName\u0026gt;. For example, if the domainUID is domain1 and the cluster name is cluster-1, the corresponding service will be named domain1-cluster-cluster-1.\nThe service name must comply with standard Kubernetes rules for naming of objects and in particular with DNS-1035:\nA DNS-1035 label must consist of lowercase alphanumeric characters or \u0026lsquo;-\u0026rsquo;, start with an alphabetic character, and end with an alphanumeric character (e.g. my-name, or abc-123, regex used for validation is [a-z]([-a-z0-9]*[a-z0-9])?).\nTo comply with these requirements, if the domainUID or the cluster name contains some upper-case characters or underscores, then in the service name the upper-case characters will be converted to lower-case and underscores will be converted to hyphens. For example, if the domainUID is myDomain_1 and the cluster name is myCluster_1, the corresponding service will be named mydomain-1-cluster-mycluster-1.\nThe service, serviceName and servicePort, of a WebLogic cluster will be used in the routing rules defined in the Ingress object and the load balancer will route traffic to the WebLogic Servers within the cluster based on the rules.\nMost common ingress controllers, for example Traefik and NGINX, understand that there are zero or more actual pods behind the service, and they actually build their backend list and route requests to those backends directly, not through the service. This means that requests are properly balanced across the pods, according to the load balancing algorithm in use. Most ingress controllers also subscribe to updates on the service and adjust their internal backend sets when additional pods become ready, or pods enter a non-ready state.\nSteps to set up an ingress load balancer Install the ingress controller.\nAfter the ingress controller is running, it monitors Ingress resources in a given namespace and acts accordingly.\nCreate Ingress resources.\nIngress resources contain routing rules to one or more backends. An ingress controller is responsible to apply the rules to the underlying load balancer. There are two approaches to create the Ingress resource:\nUse the Helm chart.\nEach ingress provider supports a number of annotations in Ingress resources. The Helm chart allows you to define the routing rules without dealing with the detailed provider-specific annotations.\nCreate the Ingress resource manually from a YAML file.\nManually create an Ingress YAML file and then apply it to the Kubernetes cluster.\nGuide and samples for Traefik and NGINX Information about how to install and configure these ingress controllers to load balance WebLogic clusters is provided here:\nTraefik guide NGINX guide For production environments, we recommend NGINX, Traefik (2.2.1 or later) ingress controllers or the load balancer provided by your cloud provider.\nSamples are also provided for the Traefik ingress controller, showing how to manage multiple WebLogic clusters as the backends, using different routing rules, host-routing and path-routing; and TLS termination: Traefik samples.\nNOTE the following Known Limitation, NGINX SSL passthrough ingress service does not work with Kubernetes headless service.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/security/service-accounts/",
	"title": "Service accounts",
	"tags": [],
	"description": "Kubernetes ServiceAccounts for the operator.",
	"content": "This document is now located in the operator user guide, see Service Accounts.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/domain-home-on-pv/",
	"title": "Domain on PV",
	"tags": [],
	"description": "Sample for creating and deploying a WebLogic domain on a persistent volume (PV).",
	"content": "Contents Introduction Domain on PV domain types (WLS and JRF) Sample directory structure Ensuring your Kubernetes cluster can access images References Introduction This sample demonstrates deploying a Domain on PV domain home source type with Domain creation images. The Domain on PV sample uses a WebLogic Deploy Tooling (WDT) model to specify your initial WebLogic configuration.\nWDT models are a convenient and simple alternative to WebLogic Scripting Tool (WLST) configuration scripts. They compactly define a WebLogic domain using model files, variable properties files, and application archive files. The WDT model format is described in the open source, WebLogic Deploy Tooling GitHub project, and the required directory structure for a WDT archive is specifically discussed here.\nFor more information, see the Domain on PV user documentation.\nDomain on PV domain types (WLS and JRF) Domain on PV is supported on two types of domains: a standard Oracle WebLogic Server (WLS) domain and an Oracle Fusion Middleware Infrastructure, Java Required Files (JRF) domain. This sample demonstrates both WLS and JRF domain types.\nThe JRF domain path through the sample includes additional steps required for JRF: deploying an infrastructure database, referencing the infrastructure database from the WebLogic configuration, setting an Oracle Platform Security Services (OPSS) wallet password, and exporting or importing an OPSS wallet file. JRF domains may be used by Oracle products that layer on top of WebLogic Server, such as SOA and OSB.\nSample directory structure The sample contains the following files and directories:\nLocation Description kubernetes/samples/scripts/create-weblogic-domain/domain-on-pv/domain-resources JRF and WLS Domain YAML files. kubernetes/samples/scripts/create-weblogic-domain/wdt-artifacts/archives Source code location for WebLogic Deploy Tooling application ZIP archives. kubernetes/samples/scripts/create-weblogic-domain/wdt-artifacts/wdt-model-files Staging for each domain creation image\u0026rsquo;s WDT YAML files, WDT properties, and WDT archive ZIP files. The directories in wdt-model-files are named for their respective images. kubernetes/samples/scripts/create-weblogic-domain/ingresses Ingress resources. kubernetes/samples/scripts/domain-lifecycle/opss-wallet.sh Utility script for exporting or importing a JRF domain OPSS wallet file. kubernetes/samples/scripts/domain-lifecycle/waitForDomain.sh Utility script that optionally waits for the pods in a domain to reach their expected Completed, image, and ready state. kubernetes/samples/scripts/domain-lifecycle/pv-pvc-helper.sh Utility script to examine or clean up the contents of shared directories on the persistent volume. Ensuring your Kubernetes cluster can access images If you run the sample from a machine that is remote to one or more of your Kubernetes cluster worker nodes, then you need to ensure that the images you create can be accessed from any node in the cluster.\nFor example, if you have permission to put the image in a container registry that the cluster can also access, then:\nAfter you\u0026rsquo;ve created an image: docker tag the image with a target image name (including the registry host name, port, repository name, and the tag, if needed). docker push the tagged image to the target repository. Before you deploy a Domain: Modify the Domain YAML file\u0026rsquo;s spec.configuration.initializeDomainOnPV.domain.domainCreationImages[*].image value to match the image tag for the image in the repository. If the repository requires a login, then also deploy a corresponding Kubernetes docker secret to the same namespace that the Domain will use, and modify the Domain YAML file\u0026rsquo;s imagePullSecrets: to reference this secret. Alternatively, if you have access to the local image cache on each worker node in the cluster, then you can use a Docker command to save the image to a file, copy the image file to each worker node, and use a Docker command to load the image file into the node\u0026rsquo;s image cache.\nFor more information, see the Cannot pull image FAQ.\nReferences For references to the relevant user documentation, see:\nDomain on PV user documentation WebLogic Deploy Tooling WebLogic Image Tool "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/model-in-image/",
	"title": "Model in image",
	"tags": [],
	"description": "Sample for supplying a WebLogic Deploy Tooling (WDT) model that the operator expands into a full domain home during runtime.",
	"content": "Contents Introduction Use cases Sample directory structure Ensuring your Kubernetes cluster can access images References Introduction This sample demonstrates deploying a Model in Image domain home source type with Auxiliary images. Model in Image eliminates the need to pre-create your WebLogic domain home prior to deploying your Domain YAML file. Instead, Model in Image uses a WebLogic Deploy Tooling (WDT) model to specify your WebLogic configuration.\nWDT models are a convenient and simple alternative to WebLogic Scripting Tool (WLST) configuration scripts. They compactly define a WebLogic domain using model files, variable properties files, and application archive files. The WDT model format is described in the open source, WebLogic Deploy Tooling GitHub project, and the required directory structure for a WDT archive is specifically discussed here.\nFurthermore, the Model in Image auxiliary image option lets you supply your WDT models files, WDT variable files, and WDT archives files in a small separate image separate from your WebLogic image.\nFor more information, see the Model in Image user guide. For a comparison of Model in Image to other domain home source types, see Choose a domain home source type.\nUse cases This sample demonstrates five Model in Image use cases:\nInitial: An initial WebLogic domain with the following characteristics:\nAuxiliary image wdt-domain-image:WLS-v1 with: A directory where the WebLogic Deploy Tooling software is installed (also known as WDT Home). A WDT archive with version v1 of an exploded Java EE web application A WDT model with: A WebLogic Administration Server A WebLogic cluster A reference to the web application A WebLogic image with a WebLogic and Java installation. Kubernetes Secrets: WebLogic credentials Required WDT runtime password A Domain with: metadata.name and weblogic.domainUID label set to sample-domain1 spec.domainHomeSourceType: FromModel spec.image set to a WebLogic image with a WebLogic and Java installation. References to the secrets Update 1: Demonstrates updating the initial domain by dynamically adding a data source using a model ConfigMap and then restarting (rolling) the domain to propagate the change. Updates:\nKubernetes Secrets: Same as Initial use case, plus secrets for data source credentials and URL Kubernetes ConfigMap with: A WDT model for a data source targeted to the cluster Domain, same as Initial use case, plus: spec.model.configMap referencing the ConfigMap References to data source secrets Update 2: Demonstrates deploying a second domain (similar to the Update 1 use case domain). Updates:\nKubernetes Secrets and ConfigMap: Similar to the Update 1 use case, except names and labels are decorated with a new domain UID A Domain, similar to Update 1 use case, except: Its metadata.name and weblogic.domainUid label become sample-domain2 instead of sample-domain1 Its secret/ConfigMap references are decorated with sample-domain2 instead of sample-domain1 Has a changed env variable that sets a new domain name Update 3: Demonstrates deploying an updated auxiliary image with an updated application to the Update 1 use case domain and then restarting (rolling) its domain to propagate the change. Updates:\nAuxiliary image wdt-domain-image:WLS-v2, similar to wdt-domain-image:WLS-v1 image with: An updated web application v2 at the myapp-v2 directory path instead of myapp-v1 An updated model that points to the new web application path Domain: Same as the Update 1 use case, except spec.image is wdt-domain-image:WLS-v2 Update 4: Demonstrates dynamically updating the running Update 1 or Update 3 WebLogic domain configuration without requiring a domain restart (roll). Updates:\nKubernetes ConfigMap with: A WDT model for Work Manager minimum and maximum threads constraints, plus the same data source as the Update 1 use case Kubernetes Secrets: Same as the Update 1 and Update 3 use case, except: An updated data source secret with a new password and an increased maximum pool capacity A Domain, same as Update 1 or Update 3 use case, plus: spec.configuration.model.onlineUpdate set to enabled: true Sample directory structure The sample contains the following files and directories:\nLocation Description kubernetes/samples/scripts/create-weblogic-domain/model-in-image/domain-resources Domain YAML files. kubernetes/samples/scripts/create-weblogic-domain/wdt-artifacts/archives Source code location for WebLogic Deploy Tooling application ZIP archives. kubernetes/samples/scripts/create-weblogic-domain/wdt-artifacts/wdt-model-files Staging for each model image\u0026rsquo;s WDT YAML files, WDT properties, and WDT archive ZIP files. The directories in model images are named for their respective images. kubernetes/samples/scripts/create-weblogic-domain/model-in-image/model-configmaps/datasource Staging files for a model ConfigMap that configures a data source. kubernetes/samples/scripts/create-weblogic-domain/model-in-image/model-configmaps/workmanager Staging files for a model ConfigMap that configures the Work Manager threads constraints. kubernetes/samples/scripts/create-weblogic-domain/ingresses Ingress resources. kubernetes/samples/scripts/create-weblogic-domain/model-in-image/utils/patch-introspect-version.sh Utility script for updating a running domain spec.introspectVersion field (which causes it to \u0026rsquo;re-instrospect\u0026rsquo; and \u0026lsquo;roll\u0026rsquo; only if non-dynamic attributes are updated). kubernetes/samples/scripts/create-weblogic-domain/model-in-image/utils/patch-restart-version.sh Utility script for updating a running domain spec.restartVersion field (which causes it to \u0026rsquo;re-instrospect\u0026rsquo; and \u0026lsquo;roll\u0026rsquo;). kubernetes/samples/scripts/create-weblogic-domain/model-in-image/utils/patch-enable-online-update.sh Utility script for updating a running domain spec.configuration.model.onlineUpdate field to enabled: true (which enables the online update feature). In addition, this sample makes use of the waitForDomain.sh sample lifecycle script that is located in the operator source kubernetes/samples/scripts/domain-lifecycle directory. This is a utility script that optionally waits for the pods in a domain to reach their expected restartVersion, introspectVersion, Completed, image, and ready state.\nEnsuring your Kubernetes cluster can access images If you run the sample from a machine that is remote to one or more of your Kubernetes cluster worker nodes, then you need to ensure that the images you create can be accessed from any node in the cluster.\nFor example, if you have permission to put the image in a container registry that the cluster can also access, then:\nAfter you\u0026rsquo;ve created an image: docker tag the image with a target image name (including the registry host name, port, repository name, and the tag, if needed). docker push the tagged image to the target repository. Before you deploy a Domain: Modify the Domain YAML file\u0026rsquo;s image: value to match the image tag for the image in the repository. If the repository requires a login, then also deploy a corresponding Kubernetes docker secret to the same namespace that the Domain will use, and modify the Domain YAML file\u0026rsquo;s imagePullSecrets: to reference this secret. Alternatively, if you have access to the local image cache on each worker node in the cluster, then you can use a Docker command to save the image to a file, copy the image file to each worker node, and use a docker command to load the image file into the node\u0026rsquo;s image cache.\nFor more information, see the Cannot pull image FAQ.\nReferences For references to the relevant user documentation, see:\nModel in Image user documentation WebLogic Deploy Tooling WebLogic Image Tool "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/rest/",
	"title": "REST APIs",
	"tags": [],
	"description": "Sample for generating a self-signed certificate and private key that can be used for the operator&#39;s external REST API.",
	"content": "Sample to create certificate and key When a user enables the operator\u0026rsquo;s external REST API (by setting externalRestEnabled to true when installing or upgrading the operator Helm chart), the user also needs to provide the certificates and private key used for the SSL/TLS identity on the external REST API endpoint by creating a kubernetes tls secret and using that secret\u0026rsquo;s name with the operator Helm chart values.\nThis sample script generates a self-signed certificate and private key that can be used for the operator\u0026rsquo;s external REST API when experimenting with the operator.\nThe certificate and key generated with this script should not be used in a production environment.\nThe syntax of the script is:\n$ kubernetes/samples/scripts/rest/generate-external-rest-identity.sh \\ -a \u0026lt;SANs\u0026gt; -n \u0026lt;operator-namespace\u0026gt; [-s \u0026lt;secret-name\u0026gt;] Where \u0026lt;SANs\u0026gt; lists the subject alternative names to put into the generated self-signed certificate for the external operator REST HTTPS interface, \u0026lt;operator-namespace\u0026gt; should match the namespace where the operator will be installed, and optionally the secret name, which defaults to weblogic-operator-external-rest-identity.\nYou should include the addresses of all masters and load balancers (for example, what a client specifies to access the external REST endpoint) in the subject alternative name list. In addition, each name must be prefaced by DNS: for a host name, or IP: for an address, as in this example:\n-a \u0026#34;DNS:myhost,DNS:localhost,IP:127.0.0.1\u0026#34; The external certificate and key can be changed after installation of the operator. For more information, see Updating operator external certificates.\nThe following script will create the tls secret named weblogic-operator-identity in the namespace weblogic-operator-ns, using a self-signed certificate and private key:\n$ echo \u0026#34;externalRestEnabled: true\u0026#34; \u0026gt; my_values.yaml $ generate-external-rest-identity.sh \\ -a \u0026#34;DNS:${HOSTNAME},DNS:localhost,IP:127.0.0.1\u0026#34; \\ -n weblogic-operator-ns -s weblogic-operator-identity \u0026gt;\u0026gt; my_values.yaml $ kubectl -n weblogic-operator-ns describe secret weblogic-operator-identity $ helm install my_operator weblogic-operator/weblogic-operator \\ --namespace weblogic-operator-ns --values my_values.yaml --wait "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/model-in-image/update2/",
	"title": "Update 2",
	"tags": [],
	"description": "",
	"content": "This use case demonstrates concurrently deploying a domain that is similar to the Update 1 use case domain to the same sample-domain1-ns namespace, but with a different domain UID, a different WebLogic domain name, and a different WebLogic domain encryption key. It does this by:\nUsing the same image, image model YAML file, and application archive as the Initial and Update 1 use cases. Using the same model update ConfigMap source file as the Update 1 use case (a data source). Using a different (unique) domain UID, sample-domain2, for the new domain. Using a different (unique) domain name, domain2, for the different domains. Deploying secrets and a model update ConfigMap that are uniquely labeled and named for the new domain. Note that this use case shows Model in Image\u0026rsquo;s unique ability to quickly deploy a copy of a WebLogic domain that has a different WebLogic domain name and domain encryption key. This is a useful capability that is not supported by the Domain in Image domain home source type:\nDomain in Image does not support overriding the domain name, but different domain names are necessary when two domains need to interoperate. This use case takes advantage of model macros to ensure that its two different domains have a different domain name:\nFirst, you define the domain name in the model YAML file using the @@ENV:CUSTOM_DOMAIN_NAME@@ environment variable macro. Second, you set the value of the CUSTOM_DOMAIN_NAME environment variable to be different using the env stanza in each Domain\u0026rsquo;s YAML file. Domain in Image requires that its images embed a WebLogic security/SerializedSystemIni.dat domain encryption key that cannot be changed for the image (see Why layering matters in CI/CD considerations). This necessarily means that two Domain in Image domains that share the same image can decrypt each other\u0026rsquo;s encrypted passwords. On the other hand, a Model in Image\u0026rsquo;s domain encryption key is not embedded in the image and instead, is dynamically and uniquely created each time the domain is started.\nOracle requires interoperating WebLogic domains to have different domain names. This is necessary when two domains communicate, or when a WebLogic Server or WebLogic Java client concurrently connects to multiple domains.\nHere are the steps for this use case:\nMake sure you have deployed the domain from the Update 1 use case.\nCreate a ConfigMap with the WDT model that contains the data source definition.\nRun the following commands:\n$ kubectl -n sample-domain1-ns create configmap sample-domain2-wdt-config-map \\ --from-file=/tmp/sample/model-configmaps/datasource $ kubectl -n sample-domain1-ns label configmap sample-domain2-wdt-config-map \\ weblogic.domainUID=sample-domain2 If you\u0026rsquo;ve created your own data source file in the Update 1 use case, then substitute the file name in the --from-file= parameter (we suggested /tmp/sample/mydatasource.yaml earlier). Note that the -from-file= parameter can reference a single file, in which case it puts the designated file in the ConfigMap, or it can reference a directory, in which case it populates the ConfigMap with all of the files in the designated directory.\nObservations:\nWe are leaving the namespace sample-domain1-ns unchanged for the ConfigMap because you will deploy domain sample-domain2 to the same namespace as sample-domain1. You name and label the ConfigMap using its associated domain UID for two reasons: To make it obvious which ConfigMap belongs to which domain. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all resources associated with a domain. You use a different ConfigMap for the new domain for two reasons: To make it easier to keep the life cycle and/or CI/CD process for the two domains simple and independent. To \u0026lsquo;future proof\u0026rsquo; the new domain so that changes to the original domain or new domain\u0026rsquo;s ConfigMap can be independent. Create the secrets that are referenced by the WDT model files in the image and ConfigMap; they also will be referenced by the Domain YAML file.\nRun the following commands:\nNOTE: Substitute a password of your choice for MY_WEBLOGIC_ADMIN_PASSWORD. This password should contain at least seven letters plus one digit.\nNOTE: Substitute a password of your choice for MY_RUNTIME_PASSWORD. It should be unique and different than the admin password, but this is not required.\n# spec.webLogicCredentialsSecret $ kubectl -n sample-domain1-ns create secret generic \\ sample-domain2-weblogic-credentials \\ --from-literal=username=weblogic --from-literal=password=MY_WEBLOGIC_ADMIN_PASSWORD $ kubectl -n sample-domain1-ns label secret \\ sample-domain2-weblogic-credentials \\ weblogic.domainUID=sample-domain2 # spec.configuration.model.runtimeEncryptionSecret $ kubectl -n sample-domain1-ns create secret generic \\ sample-domain2-runtime-encryption-secret \\ --from-literal=password=MY_RUNTIME_PASSWORD $ kubectl -n sample-domain1-ns label secret \\ sample-domain2-runtime-encryption-secret \\ weblogic.domainUID=sample-domain2 # referenced by spec.configuration.secrets and by the data source model YAML in the ConfigMap $ kubectl -n sample-domain1-ns create secret generic \\ sample-domain2-datasource-secret \\ --from-literal=\u0026#39;user=sys as sysdba\u0026#39; \\ --from-literal=\u0026#39;password=incorrect_password\u0026#39; \\ --from-literal=\u0026#39;max-capacity=1\u0026#39; \\ --from-literal=\u0026#39;url=jdbc:oracle:thin:@oracle-db.default.svc.cluster.local:1521/devpdb.k8s\u0026#39; $ kubectl -n sample-domain1-ns label secret \\ sample-domain2-datasource-secret \\ weblogic.domainUID=sample-domain2 Observations:\nWe are leaving the namespace sample-domain1-ns unchanged for each secret because you will deploy domain sample-domain2 to the same namespace as sample-domain1. You name and label the secrets using their associated domain UID for two reasons: To make it obvious which secret belongs to which domain. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all resources associated with a domain. You use a different set of secrets for the new domain for two reasons: To make it easier to keep the life cycle and/or CI/CD process for the two domains simple and independent. To \u0026lsquo;future proof\u0026rsquo; the new domain so that changes to the original domain\u0026rsquo;s secrets or new domain\u0026rsquo;s secrets can be independent. We deliberately specify an incorrect password and a low maximum pool capacity in the data source secret because we will demonstrate dynamically correcting the data source attributes for sample-domain1 in the Update 4 use case. Set up a Domain YAML file that is similar to your Update 1 use case Domain YAML file but with a different domain UID, domain name, model update ConfigMap reference, and Secret references:\nOption 1: Update a copy of your Domain YAML file from the Update 1 use case.\nIn the Update 1 use case, we suggested creating a file named /tmp/sample/mii-update1.yaml or using the /tmp/sample/domain-resources/WLS/mii-update1-d1-WLS-v1-ds.yaml file that is supplied with the sample.\nWe suggest copying this Domain YAML file and naming the copy /tmp/sample/mii-update2.yaml before making any changes.\nWorking on a copy is not strictly necessary, but it helps keep track of your work for the different use cases in this sample and provides you a backup of your previous work.\nChange the /tmp/sample/mii-update2.yaml Domain YAML file name and weblogic.domainUID label to sample-domain2.\nThe final result will look something like this:\napiVersion: \u0026#34;weblogic.oracle/v9\u0026#34; kind: Domain metadata: name: sample-domain2 namespace: sample-domain1-ns labels: weblogic.domainUID: sample-domain2 NOTE: We are leaving the namespace sample-domain1-ns unchanged because you will be deploying domain sample-domain2 to the same namespace as sample-domain1.\nChange the /tmp/sample/mii-update2.yaml Domain YAML file\u0026rsquo;s CUSTOM_DOMAIN_NAME environment variable from domain1 to domain2.\nThe model file in the image uses macro @@ENV:CUSTOM_DOMAIN_NAME@@ to reference this environment variable when setting its domain name.\nSpecifically, change the corresponding Domain spec.serverPod.env YAML file stanza to look something like this:\n... spec: ... serverPod: ... env: - name: CUSTOM_DOMAIN_NAME value: \u0026#34;domain2\u0026#34; ... Change the /tmp/sample/mii-update2.yaml Domain YAML file\u0026rsquo;s spec.domainHome value to /u01/domains/sample-domain2. The corresponding YAML file stanza will look something like this:\n... spec: ... domainHome: /u01/domains/sample-domain2 ... (This change is not strictly needed, but it is a helpful convention to decorate a WebLogic domain\u0026rsquo;s home directory with its domain name or domain UID.)\nChange the /tmp/sample/mii-update2.yaml secret references in the spec.webLogicCredentialsSecret and spec.configuration.secrets stanzas to reference this use case\u0026rsquo;s secrets. Specifically, change:\nspec: ... webLogicCredentialsSecret: name: sample-domain1-weblogic-credentials ... configuration: ... secrets: - sample-domain1-datasource-secret ... model: ... runtimeEncryptionSecret: sample-domain1-runtime-encryption-secret To this:\nspec: ... webLogicCredentialsSecret: name: sample-domain2-weblogic-credentials ... configuration: ... secrets: - sample-domain2-datasource-secret ... model: ... runtimeEncryptionSecret: sample-domain2-runtime-encryption-secret Change the Domain YAML file\u0026rsquo;s spec.configuration.model.configMap value from sample-domain1-wdt-config-map to sample-domain2-wdt-config-map. The corresponding YAML file stanza will look something like this:\nspec: ... configuration: ... model: ... configMap: sample-domain2-wdt-config-map Now, compare your original and changed Domain YAML files to double check your changes.\n$ diff /tmp/sample/mii-update1.yaml /tmp/sample/mii-update2.yaml 9c9 \u0026lt; name: sample-domain1 --- \u0026gt; name: sample-domain2 13c13 \u0026lt; weblogic.domainUID: sample-domain1 --- \u0026gt; weblogic.domainUID: sample-domain2 21c21 \u0026lt; domainHome: /u01/domains/sample-domain1 --- \u0026gt; domainHome: /u01/domains/sample-domain2 36c36 \u0026lt; name: sample-domain1-weblogic-credentials --- \u0026gt; name: sample-domain2-weblogic-credentials 46c46 \u0026lt; #logHome: /shared/logs/sample-domain1 --- \u0026gt; #logHome: /shared/logs/sample-domain2 61c61 \u0026lt; value: \u0026#34;domain1\u0026#34; --- \u0026gt; value: \u0026#34;domain2\u0026#34; 71c71 \u0026lt; # claimName: sample-domain1-weblogic-sample-pvc --- \u0026gt; # claimName: sample-domain2-weblogic-sample-pvc 110c110 \u0026lt; configMap: sample-domain1-wdt-config-map --- \u0026gt; configMap: sample-domain2-wdt-config-map 113c113 \u0026lt; runtimeEncryptionSecret: sample-domain1-runtime-encryption-secret --- \u0026gt; runtimeEncryptionSecret: sample-domain2-runtime-encryption-secret 118c118 \u0026lt; - sample-domain1-datasource-secret --- \u0026gt; - sample-domain2-datasource-secret NOTE: The diff should not contain a namespace change. You are deploying domain sample-domain2 to the same namespace as sample-domain1 (namespace sample-domain1-ns).\nApply your changed Domain YAML file:\nNOTE: Before you deploy the domain custom resource, ensure all nodes in your Kubernetes cluster can access auxiliary-image and other images.\n$ kubectl apply -f /tmp/sample/mii-update2.yaml Option 2: Use the updated Domain YAML file that is supplied with the sample:\nNOTE: Before you deploy the domain custom resource, ensure all nodes in your Kubernetes cluster can access auxiliary-image and other images.\n$ kubectl apply -f /tmp/sample/domain-resources/WLS/mii-update2-d2-WLS-v1-ds.yaml Wait for sample-domain2 to start.\nIf you run kubectl get pods -n sample-domain1-ns --watch, then you will see the introspector job for sample-domain2 run and your WebLogic Server pods start. The output will look something like this:\nClick here to expand. $ kubectl get pods -n sample-domain1-ns --watch NAME READY STATUS RESTARTS AGE sample-domain1-admin-server 1/1 Running 0 5d2h sample-domain1-managed-server1 1/1 Running 1 5d2h sample-domain1-managed-server2 1/1 Running 2 5d2h sample-domain2-introspector-plssr 0/1 Pending 0 0s sample-domain2-introspector-plssr 0/1 Pending 0 0s sample-domain2-introspector-plssr 0/1 ContainerCreating 0 0s sample-domain2-introspector-plssr 1/1 Running 0 2s sample-domain2-introspector-plssr 0/1 Completed 0 69s sample-domain2-introspector-plssr 0/1 Terminating 0 71s sample-domain2-introspector-plssr 0/1 Terminating 0 71s sample-domain2-admin-server 0/1 Pending 0 0s sample-domain2-admin-server 0/1 Pending 0 0s sample-domain2-admin-server 0/1 ContainerCreating 0 0s sample-domain2-admin-server 0/1 Running 0 1s sample-domain2-admin-server 1/1 Running 0 34s sample-domain2-managed-server1 0/1 Pending 0 0s sample-domain2-managed-server1 0/1 Pending 0 0s sample-domain2-managed-server1 0/1 ContainerCreating 0 0s sample-domain2-managed-server2 0/1 Pending 0 0s sample-domain2-managed-server2 0/1 Pending 0 0s sample-domain2-managed-server2 0/1 ContainerCreating 0 0s sample-domain2-managed-server1 0/1 Running 0 1s sample-domain2-managed-server2 0/1 Running 0 1s sample-domain2-managed-server1 1/1 Running 0 45s sample-domain2-managed-server2 1/1 Running 0 45s For a more detailed view of this activity, you can use the waitForDomain.sh sample lifecycle script. This script provides useful information about a domain\u0026rsquo;s pods and optionally waits for its Completed status condition to become True. A Completed domain indicates that all of its expected pods have reached a ready state plus their target restartVersion, introspectVersion, and image. For example:\n$ cd /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/domain-lifecycle $ ./waitForDomain.sh -n sample-domain1-ns -d sample-domain2 -p Completed After the sample-domain2 domain is running, you can call its sample web application to verify that it\u0026rsquo;s fully active.\nSend a web application request to the ingress controller for sample-domain2:\n$ curl -s -S -m 10 -H \u0026#39;host: sample-domain2-cluster-cluster-1.sample.org\u0026#39; \\ http://localhost:30305/myapp_war/index.jsp Or, if Traefik is unavailable and your domain2 Administration Server pod is running, you can run kubectl exec:\n$ kubectl exec -n sample-domain1-ns sample-domain2-admin-server -- bash -c \\ \u0026#34;curl -s -S -m 10 http://sample-domain2-cluster-cluster-1:8001/myapp_war/index.jsp\u0026#34; You will see something like the following:\n\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v1\u0026#39; of the sample JSP web-app. Welcome to WebLogic Server \u0026#39;managed-server1\u0026#39;! domain UID = \u0026#39;sample-domain2\u0026#39; domain name = \u0026#39;domain2\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found min threads constraint runtime named \u0026#39;SampleMinThreads\u0026#39; with configured count: 1 Found max threads constraint runtime named \u0026#39;SampleMaxThreads\u0026#39; with configured count: 10 Found 1 local data source: Datasource \u0026#39;mynewdatasource\u0026#39;: State=\u0026#39;Running\u0026#39;, testPool=\u0026#39;Failed\u0026#39; ---TestPool Failure Reason--- NOTE: Ignore \u0026#39;mynewdatasource\u0026#39; failures until the sample\u0026#39;s Update 4 use case. --- ... ... invalid host/username/password ... ----------------------------- ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; A TestPool Failure is expected because we will demonstrate dynamically correcting the data source attributes for sample-domain1 in Update 4.\nIf you see an error other than the expected TestPool Failure, then consult Debugging.\nYou will not be using the sample-domain2 domain again in this sample; if you wish, you can shut it down now by calling kubectl -n sample-domain1-ns delete domain sample-domain2.\nTo remove the resources you have created in the samples, see Cleanup.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/developerguide/integration-tests/",
	"title": "Integration tests",
	"tags": [],
	"description": "Learn about the available Java integration tests.",
	"content": "The project includes integration tests that can be run against a Kubernetes cluster. If you want to use these tests, you will need to provide your own Kubernetes cluster. The Kubernetes cluster must meet the version number requirements and have Helm installed. Ensure that the operator image is in a container registry visible to the Kubernetes cluster.\nYou will need to obtain the kube.config file for an administrative user and make it available on the machine running the build. To run the tests, update the KUBECONFIG environment variable to point to your config file and then execute:\n$ mvn clean verify -P java-integration-tests For more detailed information, see How to run the Java integration tests .\nWhen you run the integrations tests, they do a cleanup of any operator or domains on that cluster.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/developerguide/branching/",
	"title": "Branching",
	"tags": [],
	"description": "Understand the operator repository branching strategy.",
	"content": "The main branch is protected and contains source for the latest completed features and bug fixes. While this branch contains active work, we expect to keep it always \u0026ldquo;ready to release.\u0026rdquo; Therefore, longer running feature work will be performed on specific branches, such as feature/dynamic-clusters.\nBecause we want to balance separating destabilizing work into feature branches against the possibility of later difficult merges, we encourage developers working on features to pull out any necessary refactoring or improvements that are general purpose into their own shorter-lived branches and create pull requests to main when these smaller work items are completed.\nAll commits to main must pass the integration test suite. Please run these tests locally before submitting a pull request. Additionally, each push to a branch in our GitHub repository triggers a run of a subset of the integration tests with the results visible here.\nPlease submit pull requests to the main branch unless you are collaborating on a feature and have another target branch. Please see details on the Oracle Contributor Agreement (OCA) and guidelines for pull requests in Contribute to the operator.\nWe will create git tags for each release candidate and generally available (GA) release of the operator.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-operators/conversion-webhook/",
	"title": "Upgrade operator from version 3.x to 4.x",
	"tags": [],
	"description": "Conversion webhook for upgrading the domain resource schema.",
	"content": "Contents Introduction Conversion webhook components Install the conversion webhook Upgrade the conversion webhook Uninstall the conversion webhook Troubleshooting the conversion webhook Introduction The conversion webhook that is described in this document transparently handles a weblogic.oracle/v8 schema domain resource at runtime, but if you want to use the new fields introduced in the latest weblogic.oracle/v9 schema with a Domain that is currently weblogic.oracle/v8, then you will need to update its Domain resource file and potentially create new Cluster resource files. To simplify this conversion, see manual upgrade command line tool.\nThe Domain CustomResourceDefinition (CRD) in your Kubernetes cluster, which defines the schema of operator managed Domains, has changed significantly in operator version 4.0 from previous operator releases, therefore, we have updated the API version to weblogic.oracle/v9. For example, we have enhanced Auxiliary images in operator version 4.0, and its configuration has changed. Operator 4.0 uses the Kubernetes Webhook Conversion strategy and a WebLogic Domain resource conversion webhook to automatically and transparently upgrade domain resources with weblogic.oracle/v8 schema to weblogic.oracle/v9 schema. With the Webhook conversion strategy, the Kubernetes API server internally invokes an external REST service that pulls out the configuration of the auxiliary images defined in the weblogic.oracle/v8 schema domain resource and converts it to the equivalent weblogic.oracle/v9 schema configuration. The WebLogic Domain resource conversion webhook is a singleton Deployment in your Kubernetes cluster. It is installed by default when an operator is installed, and uninstalled when any operator is uninstalled, but you can optionally install and uninstall it independently. For details, see Install the conversion webhook and Uninstall the conversion webhook. For manually upgrading the domain resources with weblogic.oracle/v8 schema to weblogic.oracle/v9 schema, see Upgrade Domain resource.\nConversion webhook components The following table lists the components of the WebLogic Domain resource conversion webhook and their purpose.\nComponent Type Component Name Purpose Deployment webLogic-operator-webhook Manages the runtime Pod of the WebLogic Domain resource conversion webhook. Service webLogic-operator-webhook-svc The Kubernetes API server uses this service to reach the conversion webhook runtime defined in the WebLogic Domain CRD. Secret webLogic-webhook-secrets Contains the CA certificate and key used to secure the communication between the Kubernetes API server and the REST endpoint of WebLogic domain resource conversion webhook. The spec.conversion stanza in the Domain CRD Used internally by the Kubernetes API server to call an external service when a Domain conversion is required. NOTES:\nThe conversion webhook Deployment webLogic-operator-webhook uses and requires the same image as the operator image. You can scale the Deployment by increasing the number of replicas, although this is rarely required.\nThe conversion webhook Deployment sets the spec.conversion.strategy field of the Domain CRD to Webhook. It also adds webhook client configuration details such as service name, namespace, path, port, and the self-signed CA certificate used for authentication.\nInstall the conversion webhook Beginning with operator version 4.0, when you install the operator using the helm install command with the operator Helm chart, by default, it also configures a deployment and supporting resources for the conversion webhook and deploys the conversion webhook in the specified namespace. Note that a webhook is a singleton deployment in the cluster. Therefore, if the webhook is already installed in the cluster and the operator installation version is the same or older, then the operator installation will skip the webhook installation. However, if the operator version is newer, then a new webhook Deployment is created and all webhook traffic for the CRD is redirected to the new webhook.\nSecurity Considerations: The helm install step requires cluster-level permissions for listing and reading all Namespaces and Deployments to search for existing conversion webhook deployments. If you cannot grant the cluster-level permissions and have multiple operators deployed, then install the conversion webhook separately and set the Helm configuration value operatorOnly to true in the helm install command to prevent multiple conversion webhook deployments. In addition, the webhook uses a service account that is usually the same service account as an operator running in the same namespace. This service account requires permissions to create and read events in the conversion webhook namespace. For more information, see RBAC.\nOperator version 4.x requires a conversion webhook. The operatorOnly Helm configuration value is an advanced setting and should be used only when a conversion webhook is already installed.\nIf you want to install only the conversion webhook (and not the operator) in the given namespace, set the Helm configuration value webhookOnly to true in the helm install command. After meeting the prerequisite requirements, call:\n$ helm install sample-weblogic-conversion-webhook \\ weblogic-operator/weblogic-operator \\ --namespace sample-weblogic-conversion-webhook-ns \\ --set \u0026#34;webhookOnly=true\u0026#34; \\ --wait The previous command creates a Helm release named sample-weblogic-conversion-webhook in the sample-weblogic-conversion-webhook-ns namespace, configures a deployment and supporting resources for the conversion webhook, and deploys the conversion webhook. This command uses the default service account for the sample-weblogic-conversion-webhook-ns namespace.\nTo check if the conversion webhook is deployed and running, see Troubleshooting.\nTo prevent the removal of the conversion webhook during the helm uninstall command of an operator, set the Helm configuration value preserveWebhook to true during the helm install of the operator release.\nThe following table describes the behavior of different operator Helm chart commands for various Helm configuration values.\nHelm command with the operator Helm chart Helm configuration values Behavior Helm install None specified Operator and conversion webhook installed. Helm install webhookOnly=true Only conversion webhook installed. Helm install operatorOnly=true Only operator installed. Helm install preserveWebhook=true Operator and webhook installed and a future uninstall will not remove the webhook. Helm uninstall Operator and webhook deployment uninstalled. Helm uninstall with preserveWebhook=true set during helm install Operator deployment uninstalled and webhook deployment preserved. NOTE: A webhook install is skipped if there\u0026rsquo;s already a webhook deployment at the same or newer version. The helm install step requires cluster-level permissions to search for existing conversion webhook deployments in all namespaces.\nUpgrade the conversion webhook We support having exactly one installation of the webhook and having one or more installations of the operator. To have more than one installation of the operator, you would need to install a Helm release with just the webhook and then separately install multiple Helm releases with just the operator. The conversion webhook should be updated to at least match the version of the most recent operator in the cluster.\nTo upgrade the conversion webhook only, you must have first installed a Helm release with the webhook only, use the --set webhookOnly=true option, then you can update that release.\nThe following example installs the conversion webhook only (at the specified version), and then upgrades it (to a later, specified version).\nkubectl create namespace \u0026lt;your-namespace\u0026gt; helm repo add weblogic-helm-repository https://oracle.github.io/weblogic-kubernetes-operator/charts --force-update helm install \u0026lt;your-release-name\u0026gt; weblogic-helm-repository/weblogic-operator --namespace \u0026lt;your-namespace\u0026gt; --set webhookOnly=true --version \u0026lt;selected-version\u0026gt; The first two steps create the namespace and configure Helm with the chart repository.\nThe final step installs the webhook specifying that the install should be for the webhook only and at a specific version of the product.\nTo upgrade to a later version of the webhook:\nhelm upgrade \u0026lt;your-release-name\u0026gt; weblogic-helm-repository/weblogic-operator --namespace \u0026lt;your-namespace\u0026gt; --version \u0026lt;selected-new-version\u0026gt; Uninstall the conversion webhook If you are deploying Domains using weblogic.oracle/v8 or older schema and either you have multiple operators deployed or you want to create Domains without the operator deployed, then you may not want to uninstall the webhook. If the conversion webhook runtime is not available under these conditions, then you will see a conversion webhook not found error. To avoid this error, reinstall the webhook and, in the future, use the preserveWebhook=true Helm configuration value when installing an operator.\nWhen you uninstall the operator using the helm uninstall command, it removes the conversion webhook associated with the release and its resources from the Kubernetes cluster. However, if you only installed the conversion webhook using the webhookOnly=true Helm configuration value, then run the helm uninstall command separately to remove the conversion webhook associated with the release and its resources.\nFor example, assuming the Helm release name for the conversion webhook is sample-weblogic-conversion-webhook, and the conversion webhook namespace is sample-weblogic-conversion-webhook-ns:\n$ helm uninstall sample-weblogic-conversion-webhook -n sample-weblogic-conversion-webhook-ns This command deletes the conversion webhook Deployment (weblogic-operator-webhook) and it also deletes the conversion resources, such as service and Secrets.\nTroubleshooting the conversion webhook See Troubleshooting the conversion webhook.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/base-images/access-images/",
	"title": "Access domain images",
	"tags": [],
	"description": "Set up Kubernetes to access domain images.",
	"content": "In most operator samples, it is assumed that the Kubernetes cluster has a single worker node and any images that are needed by that node have either been created on that node or externally pulled to the node from a registry (using docker pull). This is fine for most demonstration purposes, and if this assumption is correct, then no additional steps are needed to ensure that Kubernetes has access to the image. Otherwise, additional steps are typically required to ensure that a Kubernetes cluster has access to domain images.\nFor example, it is typical in production deployments for the Kubernetes cluster to be remote and have multiple worker nodes, and to store domain images in a central repository that requires authentication.\nHere are two typical scenarios for supplying domain images to such deployments:\nOption 1: Store images in a central registry and set up image pull secrets on each domain resource\nOption 2: Store images in a central registry and set up a Kubernetes service account with image pull secrets in each domain namespace\nOption 1: Store images in a central registry and set up image pull secrets on each domain resource The most commonly used option is to store the image in a central registry and set up image pull secrets for a domain resource:\nA Kubernetes docker-registry secret containing the registry credentials must be created in the same namespace as domain resources with a domain.spec.image attribute that reference the image. For example, to create a secret with OCR credentials:\n$ kubectl create secret docker-registry SECRET_NAME \\ -n NAMESPACE_WHERE_YOU_DEPLOY_DOMAINS \\ --docker-server=container-registry.oracle.com \\ --docker-username=YOUR_USERNAME \\ --docker-password=YOUR_PASSWORD \\ --docker-email=YOUR_EMAIL The name of the secret must be added to these domain resources using the domain.spec.imagePullSecrets field. For example:\n... spec: ... imagePullSecrets: - name: SECRET_NAME ... If you are using the Oracle Container Registry, then you must use the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. You need to do this only once for a particular image. See Obtain images from the Oracle Container Registry.\nFor more information about creating Kubernetes Secrets for accessing the registry, see the Kubernetes documentation, Pull an image from a private registry.\nOption 2: Store images in a central registry and set up a Kubernetes service account with image pull secrets in each domain namespace An option for accessing an image that is stored in a private registry is to set up the Kubernetes ServiceAccount in the namespace running the WebLogic domain with a set of image pull secrets thus avoiding the need to set imagePullSecrets for each Domain resource being created (because each resource instance represents a WebLogic domain that the operator is managing):\nCreate a Kubernetes docker-registry secret as shown in Option 1.\nModify the ServiceAccount that is in the same namespace as your domain resources to include this image pull secret:\n$ kubectl patch serviceaccount default -n domain1-ns \\ -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;my-registry-pull-secret\u0026#34;}]}\u0026#39; Note that this patch command entirely replaces the current list of image pull secrets (if any). To include multiple secrets, use the following format: -p '{\u0026quot;imagePullSecrets\u0026quot;: [{\u0026quot;name\u0026quot;: \u0026quot;my-registry-pull-secret\u0026quot;}, {\u0026quot;name\u0026quot;: \u0026quot;my-registry-pull-secret2\u0026quot;}]}'.\nFor more information about updating a Kubernetes ServiceAccount for accessing the registry, see the Kubernetes documentation, Configure service accounts for pods.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/",
	"title": "Manage domains",
	"tags": [],
	"description": "Managing domains in Kubernetes.",
	"content": "The following sections provide information for running, managing, accessing, and monitoring domains.\nAbout WebLogic domains An overview about managing WebLogic domains and clusters in Kubernetes.\nChoose a domain home source type Choose among these domain home types depending on your requirements and create images that are appropriate for your type.\nDomain and Cluster resources Create your own Domain and Cluster resources.\nUpgrade Domain resource Upgrade Domain resources.\nPrepare to run a domain Perform these steps to prepare your Kubernetes cluster to run a WebLogic domain.\nUpgrade managed domains Upgrade managed domains, general and specific guidelines.\nDomain on Persistent Volume (PV) Create and deploy a typical domain on PV.\nModel in Image Create and deploy a typical Model in Image domain.\nConfiguration overrides Use overrides to customize domains.\nPersistent storage Use a Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC) to store WebLogic domain homes and log files.\nAccess and monitor domains Choose among several options for accessing and monitoring domains.\nDomain life cycle Learn how to start, stop, restart, and scale the WebLogic Server instances in your domain.\nCI/CD considerations Learn about managing domain images with continuous integration and continuous delivery (CI/CD).\nManage FMW Infrastructure domains FMW Infrastructure domains contain the Java Required Files (JRF) feature and are the prerequisite for upper stack products like Oracle SOA Suite.\nAzure Kubernetes Service (AKS) Deploy WebLogic Server on Azure Kubernetes Service.\nDomain debugging Debug deployed domains.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/upgrade-domain-resource/",
	"title": "Upgrade Domain resource",
	"tags": [],
	"description": "Upgrade Domain resources.",
	"content": "Contents Operator 4.0 domain resource API version change Automated upgrade of weblogic.oracle/v8 schema domain resource Upgrade the weblogic.oracle/v8 schema domain resource manually Setup Operator 4.0 domain resource API version change The Domain CustomResourceDefinition in operator version 4.0 has changed significantly from previous operator releases. For this reason, we have updated the API version of the Domain custom resource in the CRD from weblogic.oracle/v8 to weblogic.oracle/v9. We continue to support the Domains with API version weblogic.oracle/v8 and provide full backward compatibility. If you want to use the new fields introduced in the latest weblogic.oracle/v9 schema, then you will need to update the API version in your Domain resource YAML file.\nAutomated upgrade of weblogic.oracle/v8 schema domain resource The automated upgrade described in this section converts weblogic.oracle/v8 schema auxiliary image configuration into low-level Kubernetes schema, for example, init containers and volumes. Instead of relying on the generated low-level schema, Oracle recommends using a simplified weblogic.oracle/v9 schema configuration for auxiliary images, as documented in the Auxiliary Images Configuration section.\nThe 4.0 operator provides a seamless upgrade of the Domain resources with the weblogic.oracle/v8 version of the schema. When you create a Domain using a domain resource YAML file with weblogic.oracle/v8 schema in a namespace managed by the 4.0 operator, the WebLogic Domain resource conversion webhook explained in the Upgrade operator from version 3.x to 4.x document, performs an automated upgrade of the domain resource to the weblogic.oracle/v9 schema. The conversion webhook runtime converts the weblogic.oracle/v8 configuration to the equivalent configuration in operator 4.0. Similarly, when upgrading the operator version, Domain resources with weblogic.oracle/v8 schema are seamlessly upgraded.\nUpgrade the weblogic.oracle/v8 schema domain resource manually The manual upgrade tooling described in this section converts weblogic.oracle/v8 schema auxiliary image configuration into low-level Kubernetes schema, for example, init containers and volumes. Instead of relying on the generated low-level schema, Oracle recommends using a simplified weblogic.oracle/v9 schema configuration for auxiliary images, as documented in the Auxiliary Images Configuration section.\nBeginning with operator version 4.0, you can use a standalone command-line tool for manually upgrading the domain resource YAML file with weblogic.oracle/v8 schema to the weblogic.oracle/v9 schema. If you are required to keep the upgraded Domain resource YAML file in the source control repository, then you can use this tool to generate the upgraded Domain resource YAML file.\nSetup Download the Domain upgrade tool JAR file to the desired location. You can find the latest JAR file on the project releases page. Alternatively, you can download the JAR file with cURL. curl -m 120 -fL https://github.com/oracle/weblogic-kubernetes-operator/releases/latest/download/domain-upgrader.jar -o ./domain-upgrader.jar OPTIONALLY: You may build the project (mvn clean package) to create the JAR file in ./weblogic-kubernetes-operator/target (see Build From Source). Set the JAVA_HOME environment variable to the location of the Java installation (Java version 11+). The Domain upgrader tool upgrades the provided V8 schema domain resource input file and writes the upgraded domain resource YAML file to the directory specified using the -d parameter.\nUsage: java -jar domain-upgrader.jar \u0026lt;input-file\u0026gt; [-d \u0026lt;output_dir\u0026gt;] [-f \u0026lt;output_file_name\u0026gt;] [-o --overwriteExistingFile] [-h --help] Parameter Definition Default input-file (Required) Name of the operator 3.x/V8 domain resource YAML file to be converted. -d, --outputDir The directory where the tool will place the converted file. The directory of the input file. -f, --outputFile Name of the converted file. Base name of the input file name followed by \u0026quot;__converted.\u0026quot; followed by the input file extension. -h, --help Prints help message. -o, --overwriteExistingFile Enable overwriting the existing output file, if any. If the output file name is not specified using the -f parameter, then the tool generates the file name by appending \u0026quot;__converted.\u0026quot; and the input file extension to the base name of the input file name. For example, assuming the name of the V8 domain resource YAML file to be upgraded is domain-v8.yaml in the current directory:\n$ java -jar /tmp/domain-upgrader.jar domain-v8.yaml -d /tmp -f domain-v9.yaml {\u0026#34;timestamp\u0026#34;:\u0026#34;2022-04-18T23:11:09.182227Z\u0026#34;,\u0026#34;thread\u0026#34;:1,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.DomainUpgrader\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;main\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1650323469182,\u0026#34;message\u0026#34;:\u0026#34;Successfully generated upgraded domain custom resource file \u0026#39;domain-v9.yaml\u0026#39;.\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} In the previous example, the tool writes the upgraded file to the /tmp directory with the name domain-v9.yaml.\n$ ls -ltr /tmp/domain-v9.yaml -rw-r----- 1 user dba 2818 Apr 18 23:11 /tmp/domain-v9.yaml The manual upgrade tooling creates init containers with names prefixed with compat- when converting the auxiliary image configuration of the weblogic.oracle/v8 schema. The operator generates only init containers with names starting with either compat- or wls-shared- in the introspector job pod. To alter the generated init container\u0026rsquo;s name, the new name must start with either compat- or wls-shared-.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/introduction/get-help/",
	"title": "Get help",
	"tags": [],
	"description": "Where to get help, submit suggestions, or submit issues.",
	"content": "Oracle Slack We have a closely monitored public Slack channel where you can get in touch with us to ask questions about using the operator or give us feedback or suggestions about what features and improvements you would like to see. We can also create dedicated private channels upon request. We would love to hear from you.\nTo join our public channel, please visit this site to get an invitation. The invitation email will include details of how to access our Slack workspace. After you are logged in, please come to #operator and say, \u0026ldquo;hello!\u0026rdquo;\nGitHub issues To quickly get help, we strongly recommend using Oracle Slack. If this is insufficient, then feel free to submit a GitHub Issue on the WebLogic Kubernetes Operator project. All issues are public.\nOracle support To access Oracle support, see WebLogic Server Certifications on Kubernetes in My Oracle Support Doc ID 2349228.1.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/quickstart/cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Remove the domain and cluster. Remove the domain\u0026rsquo;s ingress routes by using kubectl.\n$ kubectl delete ingressroute quickstart -n sample-domain1-ns $ kubectl delete ingressroute console -n sample-domain1-ns Use kubectl to delete the domain resource.\n$ kubectl delete domain sample-domain1 -n sample-domain1-ns Use kubectl to confirm that the WebLogic Server instance Pods and Domain are gone.\n$ kubectl get pods -n sample-domain1-ns $ kubectl get domains -n sample-domain1-ns Use kubectl to delete the cluster resource.\n$ kubectl delete cluster sample-domain1-cluster-1 -n sample-domain1-ns Remove the Kubernetes Secrets associated with the domain.\n$ kubectl -n sample-domain1-ns delete secret sample-domain1-weblogic-credentials $ kubectl -n sample-domain1-ns delete secret sample-domain1-runtime-encryption-secret Remove the domain namespace. Configure the Traefik ingress controller to stop managing the ingresses in the domain namespace.\n$ helm upgrade traefik-operator traefik/traefik \\ --namespace traefik \\ --reuse-values \\ --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; Delete the domain namespace.\n$ kubectl delete namespace sample-domain1-ns Remove the operator. Remove the operator.\n$ helm uninstall sample-weblogic-operator -n sample-weblogic-operator-ns Remove the operator\u0026rsquo;s namespace.\n$ kubectl delete namespace sample-weblogic-operator-ns Remove the ingress controller. Remove the Traefik ingress controller.\n$ helm uninstall traefik-operator -n traefik Remove the Traefik namespace.\n$ kubectl delete namespace traefik "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/domain-home-on-pv/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Remove the domain and cluster Remove the domain\u0026rsquo;s ingress routes by using kubectl.\n$ kubectl delete ingressroute traefik-ingress-sample-domain1-admin-server -n sample-domain1-ns $ kubectl delete ingressroute traefik-ingress-sample-domain1-cluster-cluster-1 -n sample-domain1-ns $ kubectl delete ingressroute traefik-ingress-sample-domain2-cluster-cluster-1 -n sample-domain1-ns Use kubectl to delete the domain resource.\n$ kubectl delete domain sample-domain1 -n sample-domain1-ns Use kubectl to confirm that the WebLogic Server instance Pods and Domain are gone.\n$ kubectl get pods -n sample-domain1-ns $ kubectl get domains -n sample-domain1-ns Use kubectl to delete the cluster resource.\n$ kubectl delete cluster sample-domain1-cluster-1 -n sample-domain1-ns Remove the Kubernetes Secrets associated with the domain.\n$ kubectl -n sample-domain1-ns delete secret sample-domain1-weblogic-credentials Remove the operator Remove the operator.\n$ helm uninstall sample-weblogic-operator -n sample-weblogic-operator-ns Remove the operator\u0026rsquo;s namespace.\n$ kubectl delete namespace sample-weblogic-operator-ns Remove the ingress controller Remove the Traefik ingress controller.\n$ helm uninstall traefik-operator -n traefik Remove the Traefik namespace.\n$ kubectl delete namespace traefik "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/domain-on-pv/jrf-domain/",
	"title": "JRF domains",
	"tags": [],
	"description": "Important information about using JRF domains.",
	"content": "Contents Overview Sample WDT model for JRF domain Importance of domain home directory backup Download OPSS wallet and store in a Kubernetes Secret Disaster recovery for Domain on PV deployment Disaster recovery for Model in Image domains Overview A Java Required Files (JRF) domain consists of those components not included in a WebLogic Server installation that provide common functionality for Oracle business applications and application frameworks. They consist of a number of independently developed libraries and applications that are deployed in a common location.\nTypically, a JRF domain is used by Fusion Middleware products. The JRF domain has database requirements. The database components are created using Repository Creation Utility (RCU); a new RCU schema is created before creating a JRF-based domain.\nSample WDT model for JRF domain For the operator to create a JRF domain, your WDT model must have a RCUDbInfo section in domainInfo. This is a sample model snippet of a typical JRF domain\ndomainInfo: RCUDbInfo: rcu_prefix: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_prefix@@\u0026#39; rcu_schema_password: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_schema_password@@\u0026#39; rcu_db_conn_string: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_db_conn_string@@\u0026#39; # DBA credentials: required if operator is running the rcu in Domain On PV rcu_db_user: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:dba_user@@\u0026#39; rcu_admin_password: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:dba_password@@\u0026#39; Refer to WebLogic Deploy Tooling Connect to a database for more information.\nImportance of domain home directory backup A JRF domain has a one-to-one relationship with the RCU schema. After a domain is created using a particular RCU schema, that schema cannot be reused by another domain and the same schema cannot be shared across different domains. Any attempts to create a new domain using the schema that had already been used, will result in an error.\nIf the domain home is not properly backed up, you potentially can lose existing data if the domain home is corrupted or deleted. That\u0026rsquo;s because recreating the domain requires dropping the existing RCU schema and creating a new RCU schema. Therefore, backing up the existing domain home should be the highest priority in your Kubernetes environment.\nThis is especially important for a Domain on PV deployment, where the domain is continually updated after its initial deployment. For example, you already deployed new applications, added custom OPSS keystores, added OWSM policies, and such. The original models used to create the domain will not match the existing state of the domain (the models are not the source of truth) and, if you use the original models to create the domain again, then you will lose all the updates that you have made. In order to preserve the domain updates, you should restore the domain from a backup copy of the domain home directory and connect to the existing RCU schema from the database backup.\nDownload OPSS wallet and store in a Kubernetes Secret After the domain is created, the operator will automatically export the OPSS wallet and store it in an introspector ConfigMap; the name of the ConfigMap follows the pattern \u0026lt;domain uid\u0026gt;-weblogic-domain-introspect-cm with the key ewallet.p12. Export this file and put it in a safe place. The operator provides a OPSS wallet utility for extracting this file and storing it in a Kubernetes walletFileSecret. You should also save the wallet file in a safely backed-up location, outside of Kubernetes.\nFor example,\n$ opss-wallet.sh -n sample-ns -d sample-domain1 -s -r -wf /tmp/ewallet.p12 -ws jrf-wallet-file-secret Disaster recovery for Domain on PV deployment When a JRF domain is created, an OPSS wallet is stored in the file system where the domain home resides. This specific wallet key can be exported and used to create a new domain. There is no way to reuse the original RCU schema without this specific wallet key. Therefore, for disaster recovery, you should back up this OPSS wallet.\nAfter the operator creates the JRF domain, it stores the OPSS wallet in a ConfigMap. See Download and save the OPSS wallet.\nOracle recommends that you save the OPSS wallet file in a safe, backed-up location immediately after an initial JRF domain is created. In addition, you should make sure to store the wallet in a Kubernetes Secret in the same namespace. This will allow the secret to be available when the domain needs to be recovered in a disaster scenario or if the domain directory gets corrupted.\nIn the domain resource YAML file, you can provide two secrets in the opss section under configuration.initializeDomainOnPV.domain:\n... # For domain on PV, opss settings are under `configuration.initializeDomainOnPV.domain.opss` # DO NOT specify it under `configuration` # configuration: initializeDomainOnPV: ... domain: createIfNotExists: Domain domainType: JRF ... opss: walletFileSecret: jrf-wallet-file-secret walletPasswordSecret: sample-domain1-opss-wallet-password-secret walletPasswordSecret is required during initial deployment. This secret contains the password used to encrypt the exported OPSS wallet. You can create a Kubernetes Secret with the key walletPassword containing the password. The password must have a minimum length of eight characters and contain alphabetic characters combined with numbers or special characters.\nIf the domain home directory is corrupted, and you have a recent backup of the domain home directory, then perform the following steps to recover the domain.\nRestore the domain home directory from the backup copy.\nUpdate the restartVersion of the domain resource to restart the domain. For example,\n$ kubectl -n sample-ns patch domain sample-domain1 --type=\u0026#39;JSON\u0026#39; -p=\u0026#39;[ { \u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34; : \u0026#34;/spec/restartVersion\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;15\u0026#34; }]\u0026#39; After the domain is restarted, check the WebLogic domain configuration to ensure that it has the latest changes. NOTE: If you made any changes that are persisted in the domain home directory after your last backup, you must reapply those changes to the domain home directory. However, because the operator will reconnect to the same RCU schema, the data stored in the OPSS, MDS, or OWSM tables will be current.\nReapply any domain configuration changes persisted to the domain home directory, such as data source connections, JMS destinations, or new application EAR deployments, after your last backup. To make these changes, use WLST, the WebLogic Server Administration Console, or Enterprise Manager.\nIn the rare scenario where the domain home directory is corrupted, and you do not have a recent backup of the domain home directory, or if the backup copy is also corrupted, then you can recreate the domain from the WDT model files without losing any RCU schema data.\nDelete the existing domain home directory on PV.\nSpecify the walletFileSecret - A Kubernetes Secret with the key walletFile containing the exported OPSS wallet file ewallet.p12 you created earlier.\nFor example,\n$ kubectl -n sample-ns patch domain sample-domain1 --type=\u0026#39;JSON\u0026#39; -p=\u0026#39;[ { \u0026#34;op\u0026#34; : \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34; : \u0026#34;/spec/configuration/initializeDomainOnPV/domain/opss/walletFileSecret\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;jrf-wallet-file-secret\u0026#34; }]\u0026#39; Update the introspectVersion in the domain resource.\n$ kubectl -n sample-ns patch domain sample-domain1 --type=\u0026#39;JSON\u0026#39; -p=\u0026#39;[ { \u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34; : \u0026#34;/spec/intropsectVersion\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;15\u0026#34; }]\u0026#39; The operator will then create a new domain from the existing WDT models and reuse the original RCU schema.\nNOTE: All the updates made to the domain after the initial deployment will not be available in the recovered domain. However, this allows you to access the original RCU schema database without losing all its data.\nApply all the domain configuration changes persisted to the domain home file system, such as data source connections, JMS destinations, or new application EAR deployments, that are not in the WDT model files. These are the changes you have made after the initial domain deployment.\nUpdate the restartVersion of the domain resource to restart the domain.\n$ kubectl -n sample-ns patch domain sample-domain1 --type=\u0026#39;JSON\u0026#39; -p=\u0026#39;[ { \u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34; : \u0026#34;/spec/restartVersion\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;15\u0026#34; }]\u0026#39; Disaster recovery for Model in Image domains NOTE: JRF support in Model in Image domains has been deprecated since operator version 4.1.0; use the Domain on PV domain home source type instead.\nWhen a JRF domain is created, an OPSS wallet is stored in the file system where the domain home resides. This specific wallet key can be exported and used to create a new domain. There is no way to reuse the original RCU schema without this specific wallet key. Therefore, for disaster recovery, you should back up this OPSS wallet.\nAfter the operator creates the JRF domain, it stores the OPSS wallet in a ConfigMap. See Download and save the OPSS wallet.\nIn the domain resource YAML file, you can provide two secrets in the opss section under configuration:\nconfiguration: model: ... opss: walletFileSecret: jrf-wallet-file-secret walletPasswordSecret: sample-domain1-opss-wallet-password-secret walletPasswordSecret is required during initial deployment. This secret contains the password used to encrypt the exported OPSS wallet. You can create a Kubernetes Secret with the key walletPassword containing the password. The password must have a minimum length of eight characters and contain alphabetic characters combined with numbers or special characters.\nIn case the domain home directory is corrupted and you need to recreate the domain, and reuse the existing RCU schema:\nSpecify the walletFileSecret - A Kubernetes Secret with the key walletFile containing the exported OPSS wallet file ewallet.p12 that you have created earlier.\nFor example,\n$ kubectl -n sample-ns patch domain sample-domain1 --type=\u0026#39;JSON\u0026#39; -p=\u0026#39;[ { \u0026#34;op\u0026#34; : \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34; : \u0026#34;/spec/configuration/initializeDomainOnPV/domain/opss/walletFileSecret\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;jrf-wallet-file-secret\u0026#34; }]\u0026#39; Update the introspectVersion in the domain resource.\n$ kubectl -n sample-ns patch domain sample-domain1 --type=\u0026#39;JSON\u0026#39; -p=\u0026#39;[ { \u0026#34;op\u0026#34; : \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34; : \u0026#34;/spec/intropsectVersion\u0026#34;, \u0026#34;value\u0026#34; : \u0026#34;15\u0026#34; }]\u0026#39; The operator will create a new domain from the existing WDT models and reuse the original RCU schema.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/domain-lifecycle/introspection/",
	"title": "Domain introspection",
	"tags": [],
	"description": "This document describes domain introspection in the Oracle WebLogic Server in Kubernetes environment.",
	"content": "This document describes domain introspection, when it occurs automatically, and how and when to initiate additional introspections of the domain configuration in the Oracle WebLogic Server in a Kubernetes environment.\nContents Overview When introspection occurs automatically Initiating introspection Failed introspection Introspection use cases Adding clusters or Managed Servers to a Domain on PV configuration Distributing changes to configuration overrides Distributing changes to running Model in Image domains Overview To manage the operation of WebLogic domains in Kubernetes, the Oracle WebLogic Kubernetes Operator analyzes the WebLogic domain configuration using an \u0026ldquo;introspection\u0026rdquo; job. This Job will be named DOMAIN_UID-introspector, will be run in the same namespace as the Domain, and must successfully complete before the operator will begin to start WebLogic Server instances. Because each of the domain home source types are different (for instance, Domain on PV uses a domain home on a PersistentVolume while Model in Image generates the domain home dynamically from a WDT model), the Pod created by this Job will be as similar as possible to the Pod that will later be generated for the Administration Server. This guarantees that the operator is analyzing the same WebLogic domain configuration that WebLogic Server instances will use.\nIntrospection ensures that:\nThe operator is aware of domain topology from the WebLogic domain configuration, including servers, clusters, network access points, listen addresses, and other configurations. The operator can generate configuration overrides to adjust the WebLogic domain configuration to match the Kubernetes environment, such as modifying listen addresses. For Model in Image, the operator can generate the WebLogic domain home, including the final domain configuration. For Domain on PV and Domain in Image, the operator can use any customer-provided configuration overrides along with the operator-generated overrides to generate the final configuration overrides. When introspection occurs automatically Introspection automatically occurs when:\nThe operator is starting a WebLogic Server instance when there are currently no other servers running. This occurs when the operator first starts servers for a domain or when starting servers following a full domain shutdown. For Model in Image, the operator determines that at least one WebLogic Server instance that is currently running must be shut down and restarted. This could be a rolling of one or more clusters, the shut down and restart of one or more WebLogic Server instances, or a combination. Initiating introspection Sometimes, such as for the use cases, it is desirable to explicitly initiate introspection. To initiate introspection, change the value of your Domain introspectVersion field.\nSet introspectVersion to a new value.\nkind: Domain metadata: name: domain1 spec: introspectVersion: \u0026#34;2\u0026#34; ... As with restartVersion, the introspectVersion field has no required format; however, we recommend using a value likely to be unique such as a continually increasing number or a timestamp.\nBeginning with operator 3.1.0, if a domain resource\u0026rsquo;s spec.introspectVersion is set, each of the domain\u0026rsquo;s WebLogic Server pods will have a label with the key weblogic.introspectVersion to indicate the introspectVersion at which the pod is running.\nName: domain1-admin-server Namespace: domain1-ns Labels: weblogic.createdByOperator=true weblogic.domainName=domain1 weblogic.domainRestartVersion=abcdef weblogic.domainUID=domain1 weblogic.introspectVersion=12345 weblogic.serverName=admin-server When a domain\u0026rsquo;s spec.introspectVersion is changed, the weblogic.introspectVersion label of each WebLogic Server pod is updated to the new introspectVersion value, either when the operator restarts the pod or when the operator determines that the pod does not need to be restarted.\nFailed introspection Sometimes the Kubernetes Job, named DOMAIN_UID-introspector, created for the introspection will fail.\nWhen introspection fails, the operator will not start any WebLogic Server instances. If this is not the initial introspection and there are already WebLogic Server instances running, then a failed introspection will leave the existing WebLogic Server instances running without making any changes to the operational state of the domain.\nThe introspection will be periodically retried and then will eventually timeout with the Domain status indicating the processing failed. To recover from a failed state, correct the underlying problem and update the introspectVersion.\nPlease review the details for diagnosing introspection failures related to configuration overrides or Model in Image domain home generation.\nThe introspector log is mirrored to the Domain resource spec.logHome directory when spec.logHome is configured and spec.logHomeEnabled is true.\nIntrospection use cases The following sections describe typical use cases for rerunning the introspector.\nAdding clusters or Managed Servers to a Domain on PV configuration When you have an existing WebLogic domain home on a persistent volume (Domain on PV) and you currently have WebLogic Server instances running, it is now possible to define new WebLogic clusters or Managed Servers in the domain configuration and start these new instances without affecting the life cycle of any WebLogic Server instances that are already running.\nPrior to operator 3.0.0, this was not possible because there was no mechanism to initiate introspection other than a full domain shut down and restart and so the operator was unaware of the new clusters or Managed Servers. Now, after updating the domain configuration, you can initiate introspection by changing the introspectVersion.\nFor instance, if you had a domain configuration with a single cluster named \u0026ldquo;cluster-1\u0026rdquo; then your Domain YAML file may have content like this:\nspec: ... clusters: - clusterName: cluster-1 replicas: 3 ... If you modified your WebLogic domain configuration (using the console or WLST) to add a new dynamic cluster named \u0026ldquo;cluster-2\u0026rdquo;, then you could immediately start cluster members of this new cluster by updating your Domain YAML file like this:\nspec: ... clusters: - clusterName: cluster-1 replicas: 3 - clusterName: cluster-2 replicas: 2 introspectVersion: \u0026#34;2\u0026#34; ... When this updated Domain YAML file is applied, the operator will initiate a new introspection of the domain configuration during which it will learn about the additional WebLogic cluster and then the operator will continue to start WebLogic Server instances that are members of this new cluster. In this case, the operator will start two Managed Servers that are members of the cluster named \u0026ldquo;cluster-2\u0026rdquo;.\nDistributing changes to configuration overrides The operator supports customer-provided configuration overrides. These configuration overrides, which are supported with Domain on PV or Domain in Image, allow you to override elements of the domain configuration, such as data source URL\u0026rsquo;s or credentials.\nWith operator 3.0.0, you can now change the configuration overrides and distribute these new configuration overrides to already running WebLogic Server instances. To do this, update the ConfigMap that contains the configuration overrides or update one or more of the Secrets referenced by those configuration overrides and then initiate introspection by changing the introspectVersion field.\nWe have introduced a new field, called overrideDistributionStrategy and located under configuration, that controls whether updated configuration overrides are distributed dynamically to already running WebLogic Server instances or if the new configuration overrides are only applied when servers are started or restarted.\nThe default value for overrideDistributionStrategy is Dynamic, which means that new configuration overrides are distributed dynamically to already running WebLogic Server instances.\nAlternately, you can set overrideDistributionStrategy to OnRestart, which means that the new configuration overrides will not be distributed to already running WebLogic Server instances, but will instead be applied only to servers as they start or restart. Use of this value will not cause WebLogic Server instances to restart absent changes to other fields, such as restartVersion.\nChanges to configuration overrides distributed to running WebLogic Server instances can only take effect if the corresponding WebLogic configuration MBean attribute is \u0026ldquo;dynamic\u0026rdquo;. For instance, the Data Source \u0026ldquo;passwordEncrypted\u0026rdquo; attribute is dynamic while the \u0026ldquo;Url\u0026rdquo; attribute is non-dynamic.\nDistributing changes to running Model in Image domains The operator supports rerunning the introspector to propagate model updates to a running Model in Image domain.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/faq/node-heating/",
	"title": "Node heating problem",
	"tags": [],
	"description": "The operator creates a Pod for each WebLogic Server instance that is started. The Kubernetes Scheduler then selects a Node for each Pod. Because the default scheduling algorithm gives substantial weight to selecting a Node where the necessary container images have already been pulled, this often results in Kubernetes running many of the Pods for WebLogic Server instances on the same Node while other Nodes are not fairly utilized. This is commonly known as the Node heating problem.",
	"content": "The WebLogic Kubernetes Operator creates a Pod for each WebLogic Server instance that is started. The Kubernetes Scheduler then selects a Node for each Pod. Because the default scheduling algorithm gives substantial weight to selecting a Node where the necessary container images have already been pulled, this often results in Kubernetes running many of the Pods for WebLogic Server instances on the same Node while other Nodes are not fairly utilized. This is commonly known as the \u0026ldquo;Node heating problem.\u0026rdquo;\nOne solution is to ensure that all necessary container images are available on worker Nodes as part of node provisioning. When the necessary container images are available on each worker Node, the Kubernetes Scheduler will instead select a Node based on other factors such as available CPU and memory or a simple round-robin.\nThe operator team recommends a different solution that is based on inter-pod affinity and anti-affinity. This solution has the advantage of both resolving the Node heating problem and of explicitly directing the Kubernetes Scheduler to spread the Pods for WebLogic Server instances from a given cluster or domain more widely across the available Nodes. Inter-pod affinity and anti-affinity are features of the Kubernetes Scheduler that allow the scheduler to choose a Node for a new Pod based on details of the Pods that are already running. For WebLogic Server use cases, the intent will often be for anti-affinity with the Pods for other WebLogic Server instances so that server instances spread over the available Nodes.\nBeginning with operator version 4.0, by default, the operator adds the following content to the serverPod element at the cluster scope in the Domain Custom Resource.\nclusters: - clusterName: cluster-1 serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026#34;weblogic.clusterName\u0026#34; operator: In values: - $(CLUSTER_NAME) - key: \u0026#34;weblogic.domainUID\u0026#34; operator: In values: - $(DOMAIN_UID) topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; Because the serverPod element here is scoped to a cluster, the content of the affinity element will be added to the Pod generated for each WebLogic Server instance that is a member of this WebLogic cluster. This inter-pod anti-affinity statement expresses a preference that the scheduler select a Node for the new Pod avoiding, as much as possible, Nodes that already have Pods with the label weblogic.clusterName and the name of this cluster, and with the label weblogic.domainUID and the UID of this domain. Note that the weight is set to 100, which is the maximum weight, so that this term will outweigh any possible preference for a Node based on availability of container images.\nFor the server pods that are not part of a WebLogic cluster, the operator adds the following content to the serverPod element at the domain scope in the Domain Custom Resource. The following expresses an anti-affinity, but changes the test to have all WebLogic Server instances in the domain prefer to run on Nodes where there is not already a Pod for a running instance.\nclusters: - clusterName: cluster-1 serverStartState: \u0026#34;RUNNING\u0026#34; serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026#34;weblogic.domainUID\u0026#34; operator: In values: - $(DOMAIN_UID) topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; It is possible to customize the default affinity preferences and express many other scheduling preferences or constraints. The following example similarly expresses an anti-affinity, but changes the test to have all WebLogic Server instances in the domain prefer to run on Nodes where there is not already a Pod with a security label with the value of S1.\nserverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026#34;security\u0026#34; operator: In values: - \u0026#34;S1\u0026#34; topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; Details about how the operator generates Pods for WebLogic Server instances, including details about labels and variable substitution, are available here.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/accessing-the-domain/oci-lb/",
	"title": "Use an OCI load balancer",
	"tags": [],
	"description": "If you are running your Kubernetes cluster on Oracle Container Engine for Kubernetes (OKE), then you can have Oracle Cloud Infrastructure automatically provision load balancers for you.",
	"content": "If you are running your Kubernetes cluster on Oracle Container Engine for Kubernetes (commonly known as OKE), then you can have Oracle Cloud Infrastructure automatically provision load balancers for you by creating a Service of type LoadBalancer instead of (or in addition to) installing an ingress controller like Traefik.\nOKE Kubernetes worker nodes typically do not have public IP addresses. This means that the NodePort services created by the operator are not usable, because they would expose ports on the worker node\u0026rsquo;s private IP addresses only, which are not reachable from outside the cluster.\nInstead, you can use an Oracle Cloud Infrastructure load balancer to provide access to services running in OKE.\nIt is also possible, if desirable, to have an Oracle Cloud Infrastructure load balancer route traffic to an ingress controller running inside the Kubernetes cluster and have that ingress controller in turn route traffic to services in the cluster.\nRequesting an Oracle Cloud Infrastructure load balancer When your domain is created by the operator, a number of Kubernetes services are created by the operator, including one for the WebLogic Server Administration Server and one for each Managed Server and cluster.\nIn the following example, there is a domain called bobs-bookstore in the bob namespace. This domain has a cluster called cluster-1 which exposes traffic on port 31111.\nThe following Kubernetes YAML file defines a new Service in the same namespace. The selector targets all of the pods in this namespace which are part of the cluster cluster-1, using the annotations that are placed on those pods by the operator. It also defines the port and protocol.\nYou can include the optional oci-load-balancer-shape annotation (as shown) if you want to specify the shape of the load balancer. Otherwise the default shape (100Mbps) will be used.\napiVersion: v1 kind: Service metadata: name: bobs-bookstore-oci-lb-service namespace: bob annotations: service.beta.kubernetes.io/oci-load-balancer-shape: 400Mbps spec: ports: - name: http port: 31111 protocol: TCP targetPort: 31111 selector: weblogic.clusterName: cluster-1 weblogic.domainUID: bobs-bookstore sessionAffinity: None type: LoadBalancer When you apply this YAML file to your cluster, you will see the new service is created but initially the external IP is shown as \u0026lt;pending\u0026gt;.\n$ kubectl -n bob get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE bobs-bookstore-admin-server ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,7001/TCP,30101/TCP 9d bobs-bookstore-admin-server-ext NodePort 10.96.224.13 \u0026lt;none\u0026gt; 7001:32401/TCP 9d bobs-bookstore-cluster-cluster-1 ClusterIP 10.96.86.113 \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-oci-lb-service LoadBalancer 10.96.121.216 \u0026lt;pending\u0026gt; 31111:31671/TCP 9s After a short time (typically less than a minute), the Oracle Cloud Infrastructure load balancer will be provisioned and the external IP address will be displayed:\n$ kubectl -n bob get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE bobs-bookstore-admin-server ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,7001/TCP,30101/TCP 9d bobs-bookstore-admin-server-ext NodePort 10.96.224.13 \u0026lt;none\u0026gt; 7001:32401/TCP 9d bobs-bookstore-cluster-cluster-1 ClusterIP 10.96.86.113 \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-oci-lb-service LoadBalancer 10.96.121.216 132.145.235.215 31111:31671/TCP 55s You can now use the external IP address and port to access your pods. There are several options that can be used to configure more advanced load balancing behavior. For more information, including how to configure SSL support, supporting internal and external subnets, and so one, refer to the Oracle Cloud Infrastructure documentation.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/cicd/how-to-copy-domains/",
	"title": "Copy domains",
	"tags": [],
	"description": "How to copy domains.",
	"content": "The recommended approach to save a copy of a Domain in Image or Domain on PV domain is to simply ZIP (or tar) the domain directory. However, there is a very important caveat with this recommendation - when you unzip the domain, it must go back into exactly the same location (Domain Home) in the (new) file system. Using this approach will maintain the same domain encryption key.\nThe best practice/recommended approach is to create a \u0026ldquo;primordial domain\u0026rdquo; which does not contain any applications or resources, and to create a ZIP file of this domain before starting any servers.\nThe domain ZIP file must be created before starting servers.\nWhen servers are started the first time, they will encrypt various other data. Make sure that you create the ZIP file before starting servers for the first time. The primordial domain ZIP file should be stored in a safe place where the CI/CD can get it when needed, for example in a secured Artifactory repository (or something similar).\nRemember, anyone who gets access to this ZIP file can get access to the domain encryption key, so it needs to be protected appropriately.\nEvery time you run your CI/CD pipeline to create a new mutation of the domain, it should retrieve and unzip the primordial domain first, and then apply changes to that domain using tools like WDT or WLST (see here).\nAlways use external state.\nYou should always keep state outside the image. This means that you should use JDBC stores for leasing tables, JMS and Transaction stores, EJB timers, JMS queues, and so on. This ensures that data will not be lost when a container is destroyed.\nWe recommend that state be kept in a database to take advantage of built-in database server HA, and the fact that disaster recovery of sites across all but the shortest distances almost always requires using a single database server to consolidate and replicate data (DataGuard).\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/ingress/",
	"title": "Ingress",
	"tags": [],
	"description": "Ingress controllers and load balancer sample scripts.",
	"content": "The WebLogic Kubernetes Operator supports NGINX and Traefik. We provide samples that demonstrate how to install and configure each one.\nFor production environments, we recommend NGINX, Traefik (2.2.1 or later) ingress controllers or the load balancer provided by your cloud provider.\nThe samples are located in following folders:\nTraefik NGINX "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/model-in-image/update3/",
	"title": "Update 3",
	"tags": [],
	"description": "",
	"content": "The Update 3 use case demonstrates deploying an updated WebLogic application to the running Update 1 use case domain using an updated image.\nIn the use case, you will:\nCreate an image model-in-image:WLS-v2 that is similar to the currently active model-in-image:WLS-v1 image, but with the following updates: An updated web application v2 at the myapp-v2 directory path within the WDT application archive instead of myapp-v1. An updated model YAML file within the image that points to the new web application path. Apply an updated Domain YAML file that references the new image while still referencing the original Update 1 use case secrets and model ConfigMap. After the updated Domain YAML file is applied, the operator will:\nRerun the introspector job and generate a new domain home based on the new model. Restart the domain\u0026rsquo;s Administration Server pod so that it loads the new image and new domain home. Roll the domain\u0026rsquo;s cluster servers one at a time so that they each load the new image, new domain home, and revised application. Finally, you will call the application to verify that its revision is active.\nNote that the old version of the application v1 remains in the new image\u0026rsquo;s archive but is unused. We leave it there to demonstrate that the old version can remain in case you want to revert to it. After the new image is applied, you can revert by modifying your model\u0026rsquo;s configuration.model.configMap to override the related application path in your image model.\nHere are the steps for this use case:\nMake sure you have deployed the domain from the Update 1 use case.\nCreate an updated auxiliary image.\nRecall that a goal of the Initial use case was to demonstrate using the WebLogic Image Tool to create an auxiliary image named wdt-domain-image:WLS-v1 from files that were staged in /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/. The staged files included a web application in a WDT ZIP archive, and WDT model configuration for a WebLogic Server Administration Server called admin-server and a WebLogic cluster called cluster-1. The final image was called wdt-domain-image:WLS-v1 and, in addition to having a copy of the staged files in its /auxiliary/models directory, also contained a directory /auxiliary/weblogic-deploy where the WebLogic Deploy Tooling software is installed.\nIn this use case, you will follow similar steps to the Initial use case to create a new image with an updated application and model, plus deploy the updated model and application to the running Update 1 use case domain.\nUnderstanding your updated WDT archive.\nThe updated archive for this use case is in directory /tmp/sample/wdt-artifacts/archives/archive-v2. You will use it to create an archive ZIP file for the image. This archive is similar to the /tmp/sample/wdt-artifacts/archives/archive-v1 from the Initial use case with the following differences:\nIt includes an updated version of the application in ./wlsdeploy/applications/myapp-v2 (while keeping the original application in directory ./wlsdeploy/applications/myapp-v1). The application in ./wlsdeploy/applications/myapp-v2/myapp_war/index.jsp contains a single difference from the original application: it changes the line out.println(\u0026quot;Hello World! This is version 'v1' of the sample JSP web-app.\u0026quot;); to out.println(\u0026quot;Hello World! This is version 'v2' of the sample JSP web-app.\u0026quot;);. For additional information about archives, see Understand your first archive.\nStage a ZIP file of the WDT archive.\nWhen you create your updated image, you will use the files in the staging directory /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v2. In preparation, you need it to contain a ZIP file of the new WDT application archive.\nRun the following commands to create your application archive ZIP file and put it in the expected directory:\n# Delete existing archive.zip in case we have an old leftover version $ rm -f /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v2/archive.zip # Move to the directory which contains the source files for our new archive $ cd /tmp/sample/wdt-artifacts/archives/archive-v2 Using the WDT archive helper tool, create the archive in the location that we will use later when we run the WebLogic Image Tool. `\n$ /tmp/sample/wdt-artifacts/weblogic-deploy/bin/archiveHelper.sh add application -archive_file=/tmp/sample/wdt-artifacts/wdt-model-files/WLS-v2/archive.zip -source=wlsdeploy/applications/myapp-v2 Understanding your staged model files.\nThe WDT model YAML file and properties for this use case have already been staged for you to directory /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v2.\nThe model.10.yaml file in this directory has an updated path wlsdeploy/applications/myapp-v2 that references the updated web application in your archive, but is otherwise identical to the model staged for the original image. The final related YAML file stanza looks like this:\nappDeployments: Application: myapp: SourcePath: \u0026#39;wlsdeploy/applications/myapp-v2\u0026#39; ModuleType: ear Target: \u0026#39;cluster-1\u0026#39; If you would like to review the entire original model before this change, see Staging model files in the Initial use case.\nCreate a new auxiliary image from your staged model files using WIT.\nAt this point, you have staged all of the files needed for image wdt-domain-image:WLS-v2; they include:\n/tmp/sample/wdt-artifacts/wdt-model-files/WLS-v2/model.10.yaml /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v2/model.10.properties /tmp/sample/model-images/wdt-artifacts/wdt-model-files/WLS-v2/archive.zip Now, you use the Image Tool to create an auxiliary image named wdt-domain-image:WLS-v2. You\u0026rsquo;ve already set up this tool during the prerequisite steps.\nRun the following commands to create the auxiliary image and verify that it worked:\n$ cd /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v2 $ /tmp/sample/wdt-artifacts/imagetool/bin/imagetool.sh createAuxImage \\ --tag wdt-domain-image:WLS-v2 \\ --wdtModel ./model.10.yaml \\ --wdtVariables ./model.10.properties \\ --wdtArchive ./archive.zip If you don\u0026rsquo;t see the imagetool directory, then you missed a step in the prerequisites.\nThis command runs the WebLogic Image Tool in its Model in Image mode, and does the following:\nBuilds the final auxiliary image as a layer on a small busybox base image. Copies the WDT ZIP file that\u0026rsquo;s referenced in the WIT cache into the image. Note that you cached WDT in WIT using the keyword latest when you set up the cache during the sample prerequisites steps. This lets WIT implicitly assume it\u0026rsquo;s the desired WDT version and removes the need to pass a -wdtVersion flag. Copies the specified WDT model, properties, and application archives to image location /auxiliary/models. When the command succeeds, it will end with output like the following:\n[INFO ] Build successful. Build time=36s. Image tag=wdt-domain-image:WLS-v2 Also, if you run the docker images command, then you will see an image named wdt-domain-image:WLS-v2.\nNOTE: Before you deploy the domain custom resource, ensure all nodes in your Kubernetes cluster can access auxiliary-image and other images.\nDeploy resources - Introduction Set up and apply a Domain YAML file that is similar to your Update 1 use case Domain YAML file but with a different image:\nOption 1: Update a copy of your Domain YAML file from the Update 1 use case.\nIn the Update 1 use case, we suggested creating a file named /tmp/sample/mii-update1.yaml or using the /tmp/sample/domain-resources/WLS/mii-update1-d1-WLS-v1-ds.yaml file that is supplied with the sample.\nWe suggest copying this Domain YAML file and naming the copy /tmp/sample/mii-update3.yaml before making any changes.\nWorking on a copy is not strictly necessary, but it helps keep track of your work for the different use cases in this sample and provides you a backup of your previous work.\nChange the /tmp/sample/mii-update3.yaml Domain YAML file\u0026rsquo;s image field to reference wdt-domain-image:WLS-v2 instead of wdt-domain-image:WLS-v1.\nThe final result will look something like this:\n... spec: ... image: \u0026#34;wdt-domain-image:WLS-v2\u0026#34; Apply your changed Domain YAML file:\nNOTE: Before you deploy the domain custom resource, ensure all nodes in your Kubernetes cluster can access auxiliary-image and other images.\n$ kubectl apply -f /tmp/sample/mii-update3.yaml Option 2: Use the updated Domain YAML file that is supplied with the sample:\nNOTE: Before you deploy the domain custom resource, ensure all nodes in your Kubernetes cluster can access auxiliary-image and other images.\n$ kubectl apply -f /tmp/sample/domain-resources/WLS/mii-update3-d1-WLS-v2-ds.yaml Wait for the roll to complete.\nNow that you\u0026rsquo;ve applied a Domain YAML file with an updated image, the operator will automatically rerun the domain\u0026rsquo;s introspector job to generate a new domain home, and then will restart (\u0026lsquo;roll\u0026rsquo;) each of the domain\u0026rsquo;s pods so that they use the new domain home and the new image. You\u0026rsquo;ll need to wait for this roll to complete before you can verify that the new image and its associated new application have been deployed.\nOne way to do this is to call kubectl get pods -n sample-domain1-ns --watch and wait for the pods to cycle back to their ready state.\nFor a more detailed view of this activity, you can use the waitForDomain.sh sample lifecycle script. This script provides useful information about a domain\u0026rsquo;s pods and optionally waits for its Completed status condition to become True. A Completed domain indicates that all of its expected pods have reached a ready state plus their target restartVersion, introspectVersion, and image. For example:\n$ cd /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/domain-lifecycle $ ./waitForDomain.sh -n sample-domain1-ns -d sample-domain1 -p Completed After your domain roll is complete, you can call the sample web application to determine if the updated application was deployed.\nWhen the application is invoked, it will contain an output string like Hello World! This is version 'v2' of the sample JSP web-app..\nSend a web application request to the ingress controller:\n$ curl -s -S -m 10 -H \u0026#39;host: sample-domain1-cluster-cluster-1.sample.org\u0026#39; \\ http://localhost:30305/myapp_war/index.jsp Or, if Traefik is unavailable and your Administration Server pod is running, you can run kubectl exec:\n$ kubectl exec -n sample-domain1-ns sample-domain1-admin-server -- bash -c \\ \u0026#34;curl -s -S -m 10 http://sample-domain1-cluster-cluster-1:8001/myapp_war/index.jsp\u0026#34; You will see something like the following:\n\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v2\u0026#39; of the sample JSP web-app. Welcome to WebLogic Server \u0026#39;managed-server1\u0026#39;! domain UID = \u0026#39;sample-domain1\u0026#39; domain name = \u0026#39;domain1\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found min threads constraint runtime named \u0026#39;SampleMinThreads\u0026#39; with configured count: 1 Found max threads constraint runtime named \u0026#39;SampleMaxThreads\u0026#39; with configured count: 10 Found 1 local data source: Datasource \u0026#39;mynewdatasource\u0026#39;: State=\u0026#39;Running\u0026#39;, testPool=\u0026#39;Failed\u0026#39; ---TestPool Failure Reason--- NOTE: Ignore \u0026#39;mynewdatasource\u0026#39; failures until the sample\u0026#39;s Update 4 use case. --- ... ... invalid host/username/password ... ----------------------------- ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; A TestPool Failure is expected because we will demonstrate dynamically correcting the data source attributes in Update 4.\nIf you see an error other than the expected TestPool Failure, then consult Debugging.\nIf you plan to run the Update 4 use case, then leave your domain running.\nTo remove the resources you have created in the samples, see Cleanup.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/developerguide/coding-standards/",
	"title": "Coding standards",
	"tags": [],
	"description": "Review the project coding standards.",
	"content": "This project has adopted the following coding standards:\nCode should be formatted using Oracle / WebLogic standards, which are identical to the Google Java Style. Javadoc must be provided for all public packages, classes, and methods, and must include all parameters and returns. Javadoc is not required for methods that override or implement methods that are already documented. All non-trivial methods should include LOGGER.entering() and LOGGER.exiting() calls. The LOGGER.exiting() call should include the value that is going to be returned from the method, unless that value includes a credential or other sensitive information. All logged messages must be internationalized using the resource bundle src/main/resources/Operator.properties and using a key itemized in src/main/java/oracle/kubernetes/operator/logging/MessageKeys.java. After operator initialization, all operator work must be implemented using the asynchronous call model. In particular, worker threads must not use sleep() or IO or lock-based blocking methods. Code formatting plugins The following IDE plugins are available to assist with following the code formatting standards\nIntelliJ An IntelliJ plugin is available from the plugin repository.\nThe plugin will be enabled by default. To disable it in the current project, go to File \u0026gt; Settings... \u0026gt; google-java-format Settings (or IntelliJ IDEA \u0026gt; Preferences... \u0026gt; Other Settings \u0026gt; google-java-format Settings on macOS) and uncheck the \u0026ldquo;Enable google-java-format\u0026rdquo; checkbox.\nTo disable it by default in new projects, use File \u0026gt; Other Settings \u0026gt; Default Settings....\nWhen enabled, it will replace the normal \u0026ldquo;Reformat Code\u0026rdquo; action, which can be triggered from the \u0026ldquo;Code\u0026rdquo; menu or with the Ctrl-Alt-L (by default) keyboard shortcut.\nThe import ordering is not handled by this plugin, unfortunately. To fix the import order, download the IntelliJ Java Google Style file and import it into File→Settings→Editor→Code Style.\nEclipse An Eclipse plugin can be downloaded from the releases page. Drop it into the Eclipse drop-ins folder to activate the plugin.\nThe plugin adds a google-java-format formatter implementation that can be configured in Eclipse \u0026gt; Preferences \u0026gt; Java \u0026gt; Code Style \u0026gt; Formatter \u0026gt; Formatter Implementation.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/security/rbac/",
	"title": "RBAC",
	"tags": [],
	"description": "Operator role-based authorization.",
	"content": "This document is now located in the operator user guide, see RBAC.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-operators/using-helm/",
	"title": "Configuration reference",
	"tags": [],
	"description": "An operator runtime is installed and configured using Helm. Here are useful Helm operations and operator configuration values.",
	"content": "Contents Introduction Useful Helm operations Operator Helm configuration values Overall operator information serviceAccount kubernetesPlatform enableClusterRoleBinding Creating the operator pod image imagePullPolicy imagePullSecrets annotations labels nodeSelector affinity runAsUser WebLogic domain conversion webhook webhookOnly operatorOnly preserveWebhook WebLogic domain management domainNamespaceSelectionStrategy domainNamespaces domainNamespaceLabelSelector domainNamespaceRegExp introspectorJobNameSuffix and externalServiceNameSuffix clusterSizePaddingValidationEnabled istioLocalhostBindingsEnabled Elastic Stack integration elkIntegrationEnabled logStashImage elasticSearchHost elasticSearchPort elasticSearchProtocol createLogStashConfigMap REST interface configuration enableRest externalRestEnabled externalRestHttpsPort externalRestIdentitySecret externalOperatorCert (Deprecated) externalOperatorKey (Deprecated) tokenReviewAuthentication Debugging options javaLoggingLevel remoteDebugNodePortEnabled internalDebugHttpPort externalDebugHttpPort Introduction The operator requires Helm for its installation and tuning, and this document is a reference guide for useful Helm commands and operator configuration values.\nThis document assumes that the operator has been installed using an operator Helm chart. An operator Helm chart can be obtained from the chart repository or can be found in the operator source. For information about operator Helm chart access, installation, and upgrade, see Prepare for installation and Installation and upgrade.\nUseful Helm operations You can find out the configuration values that the operator Helm chart supports, as well as the default values, using the helm show command.\nFirst, access the operator Helm chart repository using this format, helm repo add \u0026lt;helm-chart-repo-name\u0026gt; \u0026lt;helm-chart-repo-url\u0026gt;: $ helm repo add weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts --force-update Then, use the helm show command with this format: helm show \u0026lt;helm-chart-repo-name\u0026gt;/weblogic-operator. For example, with an operator Helm chart where the repository is named weblogic-operator: $ helm show chart weblogic-operator/weblogic-operator An installed operator is maintained by a Helm release. List the Helm releases for a specified namespace or all namespaces:\n$ helm list --namespace \u0026lt;namespace\u0026gt; $ helm list --all-namespaces Get the status of the operator Helm release named sample-weblogic-operator:\n$ helm status sample-weblogic-operator --namespace \u0026lt;namespace\u0026gt; Show the history of the operator Helm release named sample-weblogic-operator:\n$ helm history sample-weblogic-operator --namespace \u0026lt;namespace\u0026gt; Roll back to a previous version of the operator Helm release named sample-weblogic-operator, in this case, the first version:\n$ helm rollback sample-weblogic-operator 1 --namespace \u0026lt;namespace\u0026gt; Show the custom values you configured for a operator Helm release named sample-weblogic-operator:\n$ helm get values sample-weblogic-operator Show all of the values your operator Helm release named sample-weblogic-operator is using:\n$ helm get values --all sample-weblogic-operator Change one or more values using helm upgrade, for example:\n$ helm repo add weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts --force-update $ helm upgrade \\ weblogic-operator/weblogic-operator \\ --reuse-values \\ --set \u0026#34;domainNamespaces={sample-domains-ns1}\u0026#34; \\ --set \u0026#34;javaLoggingLevel=FINE\u0026#34; \\ --wait NOTES:\nIn this example, the --reuse-values flag indicates that previous overrides of other values should be retained. Before changing the javaLoggingLevel setting, consult the Operator logging level advice. Operator Helm configuration values This section describes the details of the operator Helm chart\u0026rsquo;s available configuration values.\nOverall operator information serviceAccount Specifies the name of the service account in the operator\u0026rsquo;s namespace that the operator will use to make requests to the Kubernetes API server. You are responsible for creating the service account.\nThe helm install or helm upgrade command with a non-existing service account results in a Helm chart validation error.\nDefaults to default.\nExample:\nserviceAccount: \u0026#34;weblogic-operator\u0026#34; kubernetesPlatform Specify the Kubernetes platform on which the operator is running. This setting has no default, the only valid value is OpenShift; the setting should be left unset for other platforms.\nWhen set to OpenShift, the operator:\nSets the domain home file permissions in each WebLogic Server pod to work correctly in OpenShift for Model in Image, and Domain home in Image domains. Specifically, it sets file group permissions so that they match file user permissions. Sets the weblogic.SecureMode.WarnOnInsecureFileSystem Java system property to false on the command line of each WebLogic Server. This flag suppresses insecure file system warnings reported in the WebLogic Server console when the WebLogic Server is in production mode. These warnings result from setting the file permissions necessary to work with restricted security context constraints on OpenShift. For more information about the security requirements for running WebLogic in OpenShift, see the OpenShift documentation.\nExample:\nkubernetesPlatform: OpenShift enableClusterRoleBinding Specifies whether the roles necessary for the operator to manage domains will be granted using a ClusterRoleBinding rather than using RoleBindings in each managed namespace.\nDefaults to true.\nThis option greatly simplifies managing namespaces when the selection is done using label selectors or regular expressions as the operator will already have privilege in any namespace.\nCustomers who deploy the operator in Kubernetes clusters that run unrelated workloads will likely not want to use this option. With the enableClusterRoleBinding option, the operator will have privilege in all Kubernetes namespaces. If you want to limit the operator\u0026rsquo;s privilege to just the set of namespaces that it will manage, then remove this option; this will mean that the operator has privilege only in the set of namespaces that match the selection strategy at the time the Helm release was installed or upgraded.\nNOTE: If your operator Helm enableClusterRoleBinding configuration value is false, then a running operator will not have privilege to manage a newly added namespace that matches its namespace selection criteria until you upgrade the operator\u0026rsquo;s Helm release. See Ensuring the operator has permission to manage a namespace.\nCreating the operator pod image Specifies the container image containing the operator code.\nDefaults to ghcr.io/oracle/weblogic-kubernetes-operator:4.2.20 or similar (based on the default in your Helm chart, see helm show in Useful Helm operations).\nExample:\nimage: \u0026#34;ghcr.io/oracle/weblogic-kubernetes-operator:some-tag\u0026#34; imagePullPolicy Specifies the image pull policy for the operator container image.\nDefaults to IfNotPresent.\nWhen using the default images, IfNotPresent is sufficient because the operator will never update an existing image. However, if you have created your own operator image and are updating the image without changing the tag, you might want to use Always.\nExample:\nimage: \u0026#34;Always\u0026#34; imagePullSecrets Contains an optional list of Kubernetes Secrets, in the operator\u0026rsquo;s namespace, that are needed to access the registry containing the operator image. For example, you might need an operator imagePullSecret if you are using an operator image from a private registry that requires authentication to pull. You are responsible for creating the secret. If no secrets are required, then omit this property. For more information on specifying the registry credentials when the operator image is stored in a private registry, see Customizing operator image name, pull secret, and private registry.\nExamples:\nUsing YAML: imagePullSecrets: - name: \u0026#34;my-image-pull-secret\u0026#34; Using the Helm command line: --set \u0026#34;imagePullSecrets[0].name=my-image-pull-secret\u0026#34; annotations Specifies a set of key-value annotations that will be added to each pod running the operator. If no customer defined annotations are required, then omit this property.\nExample:\nannotations: stage: production You may also specify annotations using the --set parameter to the Helm install command, as follows:\n--set annotations.stage=production labels Specifies a set of key-value labels that will be added to each pod running the operator. The Helm chart will automatically add any required labels, so the customer is not required to define those here. If no customer defined labels are required, then omit this property.\nExample:\nlabels: sidecar.istio.io/inject: \u0026#34;false\u0026#34; You may also specify labels using the --set parameter to the Helm install command, as follows:\n--set labels.\u0026#34;sidecar\\.istio\\.io/inject\u0026#34;=false nodeSelector Allows you to run the operator Pod on a Node whose labels match the specified nodeSelector labels. You can use this optional feature if you want the operator Pod to run on a Node with particular labels. For more details, see Assign Pods to Nodes in the Kubernetes documentation. This is not required if the operator Pod can run on any Node.\nExample:\nnodeSelector: disktype: ssd affinity Allows you to constrain the operator Pod to be scheduled on a Node with certain labels; it is conceptually similar to nodeSelector. affinity provides advanced capabilities to limit Pod placement on specific Nodes. For more details, see Assign Pods to Nodes in the Kubernetes documentation. This is optional and not required if the operator Pod can run on any Node or when using nodeSelector.\nExample:\naffinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: nodeType operator: In values: - dev - test preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: another-node-label-key operator: In values: - another-node-label-value runAsUser Specifies the UID to run the operator and conversion webhook container processes. If not specified, it defaults to the user specified in the operator\u0026rsquo;s container image.\nExample:\nrunAsUser: 1000 WebLogic domain conversion webhook The WebLogic domain conversion webhook is automatically installed by default when an operator is installed and uninstalled when an operator is uninstalled. You can optionally install and uninstall it independently by using the operator\u0026rsquo;s Helm chart. For details, see Install the conversion webhook and Uninstall the conversion webhook.\nNOTE: By default, the conversion webhook installation uses the same serviceAccount, Elastic Stack integration, and Debugging options configuration values that are used by the operator installation. If you want to use different serviceAccount or Elastic Stack integration or Debugging options for the conversion webhook, then install the conversion webhook independently by using the following webhookOnly configuration value and provide the new value during webhook installation.\nwebhookOnly Specifies whether only the conversion webhook should be installed during the helm install and that the operator installation should be skipped. By default, the helm install command installs both the operator and the conversion webhook. If set to true, the helm install will install only the conversion webhook (and not the operator).\nDefaults to false.\noperatorOnly NOTE: This is an advanced setting and should be used only in environments where a conversion webhook is already installed. The operator version 4.x requires a conversion webhook to be installed.\nSpecifies whether only the operator should be installed during the helm install and that the conversion webhook installation should be skipped. By default, the helm install command installs both the operator and the conversion webhook. If set to true, the helm install will install only the operator (and not the conversion webhook).\nDefaults to false.\npreserveWebhook Specifies whether the existing conversion webhook deployment should be preserved (not removed) when the release is uninstalled using helm uninstall. By default, the helm uninstall removes both the webhook and the operator installation. If set to true in the helm install command, then the helm uninstall command will not remove the webhook installation. Ignored when webhookOnly is set to true in the helm install command.\nDefaults to false.\nWebLogic domain management The settings in this section determine the namespaces that an operator monitors for domain resources. For usage, also see Namespace management.\ndomainNamespaceSelectionStrategy Specifies how the operator will select the set of namespaces that it will manage. Legal values are: List, LabelSelector, RegExp, and Dedicated:\nIf set to List, then the operator will manage the set of namespaces listed by the domainNamespaces value. If set to LabelSelector, then the operator will manage the set of namespaces discovered by a list of namespaces using the value specified by domainNamespaceLabelSelector as a label selector. If set to RegExp, then the operator will manage the set of namespaces discovered by a list of namespaces using the value specified by domainNamespaceRegExp as a regular expression matched against the namespace names. Finally, if set to Dedicated, then operator will manage WebLogic domains only in the same namespace which the operator itself is deployed, which is the namespace of the Helm release. NOTES:\nDefaults to LabelSelector. For more information, see Choose a domain namespace section strategy. If your operator Helm enableClusterRoleBinding configuration value is false, note that any domain namespaces created after operator installation, requires running helm upgrade on the operator to have the operator rescan for domains to manage, even if, for example, using a LabelSelector where the namespace has a matching label. See Ensuring the operator has permission to manage a namespace.\ndomainNamespaces Specifies a list of namespaces that the operator manages. The names must be lowercase. You are responsible for creating these namespaces. The operator will only manage domains found in these namespaces. This value is required if domainNamespaceSelectionStrategy is List and ignored otherwise.\nExamples:\nExample 1: In the following configuration, the operator will manage the default and ns1 Kubernetes Namespaces: domainNamespaces: - \u0026#34;default\u0026#34; - \u0026#34;ns1\u0026#34; Example 2: In the following configuration, the operator will manage namespace1 and namespace2: domainNamespaces: [ \u0026#34;namespace1\u0026#34;, \u0026#34;namespace2\u0026#34; ] Note that this is a valid but different YAML syntax for specifying arrays in comparison to the previous example. Example 3: To specify on the Helm command line: --set \u0026#34;domainNamespaces={namespace1,namespace2}\u0026#34; NOTES:\nDefaults to the default namespace. You must include the default namespace in the list if you want the operator to monitor both the default namespace and some other namespaces. If you change domainNamespaces using a helm upgrade command, then the new list completely replaces the original list (they are not merged). For more information, see Namespace Management. domainNamespaceLabelSelector Specifies a label selector that will be used when searching for namespaces that the operator will manage. The operator will only manage domains found in namespaces matching this selector. This value is required if domainNamespaceSelectionStrategy is LabelSelector and ignored otherwise.\nExamples:\nExample 1: In the following configuration, the operator will manage namespaces that have the label weblogic-operator regardless of the value of that label: domainNamespaceLabelSelector: weblogic-operator Example 2: In the following configuration, the operator will manage all namespaces that have the label environment, but where the value of that label is not production or systemtest: domainNamespaceLabelSelector: environment notin (production,systemtest) NOTES:\nTo specify the previous sample on the Helm command line, escape the equal sign, spaces, and commas as follows: --set \u0026#34;domainNamespaceLabelSelector\\=environment\\\\ notin\\\\ (production\\\\,systemtest)\u0026#34; If your operator Helm enableClusterRoleBinding configuration value is false, then a running operator will not have privilege to manage a newly added namespace that matches its label selector until you upgrade the operator\u0026rsquo;s Helm release. See Ensuring the operator has permission to manage a namespace. domainNamespaceRegExp Specifies a regular expression that will be used when searching for namespaces that the operator will manage. The operator will only manage domains found in namespaces matching this regular expression. This value is required if domainNamespaceSelectionStrategy is RegExp and ignored otherwise.\nNOTES:\nThe regular expression functionality included with Helm is restricted to linear time constructs and, in particular, does not support lookarounds. The operator, written in Java, supports these complicated expressions. If you need to use a complex regular expression, then either: Set enableClusterRoleBinding to true. When enableClusterRoleBinding is false and namespaces are determined by list or regular expression, then the Helm chart has to iterate all of the namespaces and determine which ones match. If you set enableClusterRoleBinding to true when selecting namespaces by list or regular expression, then the Helm chart doesn\u0026rsquo;t need to do anything special per namespace. Or, create the necessary RoleBindings outside of Helm. If your operator Helm enableClusterRoleBinding configuration value is false, then a running operator will not have privilege to manage a newly added namespace that matches its regular expression until you upgrade the operator\u0026rsquo;s Helm release. See Ensuring the operator has permission to manage a namespace. introspectorJobNameSuffix and externalServiceNameSuffix Specify the suffixes that the operator uses to form the name of the Kubernetes job for the domain introspector, and the name of the external service for the WebLogic Administration Server, if the external service is enabled.\nDefaults to -introspector and -ext, respectively. The values cannot be more than 25 and 10 characters, respectively.\nPrior to the operator 3.1.0 release, the suffixes are hard-coded to -introspect-domain-job and -external. The defaults are shortened in newer releases to support longer names in the domain resource and WebLogic domain configurations, such as the domainUID, and WebLogic cluster and server names.\nTo work with Kubernetes limits to resource names, the resultant names for the domain introspector job and the external service should not be more than 63 characters. For more details, see Meet Kubernetes resource name restrictions.\nclusterSizePaddingValidationEnabled Specifies if the operator needs to reserve additional padding when validating the server service names to account for longer Managed Server names as a result of expanding a cluster\u0026rsquo;s size in WebLogic domain configurations.\nDefaults to true.\nIf clusterSizePaddingValidationEnabed is set to true, two additional characters will be reserved if the configured cluster\u0026rsquo;s size is between one and nine, and one additional character will be reserved if the configured cluster\u0026rsquo;s size is between 10 and 99. No additional character is reserved if the configured cluster\u0026rsquo;s size is greater than 99.\nistioLocalhostBindingsEnabled Default for the domain resource domain.spec.configuration.istio.localhostBindingsEnabled setting.\nFor more information, see Configuring the domain resource in Istio Support.\nElastic Stack integration The following settings are related to integrating the Elastic Stack with the operator pod.\nFor example usage, see the operator Elastic Stack (Elasticsearch, Logstash, and Kibana) integration sample.\nelkIntegrationEnabled Specifies whether or not Elastic Stack integration is enabled.\nDefaults to false.\nExample:\nelkIntegrationEnabled: true logStashImage Specifies the container image containing Logstash. This parameter is ignored if elkIntegrationEnabled is false.\nDefaults to logstash:6.8.23.\nExample:\nlogStashImage: \u0026#34;docker.elastic.co/logstash/logstash:6.8.23\u0026#34; elasticSearchHost Specifies the hostname where Elasticsearch is running. This parameter is ignored if elkIntegrationEnabled is false.\nDefaults to elasticsearch.default.svc.cluster.local.\nExample:\nelasticSearchHost: \u0026#34;elasticsearch2.default.svc.cluster.local\u0026#34; elasticSearchPort Specifies the port number where Elasticsearch is running. This parameter is ignored if elkIntegrationEnabled is false.\nDefaults to 9200.\nExample:\nelasticSearchPort: 9201 elasticSearchProtocol Specifies the protocol to use for communication with Elasticsearch. This parameter is ignored if elkIntegrationEnabled is false.\nDefaults to http.\nExample:\nelasticSearchProtocol: https createLogStashConfigMap Specifies whether a ConfigMap named weblogic-operator-logstash-cm should be created during helm install. The ConfigMap contains the Logstash pipeline configuration file logstash.conf and the Logstash settings file logstash.yml for the Logstash container running in the operator pod. If set to true, then a ConfigMap will be created during helm install using the logstash.conf and logstash.yml files in the kubernetes/samples/charts/weblogic-operator directory. Set createLogStashConfigMap to false if the ConfigMap already exists in the operator\u0026rsquo;s namespace with the Logstash configuration files. This parameter is ignored if elkIntegrationEnabled is false.\nDefaults to true.\nExample:\ncreateLogStashConfigMap: false REST interface configuration The REST interface configuration options are advanced settings for configuring the operator\u0026rsquo;s external REST interface.\nFor usage information, see the operator REST Services.\nenableRest Determines whether the operator\u0026rsquo;s REST endpoint is enabled.\nBeginning with operator version 4.0.5, the operator\u0026rsquo;s REST endpoint is disabled by default.\nDefaults to false.\nexternalRestEnabled Determines whether the operator\u0026rsquo;s REST interface will be exposed outside the Kubernetes cluster using a node port. This value is ignored if enableRest is not true.\nSee also externalRestHttpsPort for customizing the port number.\nDefaults to false.\nIf set to true, you must provide the externalRestIdentitySecret property that contains the name of the Kubernetes Secret which contains the SSL certificate and private key for the operator\u0026rsquo;s external REST interface.\nExample:\nexternalRestEnabled: true NOTE: A node port is a security risk because the port may be publicly exposed to the internet in some environments. If you need external access to the REST port, then consider alternatives such as providing access through your load balancer, or using Kubernetes port forwarding.\nexternalRestHttpsPort Specifies the node port that should be allocated for the external operator REST HTTPS interface.\nOnly used when externalRestEnabled is true, otherwise ignored.\nDefaults to 31001.\nExample:\nexternalRestHttpsPort: 32009 NOTE: A node port is a security risk because the port may be publicly exposed to the internet in some environments. If you need external access to the REST port, then consider alternatives such as providing access through your load balancer, or using Kubernetes port forwarding.\nexternalRestIdentitySecret Specifies the user supplied secret that contains the SSL/TLS certificate and private key for the external operator REST HTTPS interface. The value must be the name of the Kubernetes tls secret previously created in the namespace where the operator is deployed. This parameter is required if externalRestEnabled is true, otherwise, it is ignored. To create the Kubernetes tls secret, you can use the following command:\n$ kubectl create secret tls \u0026lt;secret-name\u0026gt; \\ -n \u0026lt;operator-namespace\u0026gt; \\ --cert=\u0026lt;path-to-certificate\u0026gt; \\ --key=\u0026lt;path-to-private-key\u0026gt; There is no default value.\nThe Helm installation will produce an error, similar to the following, if externalRestIdentitySecret is not specified (left blank) and externalRestEnabled is true:\nError: render error in \u0026#34;weblogic-operator/templates/main.yaml\u0026#34;: template: weblogic-operator/templates/main.yaml:9:3: executing \u0026#34;weblogic-operator/templates/main.yaml\u0026#34; at \u0026lt;include \u0026#34;operator.va...\u0026gt;: error calling include: template: weblogic-operator/templates/_validate-inputs.tpl:42:14: executing \u0026#34;operator.validateInputs\u0026#34; at \u0026lt;include \u0026#34;utils.endVa...\u0026gt;: error calling include: template: weblogic-operator/templates/_utils.tpl:22:6: executing \u0026#34;utils.endValidation\u0026#34; at \u0026lt;fail $scope.validati...\u0026gt;: error calling fail: string externalRestIdentitySecret must be specified Example:\nexternalRestIdentitySecret: weblogic-operator-external-rest-identity externalOperatorCert (Deprecated) Use externalRestIdentitySecret instead\nSpecifies the user supplied certificate to use for the external operator REST HTTPS interface. The value must be a string containing a Base64 encoded PEM certificate. This parameter is required if externalRestEnabled is true, otherwise, it is ignored.\nThere is no default value.\nThe Helm installation will produce an error, similar to the following, if externalOperatorCert is not specified (left blank) and externalRestEnabled is true:\nError: render error in \u0026#34;weblogic-operator/templates/main.yaml\u0026#34;: template: weblogic-operator/templates/main.yaml:4:3: executing \u0026#34;weblogic-operator/templates/main.yaml\u0026#34; at \u0026lt;include \u0026#34;operator.va...\u0026gt;: error calling include: template: weblogic-operator/templates/_validate-inputs.tpl:53:4: executing \u0026#34;operator.validateInputs\u0026#34; at \u0026lt;include \u0026#34;operator.re...\u0026gt;: error calling include: template: weblogic-operator/templates/_utils.tpl:137:6: executing \u0026#34;operator.reportValidationErrors\u0026#34; at \u0026lt;fail .validationErro...\u0026gt;: error calling fail: The string property externalOperatorCert must be specified. Example:\nexternalOperatorCert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQwakNDQXJxZ0F3S ... externalOperatorKey (Deprecated) Use externalRestIdentitySecret instead\nSpecifies user supplied private key to use for the external operator REST HTTPS interface. The value must be a string containing a Base64 encoded PEM key. This parameter is required if externalRestEnabled is true, otherwise, it is ignored.\nThere is no default value.\nThe Helm installation will produce an error, similar to the following, if externalOperatorKey is not specified (left blank) and externalRestEnabled is true:\nError: render error in \u0026#34;weblogic-operator/templates/main.yaml\u0026#34;: template: weblogic-operator/templates/main.yaml:4:3: executing \u0026#34;weblogic-operator/templates/main.yaml\u0026#34; at \u0026lt;include \u0026#34;operator.va...\u0026gt;: error calling include: template: weblogic-operator/templates/_validate-inputs.tpl:53:4: executing \u0026#34;operator.validateInputs\u0026#34; at \u0026lt;include \u0026#34;operator.re...\u0026gt;: error calling include: template: weblogic-operator/templates/_utils.tpl:137:6: executing \u0026#34;operator.reportValidationErrors\u0026#34; at \u0026lt;fail .validationErro...\u0026gt;: error calling fail: The string property externalOperatorKey must be specified. Example:\nexternalOperatorKey: QmFnIEF0dHJpYnV0ZXMKICAgIGZyaWVuZGx5TmFtZTogd2VibG9naWMtb3B ... tokenReviewAuthentication If set to true, tokenReviewAuthentication specifies whether the the operator\u0026rsquo;s REST API should:\nUse Kubernetes token review API for authenticating users. Use Kubernetes subject access review API for authorizing a user\u0026rsquo;s operation (get, list, patch, and such) on a resource. Update the domain resource using the operator\u0026rsquo;s privileges. If set to false, the operator\u0026rsquo;s REST API will use the caller\u0026rsquo;s bearer token for any update to the domain resource so that it is done using the caller\u0026rsquo;s privileges.\nDefaults to false.\nExample:\ntokenReviewAuthentication: true Debugging options javaLoggingLevel Specifies the level of Java logging that should be enabled in the operator. Valid values are: SEVERE, WARNING, INFO, CONFIG, FINE, FINER, and FINEST.\nDefaults to INFO.\nExample:\njavaLoggingLevel: \u0026#34;FINE\u0026#34; NOTE: Please consult Operator logging level before changing this setting.\nremoteDebugNodePortEnabled Specifies whether or not the operator will start a Java remote debug server on the provided port and suspend execution until a remote debugger has attached.\nDefaults to false.\nExample:\nremoteDebugNodePortEnabled: true internalDebugHttpPort Specifies the port number inside the Kubernetes cluster for the operator\u0026rsquo;s Java remote debug server.\nThis parameter is required if remoteDebugNodePortEnabled is true (default false). Otherwise, it is ignored.\nDefaults to 30999.\nExample:\ninternalDebugHttpPort: 30888 externalDebugHttpPort Specifies the node port that should be allocated for the Kubernetes cluster for the operator\u0026rsquo;s Java remote debug server.\nThis parameter is required if remoteDebugNodePortEnabled is true. Otherwise, it is ignored.\nDefaults to 30999.\nExample:\nexternalDebugHttpPort: 30777 NOTE: A node port is a security risk because the port may be publicly exposed to the internet in some environments. If you need external access to the debug port, then consider using Kubernetes port forwarding instead.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/base-images/",
	"title": "WebLogic images",
	"tags": [],
	"description": "Obtain, create, and dynamically patch images for WebLogic Server or Fusion Middleware Infrastructure deployments.",
	"content": "The following sections guide you through obtaining, creating, and dynamically updating WebLogic images.\nOCR images Obtain and inspect base images for WebLogic Server or Fusion Middleware Infrastructure deployments from the Oracle Container Registry (OCR).\nCreate custom images Create custom WebLogic images using the WebLogic Image Tool (WIT) with specified patches and/or JDK version.\nPatch running domains Dynamically update or upgrade WebLogic images in a running domain.\nAccess domain images Set up Kubernetes to access domain images.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/prepare/",
	"title": "Prepare to run a domain",
	"tags": [],
	"description": "Perform these steps to prepare your Kubernetes cluster to run a WebLogic domain.",
	"content": "Perform these steps to prepare your Kubernetes cluster to run a WebLogic domain:\nCreate the domain namespace or namespaces. One or more domains can share a namespace. A single instance of the operator can manage multiple namespaces.\n$ kubectl create namespace domain-namespace-1 Replace domain-namespace-1 with name you want to use. The name must follow standard Kubernetes naming conventions, that is, lowercase, numbers, and hyphens.\nCreate a Kubernetes Secret containing the Administration Server boot credentials. You can do this manually or by using the provided sample. To create the secret manually, use this command:\n$ kubectl -n domain-namespace-1 \\ create secret generic domain1-weblogic-credentials \\ --from-literal=username=\u0026lt;the user name\u0026gt; \\ --from-literal=password=\u0026lt;the actual password\u0026gt; Replace domain-namespace-1 with the namespace that the domain will be in. Replace domain1-weblogic-credentials with the name of the secret. It is a recommended best practice to name the secret using the domain\u0026rsquo;s domainUID followed by the literal string -weblogic-credentials where domainUID is a unique identifier for the domain. Many of the samples follow this practice and use a domainUID of domain1 or sample-domain1. In the third line, enter the user name for the administrative user. In the fourth line, enter the password. Optionally, create a PV \u0026amp; PersistentVolumeClaim (PVC) which can hold the domain home, logs, and application binaries. Even if you put your domain in an image, you may want to put the logs on a persistent volume so that they are available after the pods terminate. This may be instead of, or as well as, other approaches like streaming logs into Elasticsearch.\nOptionally, configure load balancer to manage access to any WebLogic clusters.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/introduction/prerequisites/introduction/",
	"title": "Operator prerequisites",
	"tags": [],
	"description": "Review the prerequisites for the current release of the operator.",
	"content": "For the current production release 4.2.20 :\nSupport for Kubernetes 1.32.1+ with Oracle WebLogic Server 12.2.1.4 or 14.1.1.0 requires that the WebLogic container images have patch 37788099. No patch is required when using Oracle WebLogic Server 14.1.2.0.\nKubernetes 1.26.2+, 1.27.2+, 1.28.2+, 1.29.1+, 1.30.1+, 1.31.1+, 1.32.1+ (check with kubectl version).\nFlannel networking v0.26.7 or later (check with docker images | grep flannel), Calico networking v3.29.4 or later, or OpenShift SDN on OpenShift 4.3 systems.\nDocker 25.0.10+ (check with docker version) or CRI-O 1.26.2+ (check with crictl version | grep RuntimeVersion).\nHelm 3.13.2+ (check with helm version --client --short).\nFor domain home source type Model in Image, WebLogic Deploy Tooling 4.3.4+.\nOracle WebLogic Server 12.2.1.4.0, Oracle WebLogic Server 14.1.1.0.0, or Oracle WebLogic Server 14.1.2.0.0.\nNOTE:\nAs of June, 2023, Oracle WebLogic Server 12.2.1.3 is no longer supported. The last Critical Patch Updates (CPU) images for WebLogic Server 12.2.1.3 were published in April, 2023. As of December, 2022, Fusion Middleware 12.2.1.3 is no longer supported. The last CPU images for FMW Infrastructure 12.2.1.3 were published in October, 2022. Throughout the documentation, the sample images are General Availability (GA) images. GA images are suitable for demonstration and development purposes only where the environments are not available from the public Internet; they are not acceptable for production use. In production, you should always use CPU (patched) images from OCR or create your images using the WebLogic Image Tool (WIT) with the --recommendedPatches option. For more guidance, see Apply the Latest Patches and Updates in Securing a Production Environment for Oracle WebLogic Server. For details on how to obtain or create images, see WebLogic images.\nCheck the WLS version and patches using the WebLogic Image Tool inspect command: imagetool inspect --image=container-registry.oracle.com/middleware/weblogic:12.2.1.4 --patches. For more information, see Inspect images. Container images based on Oracle Linux 8 are now supported. The Oracle Container Registry hosts container images based on both Oracle Linux 7 and 8, including Oracle WebLogic Server 14.1.1.0.0 images based on Java 8 and 11 and Oracle WebLogic Server 14.1.2.0.0 images based on Java 17 and 21.\nContainer images based on Oracle Linux 9 are now supported. The Oracle Container Registry hosts container images based on Oracle Linux 9, including Oracle WebLogic Server 14.1.2.0.0 images based on Java 17 and 21.\nYou must have the cluster-admin role to install the operator. The operator does not need the cluster-admin role at runtime. For more information, see the role-based access control, operator RBAC documentation.\nWe do not currently support running WebLogic in non-Linux containers.\nSee also Supported environments for environment and licensing requirements.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/quickstart/summary/",
	"title": "Under the covers",
	"tags": [],
	"description": "",
	"content": "Here\u0026rsquo;s some insight into what\u0026rsquo;s happening under the covers during the Quick Start tutorial.\nThe Quick Start guide first installs the WebLogic Kubernetes Operator, then creates a domain using the Model in Image domain home source type.\nFor a comparison of Model in Image to other domain home source types, see Choose a domain home source type. To learn more about Model in Image domains, see the detailed Model in Image user guide. Also recommended, review a detailed Model in Image sample here. The WebLogic domain configuration is specified using the WebLogic Deployment Tool (WDT) model YAML file in a separate auxiliary image.\nThe auxiliary image contains a WebLogic domain and WebLogic application defined by using WDT model YAML and application archive files. To learn more about auxiliary images, see the user guide. If you want to step through the auxiliary image creation process, follow the instructions in the Advanced do-it-yourself section. The operator detects the domain and cluster resources, and deploys their WebLogic Server Administration Server and Managed Server pods.\nAdvanced do-it-yourself The following instructions guide you, step-by-step, through the process of creating the Quick Start auxiliary image using the WebLogic Image Tool (WIT). These steps help you understand and customize auxiliary image creation. Then you\u0026rsquo;ll see how to use that image in the domain creation.\nPrerequisites. The JAVA_HOME environment variable must be set and must reference a valid JDK 8 or 11 installation.\nDownload the latest WebLogic Deploy Tooling (WDT) and WebLogic Image Tool (WIT) installer ZIP files to a new directory; for example, use directory /tmp/quickstart/tools. Both WDT and WIT are required to create your Model in Image auxiliary images.\nFor example:\n$ mkdir -p /tmp/quickstart/tools $ cd /tmp/quickstart/tools $ curl -m 120 -fL https://github.com/oracle/weblogic-deploy-tooling/releases/latest/download/weblogic-deploy.zip \\ -o /tmp/quickstart/tools/weblogic-deploy.zip $ curl -m 120 -fL https://github.com/oracle/weblogic-image-tool/releases/latest/download/imagetool.zip \\ -o /tmp/quickstart/tools/imagetool.zip To set up the WebLogic Image Tool, run the following commands.\n$ unzip imagetool.zip $ ./imagetool/bin/imagetool.sh cache deleteEntry --key wdt_latest $ ./imagetool/bin/imagetool.sh cache addInstaller \\ --type wdt \\ --version latest \\ --path /tmp/quickstart/tools/weblogic-deploy.zip Note that the WebLogic Image Tool cache deleteEntry command does nothing if the wdt_latest key doesn\u0026rsquo;t have a corresponding cache entry. It is included because the WIT cache lookup information is stored in the $HOME/cache/.metadata file by default, and if the cache already has a version of WDT in its --type wdt --version latest location, then the cache addInstaller command will fail. For more information about the WIT cache, see the cache documentation.\nThese steps install WIT to the /tmp/quickstart/tools/imagetool directory and put a wdt_latest entry in the tool\u0026rsquo;s cache, which points to the WDT ZIP file installer. You will use WIT and its cached reference to the WDT installer later in the sample for creating model images.\nDownload the sample WDT model, web application, and properties files to be included in the auxiliary image and put them in your /tmp/quickstart/models directory. Then use the jar command to put the web application files into a model archive ZIP file.\nFor example:\n$ mkdir -p /tmp/quickstart/models/archive/wlsdeploy/applications/quickstart/WEB-INF $ curl -m 120 -fL https://raw.githubusercontent.com/oracle/weblogic-kubernetes-operator/release/4.2/kubernetes/samples/quick-start/model.yaml -o /tmp/quickstart/models/model.yaml $ curl -m 120 -fL https://raw.githubusercontent.com/oracle/weblogic-kubernetes-operator/release/4.2/kubernetes/samples/quick-start/model.properties -o /tmp/quickstart/models/model.properties $ curl -m 120 -fL https://raw.githubusercontent.com/oracle/weblogic-kubernetes-operator/release/4.2/kubernetes/samples/quick-start/archive/wlsdeploy/applications/quickstart/index.jsp -o /tmp/quickstart/models/archive/wlsdeploy/applications/quickstart/index.jsp $ curl -m 120 -fL https://raw.githubusercontent.com/oracle/weblogic-kubernetes-operator/release/4.2/kubernetes/samples/quick-start/archive/wlsdeploy/applications/quickstart/WEB-INF/web.xml -o /tmp/quickstart/models/archive/wlsdeploy/applications/quickstart/WEB-INF/web.xml $ jar cvf /tmp/quickstart/models/archive.zip -C /tmp/quickstart/models/archive/ wlsdeploy Create the auxiliary image. Follow these steps to create the auxiliary image containing WDT model YAML files, application archives, and the WDT installation files.\nUse the createAuxImage option of the WebLogic Image Tool (WIT) to create the auxiliary image.\n$ /tmp/quickstart/tools/imagetool/bin/imagetool.sh createAuxImage \\ --tag quick-start-aux-image:v1 \\ --wdtModel /tmp/quickstart/models/model.yaml \\ --wdtVariables /tmp/quickstart/models/model.properties \\ --wdtArchive /tmp/quickstart/models/archive.zip When you run this command, the Image Tool will create an auxiliary image with the specified model, variables, and archive files in the image\u0026rsquo;s /auxiliary/models directory. It will also add the latest version of the WDT installation in its /auxiliary/weblogic-deploy directory. See Create Auxiliary Image for additional Image Tool options.\nIf you have successfully created the image, then it should now be in your local machine\u0026rsquo;s Docker repository. For example:\n$ docker images quick-start-aux-image:v1 REPOSITORY TAG IMAGE ID CREATED SIZE quick-start-aux-image v1 eac9030a1f41 1 minute ago 4.04MB After the image is created, it will have the WDT executables in /auxiliary/weblogic-deploy, and WDT model, property, and archive files in /auxiliary/models. You can run ls in the Docker image to verify this.\n$ docker run -it --rm quick-start-aux-image:v1 ls -l /auxiliary total 8 drwxr-xr-x 1 oracle root 4096 Jun 1 21:53 models drwxr-xr-x 1 oracle root 4096 May 26 22:29 weblogic-deploy $ docker run -it --rm quick-start-aux-image:v1 ls -l /auxiliary/models total 16 -rw-rw-r-- 1 oracle root 1663 Jun 1 21:52 archive.zip -rw-rw-r-- 1 oracle root 173 Jun 1 21:59 model.10.properties -rw-rw-r-- 1 oracle root 1515 Jun 1 21:59 model.10.yaml $ docker run -it --rm quick-start-aux-image:v1 ls -l /auxiliary/weblogic-deploy total 28 -rw-r----- 1 oracle root 4673 Oct 22 2019 LICENSE.txt -rw-r----- 1 oracle root 30 May 25 11:40 VERSION.txt drwxr-x--- 1 oracle root 4096 May 26 22:29 bin drwxr-x--- 1 oracle root 4096 May 25 11:40 etc drwxr-x--- 1 oracle root 4096 May 25 11:40 lib drwxr-x--- 1 oracle root 4096 Jan 22 2019 samples Copy the image to all the nodes in your cluster or put it in a container registry that your cluster can access.\nCreate the domain. If you followed the previous steps to create an auxiliary image, then use these steps to create the domain.\nPrepare the domain resource.\na. Download the domain and cluster resource sample YAML file to a file called /tmp/quickstart/domain-resource.yaml or similar.\nb. If you chose a different name and tag for the auxiliary image you created, then update the image field under the spec.configuration.model.auxiliaryImages section to use that name and tag. For example, if you named the auxiliary image my-aux-image:v1, then update the spec.configuration.model.auxiliaryImages section as shown.\nauxiliaryImages: - image: \u0026#34;my-aux-image:v1\u0026#34; c. If you chose non-default values for any other fields, such as spec.image, spec.imagePullSecrets, spec.webLogicCredentialsSecret, and spec.configuration.model.runtimeEncryptionSecret, then update those fields accordingly.\nCreate the domain by applying the domain resource.\n$ kubectl apply -f /tmp/quickstart/domain-resource.yaml Delete the generated image and directories for tools and models. Use following commands to delete the generated image and directories for tools and models.\nDelete the generated image by using the docker rmi command. Use the following command to delete an image tagged with quick-start-aux-image:v1.\n$ docker rmi quick-start-aux-image:v1 Delete the directory where WebLogic Deploy Tooling and WebLogic Image Tool are installed.\n$ rm -rf /tmp/quickstart/tools/ Delete the directory where the WDT model file, archive, and variable files are copied.\n$ rm -rf /tmp/quickstart/models/ "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/major-weblogic-version-upgrade/upgrade-major/",
	"title": "Upgrade managed domains",
	"tags": [],
	"description": "Upgrade managed domains to a higher, major version.",
	"content": "This document provides guidelines for upgrading WLS and FMW/JRF infrastructure domains to a higher, major version.\nIn general, the process for upgrading WLS and FMW/JRF infrastructure domains in Kubernetes is similar to upgrading domains on premises. For a thorough understanding, we suggest that you read the Fusion Middleware Upgrade Guide.\nBefore the upgrade, you must do the following:\nIf your domain home source type is Domain on Persistent Volume (DoPV), then back up the domain home.\nIf your domain type is JRF:\nBack up the JRF database.\nBack up the OPSS wallet file, this allows you to reuse the same JRF database schemas if you need to recreate the domain.\nThe operator provides a helper script, the OPSS wallet utility, for extracting the wallet file and storing it in a Kubernetes walletFileSecret. In addition, you should save the wallet file in a safely backed-up location, outside of Kubernetes. For example, the following command saves the OPSS wallet for the sample-domain1 domain in the sample-ns namespace to a file named ewallet.p12 in the /tmp directory and also stores it in the wallet secret named sample-domain1-opss-walletfile-secret.\n$ opss-wallet.sh -n sample-ns -d sample-domain1 -s -r -wf /tmp/ewallet.p12 -ws sample-domain1-opss-walletfile-secret Make sure nothing else is accessing the database.\nDo not delete the domain resource.\nAccording to My Oracle Support Doc ID 2752458.1, if you are using an FMW/JRF domain and upgrading from 12.2.1.3 to 12.2.1.4, then before upgrading, you do not need to run the Upgrade Assistant or Reconfiguration Wizard, but we recommend moving the domain to a persistent volume. See Move MII/JRF domains to PV.\nTo upgrade WLS and FMW/JRF infrastructure domains, use the following procedure.\nShut down the domain by patching the domain and/or cluster spec serverStartPolicy to Never. For example:\n$ kubectl -n sample-domain1-ns patch domain sample-domain1 --type=json -p=\u0026#39;[ {\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Never\u0026#34;}]\u0026#39; After the shutdown completes, upgrade the base image in the domain resource YAML file and redeploy the domain.\nYou can patch the domain resource YAML file, update the base image and change serverStartPolicy to IfNeeded again, as follows:\n$ kubectl -n sample-domain1-ns patch domain sample-domain1 --type=json -p=\u0026#39;[ {\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;IfNeeded\u0026#34;}, {\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/spec/image\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;\u0026lt;New WebLogic or Fusion Middleware base image\u0026gt;\u0026#34;]\u0026#39; "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/faq/fan/",
	"title": "Disabling Fast Application Notifications",
	"tags": [],
	"description": "To support Fast Application Notifications (FAN), Oracle databases configure GRID (Oracle Grid Infrastructure). GRID is typically associated with (and required by) Oracle RAC databases but can also be used in other configurations.  Oracle Autonomous Database-Serverless (ATP-S) does not provide GRID.",
	"content": "To support Fast Application Notifications (FAN), Oracle databases configure GRID (Oracle Grid Infrastructure). GRID is typically associated with (and required by) Oracle RAC databases but can also be used in other configurations. Oracle Autonomous Database-Serverless (ATP-S) does not provide GRID.\nWhen connecting to a database that does not have GRID, the only type of WebLogic Server data source that is supported is the Generic Data Sources. Multi Data Sources and Active GridLink data sources cannot be used because they work with RAC.\nWebLogic Server 12.2.1.3.0 shipped with the 12.2.0.1 Oracle driver. When connecting with this driver to a database that does not have GRID, you will encounter the following exception (however, the 18.3 driver does not have this problem):\noracle.simplefan.impl.FanManager configure SEVERE: attempt to configure ONS in FanManager failed with oracle.ons.NoServersAvailable: Subscription time out NOTE: As of June, 2023, Oracle WebLogic Server 12.2.1.3 is no longer supported. The last Critical Patch Updates (CPU) images for WebLogic Server 12.2.1.3 were published in April, 2023.\nTo correct the problem, you must disable FAN, in one of two places:\nThrough a system property at the domain, cluster, or server level. To do this, edit the Domain Custom Resource to set the system property oracle.jdbc.fanEnabled to false as shown in the following example:\nserverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false -Doracle.jdbc.fanEnabled=false\u0026#34; - name: WLSDEPLOY_PROPERTIES value: \u0026#34;-Doracle.jdbc.fanEnabled=false\u0026#34; Configure the data source connection pool properties. The following WLST script adds the oracle.jdbc.fanEnabled property, set to false, to an existing data source.\nfmwDb = \u0026#39;jdbc:oracle:thin:@\u0026#39; + db print \u0026#39;fmwDatabase \u0026#39; + fmwDb cd(\u0026#39;/JDBCSystemResource/LocalSvcTblDataSource/JdbcResource/LocalSvcTblDataSource\u0026#39;) cd(\u0026#39;JDBCDriverParams/NO_NAME_0\u0026#39;) set(\u0026#39;DriverName\u0026#39;, \u0026#39;oracle.jdbc.OracleDriver\u0026#39;) set(\u0026#39;URL\u0026#39;, fmwDb) set(\u0026#39;PasswordEncrypted\u0026#39;, dbPassword) stbUser = dbPrefix + \u0026#39;_STB\u0026#39; cd(\u0026#39;Properties/NO_NAME_0/Property/user\u0026#39;) set(\u0026#39;Value\u0026#39;, stbUser) ls() cd(\u0026#39;../..\u0026#39;) ls() create(\u0026#39;oracle.jdbc.fanEnabled\u0026#39;,\u0026#39;Property\u0026#39;) ls() cd(\u0026#39;Property/oracle.jdbc.fanEnabled\u0026#39;) set(\u0026#39;Value\u0026#39;, \u0026#39;false\u0026#39;) ls() After changing the data source connection pool configuration, for the attribute to take effect, make sure to undeploy and redeploy the data source, or restart WebLogic Server.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-operators/namespace-management/",
	"title": "Namespace management",
	"tags": [],
	"description": "Configure or dynamically change the namespaces that a running operator manages.",
	"content": "Contents Overview Choose a domain namespace selection strategy Ensuring the operator has permission to manage a namespace Check the namespaces that a running operator manages Altering namespaces for a running operator Add a Kubernetes namespace to a running operator Delete a Kubernetes namespace from a running operator Recreate a previously deleted Kubernetes namespace with a running operator Overview An operator deployment must be configured to manage Kubernetes namespaces, and a number of Kubernetes resources must be present in a namespace before any WebLogic Server instances can be successfully started by operator. These Kubernetes resources are created either as part of the installation of the operator\u0026rsquo;s Helm chart, or are created by the operator.\nAn operator can manage all WebLogic domains in all namespaces in a Kubernetes cluster, or only manage domains in a specific subset of the namespaces, or manage only the domains that are located in the same namespace as the operator. You can change the namespaces that an operator deployment manages while the operator is still running. You can also create and prepare a namespace for the operator to manage while the operator is still running.\nFor considerations for configuring which namespaces an operator manages, see Choose a domain namespace selection strategy, For some considerations to be aware of when you manage the namespaces while the operator is running, see Altering namespaces for a running operator. For namespace management information that is external to this document, see WebLogic domain management and Common mistakes and solutions. There can be multiple operators in a Kubernetes cluster, and in that case, you must ensure that the namespaces managed by these operators do not overlap. At most, a namespace can be managed by one operator.\nChoose a domain namespace selection strategy An operator can manage domain resources in multiple namespaces, including its own namespace, but two operators cannot manage domains that are in the same namespace. The operator installation Helm chart domainNamespaceSelectionStrategy configuration setting controls which namespaces an operator manages.\nStrategy Description Example LabelSelector This is the default. The operator will manage namespaces with Kubernetes labels that match the label selector defined by your Helm chart configuration domainNamespaceLabelSelector attribute, which defaults to weblogic-operator=enabled. With the Helm chart defaults, the operator will manage namespaces that have a label named weblogic-operator when this label has the value enabled. You can label the namespace sample-domain1-ns using the command kubectl label namespace sample-domain1-ns weblogic-operator=enabled. You can define a different label selector for your operator installation using the domainNamespaceLabelSelector, such as --set \u0026quot;domainNamespaceLabelSelector=environment\\=prod\u0026quot;. For detailed syntax requirements, see domainNamespaceLabelSelector in the Configuration Reference. RegExp The operator will manage namespaces that match the regular expression set by your domainNamespaceRegExp Helm chart configuration attribute. If you want an operator to manage namespaces that start with the string prod, then use --set \u0026quot;domainNamespaceSelectionStrategy=RegExp\u0026quot; for your operator Helm configuration setting, and set the domainNamespaceRegExp Helm chart configuration attribute using --set \u0026quot;domainNamespaceRegExp=^prod\u0026quot;. Dedicated The operator will manage only domains that are in the same namespace as the operator. If the operator is deployed to namespace my-operator-ns and was installed using --set \u0026quot;domainNamespaceSelectionStrategy=Dedicated\u0026quot; in its Helm chart configuration, then the operator will manage only domains in the my-operator-ns namespace. Customers often choose this strategy if a third-party or infrastructure team manages the Kubernetes cluster and the team installing the operator, such as an applications team, will have privilege only within one namespace. For more details on this use case, see Local namespace only with cluster role binding disabled. List The operator will manage the namespaces included in the domainNamespaces operator installation Helm chart configuration value, which is a list that defaults to {default}. If you want to manage namespaces default and ns1, then in your operator installation Helm chart configuration, use --set \u0026quot;domainNamespaceSelectionStrategy=List\u0026quot; and --set \u0026quot;domainNamespaces={default,ns1}\u0026quot;. For detailed reference information about each setting, see WebLogic domain management.\nNOTES:\nYour security strategy may determine which namespace strategy you should choose, see Choose a security strategy. As has been noted previously, two operators cannot manage domains that are in the same namespace. If two operators are configured to manage the same namespace, then their behavior is undefined but it is likely that the second operator installation will deploy a FAILED Helm release and generate an error similar to Error: release op2 failed: rolebindings.rbac.authorization.k8s.io \u0026quot;weblogic-operator-rolebinding-namespace\u0026quot; already exists. For more information about ensuring an operator has permission to manage a namespace, see Ensuring the operator has permission to manage a namespace. For more information about listing, adding, deleting, or recreating the namespaces that an already running operator manages, see Altering namespaces for a running operator. For more information about common namespace management issues, see Common mistakes and solutions. Ensuring the operator has permission to manage a namespace If your operator Helm enableClusterRoleBinding configuration value is true, then the operator has permission to manage any namespace and can automatically manage a namespace that is added after the operator was last installed or upgraded.\nIf your operator Helm enableClusterRoleBinding configuration value is false, then:\nThe operator Helm chart will create RoleBindings in each namespace that matches your domain namespace selection criteria during a call to helm install or helm upgrade. These RoleBindings give the operator\u0026rsquo;s service account the necessary privileges in the namespace.\nThe Helm chart will only create these RoleBindings in namespaces that match the operator\u0026rsquo;s domain namespace selection criteria at the time the chart is installed or upgraded.\nIf you later create namespaces that match a List, LabelSelector, or RegExp selector, then the operator will not have privilege in these namespaces until you upgrade the Helm release. You can resolve this issue by performing a helm upgrade on an already installed operator Helm release.\nFor example, with an operator release name weblogic-operator:\n$ helm upgrade weblogic-operator/weblogic-operator --reuse-values NOTE: If you still run into problems after you perform the helm upgrade to re-initialize a namespace that is deleted and recreated, then you can try forcing the operator to restart.\nFor a detailed description of the operator\u0026rsquo;s security related resources, see the operator\u0026rsquo;s role-based access control (RBAC) requirements which are documented here.\nCheck the namespaces that a running operator manages Prior to version 3.1.0, the operator supported specifying the namespaces that it would manage only through a list. Now, the operator supports a list of namespaces, a label selector, or a regular expression matching namespace names.\nWhen using a namespace list\nFor operators that specify namespaces by a list, you can find the list of the namespaces using the helm get values command. For example, the following command shows all the values of the operator release weblogic-operator; the domainNamespaces list contains default and ns1:\n$ helm get values weblogic-operator domainNamespaces: - default - ns1 elasticSearchHost: elasticsearch.default.svc.cluster.local elasticSearchPort: 9200 elkIntegrationEnabled: false externalDebugHttpPort: 30999 externalRestEnabled: false externalRestHttpsPort: 31001 image: ghcr.io/oracle/weblogic-kubernetes-operator:4.2.20 imagePullPolicy: IfNotPresent internalDebugHttpPort: 30999 javaLoggingLevel: INFO logStashImage: logstash:6.6.0 remoteDebugNodePortEnabled: false serviceAccount: default suspendOnDebugStartup: false If you don\u0026rsquo;t know the release name of the operator, you can use helm list to list all the releases for a specified namespace or all namespaces:\n$ helm list --namespace \u0026lt;namespace\u0026gt; $ helm list --all-namespaces When using a label selector\nFor operators that select namespaces with a selector, simply list namespaces using that selector:\n$ kubectl get ns --selector=\u0026#34;weblogic-operator=enabled\u0026#34; You can see the labels on all namespaces by calling:\n$ kubectl get ns --show-labels When using regular expression matching\nFor operators that select namespaces with a regular expression matching the name, you can use a combination of kubectl and any command-line tool that can process the regular expression, such as grep:\n$ kubectl get ns -o go-template=\u0026#39;{{range .items}}{{.metadata.name}}{{\u0026#34;\\n\u0026#34;}}{{end}}\u0026#39; | grep \u0026#34;^weblogic\u0026#34; Altering namespaces for a running operator This section describes the steps for adding, deleting, or recreated namespaces that are managed by a running operator.\nAdd a Kubernetes namespace to a running operator The following are steps for adding namespaces that are managed by a running operator.\nWhen using a namespace list\nWhen the operator is configured to manage a list of namespaces and you want the operator to manage an additional namespace, you need to add the namespace to the operator\u0026rsquo;s domainNamespaces list. Note that this namespace has to already exist, for example, using the kubectl create command.\nAdding a namespace to the domainNamespaces list tells the operator to initialize the necessary Kubernetes resources so that the operator is ready to manage WebLogic Server instances in that namespace.\nWhen the operator is managing the default namespace, the following example Helm command adds the namespace ns1 to the domainNamespaces list, where weblogic-operator is the release name of the operator:\n$ helm upgrade \\ weblogic-operator/weblogic-operator \\ --reuse-values \\ --set \u0026#34;domainNamespaces={default,ns1}\u0026#34; \\ --wait You can verify that the operator has initialized a namespace by confirming the existence of the required configmap resource.\n$ kubetctl get cm -n \u0026lt;namespace\u0026gt; For example, the following example shows that the domain configmap resource exists in the namespace ns1.\n$ kubectl get cm -n ns1 NAME DATA AGE weblogic-scripts-cm 14 12m When using a label selector or regular expression\nFor operators configured to select managed namespaces through the use of a label selector or regular expression, you need to create a namespace with the appropriate labels or with a name that matches the expression, respectively.\nIf your operator Helm enableClusterRoleBinding configuration value is false, then a running operator will not have privilege to manage the newly added namespace until you upgrade the operator\u0026rsquo;s Helm release. See Ensuring the operator has permission to manage a namespace.\nDelete a Kubernetes namespace from a running operator The following are steps for deleting namespaces that are managed by a running operator.\nWhen using a namespace list\nWhen the operator is configured to manage a list of namespaces and you no longer want a namespace to be managed by the operator, you need to:\nFirst, remove the namespace from the operator\u0026rsquo;s domainNamespaces list. Then, delete the namespace. While the operator is running and managing the default and ns1 namespaces, the following example Helm command removes the namespace ns1 from the domainNamespaces list, where weblogic-operator is the release name of the operator:\n$ helm upgrade \\ weblogic-operator/weblogic-operator \\ --reuse-values \\ --set \u0026#34;domainNamespaces={default}\u0026#34; \\ --wait When using a label selector or regular expression\nFor operators configured to select managed namespaces through the use of a label selector or regular expression, you need to delete the namespace. For the label selector option, you can also adjust the labels on the namespace so that the namespace no longer matches the selector.\nRecreate a previously deleted Kubernetes namespace with a running operator The following are steps for recreating previously deleted namespaces that are managed by a running operator.\nWhen using a namespace list\nIf you deleted a namespace (and the resources in it) and then want to recreate it:\nFirst, add back (recreate) the namespace, for example, using the kubectl create command. Then, add the namespace back to the domainNamespaces list using the helm upgrade command illustrated previously. If a domain custom resource is created before the namespace is ready, you might see that the introspector job pod fails to start, with a warning like the following, when you review the description of the introspector pod. Note that domain1 is the name of the domain in the following example output.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned domain1-introspector-bz6rw to slc16ffk Normal SuccessfulMountVolume 1m kubelet, slc16ffk MountVolume.SetUp succeeded for volume \u0026#34;weblogic-credentials-volume\u0026#34; Normal SuccessfulMountVolume 1m kubelet, slc16ffk MountVolume.SetUp succeeded for volume \u0026#34;default-token-jzblm\u0026#34; Warning FailedMount 27s (x8 over 1m) kubelet, slc16ffk MountVolume.SetUp failed for volume \u0026#34;weblogic-scripts-cm-volume\u0026#34; : configmaps \u0026#34;weblogic-scripts-cm\u0026#34; not found If you still run into problems after you perform the helm upgrade to re-initialize a namespace that was deleted and recreated, then you can try forcing the operator to restart.\nWhen using a label selector or regular expression\nIf your operator Helm enableClusterRoleBinding configuration value is false, then a running operator will not have privilege to manage the newly recreated namespace until you upgrade the operator\u0026rsquo;s Helm release. See Ensuring the operator has permission to manage a namespace.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/accessing-the-domain/istio/istio/",
	"title": "Istio support",
	"tags": [],
	"description": "Run the operator and WebLogic domains managed by the operator when Istio sidecar injection is enabled.",
	"content": "Contents Overview Limitations Determining the Istio version Setting up an operator with Istio support Creating a domain with Istio support Configuring the domain resource Applying a Domain YAML file Exposing applications in Istio-enabled domains Traffic management Distributed tracing Automatically added network channels Network channel for Istio versions v1.10 and later Network channel for WebLogic EJB and servlet session state replication traffic Security Mutual TLS Authorization policy Destination rule Ingress gateway Overview These instructions assume that you are using a Kubernetes cluster with Istio installed and configured already. The operator will not install Istio for you.\nIstio support lets you run the operator, and WebLogic domains managed by the operator, when Istio sidecar injection is enabled. You can use Istio gateways and virtual services to access applications deployed in these domains. If your applications have suitable tracing code in them, then you will also be able to use distributed tracing, such as Jaeger, to trace requests across domains and to other components and services that have tracing enabled.\nWebLogic Kubernetes Operator assumes that you are familiar with Istio. If you are new to Istio, we strongly recommend reading the documentation and working through the Bookinfo sample application to familiarize yourself with the mesh and verify that it is working properly in your environment, before proceeding to work with the operator.\nTo learn more about Istio, see What is Istio.\nLimitations The current support for Istio has these limitations:\nThe operator supports Istio versions 1.10 and later, and has been tested with single and multicluster Istio installations from 1.10 up to 1.13.2.\nWhen using Istio in Red Hat OpenShift, operator version 4.0 and later is compatible only with Red Hat OpenShift version 4.11.x with the latest Service Mesh 2.2.3 and later.\nYou cannot set up a NodePort using domain.spec.adminServer.adminService.channels with a channelName of default, default-secure, and default-admin. Any attempt will result in an error when deploying a domain in combination with Istio.\nIf the istio-ingressgateway service in your environment does not have an EXTERNAL-IP defined and you would like to externally run WLST commands, then see Use WLST.\nDetermining the Istio version To see the Istio build version that is installed, use the istioctl version command. For example:\n$ istioctl version client version: 1.11.1 control plane version: 1.11.1 data plane version: 1.11.1 (1 proxies) Setting up an operator with Istio support Istio support requires labeling the operator namespace and your domain namespaces to enable Istio automatic sidecar injection. In this section, we describe the steps for the operator namespace; we will describe the steps for the domain in later sections.\nBefore installing the operator, create the namespace in which you want to run the operator and label it.\n$ kubectl create namespace weblogic-operator Label the namespace as follows:\n$ kubectl label namespace weblogic-operator istio-injection=enabled After the namespace is labeled, you can install the operator.\nWhen the operator pod starts, you will notice that Istio automatically injects an initContainer called istio-init and the Envoy container istio-proxy.\nYou can validate this using the following commands:\n$ kubectl --namespace weblogic-operator get pods $ kubectl --namespace weblogic-operator get pod weblogic-operator-xxx-xxx -o yaml In the second command, change weblogic-operator-xxx-xxx to the name of your pod.\nCreating a domain with Istio support Setting up Istio support for a domain requires only enabling Istio automatic sidecar injection.\nTo allow your domains to run with Istio automatic sidecar injection enabled, create the namespace in which you want to run the domain and label it for automatic injection before deploying your domain.\n$ kubectl create namespace sample-domain1 $ kubectl label namespace sample-domain1 istio-injection=enabled Configuring the domain resource Beginning with WebLogic Kubernetes Operator release 4.0, you no longer have to provide the domain.spec.configuration.istio section to enable Istio support for a domain. The domain.spec.configuration.istio is no longer a valid field in the schema.\nApplying a Domain YAML file Apply a Domain YAML file by:\n$ kubectl apply -f domain.yaml After all the servers are up, you will see output like this:\n$ kubectl -n sample-domain1-ns get pods NAME READY STATUS RESTARTS AGE sample-domain1-admin-server 2/2 Running 0 154m sample-domain1-managed-server1 2/2 Running 0 153m sample-domain1-managed-server2 2/2 Running 0 153m If you use istioctl proxy-status, you will see the mesh status:\n$ istioctl proxy-status NAME CDS LDS EDS RDS PILOT VERSION istio-ingressgateway-5c7d8d7b5d-tjgtd.istio-system SYNCED SYNCED SYNCED NOT SENT istio-pilot-6cfcdb75dd-87lqm 1.5.4 sample-domain1-admin-server.sample-domain1-ns SYNCED SYNCED SYNCED SYNCED istio-pilot-6cfcdb75dd-87lqm 1.5.4 sample-domain1-managed-server1.sample-domain1-ns SYNCED SYNCED SYNCED SYNCED istio-pilot-6cfcdb75dd-87lqm 1.5.4 sample-domain1-managed-server2.sample-domain1-ns SYNCED SYNCED SYNCED SYNCED istio-pilot-6cfcdb75dd-87lqm 1.5.4 weblogic-operator-7d86fffbdd-5dxzt.weblogic-operator-ns SYNCED SYNCED SYNCED SYNCED istio-pilot-6cfcdb75dd-87lqm 1.5.4 Exposing applications in Istio-enabled domains When a domain is running with Istio support, you should use the Istio ingress gateway to provide external access to applications, instead of using an ingress controller like Traefik. Using the Istio ingress gateway, you can also view the traffic in Kiali and use distributed tracing from the entry point to the cluster.\nTo configure external access to your domain, you need to create an Istio Gateway and VirtualService, as shown in the following example:\n--- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: domain1-gateway namespace: sample-domain1 spec: selector: istio: ingressgateway servers: - hosts: - \u0026#39;yourdomain.dns.com\u0026#39; port: name: http number: 80 protocol: HTTP --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: domain1-virtualservice namespace: sample-domain1 spec: gateways: - domain1-gateway hosts: - \u0026#39;yourdomain.dns.com\u0026#39; http: - match: - uri: prefix: /console - port: 7001 route: - destination: host: sample-domain1-admin-server.sample-domain1-ns.svc.cluster.local port: number: 7001 - match: - uri: prefix: /testwebapp - port: 8001 route: - destination: host: sample-domain1-cluster-cluster-1.domain1.svc.cluster.local port: number: 8001 This example creates a gateway that will accept requests with any host name using HTTP on port 80, and a virtual service that will route all of those requests to the cluster service for cluster-1 in domain1 in the namespace domain1. NOTE: In a production environment, hosts should be limited to the proper DNS name.\nAfter the gateway and virtual service has been set up, you can access it through your ingress host and port. Refer to Determining the ingress IP and ports.\nFor more information about providing ingress using Istio, see the Istio documentation.\nTraffic management Istio provides traffic management capabilities, including the ability to visualize traffic in Kiali. You do not need to change your applications to use this feature. The Istio proxy (Envoy) sidecar that is injected into your pods provides it. The following image shows an example with traffic flowing: in from the Istio gateway on the left, to a domain called domain1.\nIn this example, you can see how the traffic flows to the cluster services and then to the individual Managed Servers.\nTo learn more, see Istio traffic management.\nDistributed tracing Istio provides distributed tracing capabilities, including the ability to view traces in Jaeger. To use distributed tracing though, first you will need to instrument your WebLogic application, for example, using the Jaeger Java client. The following image shows an example of a distributed trace that shows a transaction following the same path through the system as shown in the previous image.\nTo learn more, see distrubting tracing in Istio.\nAutomatically added network channels The operator will automatically add network channels to each WebLogic Server when Istio is enabled for a domain.\nNetwork channel for Istio versions v1.10 and later Background:\nBeginning with Istio version 1.10, Istio\u0026rsquo;s networking behavior was simplified. It changed so that the Istio network proxy that runs in each Istio sidecar (the Envoy proxy) no longer redirects network traffic to the current pod\u0026rsquo;s localhost interface, but instead directly forwards it to the network interface associated with the pod\u0026rsquo;s IP address. This means that the operator does not need to create additional localhost network channels on each WebLogic pod except to enable the readiness probe.\nTo learn more about changes to Istio networking beginning with Istio 1.10, see Upcoming networking changes in Istio 1.10.\nChannel behavior:\nWhen deploying a domain that is configured to support Istio versions 1.10 and later, the operator automatically adds an HTTP protocol network channel (also known as Network Access Points) to your WebLogic configuration for each server so that the pod\u0026rsquo;s readiness probe is bound to the server pod\u0026rsquo;s network interface:\nChannel Name Port Listen address Protocol Exposed as a container port http-probe-ext From configuration Istio readinessPort Server Pod\u0026rsquo;s IP address http No Network channel for WebLogic EJB and servlet session state replication traffic WebLogic Kubernetes Operator versions 4.0 and later support WebLogic EJB and servlet session state replication traffic in an Istio service mesh; it uses the default channel of the domain for replication.\nSecurity Istio provides rich sets of security features that you can use to secure the Istio service mesh environments. For details, see Istio Security. The following are some sample scenarios.\nMutual TLS By default, all traffic between the Istio sidecar proxies use mutual TLS within the mesh. However, service within the mesh can still be accessed by other pods outside the mesh. For example, you have domain-1 deployed with sidecar injection, therefore within the mesh, and another domain, domain-2, deployed without sidecar injection, therefore outside of the mesh. Services within domain-2 can still access the services within domain-1, however the traffic will be Plain unencrypted traffic. This is because by default, Istio configures the traffic using the PERMISSIVE mode, which means it can accept both Plain and mutual TLS traffic. You can restrict this behavior by allowing only mutual TLS traffic by locking down the entire mesh or by namespace within the mesh.\nFor locking down the entire mesh, you can:\nkubectl apply -n istio-system -f - \u0026lt;\u0026lt;EOF apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: \u0026#34;default\u0026#34; spec: mtls: mode: STRICT EOF For namespace only, you can:\nkubectl apply -n \u0026lt;your namespace\u0026gt; -f - \u0026lt;\u0026lt;EOF apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: \u0026#34;default\u0026#34; spec: mtls: mode: STRICT EOF See Istio Mutual TLS Migration.\nAuthorization policy Istio provides policy-based authorization using AuthorizationPolicy. You can set up policies to deny or allow access to services deployed in the mesh. For example, if you want to limit access to a particular service in the domain from another namespace only with a service account.\nCreate a service account for the client namespace.\nkubectl -n domain2-ns create serviceaccount privaccess Set up the service account in the client deployment pod. For example, if it is another WebLogic Domain in the Operator, specify the ServiceAccountName in the domain.spec.serverPod.\nspec: serverPod: serviceAccountName: privaccess Create an AuthorizationPolicy for the target service.\napiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: privaccess namespace: sample-domain1-ns spec: action: ALLOW rules: - from: - source: principals: [\u0026#34;cluster.local/ns/domain2-ns/sa/privaccess\u0026#34;] to: - operation: methods: [\u0026#34;GET\u0026#34;] paths: [\u0026#34;/domain1-priv-service\u0026#34;] See Istio Authorization Policy.\nDestination rule Istio allows you to define traffic management polices applied to the service after the routing has occurred. You can use it to control load balancing, connection pool size from the sidecar, and outlier detection settings to detect and evict unhealthy hosts from the load balancing pool. You can also set up a service-level mutual TLS requirement instead of entire mesh or namespace-based.\nFor example, to configure service-level mutual TLS:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: sample-domain1-service spec: host: sample-domain1-cluster-cluster-1.sample-domain1-ns.svc.cluster.local trafficPolicy: tls: mode: ISTIO_MUTUAL For example, to configure a sticky session for a service using hashing-based hash key user_cookie:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: sample-domain1-service spec: host: sample-domain1-cluster-cluster-1.sample-domain1-ns.svc.cluster.local trafficPolicy: loadBalancer: consistentHash: httpCookie: name: user_cookie ttl: 0s See Istio Destination Rule.\nIngress gateway Ingress gateway provides similar functions to Kubernetes Ingress but with more advanced functionality.\nI. For example, to configure an Ingress gateway for SSL termination at the gateway:\nCreate a TLS certificate and secret. $ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls1.key -out /tmp/tls1.crt -subj \u0026#34;/CN=secure-domain.org\u0026#34; $ kubectl -n istio-system create secret tls domain1-tls-cert --key /tmp/tls1.key --cert /tmp/tls1.crt Create the Ingress gateway. apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: sample-domain1-gateway namespace: sample-domain1-ns spec: selector: istio: ingressgateway servers: - port: number: 443 name: https protocol: HTTPS tls: mode: SIMPLE credentialName: domain1-tls-cert hosts: - \u0026#39;secure-domain.org\u0026#39; - port: number: 80 name: http protocol: HTTP hosts: - \u0026#39;regular-domain.org\u0026#39; If you are accessing the WebLogic Console through a secure gateway with SSL termination at the gateway level, enable WeblogicPluginEnabled in the WebLogic domain and add the appropriate request headers. For example,\nIf you are using WDT, add the resources section in the model YAML file.\nresources: WebAppContainer: WeblogicPluginEnabled: true If you are using WLST, set the WeblogicPluginEnabled for each server and cluster\nset(\u0026#39;WeblogicPluginEnabled\u0026#39;,true) Set the request headers in the virtual service: (Use kubectl explain virtualservice.spec.http.route.headers for help)\nheaders: request: remove: [\u0026#39;WL-Proxy-Client-IP\u0026#39;, \u0026#39;WL-Proxy-SSL\u0026#39;] set: X-Forwarded-Proto: https WL-Proxy-SSL: \u0026#39;true\u0026#39; II. For example, to configure an Ingress gateway for SSL passthrough:\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: sample-domain1-gateway namespace: sample-domain1-ns spec: selector: istio: ingressgateway servers: - port: number: 443 name: https protocol: HTTPS tls: mode: PASSTHROUGH hosts: - \u0026#39;secure-domain.org\u0026#39; - port: number: 80 name: http protocol: HTTP hosts: - \u0026#39;regular-domain.org\u0026#39; The virtual service will then configure to match the tls rule.\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: sample-domain1-virtualservice namespace: sample-domain1-ns spec: gateways: - sample-domain1-gateway hosts: - secure-domain.org tls: - match: - port: 443 sniHosts: - secure-domain.org route: - destination: host: sample-domain1-admin-server port: number: 9002 See Istio Ingress.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/cicd/tools/",
	"title": "Tools",
	"tags": [],
	"description": "Tools that are available to build CI/CD pipelines.",
	"content": "WebLogic Deploy Tooling (WDT) You can use several of the WDT tools in a CI/CD pipeline. For example, the createDomain tool creates a new domain based on a simple model, and updateDomain (and deployApps) uses the same model concept to update an existing domain (preserving the same domain encryption key). The deployApps tool is very similar to the updateDomain tool, but limits what can be updated to application-related configuration attributes such as data sources and application archives. The model used by these tools is a sparse set of attributes needed to create or update the domain. A model can be as sparse as providing only the WebLogic Server administrative password, although not very interesting. A good way to get a jumpstart on a model is to use the discoverDomain tool in WDT which builds a model based on an existing domain.\nA Model in Image domain takes advantage of WDT by letting you specify an operator domain directly with a model instead of requiring that you supply a domain home.\nOther than the tools themselves, there are three components to the WDT tools:\nThe Domain Model - Metadata model describing the desired domain.\nThe metadata domain model can be written in YAML or JSON and is documented here. The Archive ZIP - Binaries to supplement the model.\nAll binaries needed to supplement the model must be specified in an archive file, which is just a ZIP file with a specific directory structure. Optionally, the model can be stored inside the ZIP file, if desired. Any binaries not already on the target system must be in the ZIP file so that the tooling can extract them in the target domain. The Properties File - A standard Java properties file.\nA property file used to provide values to placeholders in the model. WDT Create Domain Samples (Kubernetes) A Model in Image sample for supplying an image that contains a WDT model only, instead of a domain home. In this case, the operator generates the domain home for you at runtime. WebLogic Scripting Tool (WLST) You can use WLST scripts to create and update domain homes in a CI/CD pipeline for Domain in Image and Domain on PV domains. We recommend that you use offline WLST for this purpose. There may be some scenarios where it is necessary to use WLST online, but we recommend that you do that as an exception only, and when absolutely necessary.\nIf you do not already have WLST scripts, we recommend that you consider using WebLogic Deploy Tooling (WDT) instead. It provides a more declarative approach to domain creation, whereas WLST is more of an imperative scripting language. WDT provides advantages like being able to use the same model with different versions of WebLogic, whereas you may need to update WLST scripts manually when migrating to a new version of WebLogic for example.\nWebLogic pack and unpack tools WebLogic Server provides tools called \u0026ldquo;pack\u0026rdquo; and \u0026ldquo;unpack\u0026rdquo; that can be used to \u0026ldquo;clone\u0026rdquo; a domain home. These tools do not preserve the domain encryption key. You can use these tools to make copies of Domain on PV and Domain in Image domain homes in scenarios when you do not need the same domain encryption key. See Creating Templates and Domains Using the Pack and Unpack Commands.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/domain-lifecycle/liveness-readiness-probe-customization/",
	"title": "Liveness and readiness probes customization",
	"tags": [],
	"description": "This document describes how to customize the liveness and readiness probes for WebLogic Server instance Pods.",
	"content": "This document describes how to customize the liveness and readiness probes for WebLogic Server instance Pods.\nContents Liveness probe customization Automatic restart of failed server instances by Node Manager Readiness probe customization Liveness probe customization The liveness probe is configured to check that a server is alive by querying the Node Manager process. By default, the liveness probe is configured to check liveness every 45 seconds, to timeout after 5 seconds, and to perform the first check after 30 seconds. The default success and failure threshold values are 1. If a pod fails the liveness probe, Kubernetes will restart that container.\nYou can customize the liveness probe initial delay, interval, timeout, and failure threshold using the livenessProbe attribute under the serverPod element of the domain or cluster resource.\nFollowing is an example configuration to change the liveness probe interval, timeout, and failure threshold value.\nserverPod: livenessProbe: periodSeconds: 30 timeoutSeconds: 10 failureThreshold: 3 NOTE: The liveness probe success threshold value must always be 1. See Configure Probes in the Kubernetes documentation for more details.\nAfter the liveness probe script (livenessProbe.sh) performs its normal checks, you can customize the liveness probe by specifying a custom script, which will be invoked by livenessProbe.sh. You can specify the custom script either by using the livenessProbeCustomScript attribute in the domain resource, or by setting the LIVENESS_PROBE_CUSTOM_SCRIPT environment variable using the env attribute under the serverPod element (see the following configuration examples). If the custom script fails with a non-zero exit status, the liveness probe will fail and Kubernetes will restart the container.\nThe spec.livenessProbeCustomScript domain resource attribute affects all WebLogic Server instance Pods in the domain. The LIVENESS_PROBE_CUSTOM_SCRIPT environment variable takes precedence over the spec.livenessProbeCustomScript domain resource attribute when both are configured, and, like all domain resource environment variables, can be customized on a per domain, per cluster, or even a per server basis. Changes to either the domain resource attribute or the environment variable on a running domain take effect on running WebLogic Server instance Pods when such Pods are restarted (rolled). NOTE: The liveness probe custom script option is for advanced usage only and its value is not set by default. If the specified script is not found, then the custom script is ignored and the existing liveness script will perform its normal checks.\nNOTE: Oracle recommends against having any long running calls (for example, any network calls or executing wlst.sh) in the liveness probe custom script.\nUse the following configuration to specify a liveness probe custom script using the livenessProbeCustomScript domain resource field.\nspec: livenessProbeCustomScript: /u01/customLivenessProbe.sh Use the following configuration to specify the liveness probe custom script using the LIVENESS_PROBE_CUSTOM_SCRIPT environment variable.\nserverPod: env: - name: LIVENESS_PROBE_CUSTOM_SCRIPT value: /u01/customLivenessProbe.sh The following operator-populated environment variables are available for use in the liveness probe custom script, which will be invoked by livenessProbe.sh.\nORACLE_HOME or MW_HOME: The Oracle Fusion Middleware software location as a file system path within the container.\nWL_HOME: The Weblogic Server installation location as a file system path within the container.\nDOMAIN_HOME: The domain home location as a file system path within the container.\nJAVA_HOME: The Java software installation location as a file system path within the container.\nDOMAIN_NAME: The WebLogic Server domain name.\nDOMAIN_UID: The domain unique identifier.\nSERVER_NAME: The WebLogic Server instance name.\nLOG_HOME: The WebLogic log location as a file system path within the container. This variable is available only if its value is set in the configuration.\nNOTES:\nAdditional operator-populated environment variables that are not listed, are not supported for use in the liveness probe custom script.\nThe custom liveness probe script can call source $DOMAIN_HOME/bin/setDomainEnv.sh if it needs to set up its PATH or CLASSPATH to access WebLogic utilities in its domain.\nA custom liveness probe must not fail (exit non-zero) when the WebLogic Server instance itself is unavailable. This could be the case when the WebLogic Server instance is booting or about to boot.\nAutomatic restart of failed server instances by Node Manager WebLogic Server provides a self-health monitoring feature to improve the reliability and availability of server instances in a domain. If an individual subsystem determines that it can no longer operate consistently and reliably, it registers its health state as FAILED with the host server. Each WebLogic Server instance, in turn, checks the health state of its registered subsystems to determine its overall viability. If one or more of its critical subsystems have reached the FAILED state, the server instance marks its health state as FAILED to indicate that it cannot reliably host an application.\nUsing Node Manager, server self-health monitoring enables the automatic restart of the failed server instances. The operator configures the Node Manager to restart the failed server a maximum of two times within a one-hour interval. It does this by setting the value of the RestartMax property (in the server startup properties file) to 2 and the value of the RestartInterval property to 3600. You can change the number of times the Node Manager will attempt to restart the server in a given interval by setting the RESTART_MAX and RESTART_INTERVAL environment variables in the domain resource using the env attribute under the serverPod element.\nUse the following configuration to specify the number of times the Node Manager can attempt to restart the server within a given interval using the RESTART_MAX and RESTART_INTERVAL environment variables.\nserverPod: env: - name: RESTART_MAX value: \u0026#34;4\u0026#34; - name: RESTART_INTERVAL value: \u0026#34;3600\u0026#34; If the Node Manager can\u0026rsquo;t restart the failed server and marks the server state as FAILED_NOT_RESTARTABLE, then the liveness probe will fail and the WebLogic Server container will be restarted. You can set the RESTART_MAX environment variable value to 0 to prevent the Node Manager from restarting the failed server and allow the liveness probe to fail immediately.\nSee Server Startup Properties for more details.\nReadiness probe customization Here are the options for customizing the readiness probe and its tuning:\nBy default, the readiness probe is configured to use the WebLogic Server ReadyApp framework. The ReadyApp framework allows fine customization of the readiness probe by the application\u0026rsquo;s participation in the framework. For more details, see Using the ReadyApp Framework. The readiness probe is used to determine if the server is ready to accept user requests. The readiness is used to determine when a server should be included in a load balancer\u0026rsquo;s endpoints, in the case of a rolling restart, when a restarted server is fully started, and for various other purposes.\nBy default, the readiness probe is configured to check readiness every 5 seconds, to timeout after 5 seconds, and to perform the first check after 30 seconds. The default success and failure thresholds values are 1. You can customize the readiness probe initial delay, interval, timeout, success and failure thresholds using the readinessProbe attribute under the serverPod element of the domain resource.\nFollowing is an example configuration to change readiness probe interval, timeout and failure threshold value.\nserverPod: readinessProbe: periodSeconds: 10 timeoutSeconds: 10 failureThreshold: 3 You can use domain resource configuration to customize the amount of time the operator will wait for a WebLogic Server pod to become ready before it forces the pod to restart. The default is 30 minutes. See the maxReadyWaitTimeSeconds attribute on domain.spec.serverPod (which applies to all pods in the domain), domain.spec.adminServer.serverPod, domain.spec.managedServers[*].serverPod, or cluster.spec.serverPod.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/major-weblogic-version-upgrade/",
	"title": "Upgrade managed domains",
	"tags": [],
	"description": "Upgrade managed domains, general and specific guidelines.",
	"content": " Upgrade managed domains Upgrade managed domains to a higher, major version.\nUpgrade managed domains to v14.1.2.0 Upgrade managed domains to v14.1.2.0.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/security/secrets/",
	"title": "Secrets",
	"tags": [],
	"description": "Kubernetes Secrets for the operator.",
	"content": "Contents Domain credentials secret Domain image pull secret Domain configuration override or runtime update secrets Operator image pull secret Operator external REST interface secret Operator internal REST interface secret Domain credentials secret The credentials for the WebLogic domain are kept in a Kubernetes Secret where the name of the secret is specified using webLogicCredentialsSecret in the WebLogic Domain resource. Also, the domain credentials secret must be created in the namespace where the Domain will be running.\nFor an example of a WebLogic Domain YAML file using webLogicCredentialsSecret, see Container Image Protection.\nThe samples supplied with the operator use a naming convention that follows the pattern \u0026lt;domainUID\u0026gt;-weblogic-credentials, where \u0026lt;domainUID\u0026gt; is the unique identifier of the domain, for example, domain1-weblogic-credentials.\nIf the WebLogic domain will be started in domain1-ns and the \u0026lt;domainUID\u0026gt; is domain1, an example of creating a Kubernetes generic secret is as follows:\n$ kubectl -n domain1-ns create secret generic domain1-weblogic-credentials \\ --from-file=username --from-file=password $ kubectl -n domain1-ns label secret domain1-weblogic-credentials \\ weblogic.domainUID=domain1 weblogic.domainName=domain1 Oracle recommends that you not include unencrypted passwords on command lines. Passwords and other sensitive data can be prompted for or looked up by shell scripts or tooling. For more information about creating Kubernetes Secrets, see the Kubernetes Secrets documentation.\nThe operator\u0026rsquo;s introspector job will expect the secret key names to be:\nusername password For example, here is the result when describing the Kubernetes Secret:\n$ kubectl -n domain1-ns describe secret domain1-weblogic-credentials Name: domain1-weblogic-credentials Namespace: domain1-ns Labels: weblogic.domainName=domain1 weblogic.domainUID=domain1 Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== password: 8 bytes username: 8 bytes Domain image pull secret The WebLogic domain that the operator manages can have images that are protected in the registry. The imagePullSecrets setting on the Domain can be used to specify the Kubernetes Secret that holds the registry credentials.\nFor more information, see Container Image Protection.\nDomain configuration override or runtime update secrets The operator supports embedding macros within configuration override templates and Model in Image model files that reference Kubernetes Secrets. These Kubernetes Secrets can be created with any name in the namespace where a domain will be running. The Kubernetes Secret names are specified using configuration.secrets in the WebLogic Domain resource.\nFor more information, see Configuration overrides and Runtime updates.\nOperator image pull secret The Helm chart for installing the operator has an imagePullSecrets option to specify the image pull secret used for the operator\u0026rsquo;s image when using a private registry; alternatively, the image pull secret can be specified on the operator\u0026rsquo;s service account.\nFor more information, see Customizing operator image name, pull secret, and private registry.\nOperator external REST interface secret The operator can expose an external REST HTTPS interface which can be accessed from outside the Kubernetes cluster. A Kubernetes tls secret is used to hold the certificates and private key.\nFor more information, see REST Services.\nOperator internal REST interface secret The operator exposes an internal REST HTTPS interface with a self-signed certificate. The certificate is kept in a Kubernetes ConfigMap with the name weblogic-operator-cm using the key internalOperatorCert. The private key is kept in a Kubernetes Secret with the name weblogic-operator-secrets using the key internalOperatorKey. These Kubernetes objects are managed by the operator\u0026rsquo;s Helm chart and are part of the namespace where the operator is installed.\nFor example, to see all the operator\u0026rsquo;s ConfigMaps and secrets when installed into the Kubernetes Namespace weblogic-operator-ns, use:\n$ kubectl -n weblogic-operator-ns get cm,secret "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/elastic-stack/",
	"title": "Elastic Stack",
	"tags": [],
	"description": "",
	"content": " Operator Sample for configuring the Elasticsearch and Kibana deployments and services for the operator\u0026#39;s logs.\nWebLogic domain Sample for using Fluentd for WebLogic domain and operator\u0026#39;s logs.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/model-in-image/update4/",
	"title": "Update 4",
	"tags": [],
	"description": "",
	"content": "This use case demonstrates dynamically changing the Work Manager threads constraint and data source configuration in your running domain without restarting (rolling) running WebLogic Servers. This use case requires that the Update 1 use case has been run and expects that its sample-domain1 domain is deployed and running.\nIn the use case, you will:\nUpdate the ConfigMap containing the WDT model created in the Update 1 use case with changes to the Work Manager threads constraint configuration. Update the data source secret created in the Update 1 use case to provide the correct password and an increased maximum pool capacity. Update the Domain YAML file to enable the Model in Image online update feature. Update the Domain YAML file to trigger a domain introspection, which applies the new configuration values without restarting servers. Optionally, start a database (to demonstrate that the updated data source attributes have taken effect). Here are the steps:\nMake sure that you have deployed the domain from the Update 1 use case, or have deployed an updated version of this same domain from the Update 3 use case.\nThere should be three WebLogic Server pods with names that start with sample-domain1 running in the sample-domain1-ns namespace, a domain named sample-domain1, a ConfigMap named sample-domain1-wdt-config-map, and a Secret named sample-domain1-datasource-secret.\nAdd the Work Manager threads constraint configuration WDT model updates to the existing data source model updates in the Model in Image model ConfigMap.\nIn this step, we will update the model ConfigMap from the Update 1 use case with the desired changes to the minimum and maximum threads constraints.\nHere\u0026rsquo;s an example model configuration that updates the configured count values for the SampleMinThreads minimum threads constraint and SampleMaxThreads maximum threads constraint:\nresources: SelfTuning: MinThreadsConstraint: SampleMinThreads: Count: 2 MaxThreadsConstraint: SampleMaxThreads: Count: 20 Optionally, place the preceding model snippet in a file named /tmp/sample/myworkmanager.yaml and then use it when deploying the updated model ConfigMap, or simply use the same model snippet that\u0026rsquo;s provided in /tmp/sample/model-configmaps/workmanager/model.20.workmanager.yaml.\nRun the following commands:\n$ kubectl -n sample-domain1-ns delete configmap sample-domain1-wdt-config-map $ kubectl -n sample-domain1-ns create configmap sample-domain1-wdt-config-map \\ --from-file=/tmp/sample/model-configmaps/workmanager \\ --from-file=/tmp/sample/model-configmaps/datasource $ kubectl -n sample-domain1-ns label configmap sample-domain1-wdt-config-map \\ weblogic.domainUID=sample-domain1 Notes:\nIf you\u0026rsquo;ve created your own model YAML file(s), then substitute the file names in the --from-file= parameters (we suggested /tmp/sample/myworkmanager.yaml and /tmp/sample/mydatasource.xml earlier). The -from-file= parameter can reference a single file, in which case it puts the designated file in the ConfigMap, or it can reference a directory, in which case it populates the ConfigMap with all of the files in the designated directory. It can be specified multiple times on the same command line to load the contents from multiple locations into the ConfigMap. You name and label the ConfigMap using its associated domain UID for two reasons: To make it obvious which ConfigMap belong to which domains. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all resources associated with a domain. Update the data source secret that you created in the Update 1 use case with the correct password as well as with an increased maximum pool capacity:\nNOTE: Replace MY_ORACLE_SYS_PASSWORD with the same database sys account password that you chose (or plan to choose) when deploying the database.\n$ kubectl -n sample-domain1-ns delete secret sample-domain1-datasource-secret $ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-datasource-secret \\ --from-literal=\u0026#39;user=sys as sysdba\u0026#39; \\ --from-literal=\u0026#39;password=MY_ORACLE_SYS_PASSWORD\u0026#39; \\ --from-literal=\u0026#39;max-capacity=10\u0026#39; \\ --from-literal=\u0026#39;url=jdbc:oracle:thin:@oracle-db.default.svc.cluster.local:1521/devpdb.k8s\u0026#39; $ kubectl -n sample-domain1-ns label secret \\ sample-domain1-datasource-secret \\ weblogic.domainUID=sample-domain1 Update your Domain YAML file to enable onlineUpdate.\nIf onlineUpdate is enabled for your domain and the only model changes are to WebLogic Domain dynamic attributes, then the operator will attempt to update the running domains online without restarting the servers when you update the domain\u0026rsquo;s introspectVersion.\nOption 1: Edit your domain custom resource.\nCall kubectl -n sample-domain1-ns edit domain sample-domain1. Add or edit the value of the spec.configuration.model.onlineUpdate stanza so it contains enabled: true and save. The updated domain should look something like this: ... spec: ... configuration: ... model: ... onlineUpdate: enabled: true Option 2: Dynamically change your domain using kubectl patch. For example:\n$ kubectl -n sample-domain1-ns patch domain sample-domain1 --type=json \u0026#39;-p=[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/configuration/model/onlineUpdate\u0026#34;, \u0026#34;value\u0026#34;: {\u0026#34;enabled\u0026#34; : \u0026#39;true\u0026#39;} }]\u0026#39; Option 3: Use the sample helper script.\nCall /tmp/sample/utils/patch-enable-online-update.sh -n sample-domain1-ns -d sample-domain1. This will perform the same kubectl patch commands as Option 2. Prompt the operator to introspect the updated WebLogic domain configuration.\nNow that the updated Work Manager configuration is deployed in an updated model ConfigMap and the updated data source configuration is reflected in the updated data source Secret, we need to have the operator rerun its introspector job to regenerate its configuration.\nChange the spec.introspectVersion of the domain to trigger domain introspection. To do this:\nOption 1: Edit your domain custom resource.\nCall kubectl -n sample-domain1-ns edit domain sample-domain1. Change the value of the spec.introspectVersion field and save. The field is a string; typically, you use a number in this field and increment it. Option 2: Dynamically change your domain using kubectl patch.\nGet the current introspectVersion:\n$ kubectl -n sample-domain1-ns get domain sample-domain1 \u0026#39;-o=jsonpath={.spec.introspectVersion}\u0026#39; Choose a new introspect version that\u0026rsquo;s different from the current introspect version.\nThe field is a string; typically, you use a number in this field and increment it. Use kubectl patch to set the new value. For example, assuming the new introspect version is 2:\n$ kubectl -n sample-domain1-ns patch domain sample-domain1 --type=json \u0026#39;-p=[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/introspectVersion\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2\u0026#34; }]\u0026#39; Option 3: Use the sample helper script.\nCall /tmp/sample/utils/patch-introspect-version.sh -n sample-domain1-ns -d sample-domain1. This will perform the same kubectl patch command as Option 2. Because we have set the enabled value in spec.configuration.model.onlineUpdate to true, and all of the model changes we have specified are for WebLogic dynamic configuration attributes, we expect that the domain introspector job will apply the changes to the WebLogic Servers without restarting (rolling) their pods.\nWait for the introspector job to run to completion. You can:\nCall kubectl get pods -n sample-domain1-ns --watch and wait for the introspector pod to get into Terminating state and exit.\nsample-domain1-introspector-vgxxl 0/1 Terminating 0 78s For a more detailed view of this activity, you can use the waitForDomain.sh sample lifecycle script. This script provides useful information about a domain\u0026rsquo;s pods and optionally waits for its Completed status condition to become True. A Completed domain indicates that all of its expected pods have reached a ready state plus their target restartVersion, introspectVersion, and image. For example:\n$ cd /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/domain-lifecycle $ ./waitForDomain.sh -n sample-domain1-ns -d sample-domain1 -p Completed If the introspector job fails, then consult Debugging.\nCall the sample web application to:\nDetermine if the configuration of the minimum and maximum threads constraints have been updated to the new values. Determine if the data source can now contact the database (assuming you deployed the database). Send a web application request to the ingress controller:\n$ curl -s -S -m 10 -H \u0026#39;host: sample-domain1-cluster-cluster-1.sample.org\u0026#39; \\ http://localhost:30305/myapp_war/index.jsp Or, if Traefik is unavailable and your Administration Server pod is running, you can run kubectl exec:\n$ kubectl exec -n sample-domain1-ns sample-domain1-admin-server -- bash -c \\ \u0026#34;curl -s -S -m 10 http://sample-domain1-cluster-cluster-1:8001/myapp_war/index.jsp\u0026#34; You will see something like the following:\n\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v2\u0026#39; of the sample JSP web-app. Welcome to WebLogic server \u0026#39;managed-server2\u0026#39;! domain UID = \u0026#39;sample-domain1\u0026#39; domain name = \u0026#39;domain1\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found min threads constraint runtime named \u0026#39;SampleMinThreads\u0026#39; with configured count: 2 Found max threads constraint runtime named \u0026#39;SampleMaxThreads\u0026#39; with configured count: 20 Found 1 local data source: Datasource \u0026#39;mynewdatasource\u0026#39;: State=\u0026#39;Running\u0026#39;, testPool=\u0026#39;Passed\u0026#39; ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; The testPool='Passed' for mynewdatasource verifies that your update to the data source Secret to correct the password succeeded.\nIf you see a testPool='Failed' error, then it is likely you did not deploy the database or your database is not deployed correctly.\nIf you see any other error, then consult Debugging.\nThis completes the sample scenarios.\nTo remove the resources you have created in the samples, see Cleanup.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/developerguide/code-structure/",
	"title": "Code structure",
	"tags": [],
	"description": "Review the project directory structure.",
	"content": "This project has the following directory structure:\ndocumentation/latest: This documentation documentation/\u0026lt;numbered directory\u0026gt;: The archived documentation for a previous release documentation/swagger: The operator REST API swagger documentation/domains: Reference for Domain and Cluster resource schemas json-schema-generator: Java model to JSON schema generator json-schema-maven-plugin: Maven plugin for schema generator kubernetes/charts: Helm charts kubernetes/samples: All samples, including for WebLogic domain creation integration-tests: JUnit 5 integration test suite operator: Operator runtime swagger-generator: Swagger file generator for the Kubernetes API server and Domain type Watch package The Watch API in the Kubernetes Java client provides a watch capability across a specific list of resources for a limited amount of time. As such, it is not ideally suited for our use case, where a continuous stream of watches is desired, with watch events generated in real time. The watch-wrapper in this repository extends the default Watch API to provide a continuous stream of watch events until the stream is specifically closed. It also provides resourceVersion tracking to exclude events that have already been seen. The watch-wrapper provides callbacks so events, as they occur, can trigger actions.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/introduction/platforms/environments/",
	"title": "Supported environments",
	"tags": [],
	"description": "The supported environments, pricing and licensing, and support details for the operator.",
	"content": "Contents Overview Important notes about specific environments Oracle Cloud Infrastructure Oracle Cloud Native Environment Oracle Private Cloud Appliance (PCA) and Oracle Private Cloud at Customer (OPCC) Microsoft Azure Microsoft Azure Kubernetes Service (AKS) Oracle WebLogic Server on AKS from the Azure Marketplace (WLS on AKS Marketplace) VMware Tanzu Kubernetes Grid (TKG) OpenShift WebLogic Server running in Kubernetes connecting to an Oracle Database also running in Kubernetes Development-focused Kubernetes distributions Pricing and licensing WebLogic Kubernetes Operator WebLogic Server Oracle Linux Oracle Java WebLogic Server or Fusion Middleware Infrastructure images Additional references Overview The operator supports running on production-quality Kubernetes environments, including on-premises, cloud offerings where Kubernetes is supplied for you, and cloud offerings where you set up Kubernetes yourself. Please refer to the support statement Doc ID 2790123.1.\nThese include, but are not limited to:\nOracle Cloud certified offerings, such as:\nOracle Cloud Infrastructure (OCI) Oracle Container Engine for Kubernetes (OKE) Oracle Cloud Native Environment (OCNE) Oracle Private Cloud Appliance (PCA) Oracle Private Cloud at Customer (OPCC) Other certified offerings, such as:\nMicrosoft Azure Platform Microsoft Azure Kubernetes Service (AKS) OpenShift Container Platform VMWare Tanzu VMware Tanzu Kubernetes Grid (TKG) Offerings which deploy the WebLogic Server and the operator for you. These include:\nOracle WebLogic Server for OKE (WLS for OKE) Oracle WebLogic Server on AKS from the Azure Marketplace (WLS on AKS Marketplace) Development-focused Kubernetes distributions are also supported.\nNOTES:\nImportant: Some supported environments have additional help or samples that are specific to the operator, or are subject to limitations and restrictions; see Important notes about specific environments.\nFor detailed virtualization licensing, cloud licensing, and support descriptions, see Pricing and licensing.\nThe operator is subject to Kubernetes, WebLogic Server, and operating system versioning prerequisites; see Operator prerequisites.\nImportant notes about specific environments Here are some important considerations for specific environments:\nOracle Cloud Infrastructure Oracle Cloud Native Environment Oracle Private Cloud Appliance (PCA) and Oracle Private Cloud at Customer (OPCC) Microsoft Azure VMware Tanzu Kubernetes Grid (TKG) OpenShift WebLogic Server running in Kubernetes connecting to an Oracle Database also running in Kubernetes Development-focused Kubernetes distributions NOTE: This section does not list all supported environments. See the Overview for a list of all supported environments.\nOracle Cloud Infrastructure The operator and WebLogic Server are supported on Oracle Cloud Infrastructure using Oracle Container Engine for Kubernetes, or in a cluster running Oracle Linux Container Services for use with Kubernetes on Oracle Cloud Infrastructure Compute, and on any other Oracle Cloud Infrastructure \u0026ldquo;Authorized Cloud Environments\u0026rdquo; as described in the Overview.\nWebLogic Kubernetes Operator is certified for use on OKE with Kubernetes 1.25.0+, 1.26.2+, and 1.27.2+.\nOracle Cloud Native Environment Oracle Cloud Native Environment is a fully integrated suite for the development and management of cloud-native applications. Based on Open Container Initiative (OCI) and Cloud Native Computing Foundation (CNCF) standards, Oracle Cloud Native Environment delivers a simplified framework for installations, updates, upgrades, and configuration of key features for orchestrating microservices.\nWebLogic Server and the WebLogic Kubernetes Operator are certified and supported on Oracle Cloud Native Environment (OCNE):\nOCNE 1.9 with Kubernetes 1.29.3+ OCNE 1.8 with Kubernetes 1.28.8+ OCNE 1.7 with Kubernetes 1.26.6+ OCNE 1.6 with Kubernetes 1.25.11+ Oracle Private Cloud Appliance (PCA) and Oracle Private Cloud at Customer (OPCC) The Oracle Private Cloud Appliance (PCA) and Oracle Private Cloud at Customer (OPCC) fully support Oracle Cloud Native Environment, including Oracle Container Runtime for Docker and Oracle Container Services for Use with Kubernetes. They provide an ideal runtime for Oracle WebLogic Server applications to run in Docker and Kubernetes with full, integrated system support from Oracle. For operator certifications that are specific to Oracle Cloud Native Environment, see Oracle Cloud Native Environment.\nThe Oracle WebLogic Server on Oracle Private Cloud Appliance and Kubernetes document describes how to deploy Oracle WebLogic Server applications on Kubernetes on PCA or OPCC, enabling you to run these applications in cloud native infrastructure that is fully supported by Oracle, and that is portable across cloud environments. The document also highlights how applications deployed on Oracle Exalogic Elastic Cloud systems can be migrated to this infrastructure without application changes, enabling you to preserve your application investment as you adopt modern cloud native infrastructure.\nWebLogic Kubernetes Operator is certified for use on PCA X9 with Kubernetes 1.24.5+ and Istio 1.14.\nWebLogic Kubernetes Operator 4.x is certified for use on PCA v3. For more information, see Using the OKE Service in the Oracle Private Cloud Appliance Container Engine for Kubernetes documentation.\nMicrosoft Azure There are three different approaches for deploying the operator to Microsoft Azure:\nMicrosoft Azure Platform Microsoft Azure Kubernetes Service (AKS) Oracle WebLogic Server on AKS from the Azure Marketplace (WLS on AKS Marketplace) Microsoft Azure Kubernetes Service (AKS) Azure Kubernetes Service (AKS) is a hosted Kubernetes environment. The WebLogic Kubernetes Operator, Oracle WebLogic Server 12c, and Oracle Fusion Middleware Infrastructure 12c are fully supported and certified on Azure Kubernetes Service (as per the documents referenced in the Overview). In this environment, it is the customer\u0026rsquo;s responsibility to install the operator and supply WebLogic Server or Fusion Middleware Infrastructure images.\nAKS support and limitations:\nWebLogic Kubernetes Operator is certified for use on AKS with Kubernetes 1.25.0+, 1.26.2+. All three domain home source types are supported (Domain in Image, Model in Image, and Domain on PV). NOTE: The Domain in Image domain home source type is deprecated in WebLogic Kubernetes Operator version 4.0. Oracle recommends that you choose either Domain on PV or Model in Image, depending on your needs. For Domain on PV, we support Azure Files volumes accessed through a persistent volume claim; see here. Azure Load Balancers are supported when provisioned using a Kubernetes Service of type=LoadBalancer. Oracle databases running in Oracle Cloud Infrastructure are supported for Fusion Middleware Infrastructure MDS data stores only when accessed through an Oracle Cloud Infrastructure FastConnect. Windows Server containers are not currently supported, only Linux containers. See also the Azure Kubernetes Service sample.\nOracle WebLogic Server on AKS from the Azure Marketplace (WLS on AKS Marketplace) The WebLogic Server on AKS Azure Marketplace offer lets you embrace cloud computing by providing greater choice and flexibility for deploying your WLS domains and applications. The offer leverages the WebLogic Kubernetes Toolkit to automate the provisioning of WebLogic Server and Azure resources so that you can easily move WLS workloads to AKS. The automatically provisioned resources include an AKS cluster, the WebLogic Kubernetes Operator, WebLogic Server images, and the Azure Container Registry (ACR). It is possible to use an existing AKS cluster or ACR instance with the offer if desired. The offer also supports configuring load balancing with Azure App Gateway or the Azure Load Balancer, DNS configuration, SSL/TLS configuration, easing database connectivity, publishing metrics to Azure Monitor as well as mounting Azure Files as Kubernetes Persistent Volumes.\nFor details, see WebLogic Server on AKS Marketplace.\nVMware Tanzu Kubernetes Grid (TKG) Tanzu Kubernetes Grid (TKG) is a managed Kubernetes Service that lets you quickly deploy and manage Kubernetes clusters. The WebLogic Kubernetes Operator and Oracle WebLogic Server are fully supported and certified for use on VMware Tanzu Kubernetes Grid Multicloud 1.1.3 (with vSphere 6.7U3).\nTKG support and limitations:\nWebLogic Kubernetes Operator is certified for use on TKG with Kubernetes 1.24.0+, 1.25.0+, 1.26.0+. The Model in Image domain home source type is supported; Domain on PV is not supported. VSphere CSI driver supports only volumes with Read-Write-Once policy. This does not allow writing stores on PV. For applications requiring HA, use JMS and JTA stores in the database. The ingress used for certification is NGINX, with MetalLB load balancer. Tanzu Kubernetes Grid Integrated Edition (TKGI) 1.16 with vSphere (7.0.3) CSI 2.7 support includes:\nBoth domain home source types (Model in Image and Domain on PV); use vSAN for PV. Kubernetes 1.25.10 Ubuntu See also the Tanzu Kubernetes Grid sample.\nOpenShift OpenShift can be a cloud platform or can be deployed on premises.\nOperator v4.0.5 is certified for use on: OpenShift Container Platform 4.11.30 with Kubernetes 1.24+, RedHat OpenShift Mesh 2.3.2, and Istio 1.14.5. OpenShift Container Platform 4.12.2 with Kubernetes 1.25+, RedHat OpenShift Mesh 2.3.2, and Istio 1.14.5. Operator v4.2.16 is certified for use on: OpenShift Container Platform 4.17.2 with Kubernetes v1.30.5+, RedHat OpenShift Mesh v3.0.1, and Istio 1.24.4. To accommodate OpenShift security requirements:\nFor security requirements to run WebLogic Server in OpenShift, see the OpenShift documentation. Beginning with operator version 3.3.2, specify the kubernetesPlatform Helm chart property with value OpenShift. For more information, see Operator Helm configuration values. WebLogic Server running in Kubernetes connecting to an Oracle Database also running in Kubernetes We have certified support for WebLogic Server domains, managed by the WebLogic Kubernetes Operator (operator), connecting to an Oracle Database, managed by the Oracle Database Operator for Kubernetes (OraOperator). For details on the supported WLS and database versions, see the following:\nOperator prerequisites Oracle Database Operator for Kubernetes prerequisites The certification includes support for both application data access and all WLS database-dependent features supported in Kubernetes. For more information, see WebLogic Server Certifications on Kubernetes in My Oracle Support Doc ID 2349228.1.\nIncluded in the certification is support for the following topologies:\nWebLogic Server, operator, Oracle Database, and OraOperator all running in the same Kubernetes cluster. WebLogic Server, operator, Oracle Database, and OraOperator all running in the same Kubernetes cluster and WebLogic Server running on an Istio mesh. WebLogic Server and operator running in a Kubernetes cluster and the Oracle Database and OraOperator in a different Kubernetes cluster. Development-focused Kubernetes distributions There are a number of development-focused distributions of Kubernetes, like kind, Minikube, Minishift, and so on. Often these run Kubernetes in a virtual machine on your development machine. We have found that these distributions present some extra challenges in areas like:\nSeparate container image caches, making it necessary to save/load images to move them between Docker file systems Default virtual machine file sizes and resource limits that are too small to run WebLogic Server or hold the necessary images Storage providers that do not always support the features that the operator or WebLogic Server rely on Load balancing implementations that do not always support the features that the operator or WebLogic Server rely on As such, we do not recommend using these distributions to run the operator or WebLogic Server. While we do not provide support for WebLogic Server or the operator running in production in these distributions, we do support some (such as, kind and Minikube) in a development or test environment.\nWe have found that Docker for Desktop does not seem to suffer the same limitations, and we do support that as a development or test option.\nPricing and licensing The WebLogic Kubernetes Operator and Oracle Linux are open source and free; WebLogic Server requires licenses in any environment. All WebLogic Server licenses are suitable for deploying WebLogic to containers and Kubernetes, including free single desktop Oracle Technology Network (OTN) developer licenses. See the following sections for more detailed information:\nWebLogic Kubernetes Operator WebLogic Server Oracle Linux Oracle Java WebLogic Server or Fusion Middleware Infrastructure Images Additional references WebLogic Kubernetes Operator The WebLogic Kubernetes Operator (the \u0026ldquo;operator\u0026rdquo;) is open source and free, licensed under the Universal Permissive license (UPL), Version 1.0. For support details, see Get help.\nWebLogic Server WebLogic Server is not open source:\nLicensing is required to run WebLogic Server instances in Kubernetes, just as with any deployment of WebLogic Server.\nLicensing is free for a single developer desktop development environment when using an Oracle Technology Network (OTN) developer license.\nFor more information, see the Fusion Middleware Licensing Information User Manual - Application Server Products and the following sections.\nOracle Linux Oracle Linux is under open source license and is completely free to download and use.\nNote that WebLogic Server licenses that include support do not include customer entitlements for direct access to Oracle Linux support or Unbreakable Linux Network (to directly access the standalone Oracle Linux patches). The latest Oracle Linux patches are included with the latest WebLogic Server or Fusion Middleware Infrastructure Images.\nOracle Java Oracle support for Java is included with WebLogic Server licenses when Java is used for running WebLogic and Coherence servers or clients.\nFor more information, see the Fusion Middleware Licensing Information User Manual - Application Server Products.\nWebLogic Server or Fusion Middleware Infrastructure images Oracle provides two different types of WebLogic Server or Fusion Middleware (FMW) Infrastructure images:\nCritical Patch Update (CPU) images: Images with the latest WebLogic Server (or Fusion Middleware Infrastructure) and Coherence PSU and other fixes released by the Critical Patch Update (CPU) program. CPU images are intended for production use.\nGeneral Availability (GA) WebLogic Server (or Fusion Middleware Infrastructure) images: Images which are not intended for production use and do not include WebLogic, Fusion Middleware Infrastructure, or Coherence PSUs.\nAll WebLogic Server and Fusion Middleware Infrastructure licenses, including free Oracle Technology Network (OTN) developer licenses, include access to the latest General Availability (GA) WebLogic Server or Fusion Middleware Infrastructure images which bundle Java SE.\nCustomers with access to WebLogic Server support additionally have:\nAccess to Critical Patch Update (CPU) WebLogic Server images which bundle Java SE. Access to WebLogic Server patches. Oracle support for WebLogic Server images. Oracle support for the WebLogic Kubernetes Toolkit. (Customers with access to Fusion Middleware Infrastructure support have similar access to its CPU images, patches, and support.)\nYou can use the free and open source WebLogic Image Tool to create new or updated (patched) WebLogic or Fusion Middleware Infrastructure images. See My Oracle Support (MOS) Doc ID 2790123.1 for community and Oracle support policies for the WebLogic Kubernetes Toolkit.\nSee WebLogic images for information about obtaining WebLogic Server or Fusion Middleware Infrastructure images, developer and production licensing details, the different types of images, creating custom images, and patching images.\nThe latest Oracle Container Registry (OCR) GA images include the latest security patches for Oracle Linux and Java, and do not include the latest security patches for WebLogic Server. Oracle strongly recommends using images with the latest security patches, such as OCR Critical Patch Updates (CPU) images or custom generated images. See Ensure you are using recently patched images.\nAdditional references For the most up-to-date and comprehensive support information, see Supported Virtualization Technologies for Oracle Fusion Middleware (search for keyword \u0026lsquo;Kubernetes\u0026rsquo;) Running and Licensing Oracle Programs in Containers and Kubernetes "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/",
	"title": "Samples",
	"tags": [],
	"description": "",
	"content": "The samples provide demonstrations of how to accomplish common tasks. These samples are provided for educational and demonstration purposes only; they are not intended to be used in production deployments or to be depended upon to create production environments.\nCredentials Sample for creating a Kubernetes Secret that contains the Administration Server credentials. This Secret can be used in creating a WebLogic Domain YAML file.\nStorage Sample for creating a PV or PVC that can be used by a Domain YAML file as the persistent storage for the WebLogic domain home or log files.\nRun a database Run an ephemeral database in Kubernetes that is suitable for sample or basic testing purposes.\nDomains These samples show various choices for working with domains.\nREST APIs Sample for generating a self-signed certificate and private key that can be used for the operator\u0026#39;s external REST API.\nIngress Ingress controllers and load balancer sample scripts.\nElastic Stack Operator Sample for configuring the Elasticsearch and Kibana deployments and services for the operator\u0026#39;s logs. WebLogic domain Sample for using Fluentd for WebLogic domain and operator\u0026#39;s logs.\nAzure Kubernetes Service Sample for using the operator to set up a WLS cluster on the Azure Kubernetes Service.\nTanzu Kubernetes Service Sample for using the operator to set up a WLS cluster on the Tanzu Kubernetes Service.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/major-weblogic-version-upgrade/upgrade-14210/",
	"title": "Upgrade managed domains to v14.1.2.0",
	"tags": [],
	"description": "Upgrade managed domains to v14.1.2.0.",
	"content": "Contents Important considerations General upgrade procedures Back up the OPSS wallet and save it in a secret Deploy a WebLogic Server pod to access the domain home on a persistent volume Upgrade the JRF database Reconfigure the domain Upgrade use cases WLS Domain on Persistent Volume FMW/JRF Domain on Persistent Volume WLS domain using Model in Image FMW/JRF domain using Model in Image Sample WDT model for secured production mode and SSL This document provides guidelines for upgrading WLS and FMW/JRF infrastructure domains to v14.1.2.0.\nImportant considerations By default, version 14.1.2.0 WLS and FMW/JRF infrastructure domains in production mode are set to secured production mode, in which their default security configuration is more secure, insecure configurations are logged as warnings, and default authorization and role mapping policies are more restrictive.\nSome important secured production mode changes are:\nPlain HTTP listen ports are disabled. Any application code, utilities, or ingresses that use plain HTTP listen ports must be changed.\nSSL listen ports must be enabled for every server in the domain. Each server must have at least one SSL listen port set up, either in the default channel or in one of the custom network channels. Note that demo SSL certificates should not be used in a production environment; you should set up SSL listen ports with valid SSL certificates in all server instances.\nDemo SSL certificates have been changed completely from previous releases; there are special considerations when using them. For more information, see Using demo SSL certificates in v14.1.2.0.0 or later.\nFor more information about secured production mode, see the secured production mode documentation.\nNOTE: If the domain is not in production mode, then none of the security changes apply.\nGeneral upgrade procedures In general, the process for upgrading WLS and FMW/JRF infrastructure domains in Kubernetes is similar to upgrading domains on premises. For a thorough understanding, we suggest that you read the Fusion Middleware Upgrade Guide.\nBefore the upgrade, you must do the following:\nIf your domain home source type is Domain on Persistent Volume (DoPV), then back up the domain home. If your domain type is JRF, then: Back up the JRF database. Back up the OPSS wallet file. See Save the OPSS wallet secret. Make sure nothing else is accessing the database. Do not delete the domain resource. Shut down the domain by patching the domain and/or cluster spec serverStartPolicy to Never. For example: $ kubectl -n sample-domain1-ns patch domain sample-domain1 --type=json -p=\u0026#39;[ {\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/serverStartPolicy\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Never\u0026#34;}]\u0026#39; If your domain is on a persistent volume, WebLogic provides two utilities for performing version upgrades of WebLogic domains: the Upgrade Assistant for upgrading FMW JRF database schemas and the Reconfiguration Wizard for upgrading the domain configuration. When running these utilities, you will need access to the existing domain home directory. Because a typical Kubernetes environment lacks a graphical interface, you must run these utilities with the command-line options.\nIf your domain is using Model in Image, because the domain will be rebuilt when the model is updated, see the Upgrade Use Cases for details.\nBack up the OPSS wallet and save it in a secret The operator provides a helper script, the OPSS wallet utility, for extracting the wallet file and storing it in a Kubernetes walletFileSecret. In addition, you should save the wallet file in a safely backed-up location, outside of Kubernetes. For example, the following command saves the OPSS wallet for the sample-domain1 domain in the sample-ns namespace to a file named ewallet.p12 in the /tmp directory and also stores it in the wallet secret named sample-domain1-opss-walletfile-secret.\n$ opss-wallet.sh -n sample-ns -d sample-domain1 -s -r -wf /tmp/ewallet.p12 -ws sample-domain1-opss-walletfile-secret Deploy a WebLogic Server pod to access the domain home on a persistent volume For domain on persistent volume, you will need to access the domain home on the shared volume with the 14.1.2.0 WebLogic Server version pod. You can launch a running pod with the PV and PVC helper script.\nFor example,\n$ ./pv-pvc-helper.sh -n sample-domain1-ns -c sample-domain1-pvc-rwm1 -m /share -i wls14120:fmw After the pod is deployed, you can kubectl -n sample-domain1-ns exec -it pvhelper -- /bin/sh' into the pod\u0026rsquo;s terminal session.\nUpgrade the JRF database The Upgrade Assistant is for upgrading schemas in a JRF database. It will detect if any schema needs to be upgraded, then upgrade the schemas and also upgrade the system-owned schema version table.\nIf you have not yet deployed a WebLogic Server pod, see Deploy the server pod.\nFrom the pvhelper pod, to discover all the command-line options:\n$ cd $ORACLE_HOME/oracle_common/upgrade/bin $ ./ua -help Create a file named response.txt with this content and modify any \u0026lt;TODO: to match your environment.\n# This is a response file for the Fusion Middleware Upgrade Assistant.\r# Individual component upgrades are performed in the order they are described here.\r# Each upgrade is introduced by a section header containing the name of the\r# component and name of the upgrade plugin. The form of the section header is\r# [ComponentName.PluginName]\r# These names can be found in the Upgrade Descriptor files for the components.\r# Individual input lines consist of a name, an equal sign, and a value.\r# The name is in two parts separated by a period. The first part is the \u0026#34;name\u0026#34;\r# attribute from the Descriptor File XML tag by which the plugin refers to the value.\r# The second part of the name identifies a field within that value. Some input\r# types have only one field, while other types can have half a dozen. Do not\r# intermix input lines that apply to different XML tags.\r[GENERAL]\r# This is the file format version number. Do not change the next line.\rfileFormatVersion = 3\r# The next section contains information for accessing a WebLogic Server domain.\r[UAWLSINTERNAL.UAWLS]\r# The following number uniquely identifies this instance of an\r# upgrade plugin. Do not change it.\rpluginInstance = 1\r# Specifies the WebLogic Server domain directory:\r#UASVR.path = /share/domains/sample-domain1\rUASVR.path = \u0026lt;TODO: provides the complete domain home path\u0026gt;\r# The next section contains the information for performing a schema\r# upgrade on Oracle Platform Security Services, as described in the Upgrade\r# Descriptor file located at\r# /u01/oracle/oracle_common/plugins/upgrade/Opss.xml\r# Do not change the next line.\r[OPSS.OPSS_SCHEMA_PLUGIN]\r# The following number uniquely identifies this instance of an\r# upgrade plugin. Do not change it.\rpluginInstance = 10\r# The next few lines describe a database connection.\r# \u0026#34;Specify the database containing the OPSS schema.\u0026#34;\r# Specifies the type of database. Supported types for this product are\r# Oracle Database, Oracle Database enabled for edition-based redefinition, Microsoft SQL Server, IBM DB2\rOPSS.databaseType = Oracle Database\r# Specifies the database connection string for the DBA user.\r# The format depends upon the database type.\r#OPSS.databaseConnectionString = //nuc:1521/orclpdb1\rOPSS.databaseConnectionString = \u0026lt;TODO: provides the connection string\u0026gt;\r# Specifies the database connection string for the user schema.\r# The format depends upon the database type.\r#OPSS.schemaConnectionString = //nuc:1521/orclpdb1\rOPSS.schemaConnectionString = \u0026lt;TODO: provides the connection string\u0026gt;\r# Specifies the name of the schema or database user\r#OPSS.schemaUserName = FMWTEST_OPSS\rOPSS.schemaUserName = \u0026lt;TODO: provides the schema name rcuprefix_OPSS \u0026gt;\r# Specifies the password for the schema, in encrypted form.\r# To specify a different password in cleartext, use the \u0026#34;cleartextSchemaPassword\u0026#34; keyword instead:\rOPSS.cleartextSchemaPassword = \u0026lt;TODO: provides the clear text password\u0026gt;\r# encrypted password can be generated with command line option -createResponse\r#OPSS.encryptedSchemaPassword = 0551CF2EACFC4FE7BCB1F860FCF68E13AA6E61A724E7CFC09E\r# Specifies the name of the database administrator account.\rOPSS.dbaUserName = \u0026lt;TODO: provide dba user name, e.g. sys as sysdba\u0026gt;\r# Specifies the password for the database administrator account, in encrypted form.\r# To specify a different password in cleartext, use the \u0026#34;cleartextDbaPassword\u0026#34; keyword\r# instead:\rOPSS.cleartextDbaPassword = \u0026lt;TODO: provides clear text dba password\u0026gt;\r#OPSS.encryptedDbaPassword = 057B3698F71FB2EE583D32EF36234174DCC2C7276FC11F77E7\r# The next section contains the information for performing a schema\r# upgrade on Oracle Metadata Services, as described in the Upgrade\r# Descriptor file located at\r# /u01/oracle/oracle_common/plugins/upgrade/mds.xml\r# Do not change the next line.\r[MDS.SCHEMA_UPGRADE]\rpluginInstance = 11\rMDS.databaseConnectionString = \u0026lt;TODO: provides the connection string\u0026gt;\rMDS.schemaConnectionString = \u0026lt;TODO: provides the connection string\u0026gt;\rMDS.schemaUserName = \u0026lt;TODO: provides the schema name rcuprefix_MDS \u0026gt;\rMDS.cleartextSchemaPassword = \u0026lt;TODO: provides the clear text password\u0026gt;\rMDS.dbaUserName = \u0026lt;TODO: provide dba user name, e.g. sys as sysdba\u0026gt;\rMDS.cleartextDbaPassword = \u0026lt;TODO: provides clear text dba password\u0026gt;\r# The next section contains the information for performing a schema\r# upgrade on Oracle Audit Services, as described in the Upgrade\r# Descriptor file located at\r# /u01/oracle/oracle_common/plugins/upgrade/audit.xml\r# Do not change the next line.\r[IAU.AUDIT_SCHEMA_PLUGIN]\rpluginInstance = 6\rIAU.databaseType = Oracle Database\rIAU.databaseConnectionString = \u0026lt;TODO: provides the connection string\u0026gt;\rIAU.schemaConnectionString = \u0026lt;TODO: provides the connection string\u0026gt;\rIAU.schemaUserName = \u0026lt;TODO: provides the schema name rcuprefix_IAU \u0026gt;\rIAU.cleartextSchemaPassword = \u0026lt;TODO: provides the clear text password\u0026gt;\rIAU.dbaUserName = \u0026lt;TODO: provide dba user name, e.g. sys as sysdba\u0026gt;\rIAU.cleartextDbaPassword = \u0026lt;TODO: provides clear text dba password\u0026gt;\r# The next section contains the information for performing a schema\r# upgrade on Common Infrastructure Services, as described in the Upgrade\r# Descriptor file located at\r# /u01/oracle/oracle_common/plugins/upgrade/cie.xml\r# Do not change the next line.\r[FMWCONFIG.CIE_SCHEMA_PLUGIN]\rpluginInstance = 4\rSTB.databaseType = Oracle Database\rSTB.databaseConnectionString = \u0026lt;TODO: provides the connection string\u0026gt;\rSTB.schemaConnectionString = \u0026lt;TODO: provides the connection string\u0026gt;\rSTB.schemaUserName = \u0026lt;TODO: provides the schema name rcuprefix_STB \u0026gt;\rSTB.cleartextSchemaPassword = \u0026lt;TODO: provides the clear text password\u0026gt;\rSTB.dbaUserName = \u0026lt;TODO: provide dba user name, e.g. sys as sysdba\u0026gt;\rSTB.cleartextDbaPassword = \u0026lt;TODO: provides clear text dba password\u0026gt;\r# This section is not needed for pure JRF domain.\r# The next section contains the information for performing a schema\r# upgrade on Oracle WebLogicServer, as described in the Upgrade\r# Descriptor file located at\r# /u01/oracle/oracle_common/plugins/upgrade/wlsservices.xml\r# Do not change the next line.\r#[WLS.WLS]\r#pluginInstance = 7\r#WLS.databaseType = Oracle Database\r#WLS.databaseConnectionString =\r#WLS.schemaConnectionString =\r#WLS.schemaUserName =\r#WLS.encryptedSchemaPassword = 05FEC474FC653B49B15ED79A53565A8B00F49ADADA72D30816\r#WLS.dbaUserName =\r# WLS.cleartextDbaPassword =\r#WLS.encryptedDbaPassword = 0543C93F9A28FBAFBF3FCC49E78EB2C6B3AA02F53098BB322C Copy the response file to the pod.\n$ kubectl -n sample-domain1-ns cp response.txt pvhelper:/tmp Run the Upgrade Assistant readiness check to verify the input values and whether the schema needs to be upgraded.\n$ ./ua -readiness -response /tmp/response.txt -logDir /tmp Check the output to see if there are any errors.\nOracle Fusion Middleware Upgrade Assistant 14.1.2.0.0\rLog file is located at: /tmp/ua2023-10-04-17-23-32PM.log\rReading installer inventory, this will take a few moments...\r...completed reading installer inventory.\rUsing response file /tmp/response.txt for input\rOracle Metadata Services schema readiness check is in progress\rOracle Audit Services schema readiness check is in progress\rOracle Platform Security Services schema readiness check is in progress\rCommon Infrastructure Services schema readiness check is in progress\rCommon Infrastructure Services schema readiness check finished with status: ready for upgrade\rOracle Metadata Services schema readiness check finished with status: ready for upgrade\rOracle Audit Services schema readiness check finished with status: ready for upgrade\rOracle Platform Security Services schema readiness check finished with status: ready for upgrade\rReadiness Check Report File: /tmp/readiness2023-10-04-17-24-55PM.txt\rUpgrade readiness check completed successfully.\rUPGAST-00281: Upgrade is being skipped because the -readiness flag is set\rActual upgrades are not done when the -readiness command line option is set.\rIf you want to perform an actual upgrade remove the -readiness flag from the command line. If you intended to perform just the readiness phase, no action is necessary. If there are no errors and you are ready to upgrade, then run the command again without the -readiness flag.\n$ ./ua -response /tmp/response.txt -logDir /tmp Check the output again.\nOracle Fusion Middleware Upgrade Assistant 14.1.2.0.0\rLog file is located at: /u01/oracle/oracle_common/upgrade/logs/ua2023-10-05-14-03-18PM.log\rReading installer inventory, this will take a few moments...\r...completed reading installer inventory.\rUsing response file /tmp/response.txt for input\rOracle Platform Security Services schema examine is in progress\rOracle Metadata Services schema examine is in progress\rOracle Audit Services schema examine is in progress\rCommon Infrastructure Services schema examine is in progress\rCommon Infrastructure Services schema examine finished with status: ready for upgrade\rOracle Platform Security Services schema examine finished with status: ready for upgrade\rOracle Audit Services schema examine finished with status: ready for upgrade\rOracle Metadata Services schema examine finished with status: ready for upgrade\rSchema Version Registry saved to: /u01/oracle/oracle_common/upgrade/logs/ua2023-10-05-14-03-18PM.xml\rOracle Platform Security Services schema upgrade is in progress\rOracle Audit Services schema upgrade is in progress\rOracle Metadata Services schema upgrade is in progress\rCommon Infrastructure Services schema upgrade is in progress\rCommon Infrastructure Services schema upgrade finished with status: succeeded\rOracle Audit Services schema upgrade finished with status: succeeded\rOracle Platform Security Services schema upgrade finished with status: succeeded\rOracle Metadata Services schema upgrade finished with status: succeeded If there are any errors, you need to correct them or contact Oracle Support for assistance.\nReconfigure the domain The Reconfiguration Wizard will upgrade the domain configuration to the 14.1.2.0 version.\nIf you have not yet deployed a WebLogic Server pod, see Deploy the server pod.\nFrom the pvhelper pod, use the following WLST commands to reconfigure a domain to the 14.1.2.0 version.\n$ /u01/oracle/oracle_common/bin/wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt; readDomainForUpgrade(\u0026#39;\u0026lt;your domain home directory path\u0026gt;\u0026#39;) wls:/offline\u0026gt; updateDomain() wls:/offline\u0026gt; closeDomain() If there are any errors, you need to correct them or contact Oracle Support for assistance.\nUpgrade use cases Consider the following use case scenarios, depending on your WebLogic domain type (WLS or JRF) and domain home source type (Domain on PV or Model in Image).\nWLS Domain on Persistent Volume FMW/JRF Domain on Persistent Volume WLS domain using Model in Image FMW/JRF domain using Model in Image WLS Domain on Persistent Volume Follow the steps in the General upgrade procedures. You can skip the database related steps. Upgrade the domain configuration using the reconfiguration WLST commands. See Reconfigure the domain. Update the domain resource to use the WebLogic 14120 base image, and patch the serverStartPolicy to IfNeeded to restart the domain. For example, kubectl -n sample-domain1-ns patch domain sample-domain1 --type=json -p='[ {\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/serverStartPolicy\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;Never\u0026quot;}, {\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;:\u0026quot;/spec/image\u0026quot;, \u0026quot;value\u0026quot;:\u0026quot;\u0026lt;WebLogic 14120 base image\u0026gt;\u0026quot;]' FMW/JRF Domain on Persistent Volume Follow the steps in the General upgrade procedures. Run the Upgrade Assistant. See Upgrade the JRF database. Upgrade the domain configuration using the reconfiguration WLST commands. See Reconfigure the domain. Update the domain resource to use the Fusion Middleware Infrastructure 14120 base image, and patch the serverStartPolicy to IfNeeded to restart the domain. For example, kubectl -n sample-domain1-ns patch domain sample-domain1 --type=json -p='[ {\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/serverStartPolicy\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;Never\u0026quot;}, {\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;:\u0026quot;/spec/image\u0026quot;, \u0026quot;value\u0026quot;:\u0026quot;\u0026lt;Fusion Middleware Infrastructure 14120 image\u0026gt;\u0026quot;]' WLS domain using Model in Image Follow the steps in the General upgrade procedures. Depending on whether your existing domain is using secured production mode, use one of the following options. Existing Domain Upgrade Actions Domain is already using secured production mode Update the domain resource to use the WebLogic 14120 base image and redeploy the domain. Domain is not using secured production mode but ready to switch to use secured production mode Enable secured production mode. See the Sample WDT YAML, update the domain resource to use the WebLogic 14120 base image and redeploy the domain. Domain is not using secured production mode but not ready to switch to use secured production mode Update the domain resource to use the WebLogic 14120 base image and redeploy the domain, the operator will automatically disable the secured production mode for you, but your domain will not have the added benefits of a secured production mode. Domain is not using secured production mode and wanted to start from scratch to rebuild the domain Delete the domain first, enable secure production mode. See the Sample WDT YAML, update the domain resource YAML to use the WebLogic 14120 base image and deploy the domain. You can use this patch command for redeploying the domain with the new WebLogic 14120 image. For example, kubectl -n sample-domain1-ns patch domain sample-domain1 --type=json -p='[ {\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/serverStartPolicy\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;Never\u0026quot;}, {\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;:\u0026quot;/spec/image\u0026quot;, \u0026quot;value\u0026quot;:\u0026quot;\u0026lt;WebLogic 14120 base image\u0026gt;\u0026quot;]' FMW/JRF domain using Model in Image FMW/JRF domains using Model in Image has been deprecated since WebLogic Kubernetes Operator 4.1. Before upgrading to FMW v14.1.2.0, we recommend moving your domain home to Domain on Persistent Volume. For more information, see Domain On Persistent Volume.\nFollow the steps in the General upgrade procedures. If are not using an auxiliary image in your domain, then create a Domain creation image. Create a new domain resource YAML file. You should have at least the following changes: # Change type to PersistentVolume\rdomainHomeSourceType: PersistentVolume\rimage: \u0026lt;Fusion Middleware Infrastructure 14120 base image\u0026gt;\r...\rserverPod:\r...\r# specify the volume and volume mount information\rvolumes:\r- name: weblogic-domain-storage-volume\rpersistentVolumeClaim:\rclaimName: sample-domain1-pvc-rwm1\rvolumeMounts:\r- mountPath: /share\rname: weblogic-domain-storage-volume\r# specify a new configuration section, remove the old configuration section.\rconfiguration:\r# secrets that are referenced by model yaml macros\r# sample-domain1-rcu-access is used for JRF domains\rsecrets: [ sample-domain1-rcu-access ]\rinitializeDomainOnPV:\rpersistentVolumeClaim:\rmetadata:\rname: sample-domain1-pvc-rwm1\rspec:\rstorageClassName: my-storage-class\rresources:\rrequests:\rstorage: 10Gi\rdomain:\rcreateIfNotExists: Domain\rdomainCreationImages:\r- image: \u0026#39;myaux:v6\u0026#39;\rdomainType: JRF\rdomainCreationConfigMap: sample-domain1-wdt-config-map\ropss:\r# Make sure you have already saved the wallet file secret. This allows the domain to use\r# an existing JRF database schemas.\rwalletFileSecret: sample-domain1-opss-walletfile-secret\rwalletPasswordSecret: sample-domain1-opss-wallet-password-secret Deploy the domain. If it is successful, then the domain has been migrated to a persistent volume. Now, you can proceed to upgrade to version 14.1.2.0, see FMW/JRF domain on PV. Sample WDT model for secured production mode and SSL If you are upgrading an existing domain to 14.1.2.0 and your existing domain does not have secured production mode enabled, the operator, by default, will disable secured production mode. If you want to override this behavior, you must enable it explicitly. Optionally, you can delete the existing domain and let the operator completely rebuild the domain and, by default, secured production mode will be enabled; you do not have to enable it explicitly.\nThe following is a code snippet of a WDT model for setting up secured production mode and SSL.\ntopology:\r# Production mode must be true for secured production mode\r#\rProductionModeEnabled: true\rSecurityConfiguration:\r# If you are updating an existing, pre 14.1.2.0 domain that does not have secure mode enabled,\r# and want to use secured production mode, make sure you enable secure mode; otherwise the\r# operator will disable it by default. #\r#\rSecureMode:\rSecureModeEnabled: true\r#\r# Make sure SSL is set up in all servers and server templates.\r#\rServer:\r\u0026#34;admin-server\u0026#34;:\rCustomTrustKeyStoreFileName: \u0026#39;wlsdeploy/servers/admin-server/trust-keystore.jks\u0026#39;\rCustomIdentityKeyStoreFileName: \u0026#39;wlsdeploy/servers/admin-server/identity-keystore.jks\u0026#39;\rKeyStores: CustomIdentityAndCustomTrust\rCustomIdentityKeyStoreType: JKS\rCustomTrustKeyStoreType: JKS CustomIdentityKeyStorePassPhraseEncrypted: CustomTrustKeyStorePassPhraseEncrypted:\rSSL:\rListenPort: 7002\rEnabled : true ServerPrivateKeyAlias: adminkey\rServerPrivateKeyPassPhraseEncrypted:\rServerTemplate:\r\u0026#34;cluster-1-template\u0026#34;:\rCustomTrustKeyStoreFileName: \u0026#39;wlsdeploy/servers/managed-server/trust-keystore.jks\u0026#39;\rCustomIdentityKeyStoreFileName: \u0026#39;wlsdeploy/servers/managed-server/identity-keystore.jks\u0026#39;\rKeyStores: CustomIdentityAndCustomTrust\rCustomIdentityKeyStoreType: JKS\rCustomTrustKeyStoreType: JKS CustomIdentityKeyStorePassPhraseEncrypted: CustomTrustKeyStorePassPhraseEncrypted:\rSSL:\rListenPort: 7102\rEnabled : true ServerPrivateKeyAlias: mykey\rServerPrivateKeyPassPhraseEncrypted: "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/domain-lifecycle/retry/",
	"title": "Domain failure retry processing",
	"tags": [],
	"description": "This document describes domain failure retry processing in the Oracle WebLogic Server in Kubernetes environment.",
	"content": "This document describes domain failure retry processing in the Oracle WebLogic Server in Kubernetes environment.\nContents Overview Domain failure severities Retry behavior Domain failure reasons Overview The WebLogic Kubernetes Operator may encounter various failures during its processing of a Domain resource. Failures are reported using Kubernetes events and conditions in the status.conditions field in the Domain resource. See Domain debugging. Failures fall into different categories and are handled differently by the operator, where most failures lead to automatic retries. Refer to Retry behavior on tuning failure retry limits and intervals.\nDomain failure severities Domain resource failures fall into three severity levels:\nWarnings Mismatch in domain spec configuration and WebLogic domain topology that usually does not prevent the domain from becoming available. For example, replicas are configured too high. Severe failures Most of the failures during domain processing are temporary failures that may either resolve at a later time without intervention, or be fixed by user actions without making any change to the domain resource or the cluster resource. The operator periodically retries when it encounters this type of failure. Examples: Introspector job time out (DeadlineExceeded error). SEVERE errors in the introspector log that do not contain the special marker string FatalIntrospectorError. Temporary network issues. Unauthorized to create some resources. An exception from the operator. Validation errors that can be fixed without changing the domain spec, for example, missing secrets or a missing ConfigMap. Fatal failures Failures that are not automatically retried. The cause of the failure must be fixed, and the retry must be manually initiated by updating the domain as described in Retry behavior. Examples: Fatal errors in the introspector log that contain the special marker string FatalIntrospectorError. Validation errors in the domain resource that require changes to the domain spec. Failures at the Severe level that have reached the expected maximum retry time. For reasons for Domain failures, see Domain failure reasons.\nClick here for an example of domain status showing a failure and its severity. Status: ... Conditions: Last Transition Time: 2022-10-10T23:48:09.157398Z Message: 10 replicas specified for cluster \u0026#39;cluster-1\u0026#39; which has a maximum cluster size of 5 10 replicas specified for cluster \u0026#39;cluster-2\u0026#39; which has a maximum cluster size of 2 Reason: ReplicasTooHigh Severity: Warning Status: True Type: Failed ... Retry behavior Domains that have failures with a severity of Fatal or Warning will not be retried. The domain status should contain a message indicating what action is needed to fix the failure condition.\nDomains failures with a severity of Severe will be retried as follows:\nThe operator calculates the next retry time based on the timestamp of the previous failure, so the next retry always occurs at the time that is the last failure timestamp plus a predefined retry interval. The timestamp of the previous failure can be found in the lastFailureTime field in the domain status. The retry interval is specified in the failureRetryIntervalSeconds field in the Domain spec. It has a default value of 120 seconds. A value of zero seconds means retry immediately after failure. The operator stops retrying a domain resource when the time elapsed since the initial failure exceeds a predefined maximum retry time. The timestamp of the initial failure can be found in the initialFailureTime field the domain status. The retry interval is specified in the failureRetryLimitMinutes field in the Domain spec. It has a default value of 1440 minutes (24 hours). A value of zero minutes will disable retries, which can be useful for accessing log files for debugging purposes. The following is an example of domain status showing a failure with pending retries. This Domain resource is configured to have a failureRetryLimitMinutes of 10 minutes. Note that the next retry is 120 seconds after the Last Failure Time, and the retry until time is 10 minutes after the Initial Failure Time.\nStatus: ... Initial Failure Time: 2022-10-11T23:16:21.851801Z Last Failure Time: 2022-10-11T23:21:53.109997Z Message: Failure on pod \u0026#39;domain1-introspector-hlvwt\u0026#39; in namespace \u0026#39;default\u0026#39;: Back-off pulling image \u0026#34;oracle/weblogic:12214\u0026#34;. Will retry next at 2022-10-11T23:23:53.109997240Z and approximately every 120 seconds afterward until 2022-10-11T23:26:21.851801Z if the failure is not resolved. In this example, all retries failed to start the domain before the predefined retry time limit, and the domain status shows a Fatal failure with Aborted reason.\nStatus: Clusters: Conditions: Last Transition Time: 2022-10-11T23:26:34.107662Z Message: The operator failed after retrying for 10 minutes. This time limit may be specified in spec.failureRetryLimitMinutes. Please resolve the error and then update domain.spec.introspectVersion to force another retry. Reason: Aborted Severity: Fatal Status: True ... To manually initiate an immediate retry, or to restart retries that have reached their spec.failureRetryLimitMinutes, update a domain field that will cause immediate action by the operator. For example, change spec.introspectVersion or spec.restartVersion as appropriate. See Startup and shutdown and Initiating introspection\nDomain failure reasons The following is a list of reasons for failures that may be encountered by the operator while processing a Domain resource.\nDomain Failure Reason Description DomainInvalid One of more configuration validation errors in the Domain resource, such as the domainUID is too long, or configuration overrides are used in a Model In Image domain. Introspection One or more SEVERE log messages is found in the introspector\u0026rsquo;s log file. Kubernetes Unrecoverable response code received from a Kubernetes API call. ServerPod One or more WebLogic Server pods failed or did not get into the ready state within a predefined maximum wait time as configured in spec.serverPod.maxReadyWaitTimeSeconds in the Domain resource, or the introspector job pod did not complete. ReplicasTooHigh The replicas field is set or changed to a value that exceeds the maximum number of servers in the WebLogic cluster configuration. Internal The operator encountered an internal exception while processing the Domain resource. TopologyMismatch One or more servers or clusters configured in the domain resource do not exist in the WebLogic domain configuration, or the monitoring exporter port is specified and it conflicts with a server port. Aborted The introspector encountered a fatal error or the operator has exceeded the maximum retry time. "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/domain-on-pv/",
	"title": "Domain on Persistent Volume (PV)",
	"tags": [],
	"description": "Create and deploy a typical domain on PV.",
	"content": " Overview Learn how to create a domain on a persistent volume.\nUsage Instructions for using Domain on PV.\nDomain creation images Domain creation images supply the WDT model for Domain on PV.\nWorking with WDT model files Learn about model file requirements, macros, and loading order.\nJRF domains Important information about using JRF domains.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/accessing-the-domain/external-clients/",
	"title": "External WebLogic clients",
	"tags": [],
	"description": "Give WebLogic applications access to WebLogic JMS or EJB resources when either the applications or their resources are located in Kubernetes.",
	"content": "Contents Overview Load balancer tunneling Adding a WebLogic custom channel When is a WebLogic custom channel needed? Configuring a WebLogic custom channel WebLogic custom channel notes Kubernetes NodePorts NodePort overview NodePort warnings NodePort steps Setting up a NodePort Sample NodePort resource Table of NodePort attributes Enabling unknown host access When is it necessary to enable unknown host access? How to enable unknown host access Cross-domain transactions RMI forwarding When is it necessary to configure RMI forwarding? How to configure RMI forwarding Configuring WebLogic Server affinity load balancing algorithms Configuring external listen addresses for WebLogic default channels Security notes Optional reading Overview If a WebLogic EJB or JMS resource is located in the same Kubernetes namespace as an application that calls the resource, then:\nNo additional configuration steps are needed.\nThe URL that the application specifies depends on both the location of the application and the location of its target resource.\nIf the application is running in a WebLogic Server JVM, and the JVM hosts the target resource or the JVM is located within the same WebLogic cluster as the target resource, then the application can simply specify the JNDI name of the resource and it should not specify a URL.\nIf the application is running outside of one of the previously listed locations, but is still in the same Kubernetes namespace as the target resource, then, in addition to a JNDI name, the application must also specify a t3 or t3s URL that includes the DNS name of the target resource\u0026rsquo;s Kubernetes ClusterIP service. To see the service DNS names for a deployed domain, run kubectl -n MYNS get services.\nExample URLs:\nFor a target resource that\u0026rsquo;s hosted anywhere in WebLogic cluster mycluster with domain UID myuid where the cluster is listening on port 7001:\nt3://myuid-cluster-mycluster:7001\nFor a target resource that\u0026rsquo;s hosted in a non-clustered WebLogic Server myserver that is part of a domain with a domain UID myuid where the server is listening on port 7001:\nt3://myuid-myserver:7001\nIf a WebLogic EJB or JMS resource is located in the same Kubernetes cluster as an application that calls the resource, but the application and resource are in different Kubernetes namespaces, then:\nThe URL used by the application can begin with t3 or t3s, and the DNS address in the URL must be fully specified to differentiate the namespace. For example:\nIf the target resource is in WebLogic cluster mycluster with domain UID myuid in namespace myns, and the cluster is listening on port 8001, then the cluster service name that is generated by the operator will be myuid-cluster-mycluster and the full URL will be t3://myuid-cluster-mycluster.myns:8001. If the target resource is a WebLogic Server myserver with domain UID myuid in namespace myns, and the server is listening on port 7001, then the server service name that is generated by the operator will be myuid-myserver and the full URL will be t3://myuid-myserver.myns:7001. The applications should access their target WebLogic Server or cluster using a WebLogic custom T3 or T3S channel that is configured on the target with the following configuration:\nSpecify a public address that matches the same fully decorated DNS address that the applications will use. For example: If the target resource is in WebLogic cluster mycluster that is part of domain with a domain UID myuid running in domain myns, then the cluster service name will be myuid-cluster-mycluster and the fully decorated name will be myuid-cluster-mycluster.myns. If the target resource is in WebLogic Server myserver that is part of domain with a domain UID myuid running in domain myns, then the server service name will be myuid-myserver and the fully decorated name will be myuid-myserver.myns. Do not specify a public port. It should be the same as the the channel\u0026rsquo;s listen port, which is the default. This is because there is no port mapping between namespaces. Do not enable outbound enabled. There is no need to enable tunneling, and you can continue to use URLs that begin with t3 or t3s. Application URLs must specify the channel\u0026rsquo;s port. If the applications run within a WebLogic Server JVM, then the WebLogic Server instances that host the target EJB or JMS resources may need to be configured to enable unknown host access. This should not be needed when the public address on the network channel is configured, as directed in the previous bulleted item.\nIf JTA transactions span between domains in different namespaces, then additional configuration is required to ensure that JTA transaction managers can communicate with each-other between the domains: configure each server\u0026rsquo;s default channel External Listen Address to a service name that is decorated with the server\u0026rsquo;s namespace. For more information, see Configuring external listen addresses for WebLogic default channels.\nIf a WebLogic EJB or JMS resource is hosted inside a Kubernetes cluster, but an application that calls the resource is hosted outside of the Kubernetes cluster, then:\nThere are two supported approaches for exposing an external address and port that the applications can use:\nLoad balancer tunneling (preferred) Kubernetes NodePorts The applications must specify a URL that resolves to the external address. If tunneling, then this URL must begin with http or https instead of t3 or t3s. For example, http://my-lb-address:my-lb-port.\nYou may need to enable unknown host access on the WebLogic Server instances that host the EJB or JMS resources.\nIf tunneling, then you may need to configure clusters that host EJB or JMS resources with a \u0026lsquo;server affinity\u0026rsquo; default load balancer algorithm. This can significantly speedup connection creation for EJB and JMS clients. See Configuring WebLogic Server affinity load balancing algorithms.\nYou may need to use RMI forwarding to support JTA transactions that involve multiple WebLogic Server domains.\nIf a WebLogic EJB or JMS resource is hosted outside of a Kubernetes cluster, and the EJB or JMS applications that call the resource are located within the cluster, then:\nYou may need to enable unknown host access on the external WebLogic Server instance or instances.\nPlus, if the target server or servers can be accessed only by tunneling through a load balancer using HTTP:\nSet up an HTTP tunneling-enabled custom channel on the external WebLogic Server instances. Specify URLs on the source server that resolve to the load balancer\u0026rsquo;s address and that start with http instead of t3. Ensure the load balancer configures the HTTP flow to be \u0026lsquo;sticky\u0026rsquo;. You may need to configure clusters that host EJB or JMS resources with a \u0026lsquo;server affinity\u0026rsquo; default load balancer algorithm. This can significantly speedup tunneling connection creation for EJB and JMS clients. See Configuring WebLogic Server affinity load balancing algorithms. You may need to use RMI forwarding to support JTA transactions that involve multiple WebLogic Server domains.\nAll DNS addresses must be \u0026lsquo;DNS-1123\u0026rsquo; compliant; this means that any DNS names created using the name of a service, pod, WebLogic Server, WebLogic cluster, and such, must be lowercase with underscores converted to dashes (hyphens). For example, if a WebLogic Server instance is named My_Server and the domain UID is MyUid, then its listen address within the namespace will be myuid-my-server. For more information on Kubernetes resource compliant naming, please see Meet Kubernetes resource name restrictions.\nLoad balancer tunneling Load balancer tunneling is the preferred approach for giving applications that are hosted outside of a Kubernetes cluster access to EJB and JMS resources that are hosted within the cluster. This approach involves configuring a network channel on the resource\u0026rsquo;s WebLogic cluster, ensuring the network channel accepts T3 protocol traffic that\u0026rsquo;s tunneled over HTTP, deploying a load balancer that redirects external HTTP network traffic to the network channel, and ensuring that the applications specify a URL that resolves the load balancer\u0026rsquo;s network address where the URL begins with http or https.\nHere are the steps:\nIn WebLogic, configure a custom channel for the T3 protocol that enables HTTP tunneling, and specifies an external address and port that correspond to the address and port that remote applications will use to access the load balancer. See Adding a WebLogic custom channel for samples and details.\nSet up a load balancer that redirects HTTP traffic to the custom channel. For more information on load balancers, see Ingress. If you\u0026rsquo;re using Oracle Container Engine for Kubernetes/Oracle Cloud Infrastructure to host your Kubernetes cluster, also see Using an Oracle Cloud Infrastructure Load Balancer.\nImportant: Ensure that the load balancer configures the HTTP flow to be \u0026lsquo;sticky\u0026rsquo; - for example, a Traefik load balancer has a sticky sessions option. This ensures that all of the packets of a tunneling client connection flow to the same pod, otherwise the connection will stall when its packets are load balanced to a different pod.\nImportant: For EJB and JMS resources that are targeted to a cluster, we recommend that you configure a default load balancer algorithm that provides server affinity (round-robin-affinity, weight-based-affinity, random-affinity) to speedup connection creation for EJB and JMS clients. See Configuring WebLogic Server affinity load balancing algorithms.\nIf you are adding access for applications that are hosted on remote WebLogic Server instances, then the Kubernetes hosted servers may need to enable unknown host access.\nRemote applications can then access the custom channel using an http:// URL instead of a t3:// URL.\nReview the Security notes.\nAdding a WebLogic custom channel The following sections describe WebLogic custom channel considerations and configuration.\nWhen is a WebLogic custom channel needed? WebLogic implicitly creates a multi-protocol default channel that spans the Listen Address and Port fields specified on each server in the cluster, but this channel is usually unsuitable for external network traffic from EJB and JMS applications. Instead, you may need to configure an additional, dedicated WebLogic custom channel to handle remote EJB or JMS application network traffic.\nA custom channel provides a way to configure an external listen address and port for use by external applications, unlike a default channel. External listen address or port configuration is needed when a channel\u0026rsquo;s configured listen address or port would not work if used to form a URL in the remote application. This is because remote EJB and JMS applications internally use their application\u0026rsquo;s channel\u0026rsquo;s configured network information to reconnect to WebLogic when needed. (The EJB and JMS applications do not always use the initial URL specified in the application\u0026rsquo;s JNDI context.)\nA custom channel can be locked down using two-way SSL as a way to prevent access by unauthorized external JMS and EJB applications, only accepts protocols that are explicitly enabled for the channel, and can be configured to be the only channel that accepts EJB/JMS applications that tunnel over HTTP. A default channel may often be deliberately unencrypted for convenient internal use, or, if used externally, is used for web traffic (not tunneling traffic) only. In addition, a default channel supports several protocols but it\u0026rsquo;s a best practice to limit the protocols that can be accessed by external applications. Finally, external applications may require access using HTTP tunneling to make connections, but it\u0026rsquo;s often inadvisable to enable tunneling for an unsecured default channel that\u0026rsquo;s already servicing external HTTP traffic. This is because enabling HTTP tunneling would potentially allow unauthorized external JMS and EJB applications unsecured access to the WebLogic cluster through the same HTTP path.\nConfiguring a WebLogic custom channel The basic requirements for configuring a custom channel for remote EJB and JMS access are:\nConfigure a T3 protocol network access point (NAP) with the same name and port on each server (the operator will set the listen address for you).\nConfigure the external listen address and port on each NAP to match the address and port component of a URL your applications can use. For example, if you are providing access to remote applications using a load balancer, then these should match the address and port of the load balancer.\nIf you want WebLogic T3 applications to tunnel through HTTP, then enable HTTP tunneling on each NAP. This is often necessary for load balancers.\nDo NOT set outbound-enabled to true on the network access point (the default is false), because this may cause internal network traffic to stall in an attempt to route through the network access point.\nFor operator controlled WebLogic clusters, ensure you haven\u0026rsquo;t enabled calculated-listen-ports for WebLogic dynamic cluster servers. The operator requires that a channel have the same port on each server in a cluster, but calculated-listen-ports causes the port to be different on each server.\nFor clusters that are not operator controlled, minimally ensure that the server\u0026rsquo;s default channel ListenAddress is configured. Oracle strongly recommends configuring a ListenAddress on all WebLogic Server instances. Note that if a NAP\u0026rsquo;s ListenAddress is left blank, then it will use the default channel\u0026rsquo;s ListenAddress. (This is not a concern for operator controlled clusters as the operator sets the listen addresses on every WebLogic Server instance.)\nFor example, here is a snippet of a WebLogic domain config.xml file for channel MyChannel defined for an operator controlled WebLogic dynamic cluster named cluster-1:\n\u0026lt;server-template\u0026gt; \u0026lt;name\u0026gt;cluster-1-template\u0026lt;/name\u0026gt; \u0026lt;listen-port\u0026gt;8001\u0026lt;/listen-port\u0026gt; \u0026lt;cluster\u0026gt;cluster-1\u0026lt;/cluster\u0026gt; \u0026lt;network-access-point\u0026gt; \u0026lt;name\u0026gt;MyChannel\u0026lt;/name\u0026gt; \u0026lt;protocol\u0026gt;t3\u0026lt;/protocol\u0026gt; \u0026lt;public-address\u0026gt;some.public.address.com\u0026lt;/public-address\u0026gt; \u0026lt;listen-port\u0026gt;7999\u0026lt;/listen-port\u0026gt; \u0026lt;public-port\u0026gt;30999\u0026lt;/public-port\u0026gt; \u0026lt;http-enabled-for-this-protocol\u0026gt;true\u0026lt;/http-enabled-for-this-protocol\u0026gt; \u0026lt;tunneling-enabled\u0026gt;true\u0026lt;/tunneling-enabled\u0026gt; \u0026lt;outbound-enabled\u0026gt;false\u0026lt;/outbound-enabled\u0026gt; \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; \u0026lt;two-way-ssl-enabled\u0026gt;false\u0026lt;/two-way-ssl-enabled\u0026gt; \u0026lt;client-certificate-enforced\u0026gt;false\u0026lt;/client-certificate-enforced\u0026gt; \u0026lt;/network-access-point\u0026gt; \u0026lt;/server-template\u0026gt; \u0026lt;cluster\u0026gt; \u0026lt;name\u0026gt;cluster-1\u0026lt;/name\u0026gt; \u0026lt;cluster-messaging-mode\u0026gt;unicast\u0026lt;/cluster-messaging-mode\u0026gt; \u0026lt;dynamic-servers\u0026gt; \u0026lt;name\u0026gt;cluster-1\u0026lt;/name\u0026gt; \u0026lt;server-template\u0026gt;cluster-1-template\u0026lt;/server-template\u0026gt; \u0026lt;maximum-dynamic-server-count\u0026gt;5\u0026lt;/maximum-dynamic-server-count\u0026gt; \u0026lt;calculated-listen-ports\u0026gt;false\u0026lt;/calculated-listen-ports\u0026gt; \u0026lt;server-name-prefix\u0026gt;managed-server\u0026lt;/server-name-prefix\u0026gt; \u0026lt;dynamic-cluster-size\u0026gt;5\u0026lt;/dynamic-cluster-size\u0026gt; \u0026lt;max-dynamic-cluster-size\u0026gt;5\u0026lt;/max-dynamic-cluster-size\u0026gt; \u0026lt;/dynamic-servers\u0026gt; \u0026lt;/cluster\u0026gt; Here is a snippet of offline WLST code that corresponds to the previous config.xml file snippet:\ntemplateName = \u0026#34;cluster-1-template\u0026#34; cd(\u0026#39;/ServerTemplates/%s\u0026#39; % templateName) templateChannelName = \u0026#34;MyChannel\u0026#34; create(templateChannelName, \u0026#39;NetworkAccessPoint\u0026#39;) cd(\u0026#39;NetworkAccessPoints/%s\u0026#39; % templateChannelName) set(\u0026#39;Protocol\u0026#39;, \u0026#39;t3\u0026#39;) set(\u0026#39;ListenPort\u0026#39;, 7999) set(\u0026#39;PublicPort\u0026#39;, 30999) set(\u0026#39;PublicAddress\u0026#39;, \u0026#39;some.public.address.com\u0026#39;) set(\u0026#39;HttpEnabledForThisProtocol\u0026#39;, true) set(\u0026#39;TunnelingEnabled\u0026#39;, true) set(\u0026#39;OutboundEnabled\u0026#39;, false) set(\u0026#39;Enabled\u0026#39;, true) set(\u0026#39;TwoWaySslEnabled\u0026#39;, false) set(\u0026#39;ClientCertificateEnforced\u0026#39;, false) Here is a snippet of WDT model YAML file configuration that corresponds to the previous snippets:\ntopology: Cluster: \u0026#39;cluster-1\u0026#39;: DynamicServers: ServerTemplate: \u0026#39;cluster-1-template\u0026#39; ServerNamePrefix: \u0026#39;managed-server\u0026#39; DynamicClusterSize: \u0026#39;5\u0026#39; MaxDynamicClusterSize: \u0026#39;5\u0026#39; MinDynamicClusterSize: \u0026#39;0\u0026#39; CalculatedListenPorts: false ServerTemplate: \u0026#39;cluster-1-template\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 NetworkAccessPoint: MyT3Channel: Protocol: \u0026#39;t3\u0026#39; ListenPort: 7999 PublicPort: 30999 PublicAddress: \u0026#39;some.public.address.com\u0026#39; HttpEnabledForThisProtocol: true TunnelingEnabled: true OutboundEnabled: false Enabled: true TwoWaySSLEnabled: false ClientCertificateEnforced: false In this example:\nWebLogic binds the custom network channel to port 7999 and the default network channel to 8001.\nThe operator will automatically create a Kubernetes Service named DOMAIN_UID-cluster-cluster-1 for both the custom and default channel.\nThe operator will automatically set the ListenAddress on each WebLogic Server instance for each of its channels.\nInternal applications running in the same Kubernetes cluster as the channel can access the cluster using t3://DOMAIN_UID-cluster-cluster-1:8001.\nExternal applications would be expected to access the cluster using the custom channel with URLs like t3://some.public.address.com:30999 or, if using tunneling, http://some.public.address.com:30999.\nWebLogic custom channel notes Channel configuration for a configured cluster requires configuring the same network access point on each server. The operator currently doesn\u0026rsquo;t test or support network channels that have a different configuration on each server in the cluster.\nAdditional steps are required for external applications beyond configuring the custom channel - see Overview.\nKubernetes NodePorts The following sections provide detailed information about Kubernetes NodePorts.\nNodePort overview Kubernetes NodePorts provide an alternative approach for giving external WebLogic EJB or JMS applications access to a Kubernetes hosted WebLogic cluster. This approach involves configuring a network channel on the desired WebLogic cluster that accepts T3 protocol traffic, and exposing a Kubernetes NodePort that redirects external network traffic on the Kubernetes Nodes to the network channel.\nNodePort warnings Although Kubernetes NodePorts are good for use in demos and getting-started guides, they are usually not suited for production systems for multiple reasons, including:\nThey can directly expose internal applications to the outside world. They bypass almost all network security in Kubernetes. They allow all protocols (load balancers can limit to the HTTP protocol). They cannot expose standard, low-numbered ports like 80 and 443 (or even 8080 and 8443). Some Kubernetes cloud environments cannot expose usable NodePorts because their Kubernetes clusters run on a private network that cannot be reached by external clients. Load balancer tunneling is the preferred approach over Kubernetes NodePorts.\nNodePort steps Here are the high-level steps:\nReview NodePort warnings.\nIn WebLogic, configure a custom channel for the T3 protocol that specifies an external address and port that are suitable for remote application use. See Adding a WebLogic custom channel.\nDefine a Kubernetes NodePort to publicly expose the WebLogic ports. See Setting up a NodePort.\nIf you are adding access for remote WebLogic Server instances, then the Kubernetes hosted servers may need to enable unknown host access.\nReview the Security notes.\nSetting up a NodePort A Kubernetes NodePort exposes a port on each worker node in the Kubernetes cluster (they are not typically exposed on masters), where the port is accessible from outside of a Kubernetes cluster. This port redirects network traffic to pods within the Kubernetes cluster. Setting up a Kubernetes NodePort is one approach for giving external WebLogic applications access to JMS or EJBs.\nIf an EJB or JMS service is running on an Administration Server, then you can skip the rest of this section and use the spec.adminServer.adminService.channels Domain field to have the operator create a NodePort for you. See Reference - Domain. Otherwise, if the EJB or JMS service is running in a WebLogic cluster or standalone WebLogic Server Managed Server, and you desire to provide access to the service using a NodePort, then the NodePort must be exposed \u0026lsquo;manually\u0026rsquo; - see the following sample and table.\nSetting up a NodePort usually also requires setting up a custom network channel. See Adding a WebLogic custom channel.\nSample NodePort resource The following NodePort YAML file exposes an external node port of 30999 and internal port 7999 for a domain UID of DOMAIN_UID, a domain name of DOMAIN_NAME, and a cluster name of CLUSTER_NAME. It assumes that 7999 corresponds to a T3 protocol port of a channel that\u0026rsquo;s configured on your WebLogic cluster.\napiVersion: v1 kind: Service metadata: namespace: default name: DOMAIN_UID-cluster-CLUSTER_NAME-ext labels: weblogic.domainUID: DOMAIN_UID spec: type: NodePort externalTrafficPolicy: Cluster sessionAffinity: ClientIP selector: weblogic.domainUID: DOMAIN_UID weblogic.clusterName: CLUSTER_NAME ports: - name: myclustert3channel nodePort: 30999 port: 7999 protocol: TCP targetPort: 7999 Table of NodePort attributes Attribute Description metadata.name For this particular use case, the NodePort name can be arbitrary as long as it is DNS compatible. But, as a convention, it\u0026rsquo;s recommended to use DOMAIN_UID-cluster-CLUSTER_NAME-ext. To ensure the name is DNS compatible, use all lowercase and convert any underscores (_) to dashes (-). metadata.namespace Must match the namespace of your WebLogic cluster. metadata.labels Optional. It\u0026rsquo;s helpful to set a weblogic.domainUid label so that cleanup scripts can locate all Kubernetes resources associated with a particular domain UID. spec.type Must be NodePort. spec.externalTrafficPolicy Set to Cluster for most use cases. This may lower performance, but ensures that a client that attaches to a node without any pods that match the spec.selector will be rerouted to a node with pods that do match. If set to Local, then connections to a particular node will route only to that node\u0026rsquo;s pods and will fail if the node doesn\u0026rsquo;t host any pods with the given spec.selector. It\u0026rsquo;s recommended for applications of a spec.externalTrafficPolicy: Local NodePort to use a URL that resolves to a list of all nodes, such as t3://mynode1,mynode2:30999, so that a client connect attempt will implicitly try mynode2 if mynode1 fails (alternatively, use a round-robin DNS address in place of mynode1,mynode2). spec.sessionAffinity Set to ClientIP to ensure an HTTP tunneling connection always routes to the same pod, otherwise the connection may hang and fail. spec.selector Specifies a weblogic.domainUID and weblogic.clusterName to associate the NodePort resource with your cluster\u0026rsquo;s pods. The operator automatically sets these labels on the WebLogic cluster pods that it deploys for you. spec.ports.name This name is arbitrary. spec.ports.nodePort The external port that applications will use. This must match the external port that\u0026rsquo;s configured on the WebLogic configured channels/network access points. By default, Kubernetes requires that this value range from 30000 to 32767. spec.ports.port and spec.targetPort These must match the port that\u0026rsquo;s configured on the WebLogic configured channel/network access points. Enabling unknown host access The following sections describe when and how to enable unknown host access.\nWhen is it necessary to enable unknown host access? If a source WebLogic Server attempts to initiate an EJB, JMS, or JTA connection with a target WebLogic Server, then the target WebLogic Server will reject the connection by default, if it cannot find the source server\u0026rsquo;s listen address in its DNS. Such a failed connection attempt can yield log messages or exceptions like \u0026quot;...RJVM has already been shutdown...\u0026quot; or \u0026quot;...address was valid earlier, but now we get...\u0026quot;.\nThis means that it\u0026rsquo;s usually necessary to enable unknown host access on an external WebLogic Server instance so that it can support EJB, JMS, or JTA communication that is initiated by an operator hosted WebLogic Server. For example, if an operator hosted WebLogic Server with service address mydomainuid-myservername initiates a JMS connection to a remote WebLogic Server, then the remote server will implicitly attempt to look up mydomainuid-myservername in its DNS as part of the connection setup, and this lookup will typically fail.\nSimilarly, this also means that it\u0026rsquo;s necessary to enable unknown host access on an operator hosted WebLogic Server that accepts EJB or JMS connection requests from external WebLogic Server instances when the external WebLogic Server\u0026rsquo;s listen addresses cannot be resolved by the DNS running in the Kuberneters cluster.\nHow to enable unknown host access To enable an \u0026lsquo;unknown host\u0026rsquo; source WebLogic Server to initiate EJB, JMS, or JTA communication with a target WebLogic Server:\nSet the weblogic.rjvm.allowUnknownHost Java system property to true on each target WebLogic Server instance. For operator hosted WebLogic Server instances, you can set this property by including -Dweblogic.rjvm.allowUnknownHost=true in the JAVA_OPTIONS Domain environment variable defined in the domain resource\u0026rsquo;s spec.serverPod.env attribute. Also apply patch 30656708 on each target WebLogic Server instance for versions 12.2.1.4 (PS4) or earlier. Cross-domain transactions JTA transactions that span WebLogic Server domains are referred to as cross-domain transactions. The WebLogic Server Transaction Manager (TM) requires direct server-to-server communications with all other global transaction server participants. For a Kubernetes hosted WebLogic cluster, this requires each server in the cluster to be individually addressable. This conflicts with the current operator requirement that a network channel in a cluster have the same port across all servers in the cluster, and the fact that we use cluster service to load balance communication in the WebLogic Server cluster.\nRMI forwarding RMI forwarding makes cross-domain transactions possible without requiring each server in a cluster to be individually addressable.\nIt requires the use of a proxy, such as a load balancer or an Ingress, that is configured to route messages to servers in the target WebLogic Server domain.\nWhen a T3 message, such as transaction coordination RMI calls issued by the WebLogic Server TM, is sent from a source WebLogic Server to a target WebLogic Server in a different WebLogic Server domain:\nThe source WebLogic Server chooses the URL of a proxy based on the destination address of the message, and sends the message to that URL. The proxy then routes the message to one of the WebLogic Servers in the target WebLogic Server domain according to its routing rules. Note that the message may or may not be routed to the target WebLogic Server. If the message is routed to a WebLogic Server other than the target WebLogic Server, that WebLogic Server will forward the message to the intended recipient within the same WebLogic Server domain. When is it necessary to configure RMI forwarding? RMI forwarding may be needed when transaction server participants are not able to establish connection directly with one another.\nFor example, if a cross-domain transaction communication has failed, and messages such as the following can be found in the WebLogic server log files:\n\u0026lt;BEA-111015\u0026gt; \u0026lt;The commit operation for transaction BEA1-0000993203DB6CDB7DE9 timed out after 30 seconds.\u0026gt;\nWith additional JTA debugging turned on using the Java system property -Dweblogic.debug.DebugJTA2PC=true, messages, such as the following, confirm that the failed transaction is caused by transaction participants not being able to reach the transaction coordinator:\n...\u0026lt;startPrepare FAILED javax.transaction.SystemException: Could not obtain coordinator at managed-server1+domain1-managed-server1:8001+domain1+t3+...\nHow to configure RMI forwarding Configure a proxy for each WebLogic Server domain that is a participant in global transactions but is not reachable by all other global transaction participants, because the listen addresses of the WebLogic Server instances in the domain cannot be resolved by the DNS from other participants. For example, if servers in domain1 cannot be reached from servers in domain2, configure a proxy for domain1 such that messages sent from servers in domain2 can be routed to servers in domain1 through the proxy.\nIn each source WebLogic Server domain, specify the URL for the proxy configured for each target WebLogic Server domain:\nSet the Java system property as weblogic.rjvm.domain.proxy.\u0026lt;prefix\u0026gt; where \u0026lt;prefix\u0026gt; is the domain UID of the target WebLogic Server domain. Multiple Java system properties with different values of \u0026lt;prefix\u0026gt; can be specified. For WebLogic Server domains running in Kubernetes managed by the operator, you can set this property by including the system property in the JAVA_OPTIONS Domain environment variable defined in the domain resource\u0026rsquo;s spec.serverPod.env attribute. For example, if the URL of the proxy for domain1 is t3://proxy-host:31234, specify Java system property -Dweblogic.rjvm.domain.proxy.domain1=t3://proxy-host:31234 in WebLogic Server domain2. A message originating from WebLogic Server instances in domain2 that are addressed to the host name that starts with domain1, such as t3://domain1-managed-server1:8001, will be sent to the proxy at t3://proxy-host:31234. The proxy then routes the message to a WebLogic Server instance in domain2, and RMI forwarding will ensure that the message will reach WebLogic Server managed-server1 in domain1.\nPatch 32408938 is required in each WebLogic Server instance that participates in cross-domain transactions, or that is the routing destination of a proxy. The patch is available for WebLogic versions 12.2.1.4.0 (PS4), 14.1.1.0.0 and 14.1.2.0.0, and is already included in the PSUs for these releases since July 2022.\nConfiguring WebLogic Server affinity load balancing algorithms When providing external clients access to EJB and JMS resources using load balancer tunneling, we recommend that you configure a server affinity load balancing algorithm for the cluster in which the resources are targeted. Using a server affinity based algorithm reduces the amount of time it takes for EJB and JMS standalone or server hosted clients to establish a connection through a tunneling port. It does this by ensuring that the clients prefer communicating with a target server instance which already has an established connection to the client, where this server instance is implicitly picked when such clients create their JNDI context. See Load Balancing for EJBs and RMI Objects in Load Balancing in a Cluster.\nHere is a snippet of offline WLST code for enabling server affinity in a cluster \u0026lsquo;cluster-1\u0026rsquo;:\nclusterName = \u0026#34;cluster-1\u0026#34; cd(\u0026#39;/Clusters/%s\u0026#39; % clusterName) set(\u0026#39;DefaultLoadAlgorithm\u0026#39;, \u0026#39;round-robin-affinity\u0026#39;) Here is a snippet of WDT model YAML file configuration for enabling server affinity in a cluster \u0026lsquo;cluster-1\u0026rsquo;:\ntopology: Cluster: \u0026#39;cluster-1\u0026#39;: DynamicServers: ServerTemplate: \u0026#39;cluster-1-template\u0026#39; ServerNamePrefix: \u0026#39;managed-server\u0026#39; DynamicClusterSize: \u0026#39;5\u0026#39; MaxDynamicClusterSize: \u0026#39;5\u0026#39; MinDynamicClusterSize: \u0026#39;0\u0026#39; CalculatedListenPorts: false DefaultLoadAlgorithm: \u0026#39;round-robin-affinity\u0026#39; Configuring external listen addresses for WebLogic default channels WebLogic Server provides an external listen address/DNS name feature to enable external JMS and EJB clients to properly reestablish and/or load balance connections to servers with listen addresses that aren\u0026rsquo;t directly available in the client\u0026rsquo;s DNS (such as through a firewall or a load balancer, or between different namespaces in Kubernetes). For more information on external listen addresses, see the ExternalDNSName attribute in the ServerMBean reference.\nHere is a snippet of offline WLST code for setting the external listen address for a standalone server named AdminServer in namespace weblogic-domain for domain-uid domain1:\nserverName = \u0026#34;AdminServer\u0026#34; domainUid = \u0026#34;domain1\u0026#34; nameSpace = \u0026#34;weblogic-domain\u0026#34; address = domainUid + \u0026#39;_\u0026#39; + serverName + \u0026#39;_\u0026#39; + nameSpace externalDNSName = address.lower().replace(\u0026#39;_\u0026#39;,\u0026#39;-\u0026#39;) # DNS names must be DNS-1123 compliant cd(\u0026#39;/Servers/%s\u0026#39; % serverName) set(\u0026#39;ExternalDNSName\u0026#39;, externalDNSName) Here is a snippet of offline WLST code for setting the external listen address for a server template named cluster-1-template in namespace weblogic-domain for domain-uid domain1:\ntemplateName = \u0026#34;cluster-1-template\u0026#34; serverName = \u0026#34;managed-server${id}\u0026#34; domainUid = \u0026#34;domain1\u0026#34; nameSpace = \u0026#34;weblogic-domain\u0026#34; address = domainUid + \u0026#39;_\u0026#39; + serverName + \u0026#39;_\u0026#39; + nameSpace externalDNSName = address.lower().replace(\u0026#39;_\u0026#39;,\u0026#39;-\u0026#39;) # DNS names must be DNS-1123 compliant cd(\u0026#39;/ServerTemplates/%s\u0026#39; % templateName) set(\u0026#39;ExternalDNSName\u0026#39;, externalDNSName) The server name contains the server template macro \u0026ldquo;${id}\u0026rdquo; in which the correct instance ID will be substituted by the WebLogic Server runtime. See Using Macros in Server Templates.\nHere is a snippet of WDT model YAML file configuration to set the ExternalDNSName attribute for both a standalone server and for a server template:\ntopology: Cluster: \u0026#34;cluster-1\u0026#34;: DynamicServers: ServerTemplate: \u0026#34;cluster-1-template\u0026#34; ServerNamePrefix: \u0026#34;managed-server\u0026#34; DynamicClusterSize: 5 MaxDynamicClusterSize: 5 CalculatedListenPorts: false Server: \u0026#34;admin-server\u0026#34;: ListenPort: 7001 ExternalDNSName: \u0026#39;@@ENV:DOMAIN_UID@@-admin-server.@@ENV:NAMESPACE@@\u0026#39; ServerTemplate: \u0026#34;cluster-1-template\u0026#34;: Cluster: \u0026#34;cluster-1\u0026#34; ListenPort : 8001 ExternalDNSName: \u0026#39;@@ENV:DOMAIN_UID@@-managed-server${id}.@@ENV:NAMESPACE@@\u0026#39; In the previous example, DOMAIN_UID and NAMESPACE are assumed to already be \u0026lsquo;DNS-1123\u0026rsquo; compliant. Alternatively, you can substitute the macros with DNS-1123 acceptable values (changed to lowercase and underscores converted to dashes). For more information on Kubernetes resource compliant naming, please see Meet Kubernetes resource name restrictions.\nSecurity notes With some cloud providers, a load balancer or NodePort may implicitly expose a port to the public Internet. See also NodePort warnings.\nIf an externally available port supports a protocol suitable for WebLogic applications, note that WebLogic allows access to JNDI entries, EJB/RMI applications, and JMS by anonymous users by default.\nYou can configure a custom channel with a secure protocol and two-way SSL to help prevent external access by unwanted applications. See When is a WebLogic custom channel needed?.\nFor a detailed description of external network access security, see External network access security.\nOptional reading For a description of the WebLogic URL syntax for JMS, EJB, and JNDI applications see Understanding WebLogic URLs.\nIf JMS applications need to specify a URL and the applications are hosted on a WebLogic Server or cluster, then, as a best practice, the application should simply be coded to use local JNDI names for its JMS Connection Factories and JMS Destinations, and these local JNDI names should be configured in WebLogic to map to the remote location using a Foreign JMS Server. A Foreign JMS Server maps the local JNDI name of a JMS Connection Factory or JMS Destination to a remote JNDI name located at a different URL, where the local JNDI name, remote JNDI name, remote user name, remote password, and remote URL are all configurable. See Integrating Remote JMS Destinations.\nFor a detailed description of using T3 in combination with port mapping, see T3 RMI Communication for WebLogic Server Running on Kubernetes.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/security/openshift/",
	"title": "OpenShift",
	"tags": [],
	"description": "OpenShift information for the operator.",
	"content": "Set the Helm chart property kubernetesPlatform to OpenShift Beginning with operator version 3.3.2, set the operator kubernetesPlatform Helm chart property to OpenShift. This property accommodates OpenShift security requirements. Specifically, the operator\u0026rsquo;s deployment and any pods created by the operator for WebLogic Server instances will not contain runAsUser: 1000 in the configuration of the securityContext. This is to accommodate OpenShift\u0026rsquo;s default restricted security context constraint. For more information, see Operator Helm configuration values.\nUse a dedicated namespace When the user that installs an individual instance of the operator does not have the required privileges to create resources at the Kubernetes cluster level, they can use a Dedicated namespace selection strategy for the operator instance to limit it to managing domain resources in its local namespace only (see Operator namespace management), and they may need to manually install the Domain Custom Resource (CRD) (see Prepare for installation).\nWebLogic Kubernetes Operator expects to connect to the auto-created services for each domain it manages. Any networking configuration in place must allow that connection because without it, the operator cannot accurately report the status of the domain. For more information, see MOS 2988024.1.\nWith WIT, set the target parameter to OpenShift When using the WebLogic Image Tool (WIT), create, rebase, or update command, to create a Domain in Image domain home, Model in Image image, or Model in Image auxiliary image, you can specify the --target parameter for the target Kubernetes environment. Its value can be either Default or OpenShift. The OpenShift option changes the domain directory files such that the group permissions for those files will be the same as the user permissions (group writable, in most cases). If you do not supply the OS group and user setting with --chown, then the Default setting for this option is changed from oracle:oracle to oracle:root to be in line with the expectations of an OpenShift environment.\nSecurity requirements to run WebLogic in OpenShift WebLogic Kubernetes Operator images starting with version 3.1 and WebLogic Server or Fusion Middleware Infrastructure images obtained from Oracle Container Registry after August 2020 have an oracle user with UID 1000 with the default group set to root.\nHere is an excerpt from a standard WebLogic Dockerfile that demonstrates how the file system group ownership is configured in the standard WebLogic Server images:\n# Setup filesystem and oracle user # Adjust file permissions, go to /u01 as user \u0026#39;oracle\u0026#39; to proceed with WLS installation # ------------------------------------------------------------ RUN mkdir -p /u01 \u0026amp;\u0026amp; \\ chmod 775 /u01 \u0026amp;\u0026amp; \\ useradd -b /u01 -d /u01/oracle -m -s /bin/bash oracle \u0026amp;\u0026amp; \\ chown oracle:root /u01 COPY --from=builder --chown=oracle:root /u01 /u01 OpenShift, by default, enforces the restricted security context constraint which allocates a high, random UID in the root group for each container. The standard images mentioned previously are designed to work with the restricted security context constraint.\nHowever, if you build your own image, have an older version of an image, or obtain an image from another source, it may not have the necessary permissions. You may need to configure similar file system permissions to allow your image to work in OpenShift. Specifically, you need to make sure the following directories have root as their group, and that the group read, write and execute permissions are set (enabled):\nFor the operator, /operator and /logs. For WebLogic Server and Fusion Middleware Infrastructure images, /u01 (or the ultimate parent directory of your Oracle Home and domain if you put them in different locations). For additional information about OpenShift requirements and the operator, see OpenShift.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-operators/service-accounts/",
	"title": "Service accounts",
	"tags": [],
	"description": "Kubernetes ServiceAccounts for the operator.",
	"content": "WebLogic Kubernetes Operator ServiceAccounts When the operator is installed, the Helm chart property, serviceAccount, can be specified where the value contains the name of the Kubernetes ServiceAccount in the namespace in which the operator will be installed.\nThe operator will use this service account when calling the Kubernetes API server and the appropriate access controls will be created for this ServiceAccount by the operator\u0026rsquo;s Helm chart.\nTo display the service account used by the operator, where the operator was installed using the Helm release name weblogic-operator, look for the serviceAccount value using the Helm command:\n$ helm get values --all weblogic-operator If the operator\u0026rsquo;s service account cannot have the privileges to access the cluster-level resources, such as CustomResourceDefinitions, Namespaces, and PersistentVolumes, then consider using the same dedicated namespace for each operator and the domains that each operator manages. See the Dedicated option for the domainNamespaceSelectionStrategy setting.\nAdditional reading See Prepare an operator namespace and service account. See the operator Helm chart serviceAccount setting. For more information about access controls, see RBAC. "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/model-in-image/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "To remove the resources you have created in these samples:\nDelete the resources associated with the domain.\n$ /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/delete-domain/delete-weblogic-domain-resources.sh -d sample-domain1 $ /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/delete-domain/delete-weblogic-domain-resources.sh -d sample-domain2 This deletes the domain and any related resources that are labeled with the domain UID sample-domain1 and sample-domain2.\nIt leaves the namespace intact, the operator running, the load balancer running (if installed), and the database running (if installed).\nNOTE: When you delete a domain, the operator will detect your domain deletion and shut down its pods. Wait for these pods to exit before deleting the operator that monitors the sample-domain1-ns namespace. You can monitor this process using the command kubectl get pods -n sample-domain1-ns --watch (ctrl-c to exit).\nIf you set up the Traefik ingress controller:\n$ helm uninstall traefik-operator -n traefik $ kubectl delete namespace traefik If you set up a database for the Update 4 use case:\n$ /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oracle-db-service/stop-db-service.sh Delete the operator and its namespace:\n$ helm uninstall sample-weblogic-operator -n sample-weblogic-operator-ns $ kubectl delete namespace sample-weblogic-operator-ns Delete the domain\u0026rsquo;s namespace:\n$ kubectl delete namespace sample-domain1-ns Delete the images you may have created in this sample:\n$ docker image rm wdt-domain-image:WLS-v1 $ docker image rm wdt-domain-image:WLS-v2 "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/delete-domain/",
	"title": "Delete resources associated with the domain",
	"tags": [],
	"description": "Delete the Kubernetes resources associated with the domain created while executing the samples.",
	"content": "After running the samples, you will need to release resources associated with the domain that can then be used for other purposes. The script in this sample demonstrates one approach to releasing these resources.\nUse this script to delete resources associated with the domain $ ./delete-weblogic-domain-resources.sh \\ -d domain-uid[,domain-uid...] \\ [-s max-seconds] \\ [-t] The required option -d takes domain-uid values (separated by commas and no spaces) to identify the resources that should be deleted.\nTo limit the amount of time spent on attempting to delete resources, use -s. The option must be followed by an integer that represents the total number of seconds that will be spent attempting to delete resources. The default number of seconds is 120.\nThe optional -t option shows what the script will delete without executing the deletion.\nTo see the help associated with the script:\n$ ./delete-weblogic-domain-resources.sh -h "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/developerguide/asynchronous-call-model/",
	"title": "Asynchronous call model",
	"tags": [],
	"description": "The operator employs an efficient, user-level threads pattern to implement an asynchronous call model for Kubernetes API requests.",
	"content": "Our expectation is that customers will task the operator with managing hundreds of WebLogic domains across dozens of Kubernetes Namespaces. Therefore, we have designed the operator with an efficient user-level threads pattern. We\u0026rsquo;ve used that pattern to implement an asynchronous call model for Kubernetes API requests. This call model has built-in support for timeouts, retries with exponential back-off, and lists that exceed the requested maximum size using the continuance functionality.\nUser-level thread pattern The user-level thread pattern is implemented by the classes in the oracle.kubernetes.operator.work package.\nEngine: The executor service and factory for Fibers. Fiber: The user-level thread. Fibers represent the execution of a single processing flow through a series of Steps. Fibers may be suspended and later resumed, and do not consume a Thread while suspended. Step: Individual CPU-bound activity in a processing flow. Packet: Context of the processing flow. NextAction: Used by a Step when it returns control to the Fiber to indicate what should happen next. Common \u0026rsquo;next actions\u0026rsquo; are to execute another Step or to suspend the Fiber. Component: Provider of SPI\u0026rsquo;s that may be useful to the processing flow. Container: Represents the containing environment and is a Component. Each Step has a reference to the next Step in the processing flow; however, Steps are not required to indicate that the next Step be invoked by the Fiber when the Step returns a NextAction to the Fiber. This leads to common use cases where Fibers invoke a series of Steps that are linked by the \u0026lsquo;is-next\u0026rsquo; relationship, but just as commonly, use cases where the Fiber will invoke sets of Steps along a detour before returning to the normal flow.\nIn this sample, the caller creates an Engine, Fiber, linked set of Step instances, and Packet. The Fiber is then started. The Engine would typically be a singleton, since it\u0026rsquo;s backed by a ScheduledExecutorService. The Packet would also typically be pre-loaded with values that the Steps would use in their apply() methods.\nstatic class SomeClass { public static void main(String[] args) { Engine engine = new Engine(\u0026#34;worker-pool\u0026#34;); Fiber fiber = engine.createFiber(); Step step = new StepOne(new StepTwo(new StepThree(null))); Packet packet = new Packet(); fiber.start( step, packet, new CompletionCallback() { @Override public void onCompletion(Packet packet) { // Fiber has completed successfully } @Override public void onThrowable(Packet packet, Throwable throwable) { // Fiber processing was terminated with an exception } }); } } Steps must not invoke sleep or blocking calls from within apply(). This prevents the worker threads from serving other Fibers. Instead, use asynchronous calls and the Fiber suspend/resume pattern. Step provides a method, doDelay(), which creates a NextAction to drive Fiber suspend/resume that is a better option than sleep precisely because the worker thread can serve other Fibers during the delay. For asynchronous IO or similar patterns, suspend the Fiber. In the callback as the Fiber suspends, initiate the asynchronous call. Finally, when the call completes, resume the Fiber. The suspend/resume functionality handles the case where resumed before the suspending callback completes.\nIn this sample, the step uses asynchronous file IO and the suspend/resume Fiber pattern.\nstatic class StepTwo extends Step { public StepTwo(Step next) { super(next); } @Override public NextAction apply(Packet packet) { return doSuspend((fiber) -\u0026gt; { // The Fiber is now suspended // Start the asynchronous call try { Path path = Paths.get(URI.create(this.getClass().getResource(\u0026#34;/somefile.dat\u0026#34;).toString())); AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.READ); ByteBuffer buffer = ByteBuffer.allocate(1024); fileChannel.read(buffer, 0, buffer, new CompletionHandler\u0026lt;Integer, ByteBuffer\u0026gt;() { @Override void completed(Integer result, ByteBuffer attachment) { // Store data in Packet and resume Fiber packet.put(\u0026#34;DATA_SIZE_READ\u0026#34;, result); packet.put(\u0026#34;DATA_FROM_SOMEFILE\u0026#34;, attachment); fiber.resume(packet); } @Override public void failed(Throwable exc, ByteBuffer attachment) { // log exc completed(0, null); } }); } catch (IOException e) { // log exception // If not resumed here, Fiber will never be resumed } }); } } Call builder pattern The asynchronous call model is implemented by classes in the oracle.kubernetes.operator.helpers package, including CallBuilder and ResponseStep. The model is based on the Fiber suspend/resume pattern described previously. CallBuilder provides many methods having names ending with \u0026ldquo;Async\u0026rdquo;, such as listPodAsync() or deleteServiceAsync(). These methods return a Step that can be returned as part of a NextAction. When creating these Steps, the developer must provide a ResponseStep. Only ResponseStep.onSuccess() must be implemented; however, it is often useful to override onFailure() as Kubernetes treats 404 (Not Found) as a failure.\nIn this sample, the developer is using the pattern to list pods from the default namespace that are labeled as part of cluster-1.\nstatic class StepOne extends Step { public StepOne(Step next) { super(next); } @Override public NextAction apply(Packet packet) { String namespace = \u0026#34;default\u0026#34;; Step step = CallBuilder.create().with($ -\u0026gt; { $.labelSelector = \u0026#34;weblogic.clusterName=cluster-1\u0026#34;; $.limit = 50; $.timeoutSeconds = 30; }).listPodAsync(namespace, new ResponseStep\u0026lt;V1PodList\u0026gt;(next) { @Override public NextAction onFailure(Packet packet, ApiException e, int statusCode, Map\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; responseHeaders) { if (statusCode == CallBuilder.NOT_FOUND) { return onSuccess(packet, null, statusCode, responseHeaders); } return super.onFailure(packet, e, statusCode, responseHeaders); } @Override NextAction onSuccess(Packet packet, V1PodList result, int statusCode, Map\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; responseHeaders) { // do something with the result Pod, if not null return doNext(packet); } }); return doNext(step, packet); } } Notice that the required parameters, such as namespace, are method arguments, but optional parameters are designated using a simplified builder pattern using with() and a lambda.\nThe default behavior of onFailure() will retry with an exponential backoff the request on status codes 429 (TooManyRequests), 500 (InternalServerError), 503 (ServiceUnavailable), 504 (ServerTimeout) or a simple timeout with no response from the server.\nIf the server responds with status code 409 (Conflict), then this indicates an optimistic locking failure. Common use cases are that the code read a Kubernetes object in one asynchronous step, modified the object, and attempted to replace the object in another asynchronous step; however, another activity replaced that same object in the interim. In this case, retrying the request would give the same result. Therefore, developers may provide an \u0026ldquo;on conflict\u0026rdquo; step when calling super.onFailure(). The conflict step will be invoked after an exponential backoff delay. In this example, that conflict step should be the step that reads the existing Kubernetes object.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/developerguide/",
	"title": "Developer Guide",
	"tags": [],
	"description": "",
	"content": "The Developer Guide provides information for developers who want to understand or contribute to the code.\nContribute to the operator Learn how to contribute to the operator project.\nRequirements Review the software requirements to obtain and build the operator.\nBuilding Learn about the operator build process.\nIntegration tests Learn about the available Java integration tests.\nBranching Understand the operator repository branching strategy.\nCoding standards Review the project coding standards.\nCode structure Review the project directory structure.\nAsynchronous call model The operator employs an efficient, user-level threads pattern to implement an asynchronous call model for Kubernetes API requests.\nDomain processing Understand how the operator processes domains and creates resources.\nDocumentation Learn how to contribute to the operator documentation.\nBackward compatibility Review the operator\u0026#39;s backward compatibility and maintenance.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/domain-lifecycle/scripts/",
	"title": "Lifecycle scripts",
	"tags": [],
	"description": "A collection of useful domain lifecycle sample scripts.",
	"content": "Beginning in version 3.1.0, the operator provides sample scripts to start up or shut down a specific Managed Server or cluster in a deployed domain, or the entire deployed domain.\nVersions 3.2 and 3.3 have subsequently added sample scripts for restarting a server, scaling a cluster, rolling a domain or a cluster, monitoring a cluster, and reinitiating introspection.\nThe scripts are located in the kubernetes/samples/scripts/domain-lifecycle directory. They are helpful when scripting the life cycle of a WebLogic Server domain.\nFor more information, see the README.\nNOTE: Prior to running these scripts, you must have previously created and deployed the domain.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/accessing-the-domain/domain-events/",
	"title": "Domain events",
	"tags": [],
	"description": "Monitor domain resources using operator-generated events about resources that it manages.",
	"content": "Contents Overview What\u0026rsquo;s new Operator-generated event types Operator-generated event details How to access the events Examples of generated events Overview This document describes Kubernetes events that the operator generates about resources that it manages, during key points of its processing workflow. These events provide an additional way of monitoring your domain resources. Most of the operator-generated events are associated with a domain resource, and those events are included in the Domain resource object as well. Note that the Kubernetes server also generates events for standard Kubernetes resources, such as pods, services, and jobs that the operator generates on behalf of deployed domain custom resources.\nWhat\u0026rsquo;s new The domain events have been enhanced in 4.0. Here is a summary of the changes in this area:\nRemoved two event types: DomainProcessingStarting and DomainProcessingRetrying. Simplified the event type names with the following changes: DomainProcessingFailed to Failed. DomainProcessingCompleted to Completed. DomainCreated to Created. DomainChanged to Changed. DomainDeleted to Deleted. DomainRollStarting to RollStarting. DomainRollCompleted to RollCompleted. Changed DomainProcessingAborted event to Failed event with an explicit message indicating that no retry will occur. Changed DomainValidationError event to Failed event. Enhanced Failed event to: Have a better failure categorization (see Operator-generated event types for more details). Include the categorization information in the event message. Provide more information in the event message to indicate what has gone wrong, what you need to do to resolve the problem, and if the operator will retry the failed operation. Added four event types: Available, Unavailable, Incomplete, and FailureResolved, to record the transition of their corresponding Domain resource status conditions. Added seven event types: ClusterAvailable, ClusterChanged, ClusterCompleted, ClusterCreated, ClusterDeleted, ClusterIncomplete, and ClusterUnavailable, to record the transition of their corresponding Cluster resource status conditions. Operator-generated event types The operator generates these event types in a domain namespace, which indicate the following:\nCreated: A new domain is created. Changed: A change has been made to an existing domain. Deleted: An existing domain has been deleted. Available: An existing domain is available, which means that a sufficient number of servers are ready such that the customer\u0026rsquo;s applications are available. For details, see the corresponding condition. Failed: The domain resource encountered a problem which prevented it from becoming fully up. For details, see the corresponding condition. The possible failure could be one or more of the following conditions: Invalid configurations in the domain resource. A Kubernetes API call error. Introspection failures. An unexpected error in a server pod. A topology mismatch between the Domain resource configuration and the WebLogic domain configuration. The replicas of a cluster in the Domain resource exceeds the maximum number of servers configured for the WebLogic cluster. An internal error. A failure that retries will not help, or has been retried and has exceeded the pre-defined maximum retry time. Completed: The domain resource is complete because all of the following are true: there is no failure detected, there are no pending server shutdowns, and all servers expected to be running are ready and at their target image, auxiliary images, restart version, and introspect version.all servers that are supposed to be started are up running. For details, see the corresponding condition. Unavailable: The domain resource is unavailable, which means that the domain does not have a sufficient number of servers active. For details, see the corresponding condition. Incomplete: The domain resource is incomplete for one or more of the following reasons: there are failures detected, there are pending server shutdowns, or not all servers expected to be running are ready and at their target image, auxiliary images, restart version, and introspect version. For details, see the corresponding condition. FailureResolved: The failure condition that the domain was in, has been resolved. For details, see the corresponding condition. RollStarting: The operator has detected domain resource or Model in Image model updates that require it to perform a rolling restart of the domain. For details, see the corresponding condition. RollCompleted: The operator has successfully completed a rolling restart of a domain. For details, see the corresponding condition. ClusterCreated: A new Cluster resource is created. ClusterChanged: A change has been made to an existing Cluster resource. ClusterDeleted: An existing Cluster resource has been deleted. ClusterAvailable: An existing cluster is available, which means that a sufficient number of its servers have reached the ready state. For details, see the corresponding condition. ClusterCompleted: The cluster is complete because all of the following are true: there is no failure detected, there are no pending server shutdowns, and all servers expected to be running are ready and at their target image, auxiliary images, restart version, and introspect version. For details, see the corresponding condition. ClusterIncomplete: The cluster is incomplete for one or more of the following reasons: there are failures detected, there are pending server shutdowns, or not all servers expected to be running are ready and at their target image, auxiliary images, restart version, or introspect version. For details, see the corresponding condition. ClusterUnavailable: The cluster is unavailable because an insufficient number of its servers that are expected to be running are ready. For details, see the corresponding condition. PodCycleStarting: The operator has started to replace a server pod after it detects that the current pod does not conform to the current domain resource or WebLogic domain configuration. NamespaceWatchingStarted: The operator has started watching for domains in a namespace. NamespaceWatchingStopped: The operator has stopped watching for domains in a namespace. Note that the creation of this event in a domain namespace is the operator\u0026rsquo;s best effort only; the event will not be generated if the required Kubernetes privilege is removed when a namespace is no longer managed by the operator. The operator also generates these event types in the operator\u0026rsquo;s namespace, which indicate the following:\nStartManagingNamespace: The operator has started managing domains in a namespace. StopManagingNamespace: The operator has stopped managing domains in a namespace. StartManagingNamespaceFailed: The operator failed to start managing domains in a namespace because it does not have the required privileges. Operator-generated event details Each operator-generated event contains the following fields:\nmetadata namespace: Namespace in which the event is generated. labels: weblogic.createdByOperator=true and, for a domain event, weblogic.domainUID=\u0026lt;domainUID\u0026gt;. type: String that describes the type of the event. Possible values are Normal or Warning. count: Integer that indicates the number of occurrences of the event. Note that the events are matched by the combination of the reason, involvedObject, and message fields. reportingComponent: String that describes the component that reports the event. The value is weblogic.operator for all operator-generated events. reportingInstance: String that describes the instance that reports the event. The value is the Kubernetes pod name of the operator instance that generates the event. firstTimestamp: DateTime field that presents the timestamp of the first occurrence of this event. lastTimestamp: DateTime field that presents the timestamp of the last occurrence of this event. reason: Short, machine understandable string that gives the reason for the transition to the object\u0026rsquo;s current status. message: String that describes the details of the event. involvedObject: V1ObjectReference object that describes the Kubernetes resources with which this event is associated. name: String that describes the name of the resource with which the event is associated. It may be the domainUID, the name of the namespace that the operator watches, or the name of the operator pod. namespace: String that describes the namespace of the event, which is either the namespace of the domain resource or the namespace of the operator. kind: String that describes the kind of resource this object represents. The value is Domain for a domain event, Namespace for a namespace event in the domain namespace, or Pod for the operator pod. apiVersion: String that describes the apiVersion of the involved object, which is the apiVersion of the domain resource, for example, weblogic.oracle/v9, for a domain event or unset for a namespace event. UID: String that describes the unique identifier of the object that is generated by the Kubernetes server. How to access the events To access the events that are associated with all resources in a particular namespace, run:\n$ kubectl get events -n [namespace] To get the events and sort them by their last timestamp, run:\n$ kubectl get events -n [namespace] --sort-by=lastTimestamp To get all the events that are generated by the operator, run:\n$ kubectl get events -n [namespace] --selector=weblogic.createdByOperator=true To get all the events that are generated by the operator for a particular domain resource, for example sample-domain1, run:\n$ kubectl get events -n [namespace] --selector=weblogic.domainUID=sample-domain1,weblogic.createdByOperator=true --sort-by=lastTimestamp Examples of generated events Here are some examples of operator-generated events from the output of the kubectl describe event or kubectl get events commands for sample-domain1 in namespace sample-domain1-ns.\nExample of a Available event:\nName: sample-domain1.Available.b9c1ddf08e489867 Namespace: sample-domain1-ns Labels: weblogic.createdByOperator=true weblogic.domainUID=sample-domain1 Annotations: \u0026lt;none\u0026gt; API Version: v1 Count: 2 Event Time: \u0026lt;nil\u0026gt; First Timestamp: 2021-12-14T16:23:49Z Involved Object: API Version: weblogic.oracle/v9 Kind: Domain Name: sample-domain1 Namespace: sample-domain1-ns UID: 358f9335-61b2-499a-9d2a-61ae625db2ea Kind: Event Last Timestamp: 2021-12-14T16:23:53Z Message: Domain sample-domain1 is available: a sufficient number of its servers have reached the ready state. Metadata: Creation Timestamp: 2021-12-14T16:23:49Z Resource Version: 5366831 Self Link: /api/v1/namespaces/sample-domain1-ns/events/sample-domain1.Available.b9c1ddf08e489867 UID: 5240d6c6-bfbe-4f06-8ffa-c62cd776cd28 Reason: Available Reporting Component: weblogic.operator Reporting Instance: weblogic-operator-588b9794f5-757b9 Source: Type: Normal Events: \u0026lt;none\u0026gt; Example of a Incomplete event:\nName: sample-domain1.Incomplete.b9c1dc2a2977cd95 Namespace: sample-domain1-ns Labels: weblogic.createdByOperator=true weblogic.domainUID=sample-domain1 Annotations: \u0026lt;none\u0026gt; API Version: v1 Count: 1 Event Time: \u0026lt;nil\u0026gt; First Timestamp: 2021-12-14T16:23:49Z Involved Object: API Version: weblogic.oracle/v9 Kind: Domain Name: sample-domain1 Namespace: sample-domain1-ns UID: 358f9335-61b2-499a-9d2a-61ae625db2ea Kind: Event Last Timestamp: 2021-12-14T16:23:49Z Message: Domain sample-domain1 is incomplete for one or more of the following reasons: there are failures detected, there are pending server shutdowns, or not all servers expected to be running are ready and at their target image, auxiliary images, restart version, and introspect version. Metadata: Creation Timestamp: 2021-12-14T16:23:49Z Resource Version: 5366820 Self Link: /api/v1/namespaces/sample-domain1-ns/events/sample-domain1.Incomplete.b9c1dc2a2977cd95 UID: 97a425b9-175c-43ac-81b0-86edff04fd2b Reason: Incomplete Reporting Component: weblogic.operator Reporting Instance: weblogic-operator-588b9794f5-757b9 Source: Type: Warning Events: \u0026lt;none\u0026gt; Example of a Failed event:\nName: sample-domain1.Failed.b9431b0717a5fc57 Namespace: sample-domain1-ns Labels: weblogic.createdByOperator=true weblogic.domainUID=sample-domain1 Annotations: \u0026lt;none\u0026gt; API Version: v1 Count: 42 Event Time: \u0026lt;nil\u0026gt; First Timestamp: 2021-12-14T14:05:22Z Involved Object: API Version: weblogic.oracle/v9 Kind: Domain Name: sample-domain1 Namespace: sample-domain1-ns UID: c64d95c5-a8b9-4236-a2ab-43879192972b Kind: Event Last Timestamp: 2021-12-14T15:26:56Z Message: Domain sample-domain1 failed due to \u0026#39;Domain validation error\u0026#39;: WebLogicCredentials secret \u0026#39;sample-domain1-weblogic-credentials2\u0026#39; not found in namespace \u0026#39;sample-domain1-ns\u0026#39;. Update the domain resource to correct the validation error. Metadata: Creation Timestamp: 2021-12-14T14:05:22Z Resource Version: 5358407 Self Link: /api/v1/namespaces/sample-domain1-ns/events/sample-domain1.Failed.b9431b0717a5fc57 UID: 6e7e877c-6440-4c0b-888b-bb47ea618400 Reason: Failed Reporting Component: weblogic.operator Reporting Instance: weblogic-operator-588b9794f5-lff29 Source: Type: Warning Events: \u0026lt;none\u0026gt; Example of domain processing completed after failure and retries:\nThe scenario is that the operator initially failed to process the domain resource because the specified image was missing, and then completed the processing during a retry after the image was recreated. Note that this is not a full list of events; some of the events that are generated by the Kubernetes server have been removed to make the list less cluttered.\nThe output of command kubectl get events -n sample-domain1-ns --sort-by=lastTimestamp\nLAST SEEN TYPE REASON OBJECT MESSAGE 7m51s Normal Created domain/sample-domain1 Domain sample-domain1 was created. 7m51s Normal SuccessfulCreate job/sample-domain1-introspector Created pod: sample-domain1-introspector-5ggh6 7m50s Normal Pulled pod/sample-domain1-introspector-5ggh6 Container image \u0026#34;model-in-image:WLS-v1\u0026#34; already present on machine 7m50s Normal Created pod/sample-domain1-introspector-5ggh6 Created container sample-domain1-introspector 7m50s Normal Scheduled pod/sample-domain1-introspector-5ggh6 Successfully assigned sample-domain1-ns/sample-domain1-introspector-5ggh6 to doxiao-1 7m49s Normal Started pod/sample-domain1-introspector-5ggh6 Started container sample-domain1-introspector 6m36s Warning DNSConfigForming pod/sample-domain1-introspector-5ggh6 Search Line limits were exceeded, some search paths have been omitted, the applied search line is: sample-domain1-ns.svc.cluster.local svc.cluster.local cluster.local subnet1ad3phx.devweblogicphx.oraclevcn.com us.oracle.com oracle.com 6m36s Normal Completed job/sample-domain1-introspector Job completed 6m35s Normal Scheduled pod/sample-domain1-admin-server Successfully assigned sample-domain1-ns/sample-domain1-admin-server to doxiao-1 6m35s Normal Created pod/sample-domain1-admin-server Created container weblogic-server 6m35s Normal Pulled pod/sample-domain1-admin-server Container image \u0026#34;model-in-image:WLS-v1\u0026#34; already present on machine 6m34s Normal Started pod/sample-domain1-admin-server Started container weblogic-server 6m3s Normal Scheduled pod/sample-domain1-managed-server2 Successfully assigned sample-domain1-ns/sample-domain1-managed-server2 to doxiao-1 6m3s Normal Scheduled pod/sample-domain1-managed-server1 Successfully assigned sample-domain1-ns/sample-domain1-managed-server1 to doxiao-1 6m3s Normal NoPods poddisruptionbudget/sample-domain1-cluster-1 No matching pods found 6m2s Normal Started pod/sample-domain1-managed-server1 Started container weblogic-server 6m2s Normal Pulled pod/sample-domain1-managed-server1 Container image \u0026#34;model-in-image:WLS-v1\u0026#34; already present on machine 6m2s Normal Started pod/sample-domain1-managed-server2 Started container weblogic-server 6m2s Normal Pulled pod/sample-domain1-managed-server2 Container image \u0026#34;model-in-image:WLS-v1\u0026#34; already present on machine 6m2s Normal Created pod/sample-domain1-managed-server2 Created container weblogic-server 6m2s Normal Created pod/sample-domain1-managed-server1 Created container weblogic-server 5m28s Warning Unhealthy pod/sample-domain1-managed-server2 Readiness probe failed: Get \u0026#34;http://192.168.0.162:8001/weblogic/ready\u0026#34;: dial tcp 192.168.0.162:8001: connect: connection refused 5m24s Warning Unhealthy pod/sample-domain1-managed-server1 Readiness probe failed: Get \u0026#34;http://192.168.0.161:8001/weblogic/ready\u0026#34;: dial tcp 192.168.0.161:8001: connect: connection refused 4m30s Warning Unavailable domain/sample-domain1 Domain sample-domain1 is unavailable: an insufficient number of its servers that are expected to be running are ready. 4m30s Normal SuccessfulCreate job/sample-domain1-introspector Created pod: sample-domain1-introspector-845w9 4m30s Warning Incomplete domain/sample-domain1 Domain sample-domain1 is incomplete for one or more of the following reasons: there are failures detected, there are pending server shutdowns, or not all servers expected to be running are ready and at their target image, auxiliary images, restart version, and introspect version. 4m30s Normal Scheduled pod/sample-domain1-introspector-845w9 Successfully assigned sample-domain1-ns/sample-domain1-introspector-845w9 to doxiao-1 3m44s Normal Pulling pod/sample-domain1-introspector-845w9 Pulling image \u0026#34;model-in-image:WLS-v2\u0026#34; 3m43s Warning Failed pod/sample-domain1-introspector-845w9 Failed to pull image \u0026#34;model-in-image:WLS-v2\u0026#34;: rpc error: code = Unknown desc = pull access denied for model-in-image, repository does not exist or may require \u0026#39;docker login\u0026#39; 3m43s Warning Failed pod/sample-domain1-introspector-845w9 Error: ErrImagePull 3m36s Normal Changed domain/sample-domain1 Domain sample-domain1 was changed. 3m16s Normal BackOff pod/sample-domain1-introspector-845w9 Back-off pulling image \u0026#34;model-in-image:WLS-v2\u0026#34; 3m16s Warning DNSConfigForming pod/sample-domain1-introspector-845w9 Search Line limits were exceeded, some search paths have been omitted, the applied search line is: sample-domain1-ns.svc.cluster.local svc.cluster.local cluster.local subnet1ad3phx.devweblogicphx.oraclevcn.com us.oracle.com oracle.com 3m16s Warning Failed pod/sample-domain1-introspector-845w9 Error: ImagePullBackOff 3m16s Warning Failed domain/sample-domain1 Domain sample-domain1 failed due to \u0026#39;Server pod error\u0026#39;: Failure on pod \u0026#39;sample-domain1-introspector-845w9\u0026#39; in namespace \u0026#39;sample-domain1-ns\u0026#39;: Back-off pulling image \u0026#34;model-in-image:WLS-v2\u0026#34;. 2m30s Warning Failed domain/sample-domain1 Domain sample-domain1 failed due to \u0026#39;Internal error\u0026#39;: Job sample-domain1-introspector failed due to reason: DeadlineExceeded. ActiveDeadlineSeconds of the job is configured with 120 seconds. The job was started 120 seconds ago. Ensure all domain dependencies have been deployed (any secrets, config-maps, PVs, and PVCs that the domain resource references). Use kubectl describe for the job and its pod for more job failure information. The job may be retried by the operator with longer `ActiveDeadlineSeconds` value in each subsequent retry. Use spec.configuration.introspectorJobActiveDeadlineSeconds to increase the job timeout interval if the job still fails after the retries are exhausted. The time limit for retries can be configured in `domain.spec.failureRetryLimitMinutes`.. Will retry next at 2022-10-06T23:17:03.051414370Z and approximately every 120 seconds afterward until 2022-10-06T23:20:03.051414370Z if the failure is not resolved.. Will retry. 2m30s Warning DeadlineExceeded job/sample-domain1-introspector Job was active longer than specified deadline 2m30s Normal SuccessfulDelete job/sample-domain1-introspector Deleted pod: sample-domain1-introspector-845w9 2m29s Warning Failed domain/sample-domain1 Domain sample-domain1 failed due to \u0026#39;Server pod error\u0026#39;: Failure on pod \u0026#39;sample-domain1-introspector-845w9\u0026#39; in namespace \u0026#39;sample-domain1-ns\u0026#39;: rpc error: code = Unknown desc = pull access denied for model-in-image, repository does not exist or may require \u0026#39;docker login\u0026#39;. 2m25s Normal FailureResolved domain/sample-domain1 Domain sample-domain1 encountered some failures before, and those failures have been resolved 2m20s Warning Failed domain/sample-domain1 Domain sample-domain1 failed due to \u0026#39;Kubernetes Api call error\u0026#39;: Failure invoking \u0026#39;create\u0026#39; on job in namespace sample-domain1-ns: : object is being deleted: jobs.batch \u0026#34;sample-domain1-introspector\u0026#34; already exists. 2m19s Warning Failed domain/sample-domain1 Domain sample-domain1 failed due to \u0026#39;Internal error\u0026#39;: io.kubernetes.client.openapi.ApiException: . Introspection failed on try 2 of 5. Introspection Error: io.kubernetes.client.openapi.ApiException: Will retry. 2m9s Normal Available domain/sample-domain1 Domain sample-domain1 is available: a sufficient number of its servers have reached the ready state. 2m8s Normal Completed domain/sample-domain1 Domain sample-domain1 is complete because all of the following are true: there is no failure detected, there are no pending server shutdowns, and all servers expected to be running are ready and at their target image, auxiliary images, restart version, and introspect version. Example of a StartManagingNamespace event in the operator\u0026rsquo;s namespace:\nName: weblogic-operator-588b9794f5-fwstz.StartManagingNamespace.sample-domain1-ns.ba7fca932263194 Namespace: sample-weblogic-operator-ns Labels: weblogic.createdByOperator=true Annotations: \u0026lt;none\u0026gt; API Version: v1 Count: 1 Event Time: \u0026lt;nil\u0026gt; First Timestamp: 2021-12-14T19:51:17Z Involved Object: Kind: Pod Name: weblogic-operator-588b9794f5-fwstz Namespace: sample-weblogic-operator-ns UID: dd454033-f334-49de-8013-2955cf00449e Kind: Event Last Timestamp: 2021-12-14T19:51:17Z Message: Start managing namespace sample-domain1-ns Metadata: Creation Timestamp: 2021-12-14T19:51:17Z Resource Version: 5395096 Self Link: /api/v1/namespaces/sample-weblogic-operator-ns/events/weblogic-operator-588b9794f5-fwstz.StartManagingNamespace.sample-domain1-ns.ba7fca932263194 UID: a19800f5-12d5-43bc-a694-2f25710da8d4 Reason: StartManagingNamespace Reporting Component: weblogic.operator Reporting Instance: weblogic-operator-588b9794f5-fwstz Source: Type: Normal Events: \u0026lt;none\u0026gt; Example of the sequence of operator generated events in a domain rolling restart after the domain resource\u0026rsquo;s image changed, which is the output of the command `kubectl get events -n sample-domain1-ns \u0026ndash;selector=weblogic.domainUID=sample-domain1,weblogic.createdByOperator=true \u0026ndash;sort-by=lastTimestamp'.\nLAST SEEN TYPE REASON OBJECT MESSAGE 4m31s Normal Changed domain/sample-domain1 Domain sample-domain1 was changed. 4m28s Warning Incomplete domain/sample-domain1 Domain sample-domain1 is incomplete for one or more of the following reasons: there are failures detected, there are pending server shutdowns, or not all servers expected to be running are ready and at their target image, auxiliary images, restart version, and introspect version. 4m28s Warning Unavailable domain/sample-domain1 Domain sample-domain1 is unavailable: an insufficient number of its servers that are expected to be running are ready. 4m27s Normal PodCycleStarting domain/sample-domain1 Replacing pod sample-domain1-admin-server 4m27s Normal RollStarting domain/sample-domain1 Rolling restart WebLogic server pods in domain sample-domain1 3m28s Normal Available domain/sample-domain1 Domain sample-domain1 became available 3m27s Normal PodCycleStarting domain/sample-domain1 Replacing pod sample-domain1-managed-server1 22m Normal PodCycleStarting domain/sample-domain1 Replacing pod sample-domain1-managed-server2 64s Normal RollCompleted domain/sample-domain1 Rolling restart of domain sample-domain1 completed 12s Normal Completed domain/sample-domain1 Domain sample-domain1 is complete because all of the following are true: there is no failure detected, there are no pending server shutdowns, and all servers expected to be running are ready and at their target image, auxiliary images, restart version, and introspect version. Example of a RollStarting event:\nName: sample-domain1.RollStarting.ba923815e652c0c9 Namespace: sample-domain1-ns Labels: weblogic.createdByOperator=true weblogic.domainUID=sample-domain1 Annotations: \u0026lt;none\u0026gt; API Version: v1 Count: 1 Event Time: \u0026lt;nil\u0026gt; First Timestamp: 2021-12-14T20:11:24Z Involved Object: API Version: weblogic.oracle/v9 Kind: Domain Name: sample-domain1 Namespace: sample-domain1-ns UID: 86e65656-39cc-4cd5-af61-a7cbaef51b83 Kind: Event Last Timestamp: 2021-12-14T20:11:24Z Message: Rolling restart WebLogic server pods in domain sample-domain1 Metadata: Creation Timestamp: 2021-12-14T20:11:24Z Resource Version: 5398296 Self Link: /api/v1/namespaces/sample-domain1-ns/events/sample-domain1.RollStarting.ba923815e652c0c9 UID: 394d6cab-86d8-4686-bd6b-2d906bc4eac7 Reason: RollStarting Reporting Component: weblogic.operator Reporting Instance: weblogic-operator-588b9794f5-fwstz Source: Type: Normal Events: \u0026lt;none\u0026gt; Example of a PodCycleStarting event:\nName: sample-domain1.PodCycleStarting.7d34bc3232231f49 Namespace: sample-domain1-ns Labels: weblogic.createdByOperator=true weblogic.domainUID=sample-domain1 Annotations: \u0026lt;none\u0026gt; API Version: v1 Count: 1 Event Time: \u0026lt;nil\u0026gt; First Timestamp: 2021-05-18T02:01:18Z Involved Object: API Version: weblogic.oracle/v9 Kind: Domain Name: sample-domain1 Namespace: sample-domain1-ns UID: 5df7dcda-d606-4509-9a06-32f25e16e166 Kind: Event Last Timestamp: 2021-05-18T02:01:18Z Message: Replacing pod sample-domain1-managed-server1 Metadata: Creation Timestamp: 2021-05-18T02:01:18Z Resource Version: 12842530 Self Link: /api/v1/namespaces/sample-domain1-ns/events/sample-domain1.PodCycleStarting.7d34bc3232231f49 UID: 4c6a203e-9b93-4b46-b9e3-1a448b52c7ca Reason: PodCycleStarting Reporting Component: weblogic.operator Reporting Instance: weblogic-operator-fc4ccc8b5-rh4v6 Source: Type: Normal Events: \u0026lt;none\u0026gt; "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/azure-kubernetes-service/",
	"title": "Azure Kubernetes Service",
	"tags": [],
	"description": "Sample for using the operator to set up a WLS cluster on the Azure Kubernetes Service.",
	"content": "Contents Introduction Azure Kubernetes Service cluster Domain home source types References Introduction This sample demonstrates how to use the WebLogic Kubernetes Operator (hereafter \u0026ldquo;the operator\u0026rdquo;) to set up a WebLogic Server (WLS) cluster on the Azure Kubernetes Service (AKS). After going through the steps, your WLS domain runs on an AKS cluster. You have several options for managing the cluster, depending on which domain home source type you choose. With Domain on PV, you can manage your WLS domain by accessing the WebLogic Server Administration Console or WLST. With Model in Image, you use the operator to perform WLS administrative operations.\nFor an alternative approach to this sample, see the Oracle WebLogic Server on AKS Azure Marketplace offering, which automates the provisioning of the AKS cluster, AKS resources, Azure Container Registry (ACR), load-balancer, WebLogic Kubernetes Operator, and WebLogic Server images.\nAzure Kubernetes Service cluster Azure Kubernetes Service (AKS) makes it simple to deploy a managed Kubernetes cluster in Azure. AKS reduces the complexity and operational overhead of managing Kubernetes by offloading much of that responsibility to Azure. As a hosted Kubernetes service, Azure handles critical tasks like health monitoring and maintenance for you. The Kubernetes masters are managed by Azure. You manage and maintain only the agent nodes. As a managed Kubernetes service, AKS is free - you pay for only the agent nodes within your clusters, not for the masters.\nTo learn more, see What is Azure Kubernetes Service?\nDomain home source types This sample demonstrates running the WebLogic cluster on AKS using two domain home types. The instructions for each are self-contained and independent. This section lists the domain home source types recommended for use with AKS, along with some benefits of each. For complete details on domain home source types, see Choose a domain home source type.\nModel in Image: running the WebLogic cluster on AKS with model in image offers the following benefits:\nReuse image to create different domains with different domainUID and different configurations. Mutate the domain home configuration with additional model files supplied in a ConfigMap. Many such changes do not need to restart the entire domain for the change to take effect. The model file syntax is far simpler and less error prone than the configuration override syntax, and, unlike configuration overrides, allows you to directly add data sources and JMS modules. Domain on PV: running the WebLogic cluster on AKS with domain home on PV offers the following benefits:\nUse standard Oracle-provided images with patches installed. No Docker environment required. You are able to run your business quickly without building knowledge of Docker. Mutate the live domain configuration with Administration Console from a browser or WLST. Stop and Start an Azure Kubernetes Service (AKS) cluster using Azure CLI, as described in the azure docs. This allows you to optimize costs during your AKS cluster\u0026rsquo;s idle time. Don\u0026rsquo;t pay for running development clusters unless they are actively being used. You can pick up objects and cluster state right where you were left off.\nReferences For references to the relevant user documentation, see:\nChoose a domain home source type user documentation Model in Image user documentation "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/model-in-image/",
	"title": "Model in Image",
	"tags": [],
	"description": "Create and deploy a typical Model in Image domain.",
	"content": " Overview Introduction to Model in Image, description of its runtime behavior, and references.\nUsage Steps for creating and deploying Model in Image images and their associated Domain YAML files.\nAuxiliary images Auxiliary images are an alternative approach for supplying a domain\u0026#39;s model files or other types of files.\nModel files Model file requirements, macros, and loading order.\nRuntime updates Updating a running Model in Image domain\u0026#39;s images and model files.\nMove MII/JRF domains to PV Moving an MII/JRF domain to a persistent volume.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/domains/lifecycle/",
	"title": "Domain lifecycle operations",
	"tags": [],
	"description": "Start and stop Managed Servers, clusters, and domains.",
	"content": "Domain lifecycle sample scripts Beginning in version 3.1.0, the operator provides sample scripts to start up or shut down a specific Managed Server or cluster in a deployed domain, or the entire deployed domain. Beginning in version 3.2.0, additional scripts are provided for scaling a WebLogic cluster, displaying the WebLogic cluster status, initiating rolling restart of a domain or a WebLogic cluster, and initiating explicit introspection of a domain.\nNOTE: Prior to running these scripts, you must have previously created and deployed the domain.\nThe scripts are located in the kubernetes/samples/scripts/domain-lifecycle directory. They are helpful when scripting the life cycle of a WebLogic Server domain. For more information, see the README.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/developerguide/domain-processing/",
	"title": "Domain processing",
	"tags": [],
	"description": "Understand how the operator processes domains and creates resources.",
	"content": "When the operator starts, it lists all the existing Domains and then processes these Domains to create the necessary Kubernetes resources, such as Pods and Services, if they don\u0026rsquo;t already exist. This initialization also includes looking for any stranded resources that, while created by the operator, no longer correlate with a Domain.\nAfter this, the operator starts watches for changes to Domains and any changes to other resources created by the operator. When a watch event is received, the operator processes the modified Domain to again bring the runtime presence into alignment with the desired state.\nThe operator ensures that at most one Fiber is running for any given Domain. For instance, if the customer modifies a Domain to initiate a rolling restart, then the operator will create a Fiber to process this activity. However, if while the rolling restart is in process, the customer makes another change to the Domain, such as to increase the replicas field for a cluster, then the operator will cancel the in-flight Fiber and replace it with a new Fiber. This replacement processing must be able to handle taking over for the cancelled work regardless of where the earlier processing may have been in its flow. Therefore, domain processing always starts at the beginning of the \u0026ldquo;make right\u0026rdquo; flow without any state other than the current Domain resource and referenced Cluster resources.\nFinally, the operator periodically lists all Domains and rechecks them. This is a backstop against the possibility that a watch event is missed, such as because of a temporary network outage. Recheck activities will not interrupt already running processes for a given Domain.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-operators/rbac/",
	"title": "RBAC",
	"tags": [],
	"description": "Operator role-based authorization.",
	"content": "Contents Overview Operator RBAC definitions Kubernetes Role and RoleBinding naming conventions Kubernetes ClusterRole and ClusterRoleBinding naming conventions RoleBindings ClusterRoleBindings Overview This document describes the Kubernetes Role-Based Access Control (RBAC) roles that an operator installation Helm chart automatically creates for you.\nThe general design goal of the operator installation is to automatically provide the operator with the minimum amount of permissions that the operator requires and to favor built-in roles over custom roles where it makes sense to use the Kubernetes built-in roles.\nThe operator installation Helm chart automatically creates RBAC ClusterRoles, ClusterRoleBindings, Roles, and RoleBindings for the ServiceAccount that is used by the operator. A running operator assumes that these roles are created in the Kubernetes cluster and will automatically attempt to verify that they are correct when it starts.\nNote that the operator installation Helm chart creates ClusterRoles and ClusterRoleBindings when the enableClusterRoleBinding Helm chart configuration setting is set to true (the default), and the chart creates Roles and RoleBindings when the setting is set to false.\nReferences\nFor more information about:\nInstalling the operator, see Prepare for installation and Installation. The enableClusterRoleBinding operator Helm chart setting, see Choose a security strategy. The Kubernetes ServiceAccount used by the operator, see Service accounts. Kubernetes Roles, see the Kubernetes RBAC documentation. Operator RBAC definitions To display the Kubernetes Roles and related Bindings used by the operator, where the operator was installed using the Helm release name weblogic-operator, look for the Kubernetes objects, Role, RoleBinding, ClusterRole, and ClusterRoleBinding, when using the Helm status command:\n$ helm status weblogic-operator Assuming the operator was installed into the namespace weblogic-operator-ns with a target namespaces of domain1-ns, the following commands can be used to display a subset of the Kubernetes Roles and related RoleBindings:\n$ kubectl describe clusterrole \\ weblogic-operator-ns-weblogic-operator-clusterrole-general $ kubectl describe clusterrolebinding \\ weblogic-operator-ns-weblogic-operator-clusterrolebinding-general $ kubectl -n weblogic-operator-ns \\ describe role weblogic-operator-role $ kubectl -n domain1-ns \\ describe rolebinding weblogic-operator-rolebinding-namespace Kubernetes Role and RoleBinding naming conventions The following naming pattern is used for the Role and RoleBinding objects:\nweblogic-operator-\u0026lt;type\u0026gt;-\u0026lt;optional-role-name\u0026gt; Using:\n\u0026lt;type\u0026gt; as the kind of Kubernetes object: role rolebinding \u0026lt;optional-role-name\u0026gt; as an optional name given to the Role or RoleBinding For example: namespace A complete name for an operator created Kubernetes RoleBinding would be:\nweblogic-operator-rolebinding-namespace\nKubernetes ClusterRole and ClusterRoleBinding naming conventions The following naming pattern is used for the ClusterRole and ClusterRoleBinding objects:\n\u0026lt;operator-ns\u0026gt;-weblogic-operator-\u0026lt;type\u0026gt;-\u0026lt;role-name\u0026gt; Using:\n\u0026lt;operator-ns\u0026gt; as the namespace in which the operator is installed For example: weblogic-operator-ns \u0026lt;type\u0026gt; as the kind of Kubernetes object: clusterrole clusterrolebinding \u0026lt;role-name\u0026gt; as the name given to the Role or RoleBinding For example: general A complete name for an operator created Kubernetes ClusterRoleBinding would be:\nweblogic-operator-ns-weblogic-operator-clusterrolebinding-general\nRoleBindings Assuming that the operator was installed into the Kubernetes Namespace weblogic-operator-ns, and a target namespace for the operator is domain1-ns, the following RoleBinding entries are mapped to a Role or ClusterRole granting permission to the operator.\nRoleBinding Mapped to Role Resource Access Notes weblogic-operator-rolebinding weblogic-operator-role Edit: secrets, configmaps, events The RoleBinding is created in the namespace weblogic-operator-ns 1 weblogic-operator-rolebinding-namespace Operator Cluster Role namespace Read: secrets, pods/log, pods/exec The RoleBinding is created in the namespace domain1-ns 2 Edit: configmaps, events, pods, services, jobs.batch, poddisruptionbudgets.policy Create: pods/exec ClusterRoleBindings Assuming that the operator was installed into the Kubernetes Namespace weblogic-operator-ns, the following ClusterRoleBinding entries are mapped to a ClusterRole granting permission to the operator.\nNOTE: The operator names in following table represent the \u0026lt;role-name\u0026gt; from the cluster names section.\nClusterRoleBinding Mapped to Cluster Role Resource Access Notes Operator general Operator general Read: namespaces 3 Edit: customresourcedefinitions Update: domains (weblogic.oracle), domains/status Create: tokenreviews, selfsubjectrulesreviews Operator nonresource Operator nonresource Get: /version/* 1 The binding is assigned to the operator ServiceAccount.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe binding is assigned to the operator ServiceAccount in each namespace that the operator is configured to manage. See Namespace management\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe binding is assigned to the operator ServiceAccount. In addition, the Kubernetes RBAC resources that the operator installation actually sets up will be adjusted based on whether the operator is in dedicated mode. By default, the operator does not run in dedicated mode and those security resources are created as ClusterRole and ClusterRoleBindings. If the operator is running in dedicated mode, then those resources will be created as Roles and RoleBindings in the namespace of the operator. See the Dedicated option for the domainNamespaceSelectionStrategy setting.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/reference/",
	"title": "Reference",
	"tags": [],
	"description": "",
	"content": "See the following reference documentation.\nDomain resource Use this document to set up and configure your own YAML file containing Domains and Clusters.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/accessing-the-domain/status-conditions/",
	"title": "Status conditions",
	"tags": [],
	"description": "Monitor Domain and Cluster resources using operator-generated status conditions.",
	"content": "Contents Overview Checking domain or cluster conditions Attributes in a condition Types of domain conditions Failed Completed Available ConfigChangesPendingRestart Rolling Types of cluster conditions Completed Available Conditions life cycle Conditions and generations Conditions and events Overview The WebLogic Kubernetes Operator populates status conditions on Domain and Cluster resources to provide high-level status reporting. Status conditions are a Kubernetes standard mechanism and the conditions generated by the operator are similar to those that Kubernetes provides for Pod and Deployment resources.\nChecking domain or cluster conditions Conditions can be found under the status.conditions field in a Domain resource or a Cluster resource.\nYou can check the conditions in a Domain resource by using: kubectl -n MY_NAMESPACE describe domain MY_DOMAIN_RESOURCE_NAME. Similarly, you can check the conditions in a Cluster resource by using: kubectl -n MY_NAMESPACE describe cluster MY_CLUSTER_NAME.\nClick here for an example of a cluster status showing its conditions. Status: Cluster Name: cluster-1 Conditions: Last Transition Time: 2022-10-25T16:31:22.682605Z Status: True Type: Available Last Transition Time: 2022-10-25T16:31:22.683156Z Status: True Type: Completed The conditions for Cluster resources referenced by a domain are also listed in the domain status under domain.status.clusters.\nClick here for an example of a domain status showing both domain and cluster conditions. Status: Clusters: Cluster Name: cluster-1 Conditions: Last Transition Time: 2022-10-25T16:31:22.682605Z Status: True Type: Available Last Transition Time: 2022-10-25T16:31:22.682605Z Status: True Type: Completed Label Selector: weblogic.domainUID=sample-domain1,weblogic.clusterName=cluster-1 Maximum Replicas: 5 Minimum Replicas: 0 Observed Generation: 1 Ready Replicas: 1 Replicas: 1 Replicas Goal: 1 Conditions: Last Transition Time: 2022-10-25T16:44:27.104854Z Status: True Type: Available Last Transition Time: 2022-10-25T16:44:27.104766Z Status: True Type: Completed Attributes in a condition The following attributes can be found in a condition:\ntype - The type of the condition, such as Failed or Available. See Types of domain status conditions. status - The status of the condition, such as True or False. message - An optional, human-readable message providing more details about the condition. reason - The reason for the Failed condition. Not applicable to other types of condition. severity - The severity for the Failed condition. Not applicable to other types of condition. lastTransitionTime - A timestamp of when the condition was created or the last time time the condition transitioned from one status to another. Types of domain conditions The following is a list of condition types for a Domain resource.\nFailed The desired state of the Domain resource cannot be achieved due to failures encountered in processing the Domain resource. For information about how to diagnose failures, see debugging. The status attribute is always True for a Failed condition. The Failed condition is removed from the domain status when the underlying failure is resolved. The message attribute contains an error message with details of the failure. The reason attribute is set to one of the reasons listed in domain failure reasons. The severity attribute is set to one of the severity levels listed in domain failure severities. Click here for an example of a domain status with a Failed condition. Status: ... Conditions: Last Transition Time: 2022-10-24T23:54:49.486543Z Message: One or more server pods that are supposed to be available did not start within the period of time defined in \u0026#39;serverPod.maxPendingWaitTimeSeconds\u0026#39; under \u0026#34;domain.spec\u0026#39;, \u0026#39;domain.adminServer\u0026#39;, \u0026#39;managedServer\u0026#39;, or \u0026#39;domain.cluster\u0026#39;. Check the server status in the domain status, the server pod status and logs, and WebLogic Server logs for possible reasons. One common cause of this issue is a problem pulling the WebLogicServer image. Adjust the value of \u0026#39;serverPod.maxPendingWaitTimeSeconds\u0026#39; setting if needed.\u0026#34; Reason: ServerPod Severity: Severe Status: True Type: Failed Last Transition Time: 2022-10-24T23:49:35.974905Z Message: No application servers are ready. Status: False Type: Available Last Transition Time: 2022-10-24T23:49:35.974897Z Status: False Type: Completed Last Transition Time: 2022-10-24T23:49:36.173977Z Status: True Type: Rolling Completed The status attribute of a Completed condition indicates whether the desired state of the Domain resource has been fully achieved. The status attribute is set to True when all of the following are true: There are no Failed conditions, for example, no failures are detected. One of the following conditions are met: All WebLogic Server pods that are expected to be running are ready at their target image or images, restartVersion, and introspectVersion. No WebLogic Server pods are running and this is the expected state. The domain is configured to have a spec.serverStartPolicy value of AdminOnly and the Administration Server pod is running and ready. There are no pending server shutdown requests. Available The status attribute is set to True when a sufficient number of pods are ready such that all of the following are true: At least one WebLogic Server pod is ready. Every non-clustered server with a serverStartPolicy value of IfNeeded or Always is ready. Every Cluster resource referenced by the domain has True in its Available condition. Clusters that are configured with a replicas value of 0 or a serverStartPolicy value of Never are ignored. NOTE: The Available status can be True even when the status for the Completed condition is False, a Failed condition is reported, or a cluster has up to cluster.spec.maxUnavailable pods that are not ready. ConfigChangesPendingRestart This condition tracks the progress of runtime updates to the WebLogic Deploy Tooling model of a Model in Image domain home source type. The status attribute is True if all of the following are true: The Domain resource attribute domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges is CommitUpdateOnly. The Domain resource attribute domain.spec.configuration.model.onlineUpdate.enabled is True. There were model changes and these changes modify non-dynamic WebLogic configuration. Processing successfully completed, including the introspector job. The administrator has not subsequently rolled or restarted each WebLogic Server pod (to propagate the pending non-dynamic changes). The ConfigChangesPendingRestart condition is removed from the domain status when pending non-dynamic runtime updates complete their processing due to a roll or restart. For how to see which pods are awaiting restart using WebLogic pod labels, see Online update status and labels. Click here for an example of a domain status with a ConfigChangesPendingRestart condition. Status: ... Conditions: Last Transition Time: 2021-01-20T15:09:15.209Z Message: Online update completed successfully, but the changes require restart and the Domain resource specified \u0026#39;spec.configuration.model.onlineUpdate.onNonDynamicChanges=CommitUpdateOnly\u0026#39; or not set. The changes are committed but the domain require manually restart to make the changes effective. The changes are: Server re-start is REQUIRED for the set of changes in progress. The following non-dynamic attribute(s) have been changed on MBeans that require server re-start: MBean Changed : com.bea:Name=oracle.jdbc.fanEnabled,Type=weblogic.j2ee.descriptor.wl.JDBCPropertyBean,Parent=[sample-domain1]/JDBCSystemResources[Bubba-DS],Path=JDBCResource[Bubba-DS]/JDBCDriverParams/Properties/Properties[oracle.jdbc.fanEnabled] Attributes changed : Value Reason: Online update applied, introspectVersion updated to 82 Status: True Type: ConfigChangesPendingRestart Rolling This condition indicates that the operator is rolling the server pods in a domain, such as after it has detected an update to the Domain resource or Model in Image model that requires it to perform a rolling restart of the domain. The status attribute is always True for a Rolling condition. The Rolling condition is removed from the domain status when the rolling is completed. Types of cluster conditions The following is a list of condition types for a Cluster resource.\nCompleted The status attribute of a Completed condition indicates whether the desired state of the Cluster resource has been fully achieved. The status attribute is set to True when all of the following are true: There are no Failed domain.status.conditions. No WebLogic Server pods are expected to be running, or all server pods in the cluster that are expected to be running are ready at their target image or images, restartVersion, and introspectVersion There are no pending server shutdown requests. Available The status attribute is set to True when a sufficient number of WebLogic Server pods are ready in the cluster. Both of the following must be true: At least one pod in the cluster is expected to run and is ready. The number of not ready server pods, which are expected to run, is less than or equal to the value of cluster.spec.maxUnavailable, which defaults to 1. Examples: If a cluster has a serverStartPolicy of Never, or replicas is 0, or a cluster is in a domain with serverStartPolicy of AdminOnly or Never, then the cluster will have an Available condition that is False. If a cluster and domain both have a serverStartPolicy of IfNeeded, and cluster.spec.replicas is 1, then the cluster will have an Available condition that is True only when its single pod is ready. If a cluster and domain both have a serverStartPolicy of IfNeeded, cluster.spec.replicas is 4, and cluster.spec.maxUnavailable is 1 (the default), then the cluster will have an Available condition that is True only when three or four of its pods are ready. NOTE: The Available condition\u0026rsquo;s status can be True even when the status for the Completed condition is False, a Failed condition is reported on the Domain resource, or the cluster has up to cluster.spec.maxUnavailable pods that are not ready. Conditions life cycle After the operator detects a Cluster or Domain resource, it will ensure that each resource always has exactly one occurrence of an Available and of a Completed condition. The operator will not subsequently remove these conditions but instead, will change their status to True or False as appropriate. You can use this fact as a way to determine when an operator has detected a Domain or Cluster resource.\nThe Failure, Rolling, and ConfigChangesPendingRestart conditions are ephemeral; they are only added when they apply, and are removed when they no longer apply.\nThe Failure condition is the only condition that can have multiple occurrences. This occurs when there are multiple concurrent failures of different types.\nConditions and generations Use metadata.generation and status.observedGeneration in Domain and Cluster resources to detect when their conditions and other status are up-to-date. Specifically, conditions may not be up-to-date when domain.status.observedGeneration generation does not equal domain.metadata.generation, or when cluster.status.observedGeneration does not equal cluster.metadata.generation in any of the domain\u0026rsquo;s Cluster resources. This may indicate either the operator is in the process of updating the status or the operator is not running.\nConditions and events A corresponding event is generated when an Available, Completed, Failure, or Rolling condition has changed. For details, see operator-generated event types.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/samples/tanzu-kubernetes-service/",
	"title": "Tanzu Kubernetes Service",
	"tags": [],
	"description": "Sample for using the operator to set up a WLS cluster on the Tanzu Kubernetes Service.",
	"content": "This sample demonstrates how to use the WebLogic Kubernetes Operator (hereafter “the operator”) to set up a WebLogic Server (WLS) cluster on the Tanzu Kubernetes Grid (TKG). After performing the sample steps, your WLS domain with a Model in Image domain source type runs on a TKG Kubernetes cluster instance. After the domain has been provisioned, you can monitor it using the WebLogic Server Administration console.\nTKG is a managed Kubernetes Service that lets you quickly deploy and manage Kubernetes clusters. To learn more, see the Tanzu Kubernetes Grid (TKG) overview page.\nContents Prerequisites Create a Tanzu Kubernetes cluster Oracle Container Registry Install WebLogic Kubernetes Operator Create an image Image creation prerequisites Image creation - Introduction Understanding your first archive Staging a ZIP file of the archive Staging model files Creating the image with WIT Create WebLogic domain Namespace Secrets Domain resource Invoke the web application Prerequisites This sample assumes the following prerequisite environment setup:\nWebLogic Kubernetes Operator: This document was tested with version v3.1.0. Operating System: GNU/Linux. Git; use git --version to test if git works. This document was tested with version 2.17.1. TKG CLI; use tkg version to test if TKG works. This document was tested with version v1.1.3. kubectl; use kubectl version to test if kubectl works. This document was tested with version v1.18.6. Helm version 3.1 or later; use helm version to check the helm version. This document was tested with version v3.2.1. See Supported environments for general operator prerequisites and operator support limitations that are specific to Tanzu.\nCreate a Tanzu Kubernetes cluster Create the Kubernetes cluster using the TKG CLI. See the Tanzu documentation to set up your Kubernetes cluster. After your Kubernetes cluster is up and running, run the following commands to make sure kubectl can access the Kubernetes cluster:\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-cluster-101-control-plane-8nj7t NotReady master 2d20h v1.18.6+vmware.1 192.168.100.147 192.168.100.147 VMware Photon OS/Linux 4.19.132-1.ph3 containerd://1.3.4 k8s-cluster-101-md-0-577b7dc766-552hn Ready \u0026lt;none\u0026gt; 2d20h v1.18.6+vmware.1 192.168.100.148 192.168.100.148 VMware Photon OS/Linux 4.19.132-1.ph3 containerd://1.3.4 k8s-cluster-101-md-0-577b7dc766-m8wrc Ready \u0026lt;none\u0026gt; 2d20h v1.18.6+vmware.1 192.168.100.149 192.168.100.149 VMware Photon OS/Linux 4.19.132-1.ph3 containerd://1.3.4 k8s-cluster-101-md-0-577b7dc766-p2gkz Ready \u0026lt;none\u0026gt; 2d20h v1.18.6+vmware.1 192.168.100.150 192.168.100.150 VMware Photon OS/Linux 4.19.132-1.ph3 containerd://1.3.4 Oracle Container Registry You will need an Oracle Container Registry account. The following steps will direct you to accept the Oracle Standard Terms and Restrictions to pull the WebLogic Server images. Make note of your Oracle Account password and email. This sample pertains to 12.2.1.4, but other versions may work as well.\nInstall WebLogic Kubernetes Operator The WebLogic Kubernetes Operator is an adapter to integrate WebLogic Server and Kubernetes, allowing Kubernetes to serve as a container infrastructure hosting WLS instances. The operator runs as a Kubernetes Pod and stands ready to perform actions related to running WLS on Kubernetes.\nClone the WebLogic Kubernetes Operator repository to your machine. We will use several scripts in this repository to create a WebLogic domain. Kubernetes Operators use Helm to manage Kubernetes applications. The operator’s Helm chart is located in the kubernetes/charts/weblogic-operator directory. Install the operator by running the following commands.\nClone the repository.\n$ git clone --branch v4.2.20 https://github.com/oracle/weblogic-kubernetes-operator.git $ cd weblogic-kubernetes-operator Grant the Helm service account the cluster-admin role.\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: helm-user-cluster-admin-role roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: default namespace: kube-system EOF Create a namespace and service account for the operator.\n$ kubectl create namespace sample-weblogic-operator-ns namespace/sample-weblogic-operator-ns created $ kubectl create serviceaccount -n sample-weblogic-operator-ns sample-weblogic-operator-sa serviceaccount/sample-weblogic-operator-sa created Install the operator.\n$ helm install weblogic-operator kubernetes/charts/weblogic-operator \\ --namespace sample-weblogic-operator-ns \\ --set serviceAccount=sample-weblogic-operator-sa \\ --wait NAME: weblogic-operator LAST DEPLOYED: Tue Nov 17 09:33:58 2020 NAMESPACE: sample-weblogic-operator-ns STATUS: deployed REVISION: 1 TEST SUITE: None Verify the operator with the following commands; the status will be running.\n$ helm list -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION sample-weblogic-operator sample-weblogic-operator-ns 1 2020-11-17 09:33:58.584239273 -0700 PDT deployed weblogic-operator-3.1 $ kubectl get pods -n sample-weblogic-operator-ns NAME READY STATUS RESTARTS AGE weblogic-operator-775b668c8f-nwwnn 1/1 Running 0 32s Create an image Image creation prerequisites Image creation - Introduction Understanding your first archive Staging a ZIP file of the archive Staging model files Creating the image with WIT Image creation prerequisites The JAVA_HOME environment variable must be set and must reference a valid JDK 8 or 11 installation. Copy the sample to a new directory; for example, use the directory /tmp/mii-sample. $ mkdir /tmp/mii-sample $ cp -r /root/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain/model-in-image/* /tmp/mii-sample NOTE: We will refer to this working copy of the sample as /tmp/mii-sample; however, you can use a different location.\nDownload the latest WebLogic Deploying Tooling (WDT) and WebLogic Image Tool (WIT) installer ZIP files to your /tmp/mii-sample/model-images directory. Both WDT and WIT are required to create your Model in Image container images.\n$ cd /tmp/mii-sample/model-images $ curl -m 120 -fL https://github.com/oracle/weblogic-deploy-tooling/releases/latest/download/weblogic-deploy.zip \\ -o /tmp/mii-sample/model-images/weblogic-deploy.zip $ curl -m 120 -fL https://github.com/oracle/weblogic-image-tool/releases/latest/download/imagetool.zip \\ -o /tmp/mii-sample/model-images/imagetool.zip To set up the WebLogic Image Tool, run the following commands:\n$ cd /tmp/mii-sample/model-images $ unzip imagetool.zip $ ./imagetool/bin/imagetool.sh cache addInstaller \\ --type wdt \\ --version latest \\ --path /tmp/mii-sample/model-images/weblogic-deploy.zip These steps will install WIT to the /tmp/mii-sample/model-images/imagetool directory, plus put a wdt_latest entry in the tool’s cache which points to the WDT ZIP file installer. You will use WIT later in the sample for creating model images.\nImage creation - Introduction The goal of image creation is to demonstrate using the WebLogic Image Tool to create an image named model-in-image:WLS-v1 from files that you will stage to /tmp/mii-sample/model-images/model-in-image:WLS-v1/. The staged files will contain a web application in a WDT archive, and WDT model configuration for a WebLogic Administration Server called admin-server and a WebLogic cluster called cluster-1.\nOverall, a Model in Image image must contain a WebLogic Server installation and a WebLogic Deploy Tooling installation in its /u01/wdt/weblogic-deploy directory. In addition, if you have WDT model archive files, then the image must also contain these files in its /u01/wdt/models directory. Finally, an image optionally may also contain your WDT model YAML file and properties files in the same /u01/wdt/models directory. If you do not specify a WDT model YAML file in your /u01/wdt/models directory, then the model YAML file must be supplied dynamically using a Kubernetes ConfigMap that is referenced by your Domain spec.model.configMap field. We provide an example of using a model ConfigMap later in this sample.\nThe following sections contain the steps for creating the image model-in-image:WLS-v1.\nUnderstanding your first archive The sample includes a predefined archive directory in /tmp/mii-sample/archives/archive-v1 that you will use to create an archive ZIP file for the image.\nThe archive top directory, named wlsdeploy, contains a directory named applications, which includes an ‘exploded’ sample JSP web application in the directory, myapp-v1. Three useful aspects to remember about WDT archives are:\nA model image can contain multiple WDT archives. WDT archives can contain multiple applications, libraries, and other components. WDT archives have a well defined directory structure, which always has wlsdeploy as the top directory. The application displays important details about the WebLogic Server instance that it’s running on: namely its domain name, cluster name, and server name, as well as the names of any data sources that are targeted to the server.\nStaging a ZIP file of the archive When you create the image, you will use the files in the staging directory, /tmp/mii-sample/model-in-image__WLS-v1. In preparation, you need it to contain a ZIP file of the WDT application archive.\nRun the following commands to create your application archive ZIP file and put it in the expected directory:\n# Delete existing archive.zip in case we have an old leftover version $ rm -f /tmp/mii-sample/model-images/model-in-image__WLS-v1/archive.zip # Move to the directory which contains the source files for our archive $ cd /tmp/mii-sample/archives/archive-v1 # Zip the archive to the location will later use when we run the WebLogic Image Tool $ zip -r /tmp/mii-sample/model-images/model-in-image__WLS-v1/archive.zip wlsdeploy Staging model files In this step, you explore the staged WDT model YAML file and properties in the /tmp/mii-sample/model-in-image__WLS-v1 directory. The model in this directory references the web application in your archive, configures a WebLogic Server Administration Server, and configures a WebLogic cluster. It consists of only two files, model.10.properties, a file with a single property, and, model.10.yaml, a YAML file with your WebLogic configuration model.10.yaml.\nCLUSTER_SIZE=5 Here is the WLS model.10.yaml:\ndomainInfo: AdminUserName: \u0026#39;@@SECRET:__weblogic-credentials__:username@@\u0026#39; AdminPassword: \u0026#39;@@SECRET:__weblogic-credentials__:password@@\u0026#39; ServerStartMode: \u0026#39;prod\u0026#39; topology: Name: \u0026#39;@@ENV:CUSTOM_DOMAIN_NAME@@\u0026#39; AdminServerName: \u0026#39;admin-server\u0026#39; Cluster: \u0026#39;cluster-1\u0026#39;: DynamicServers: ServerTemplate: \u0026#39;cluster-1-template\u0026#39; ServerNamePrefix: \u0026#39;managed-server\u0026#39; DynamicClusterSize: \u0026#39;@@PROP:CLUSTER_SIZE@@\u0026#39; MaxDynamicClusterSize: \u0026#39;@@PROP:CLUSTER_SIZE@@\u0026#39; MinDynamicClusterSize: \u0026#39;0\u0026#39; CalculatedListenPorts: false Server: \u0026#39;admin-server\u0026#39;: ListenPort: 7001 ServerTemplate: \u0026#39;cluster-1-template\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 appDeployments: Application: myapp: SourcePath: \u0026#39;wlsdeploy/applications/myapp-v1\u0026#39; ModuleType: ear Target: \u0026#39;cluster-1\u0026#39; The model files:\nDefine a WebLogic domain with:\nCluster cluster-1 Administration Server admin-server A cluster-1 targeted EAR application that’s located in the WDT archive ZIP file at wlsdeploy/applications/myapp-v1 Leverage macros to inject external values:\nThe property file CLUSTER_SIZE property is referenced in the model YAML file DynamicClusterSize and MaxDynamicClusterSize fields using a PROP macro. The model file domain name is injected using a custom environment variable named CUSTOM_DOMAIN_NAME using an ENV macro. You set this environment variable later in this sample using an env field in its Domain. This conveniently provides a simple way to deploy multiple differently named domains using the same model image. The model file administrator user name and password are set using a weblogic-credentials secret macro reference to the WebLogic credential secret. This secret is in turn referenced using the webLogicCredentialsSecret field in the Domain. The weblogic-credentials is a reserved name that always dereferences to the owning Domain actual WebLogic credentials secret name. A Model in Image image can contain multiple properties files, archive ZIP files, and YAML files but in this sample you use just one of each. For a complete description of Model in Images model file naming conventions, file loading order, and macro syntax, see Model files files in the Model in Image user documentation.\nCreating the image with WIT At this point, you have staged all of the files needed for the image model-in-image:WLS-v1; they include:\n/tmp/mii-sample/model-images/weblogic-deploy.zip /tmp/mii-sample/model-images/model-in-image__WLS-v1/model.10.yaml /tmp/mii-sample/model-images/model-in-image__WLS-v1/model.10.properties /tmp/mii-sample/model-images/model-in-image__WLS-v1/archive.zip If you don’t see the weblogic-deploy.zip file, then you missed a step in the prerequisites.\nNow, you use the Image Tool to create an image named model-in-image:WLS-v1 that’s layered on a base WebLogic image. You’ve already set up this tool during the prerequisite steps.\nRun the following commands to create the model image and verify that it worked:\n$ cd /tmp/mii-sample/model-images $ ./imagetool/bin/imagetool.sh update \\ --tag model-in-image:WLS-v1 \\ --fromImage container-registry.oracle.com/middleware/weblogic:12.2.1.4 \\ --wdtModel ./model-in-image__WLS-v1/model.10.yaml \\ --wdtVariables ./model-in-image__WLS-v1/model.10.properties \\ --wdtArchive ./model-in-image__WLS-v1/archive.zip \\ --wdtModelOnly \\ --wdtDomainType WLS \\ --chown oracle:root If you don’t see the imagetool directory, then you missed a step in the prerequisites.\nThis command runs the WebLogic Image Tool in its Model in Image mode, and does the following:\nBuilds the final image as a layer on the container-registry.oracle.com/middleware/weblogic:12.2.1.4 base image. Copies the WDT ZIP file that’s referenced in the WIT cache into the image. Note that you cached WDT in WIT using the keyword latest when you set up the cache during the sample prerequisites steps. This lets WIT implicitly assume it’s the desired WDT version and removes the need to pass a -wdtVersion flag. Copies the specified WDT model, properties, and application archives to image location /u01/wdt/models. When the command succeeds, it should end with output like the following:\n[INFO ] Build successful. Build time=36s. Image tag=model-in-image:WLS-v1 Also, if you run the docker images command, then you will see an image named model-in-image:WLS-v1.\nNOTE: If you have Kubernetes cluster worker nodes that are remote to your local machine, then you need to put the image in a location that these nodes can access. See Ensuring your Kubernetes cluster can access images.\nThis sample uses General Availability (GA) images. GA images are suitable for demonstration and development purposes only where the environments are not available from the public Internet; they are not acceptable for production use. In production, you should always use CPU (patched) images from OCR or create your images using the WebLogic Image Tool (WIT) with the --recommendedPatches option. For more guidance, see Apply the Latest Patches and Updates in Securing a Production Environment for Oracle WebLogic Server.\nCreate WebLogic domain In this section, you will deploy the new image to namespace sample-domain1-ns, including the following steps:\nCreate a namespace for the WebLogic domain. Upgrade the operator to manage the WebLogic domain namespace. Create a Secret containing your WebLogic administrator user name and password. Create a Secret containing your Model in Image runtime encryption password: All Model in Image domains must supply a runtime encryption Secret with a password value. It is used to encrypt configuration that is passed around internally by the operator. The value must be kept private but can be arbitrary; you can optionally supply a different secret value every time you restart the domain. Deploy a Domain YAML file that references the new image. Wait for the domain’s Pods to start and reach their ready state. Namespace Create a namespace that can host one or more domains:\n$ kubectl create namespace sample-domain1-ns ## label the domain namespace so that the operator can autodetect and create WebLogic Server pods. $ kubectl label namespace sample-domain1-ns weblogic-operator=enabled Secrets First, create the secrets needed by the WLS type model domain. In this case, you have two secrets.\nRun the following kubectl commands to deploy the required secrets:\n$ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-weblogic-credentials \\ --from-literal=username=\u0026lt;wl admin username\u0026gt; \\ --from-literal=password=\u0026lt;wl admin password\u0026gt; $ kubectl -n sample-domain1-ns label secret \\ sample-domain1-weblogic-credentials \\ weblogic.domainUID=sample-domain1 $ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-runtime-encryption-secret \\ --from-literal=password=\u0026lt;mii runtime encryption pass\u0026gt; $ kubectl -n sample-domain1-ns label secret \\ sample-domain1-runtime-encryption-secret \\ weblogic.domainUID=sample-domain1 Some important details about these secrets:\nChoosing passwords and usernames:\nReplace \u0026lt;wl admin username\u0026gt; and \u0026lt;wl admin password\u0026gt; with a username and password of your choice. The password should be at least eight characters long and include at least one digit. Remember what you specified. These credentials may be needed again later. Replace \u0026lt;mii runtime encryption pass\u0026gt; with a password of your choice. The WebLogic credentials secret:\nIt is required and must contain username and password fields. It must be referenced by the spec.webLogicCredentialsSecret field in your Domain. It also must be referenced by macros in the domainInfo.AdminUserName and domainInfo.AdminPassWord fields in your model YAML file. The Model WDT runtime secret:\nThis is a special secret required by Model in Image. It must contain a password field. It must be referenced using the spec.model.runtimeEncryptionSecret field in its Domain. It must remain the same for as long as the domain is deployed to Kubernetes but can be changed between deployments. It is used to encrypt data as it\u0026rsquo;s internally passed using log files from the domain\u0026rsquo;s introspector job and on to its WebLogic Server pods. Deleting and recreating the secrets:\nYou delete a secret before creating it, otherwise the create command will fail if the secret already exists. This allows you to change the secret when using the kubectl create secret command. You name and label secrets using their associated domain UID for two reasons:\nTo make it obvious which secrets belong to which domains. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all resources associated with a domain. Domain resource Now, you create a Domain YAML file. A Domain is the key resource that tells the operator how to deploy a WebLogic domain.\nCopy the following to a file called /tmp/mii-sample/mii-initial.yaml or similar, or use the file /tmp/mii-sample/domain-resources/WLS/mii-initial-d1-WLS-v1.yaml that is included in the sample source.\nClick here to view the WLS Domain YAML file. # # This is an example of how to define a Domain resource. # apiVersion: \u0026#34;weblogic.oracle/v9\u0026#34; kind: Domain metadata: name: sample-domain1 namespace: sample-domain1-ns labels: weblogic.domainUID: sample-domain1 spec: # Set to \u0026#39;FromModel\u0026#39; to indicate \u0026#39;Model in Image\u0026#39;. domainHomeSourceType: FromModel # The WebLogic Domain Home, this must be a location within # the image for \u0026#39;Model in Image\u0026#39; domains. domainHome: /u01/domains/sample-domain1 # The WebLogic Server image that the Operator uses to start the domain image: \u0026#34;model-in-image:WLS-v1\u0026#34; # Defaults to \u0026#34;Always\u0026#34; if image tag (version) is \u0026#39;:latest\u0026#39; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: regsecret # Identify which Secret contains the WebLogic Admin credentials, # the secret must contain \u0026#39;username\u0026#39; and \u0026#39;password\u0026#39; fields. webLogicCredentialsSecret: name: sample-domain1-weblogic-credentials # Whether to include the WebLogic Server stdout in the pod\u0026#39;s stdout, default is true includeServerOutInPodLog: true # Whether to enable overriding your log file location, see also \u0026#39;logHome\u0026#39; #logHomeEnabled: false # The location for domain log, server logs, server out, introspector out, and Node Manager log files # see also \u0026#39;logHomeEnabled\u0026#39;, \u0026#39;volumes\u0026#39;, and \u0026#39;volumeMounts\u0026#39;. #logHome: /shared/logs/sample-domain1 # Set which WebLogic Servers the Operator will start # - \u0026#34;Never\u0026#34; will not start any server in the domain # - \u0026#34;AdminOnly\u0026#34; will start up only the administration server (no managed servers will be started) # - \u0026#34;IfNeeded\u0026#34; will start all non-clustered servers, including the administration server, and clustered servers up to their replica count. serverStartPolicy: IfNeeded # Settings for all server pods in the domain including the introspector job pod serverPod: # Optional new or overridden environment variables for the domain\u0026#39;s pods # - This sample uses CUSTOM_DOMAIN_NAME in its image model file # to set the WebLogic domain name env: - name: CUSTOM_DOMAIN_NAME value: \u0026#34;domain1\u0026#34; - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false\u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-XX:+UseContainerSupport -Djava.security.egd=file:/dev/./urandom \u0026#34; # Optional volumes and mounts for the domain\u0026#39;s pods. See also \u0026#39;logHome\u0026#39;. #volumes: #- name: weblogic-domain-storage-volume # persistentVolumeClaim: # claimName: sample-domain1-weblogic-sample-pvc #volumeMounts: #- mountPath: /shared # name: weblogic-domain-storage-volume # The desired behavior for starting the domain\u0026#39;s administration server. # adminServer: # Setup a Kubernetes node port for the administration server default channel #adminService: # channels: # - channelName: default # nodePort: 30701 # The number of managed servers to start for unlisted clusters replicas: 1 # The desired behavior for starting a specific cluster\u0026#39;s member servers clusters: - clusterName: cluster-1 replicas: 2 # Change the `restartVersion` to force the introspector job to rerun # and apply any new model configuration, to also force a subsequent # roll of your domain\u0026#39;s WebLogic Server pods. restartVersion: \u0026#39;1\u0026#39; configuration: # Settings for domainHomeSourceType \u0026#39;FromModel\u0026#39; model: # Valid model domain types are \u0026#39;WLS\u0026#39;, \u0026#39;JRF\u0026#39;, and \u0026#39;RestrictedJRF\u0026#39;, default is \u0026#39;WLS\u0026#39; domainType: \u0026#34;WLS\u0026#34; # Optional configmap for additional models and variable files #configMap: sample-domain1-wdt-config-map # All \u0026#39;FromModel\u0026#39; domains require a runtimeEncryptionSecret with a \u0026#39;password\u0026#39; field runtimeEncryptionSecret: sample-domain1-runtime-encryption-secret # Secrets that are referenced by model yaml macros # (the model yaml in the optional configMap or in the image) #secrets: #- sample-domain1-datasource-secret NOTE: Before you deploy the domain custom resource, determine if you have Kubernetes cluster worker nodes that are remote to your local machine. If so, you need to put the Domain\u0026rsquo;s image in a location that these nodes can access and you may also need to modify your Domain YAML file to reference the new location. See Ensuring your Kubernetes cluster can access images.\nRun the following command to create the domain custom resource:\n$ kubectl apply -f /tmp/mii-sample/domain-resources/WLS/mii-initial-d1-WLS-v1.yaml NOTE: If you are choosing not to use the predefined Domain YAML file and instead created your own Domain YAML file earlier, then substitute your custom file name in the previously listed command. Previously, we suggested naming it /tmp/mii-sample/mii-initial.yaml.\nVerify the WebLogic Server pods are all running:\n$ kubectl get all -n sample-domain1-ns NAME READY STATUS RESTARTS AGE pod/sample-domain1-admin-server 1/1 Running 0 41m pod/sample-domain1-managed-server1 1/1 Running 0 40m pod/sample-domain1-managed-server2 1/1 Running 0 40m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/sample-domain1-admin-server ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 41m service/sample-domain1-cluster-cluster-1 ClusterIP 100.66.99.27 \u0026lt;none\u0026gt; 8001/TCP 40m service/sample-domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 40m service/sample-domain1-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 40m Invoke the web application Create a load balancer to access the WebLogic Server Administration Console and applications deployed in the cluster. Tanzu supports the MetalLB load balancer and NGINX ingress for routing.\nInstall the MetalLB load balancer by running following commands:\n## create namespace metallb-system $ kubectl create ns metallb-system ## deploy MetalLB load balancer $ kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.9.2/manifests/metallb.yaml -n metallb-system ## create secret $ kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=\u0026#34;$(openssl rand -base64 128)\u0026#34; $ cat metallb-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.100.50-192.168.100.65 $ kubectl apply -f metallb-configmap.yaml configmap/config created $ kubectl get all -n metallb-system NAME READY STATUS RESTARTS AGE pod/controller-684f5d9b49-jkzfk 1/1 Running 0 2m14s pod/speaker-b457r 1/1 Running 0 2m14s pod/speaker-bzmmj 1/1 Running 0 2m14s pod/speaker-gphh5 1/1 Running 0 2m14s pod/speaker-lktgc 1/1 Running 0 2m14s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/speaker 4 4 4 4 4 beta.kubernetes.io/os=linux 2m14s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/controller 1/1 1 1 2m14s NAME DESIRED CURRENT READY AGE replicaset.apps/controller-684f5d9b49 1 1 1 2m14s Install NGINX.\n$ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx --force-update $ helm install ingress-nginx ingress-nginx/ingress-nginx Create ingress for accessing the application deployed in the cluster and to access the Administration console.\n$ cat ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: sample-nginx-ingress-pathrouting namespace: sample-domain1-ns spec: ingressClassName: nginx rules: - host: http: paths: - path: / pathType: Prefix backend: service: name: sample-domain1-cluster-cluster-1 port: number: 8001 - path: /console pathType: Prefix backend: service: name: sample-domain1-admin-server port: number: 7001 $ kubectl apply -f ingress.yaml Verify ingress is running.\n$ kubectl get ingresses -n sample-domain1-ns NAME CLASS HOSTS ADDRESS PORTS AGE sample-nginx-ingress-pathrouting \u0026lt;none\u0026gt; * 192.168.100.50 80 7m18s Access the Administration Console using the load balancer IP address, http://192.168.100.50/console. The console login screen expects the WebLogic administration credentials that you specified in the Secrets.\nAccess the sample application.\n# Access the sample application using the load balancer IP (192.168.100.50) $ curl http://192.168.100.50/myapp_war/index.jsp \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v1\u0026#39; of the mii-sample JSP web-app. Welcome to WebLogic Server \u0026#39;managed-server1\u0026#39;! domain UID = \u0026#39;sample-domain1\u0026#39; domain name = \u0026#39;domain1\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found 0 local data sources: ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; $ curl http://192.168.100.50/myapp_war/index.jsp \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v1\u0026#39; of the mii-sample JSP web-app. Welcome to WebLogic Server \u0026#39;managed-server2\u0026#39;! domain UID = \u0026#39;sample-domain1\u0026#39; domain name = \u0026#39;domain1\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found 0 local data sources: ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/developerguide/documentation/",
	"title": "Documentation",
	"tags": [],
	"description": "Learn how to contribute to the operator documentation.",
	"content": "This documentation is produced using Hugo. To make an update to the documentation, follow this process:\nIf you have not already done so, clone the repository.\n$ git clone https://github.com/oracle/weblogic-kubernetes-operator Create a new branch.\n$ git checkout -b your-branch Make your documentation updates by editing the source files in documentation/site/content. Make sure you check in the changes from the documentation/site/content area only; do not build the site and check in the static files.\nIf you wish to view your changes, you can run the site locally using these commands. The site will be available on the URL shown here:\n$ cd documentation/site $ hugo server -b http://localhost:1313/weblogic-kubernetes-operator You can also run the runlocal.sh script in that directory to start the Hugo server locally.\nWhen you are ready to submit your changes, push your branch to origin and submit a pull request. Remember to follow the guidelines in the Contribute to the operator document.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": " Certificates Operator SSL/TLS certificate handling.\nDomain security WebLogic domain security and the operator.\nEncryption WebLogic domain encryption and the operator.\nService accounts Kubernetes ServiceAccounts for the operator.\nRBAC Operator role-based authorization.\nSecrets Kubernetes Secrets for the operator.\nOpenShift OpenShift information for the operator.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/developerguide/backwards-compatibility/",
	"title": "Backward compatibility",
	"tags": [],
	"description": "Review the operator&#39;s backward compatibility and maintenance.",
	"content": "Starting with the 2.0.1 release, operator releases must be backward compatible with respect to the Domain schema, operator Helm chart input values, configuration overrides template, Kubernetes resources created by the operator Helm chart, Kubernetes resources created by the operator, and the operator REST interface. We will maintain compatibility for three releases, except in the case of a clearly communicated deprecated feature, which will be maintained for one release after a replacement is available.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-operators/the-rest-api/",
	"title": "REST services",
	"tags": [],
	"description": "Use the operator&#39;s REST services.",
	"content": "Contents Introduction Configure the operator\u0026rsquo;s external REST HTTPS interface Updating operator external certificates Use the operator\u0026rsquo;s REST services How to add your certificate to your operating system trust store Sample SSL certificate and private key for the REST interface Sample operator REST client script Introduction The operator provides an optional REST API for advanced users. You can use the API as an alternative method for getting a list of WebLogic domains and clusters (for example, instead of calling kubectl get domains), or for getting certain aspects of a domain\u0026rsquo;s status (for example, instead of calling kubectl get domain MYDOMAIN -o yaml). You also can use the REST API as an alternative approach for initiating scaling operations (instead of using the Kubernetes API or command line to alter a domain resource\u0026rsquo;s replicas values).\nBeginning with operator version 4.0.5, the operator\u0026rsquo;s REST endpoint is disabled by default. Install the operator with the Helm install option --set \u0026quot;enableRest=true\u0026quot; to enable the REST endpoint.\nConfigure the operator\u0026rsquo;s external REST HTTPS interface The operator can expose an external REST HTTPS interface which can be accessed from outside the Kubernetes cluster. As with the operator\u0026rsquo;s internal REST interface, the external REST interface requires an SSL/TLS certificate and private key that the operator will use as the identity of the external REST interface.\nTo enable the external REST interface, configure these values in a custom configuration file or on the Helm command line:\nSet externalRestEnabled to true. Set externalRestIdentitySecret to the name of the Kubernetes tls secret that contains the certificates and private key. For more information about the REST identity secret, see Updating operator external certificates. Optionally, set externalRestHttpsPort to the external port number for the operator REST interface (defaults to 31001). NOTE: A node port is a security risk because the port may be publicly exposed to the Internet in some environments. If you need external access to the REST port, then consider alternatives, such as providing access through your load balancer or using Kubernetes port forwarding.\nUpdating operator external certificates If the operator needs to update the external certificate and key currently being used or was installed without an external REST API SSL/TLS identity, then use the helm upgrade command to restart the operator with the new or updated Kubernetes tls secret that contains the desired certificates.\nThe operator requires a restart to begin using the new or updated external certificate. Use the Helm --recreate-pods flag to cause the existing Kubernetes Pod to be terminated and a new pod to be started with the updated configuration.\nFor example, if the operator was installed with the Helm release name weblogic-operator in the namespace weblogic-operator-ns and the Kubernetes tls secret is named weblogic-operator-cert, then you can use the following commands to update the operator certificates and key:\n$ kubectl create secret tls weblogic-operator-cert -n weblogic-operator-ns \\ --cert=\u0026lt;path-to-certificate\u0026gt; --key=\u0026lt;path-to-private-key\u0026gt; $ helm get values weblogic-operator -n weblogic-operator-ns $ helm -n weblogic-operator-ns upgrade weblogic-operator weblogic-operator/weblogic-operator \\ --wait --recreate-pods --reuse-values \\ --set externalRestEnabled=true \\ --set externalRestIdentitySecret=weblogic-operator-cert Additional resources:\nREST interface configuration settings Sample to create external certificate and key Use the operator\u0026rsquo;s REST services You can access most of the REST services using GET, for example:\nTo obtain a list of domains, send a GET request to the URL /operator/latest/domains To obtain a list of clusters in a domain, send a GET request to the URL /operator/latest/domains/\u0026lt;domainUID\u0026gt;/clusters All of the REST services require authentication. Callers must pass in a valid token header and must have a properly configured CA certificate in their trust store, so that the X.509 certificate presented by the server is trusted by the client (see How to add your certificate to your operating system trust store). In previous operator versions, the operator performed checks using the Kubernetes token review and subject access review APIs, and then updated the Domain resource using the operator\u0026rsquo;s privileges. Now, by default, the operator will use the caller\u0026rsquo;s bearer token to perform the underlying update to the Domain resource using the caller\u0026rsquo;s privileges and thus delegating authentication and authorization checks directly to the Kubernetes API Server (see REST interface configuration). Depending on your Kubernetes cluster authentication and authorization configuration, there are multiple ways of getting a valid token. For an example using a Kubernetes service account, see the Sample operator REST client script.\nWhen using the operator\u0026rsquo;s REST services to scale up or down a WebLogic cluster, you may need to grant patch access to the user or service account associated with the caller\u0026rsquo;s bearer token. This can be done with an RBAC ClusterRoleBinding between the user or service account and the ClusterRole that defines the permissions for the WebLogic domains resource. For more information, see Using RBAC Authorization in the Kubernetes documentation.\nCallers should pass in the Accept:/application/json header.\nTo protect against Cross Site Request Forgery (CSRF) attacks, the operator REST API requires that you send in a X-Requested-By header when you invoke a REST endpoint that makes a change (for example, when you POST to the /scale endpoint). The value is an arbitrary name such as MyClient. For example, when using curl:\n$ curl ... -H X-Requested-By:MyClient ... -X POST .../scaling If you do not pass in the X-Requested-By header, then you\u0026rsquo;ll get a 400 (bad request) response without any details explaining why the request is bad. The X-Requested-By header is not needed for requests that only read, for example, when you GET any of the operator\u0026rsquo;s REST endpoints.\nBefore using the Sample operator REST client script, you must:\nUpdate it to ensure that it has the correct service account, namespaces, and such, and it points to the values.yaml file that you used to install the operator (so that it can get the certificates). Add your operator\u0026rsquo;s certificate to your operating system\u0026rsquo;s trust store (see How to add your certificate to your operating system trust store). If you are using a self-signed certificate and your client is macOS, you may need to update the version of curl you have installed (though newer versions of macOS come with newer versions of curl). Oracle recommends curl 7.63.0 (x86_64-apple-darwin17.7.0) libcurl/7.63.0 SecureTransport zlib/1.2.11 or later. If you are unsure, then check with curl --version. How to add your certificate to your operating system trust store For macOS, find the certificate in Finder, and double-click on it. This will add it to your keystore and open Keychain Access. Find the certificate in Keychain Access and double-click on it to open the details. Open the Trust drop-down menu and set the value of When using this certificate to Always Trust, then close the detail window. Enter your password when prompted.\nFor Oracle Linux, run the following script once to copy the certificate into /tmp/operator.cert.pem, and then run these commands to add the certificate to the trust store:\n$ sudo cp /tmp/operator.cert.pem /etc/pki/ca-trust/source/anchors/ $ sudo update-ca-trust enable; sudo update-ca-trust extract $ openssl x509 -noout -hash -in /tmp/operator.cert.pem $ sudo ln -s /etc/pki/ca-trust/source/anchors/operator.cert.pem /etc/pki/tls/certs/e242d2da.0 In the final command, the file name e242d2da.0 should be the output of the previous command plus the suffix .0.\nFor other operating systems, consult your operating system\u0026rsquo;s documentation (or Google).\nSample SSL certificate and private key for the REST interface For testing purposes, the WebLogic Kubernetes Operator project provides a sample script that generates a self-signed certificate and private key for the operator external REST interface. The generated certificate and key are stored in a Kubernetes tls secret and the sample script outputs the corresponding configuration values in YAML format. These values can be added to your custom YAML configuration file, for use when the operator\u0026rsquo;s Helm chart is installed. For more detailed information about the sample script and how to run it, see the Sample to create certificate and key.\nThe sample script should not be used in a production environment because typically a self-signed certificate for external communication is not considered safe. A certificate signed by a commercial certificate authority is more widely accepted and should contain valid host names, expiration dates, and key constraints.\nSample operator REST client script Here is a small, sample BASH script that may help to prepare the necessary token, certificates, and such, to call the operator\u0026rsquo;s REST services:\n#!/bin/bash KUBERNETES_SERVER=$1 URL_TAIL=$2 REST_PORT=`kubectl get services -n weblogic-operator -o jsonpath=\u0026#39;{.items[?(@.metadata.name == \u0026#34;external-weblogic-operator-svc\u0026#34;)].spec.ports[?(@.name == \u0026#34;rest\u0026#34;)].nodePort}\u0026#39;` REST_ADDR=\u0026#34;https://${KUBERNETES_SERVER}:${REST_PORT}\u0026#34; SECRET=`kubectl get serviceaccount weblogic-operator -n weblogic-operator -o jsonpath=\u0026#39;{.secrets[0].name}\u0026#39;` ENCODED_TOKEN=`kubectl get secret ${SECRET} -n weblogic-operator -o jsonpath=\u0026#39;{.data.token}\u0026#39;` TOKEN=`echo ${ENCODED_TOKEN} | base64 --decode` OPERATOR_CERT_DATA=`kubectl get secret -n weblogic-operator weblogic-operator-external-rest-identity -o jsonpath=\u0026#39;{.data.tls\\.crt}\u0026#39;` OPERATOR_CERT_FILE=\u0026#34;/tmp/operator.cert.pem\u0026#34; echo ${OPERATOR_CERT_DATA} | base64 --decode \u0026gt; ${OPERATOR_CERT_FILE} cat ${OPERATOR_CERT_FILE} echo \u0026#34;Ready to call operator REST APIs\u0026#34; STATUS_CODE=`curl \\ -v \\ --cacert ${OPERATOR_CERT_FILE} \\ -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ -H Accept:application/json \\ -X GET ${REST_ADDR}/${URL_TAIL} \\ -o curl.out \\ --stderr curl.err \\ -w \u0026#34;%{http_code}\u0026#34;` cat curl.err cat curl.out | jq . You can use the -k option to bypass the check to verify that the operator\u0026rsquo;s certificate is trusted (instead of curl --cacert), but this is insecure.\nTo use this script, pass in the Kubernetes server address and then the URL you want to call. The script assumes jq is installed and uses it to format the response. This can be removed if desired. In addition to the response, the script also prints out quite a bit of useful debugging information. Here is an example of the output of this script:\n$ ./rest.sh kubernetes001 operator/latest/domains/domain1/clusters Ready to call operator REST APIs Note: Unnecessary use of -X or --request, GET is already inferred. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0* Trying 10.139.151.214... * TCP_NODELAY set * Connected to kubernetes001 (10.1.2.3) port 31001 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH * error setting certificate verify locations, continuing anyway: * CAfile: /tmp/operator.cert.pem CApath: none * TLSv1.2 (OUT), TLS handshake, Client hello (1): } [512 bytes data] * TLSv1.2 (IN), TLS handshake, Server hello (2): { [81 bytes data] * TLSv1.2 (IN), TLS handshake, Certificate (11): { [799 bytes data] * TLSv1.2 (IN), TLS handshake, Server key exchange (12): { [413 bytes data] * TLSv1.2 (IN), TLS handshake, Server finished (14): { [4 bytes data] * TLSv1.2 (OUT), TLS handshake, Client key exchange (16): } [150 bytes data] * TLSv1.2 (OUT), TLS change cipher, Client hello (1): } [1 bytes data] * TLSv1.2 (OUT), TLS handshake, Finished (20): } [16 bytes data] * TLSv1.2 (IN), TLS change cipher, Client hello (1): { [1 bytes data] * TLSv1.2 (IN), TLS handshake, Finished (20): { [16 bytes data] * SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256 * ALPN, server did not agree to a protocol * Server certificate: * subject: CN=weblogic-operator * start date: Jan 18 16:30:01 2018 GMT * expire date: Jan 16 16:30:01 2028 GMT * issuer: CN=weblogic-operator * SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway. \u0026gt; GET /operator/latest/domains/domain1/clusters HTTP/1.1 \u0026gt; Host: kubernetes001:31001 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJ3ZWJsb2dpYy1vcGVyYXRvciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQ (truncated) 1vcGVyYXRvcjp3ZWJsb2dpYy1vcGVyYXRvciJ9.NgaGR0NbzbJpVXguQDjRKyDBnNTqwgwPEXv3NjWwMcaf0OlN54apHubdrIx6KYz9ONGz-QeTLnoMChFY7oWA6CBfbvjt-GQX6JvdoJYxsQo1pt-E6sO2YvqTFE4EG-gpEDaiCE_OjZ_bBpJydhIiFReToA3-mxpDAUK2_rUfkWe5YEaLGMWoYQfXPAykzFiH4vqIi_tzzyzNnGxI2tUcBxNh3tzWFPGXKhzG18HswiwlFU5pe7XEYv4gJbvtV5tlGz7YdmH74Rc0dveV-54qHD_VDC5M7JZVh0ZDlyJMAmWe4YcdwNQQNGs91jqo1-JEM0Wj8iQSDE3cZj6MB0wrdg \u0026gt; Accept:application/json \u0026gt; 0 0 0 0 0 0 0 0 --:--:-- 0:00:01 --:--:-- 0\u0026lt; HTTP/1.1 200 OK \u0026lt; Content-Type: application/json \u0026lt; Content-Length: 463 \u0026lt; { [463 bytes data] 100 463 100 463 0 0 205 0 0:00:02 0:00:02 --:--:-- 205 * Connection #0 to host kubernetes001 left intact { \u0026#34;links\u0026#34;: [ { \u0026#34;rel\u0026#34;: \u0026#34;self\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;href\u0026#34;: \u0026#34;/operator/latest/domains/domain1/clusters\u0026#34; }, { \u0026#34;rel\u0026#34;: \u0026#34;canonical\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;href\u0026#34;: \u0026#34;/operator/latest/domains/domain1/clusters\u0026#34; }, { \u0026#34;rel\u0026#34;: \u0026#34;parent\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;href\u0026#34;: \u0026#34;/operator/latest/domains/domain1\u0026#34; } ], \u0026#34;items\u0026#34;: [ { \u0026#34;links\u0026#34;: [ { \u0026#34;rel\u0026#34;: \u0026#34;self\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;href\u0026#34;: \u0026#34;/operator/latest/domains/domain1/clusters/cluster-1\u0026#34; }, { \u0026#34;rel\u0026#34;: \u0026#34;canonical\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;href\u0026#34;: \u0026#34;/operator/latest/domains/domain1/clusters/cluster-1\u0026#34; } ], \u0026#34;cluster\u0026#34;: \u0026#34;cluster-1\u0026#34; } ] } "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/configoverrides/",
	"title": "Configuration overrides",
	"tags": [],
	"description": "Use overrides to customize domains.",
	"content": "Contents Overview How do you specify overrides? How do overrides work during runtime? Prerequisites Typical overrides Unsupported overrides Overrides distribution Override template names and syntax Override template names Override template schemas Override template macros Override template syntax special requirements Override template samples Overriding config.xml Overriding a data source module Step-by-step guide Debugging Internal design flow Overview Configuration overrides can only be used in combination with Domain in Image and Domain on PV domains. For Model in Image domains, use Model in Image Runtime Updates instead.\nUse configuration overrides (also called situational configuration) to customize a Domain in Image or Domain on PV domain\u0026rsquo;s WebLogic domain configuration without modifying the domain\u0026rsquo;s actual config.xml or system resource files. For example, you may want to override a JDBC data source XML module user name, password, and URL so that it references a local database.\nNOTE: The Domain in Image domain home source type is deprecated in WebLogic Kubernetes Operator version 4.0. Oracle recommends that you choose either Domain on PV or Model in Image, depending on your needs.\nYou can use overrides to customize domains as they are moved from QA to production, are deployed to different sites, or are even deployed multiple times at the same site. Beginning with operator version 3.0.0, you can now modify configuration overrides for running WebLogic Server instances and have these new overrides take effect dynamically. There are limitations to the WebLogic configuration attributes that can be modified by overrides and only changes to dynamic configuration MBean attributes may be changed while a server is running. Other changes, specifically overrides to non-dynamic MBeans, must be applied when servers are starting or restarting.\nHow do you specify overrides? Make sure your domain home meets the prerequisites. See Prerequisites. Make sure your overrides are supported. See Typical overrides and Unsupported overrides. Create a Kubernetes ConfigMap that contains: Override templates (also known as situational configuration templates), with names and syntax as described in Override template names and syntax. A file named version.txt that contains the exact string 2.0. Set your Domain configuration.overridesConfigMap field to the name of this ConfigMap. If templates leverage secret macros: Create Kubernetes Secrets that contain template macro values. Set your domain configuration.secrets to reference the aforementioned Secrets. If your configuration overrides modify non-dynamic MBean attributes and you currently have WebLogic Server instances from this domain running: Decide if the changes you are making to non-dynamic MBean attributes can be applied by rolling the affected clusters or Managed Server instances or if the change required a full domain shutdown. If a full domain shut down is required, stop all running WebLogic Server instance Pods in your domain and then restart them. (See Starting and stopping servers.) Otherwise, simply restart your domain, which includes rolling clusters. (See Restarting servers.) Verify your overrides are taking effect. (See Debugging). For a detailed walk-through of these steps, see the Step-by-step guide.\nHow do overrides work during runtime? Configuration overrides are processed during the operator\u0026rsquo;s introspection phase. Introspection automatically occurs when: The operator is starting a WebLogic Server instance when there are currently no other servers running. This occurs when the operator first starts servers for a domain or when starting servers following a full domain shutdown. For Model in Image, the operator determines that at least one WebLogic Server instance that is currently running must be shut down and restarted. This could be a rolling of one or more clusters, the shut down and restart of one or more WebLogic Server instances, or a combination. You can initiate introspection by changing the value of the Domain introspectVersion field. For configuration overrides and during introspection, the operator will: Resolve any macros in your override templates. Place expanded override templates in the optconfig directory located in each WebLogic domain home directory. When the WebLogic Server instances start, they will: Automatically load the override files from the optconfig directory. Use the override values in the override files instead of the values specified in their config.xml or system resource XML files. WebLogic Server instances monitor the files in the optconfig directory so that if these files change while the server is running, WebLogic will detect and use the new configuration values based on the updated contents of these files. This only works for changes to configuration overrides related to dynamic configuration MBean attributes. For a detailed walk-through of the runtime flow, see the Internal design flow.\nPrerequisites Configuration overrides can be used in combination with Domain in Image and Domain on PV domains. For Model in Image domains (introduced in 3.0.0), use Model in Image Runtime Updates instead.\nA WebLogic domain home must not contain any configuration overrides XML file in its optconfig directory that was not placed there by the operator. Any existing configuration overrides XML files in this directory will be deleted and replaced by your operator override templates, if any.\nIf you want to override a JDBC, JMS, or WLDF (diagnostics) module, then the original module must be located in your domain home config/jdbc, config/jms, and config/diagnostics directory, respectively. These are the default locations for these types of modules.\nTypical overrides Typical attributes for overrides include:\nUser names, passwords, and URLs for: JDBC data sources JMS bridges, foreign servers, and SAF Network channel external/public addresses For remote RMI clients (T3, JMS, EJB) For remote WLST clients Network channel external/public ports For remote RMI clients (T3, JMS, EJB) Debugging Tuning (MaxMessageSize, and such) See overrides distribution for a description of distributing new or changed configuration overrides to already running WebLogic Server instances.\nUnsupported overrides IMPORTANT: The operator does not support customer-provided overrides in the following areas.\nDomain topology (cluster members) Network channel listen address, port, and enabled fields Server and domain log locations Default or custom file store directories when domain.spec.dataHome is set Node Manager related configuration Changing any existing MBean name Adding or removing a module (for example, a JDBC module) Specifically, do not use custom overrides for:\nAdding or removing: Servers Clusters Network Access Points (custom channels) Modules Changing any of the following: Dynamic cluster size Default, SSL, and Admin channel Enabled, listen address, and port Network Access Point (custom channel), listen address, or port Server and domain log locations \u0026ndash; use the domain.spec.logHome setting instead and ensure that domain.spec.logHomeEnabled is set to true Default or custom file store directories when domain.spec.dataHome is set Node Manager access credentials Any existing MBean name (for example, you cannot change the domain name) Note that it\u0026rsquo;s supported, even expected, to override network access point public or external addresses and ports. Also note that external access to JMX (MBean) or online WLST requires that the network access point internal port and external port match (external T3 or HTTP tunneling access to JMS, RMI, or EJBs don\u0026rsquo;t require port matching).\nThe behavior when using an unsupported override is undefined.\nOverrides distribution The operator generates the final configuration overrides, combining customer-provided configuration overrides and operator-generated overrides, during the operator\u0026rsquo;s introspection phase. These overrides are then used when starting or restarting WebLogic Server instances. Starting with operator version 3.0.0, these overrides can also be distributed and applied to already running WebLogic Server instances.\nFor Domain on PV, the ability to change WebLogic domain configuration using traditional management transactions involving the Administration Console or WLST can be combined with the ability to initiate a repeat introspection and distribute updated configuration overrides. This combination supports use cases such as defining a new WebLogic cluster and then immediately starting Managed Server cluster members.\nOverride template names and syntax Overrides leverage a built-in WebLogic feature called \u0026ldquo;Configuration Overriding\u0026rdquo; which is often informally called \u0026ldquo;Situational Configuration.\u0026rdquo; Configuration overriding consists of XML formatted files that closely resemble the structure of WebLogic config.xml and system resource module XML files. In addition, the attribute fields in these files can embed add, replace, and delete verbs to specify the desired override action for the field.\nOverride template names The operator requires a different file name format for override templates than WebLogic\u0026rsquo;s built-in configuration overrides feature. It converts the names to the format required by the configuration overrides feature when it moves the templates to the domain home optconfig directory. The following table describes the format:\nOriginal Configuration Required Override Name config.xml config.xml JMS module jms-MODULENAME.xml JDBC module jdbc-MODULENAME.xml Diagnostics module diagnostics-MODULENAME.xml A MODULENAME must correspond to the MBean name of a system resource defined in your original config.xml file. It\u0026rsquo;s not possible to add a new module by using overrides. If you need your overrides to set up a new module, then have your original configuration specify \u0026lsquo;skeleton\u0026rsquo; modules that can be overridden.\nOverride template schemas An override template must define the exact schemas required by the configuration overrides feature. The schemas vary based on the file type you wish to override.\nconfig.xml\n\u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;d:domain xmlns:d=\u0026#34;http://xmlns.oracle.com/weblogic/domain\u0026#34; xmlns:f=\u0026#34;http://xmlns.oracle.com/weblogic/domain-fragment\u0026#34; xmlns:s=\u0026#34;http://xmlns.oracle.com/weblogic/situational-config\u0026#34;\u0026gt; ... \u0026lt;/d:domain\u0026gt; jdbc-MODULENAME.xml\n\u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;jdbc:jdbc-data-source xmlns:jdbc=\u0026#34;http://xmlns.oracle.com/weblogic/jdbc-data-source\u0026#34; xmlns:f=\u0026#34;http://xmlns.oracle.com/weblogic/jdbc-data-source-fragment\u0026#34; xmlns:s=\u0026#34;http://xmlns.oracle.com/weblogic/situational-config\u0026#34;\u0026gt; ... \u0026lt;/jdbc:jdbc-data-source\u0026gt; jms-MODULENAME.xml\n\u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;jms:weblogic-jms xmlns:jms=\u0026#34;http://xmlns.oracle.com/weblogic/weblogic-jms\u0026#34; xmlns:f=\u0026#34;http://xmlns.oracle.com/weblogic/weblogic-jms-fragment\u0026#34; xmlns:s=\u0026#34;http://xmlns.oracle.com/weblogic/situational-config\u0026#34; \u0026gt; ... \u0026lt;/jms:weblogic-jms\u0026gt; diagnostics-MODULENAME.xml\n\u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;wldf:wldf-resource xmlns:wldf=\u0026#34;http://xmlns.oracle.com/weblogic/weblogic-diagnostics\u0026#34; xmlns:f=\u0026#34;http://xmlns.oracle.com/weblogic/weblogic-diagnostics-fragment\u0026#34; xmlns:s=\u0026#34;http://xmlns.oracle.com/weblogic/situational-config\u0026#34; \u0026gt; ... \u0026lt;/wldf:wldf-resource\u0026gt; Override template macros The operator supports embedding macros within override templates. This helps make your templates flexibly handle multiple use cases, such as specifying a different URL, user name, and password for a different deployment.\nTwo types of macros are supported, environment variable macros and secret macros:\nEnvironment variable macros have the syntax ${env:ENV-VAR-NAME}, where the supported environment variables include DOMAIN_UID, DOMAIN_NAME, DOMAIN_HOME, and LOG_HOME.\nSecret macros have the syntax ${secret:SECRETNAME.SECRETKEY} and ${secret:SECRETNAME.SECRETKEY:encrypt}.\nThe secret macro SECRETNAME field must reference the name of a Kubernetes Secret, and the SECRETKEY field must reference a key within that Secret. For example, if you have created a Secret named dbuser with a key named username that contains the value scott, then the macro ${secret:dbuser.username} will be replaced with the word scott before the template is copied into its WebLogic Server instance Pod.\nSECURITY NOTE: Use the :encrypt suffix in a secret macro to encrypt its replacement value with the WebLogic WLST encrypt command (instead of leaving it at its plain text value). This is useful for overriding MBean attributes that expect encrypted values, such as the password-encrypted field of a data source, and is also useful for ensuring that a custom overrides configuration file the operator places in the domain home does not expose passwords in plain-text.\nOverride template syntax special requirements Check each of the following items for best practices and to ensure custom overrides configuration takes effect:\nReference the name of the current bean and each parent bean in any hierarchy you override. Note that the combine-mode verbs (add and replace) should be omitted for beans that are already defined in your original domain home configuration. See Override template samples for examples. Use replace and add verbs as follows: If you are adding a new bean that doesn\u0026rsquo;t already exist in your original domain home config.xml, then specify add on the MBean itself and on each attribute within the bean. See the server-debug stanza in Override template samples for an example. If you are adding a new attribute to an existing bean in the domain home config.xml, then the attribute needs an add verb. See the max-message-size stanza in Override template samples for an example. If you are changing the value of an existing attribute within a domain home config.xml, then the attribute needs a replace verb. See the public-address stanza in Override template samples for an example. When overriding config.xml: The XML namespace (xmlns: in the XML) must be exactly as specified in Override template schemas. For example, use d: to reference config.xml beans and attributes, f: for add and replace domain-fragment verbs, and s: to reference the configuration overrides schema. Avoid specifying the domain name stanza, as this may cause some overrides to be ignored (for example, server-template scoped overrides). When overriding modules: It is a best practice to use XML namespace abbreviations jms:, jdbc:, and wldf: respectively for JMS, JDBC, and WLDF (diagnostics) module override files. A module must already exist in your original configuration if you want to override it; it\u0026rsquo;s not possible to add a new module by using overrides. If you need your overrides to set up a new module, then have your original configuration specify \u0026lsquo;skeleton\u0026rsquo; modules that can be overridden. See Overriding a data source module for best practice advice. Note that similar advice applies generally to other module types. Consider having your original configuration reference invalid user names, passwords, and URLs: If your original (non-overridden) configuration references non-working user names, passwords, and URLS, then this helps guard against accidentally deploying a working configuration that\u0026rsquo;s invalid for the intended environment. For example, if your base configuration references a working QA database, and there is some mistake in setting up overrides, then it\u0026rsquo;s possible the running servers will connect to the QA database when you deploy to your production environment. Override template samples Here are some sample template override files.\nOverriding config.xml The following config.xml override file demonstrates:\nSetting the max-message-size field on a WebLogic Server named admin-server. It assumes the original config.xml does not define this value, and so uses add instead of replace. Sets the public-address and public-port fields with values obtained from a Secret named test-host with keys hostname and port. It assumes the original config.xml already sets these fields, and so uses replace instead of add. Sets two debug settings. It assumes the original config.xml does not have a server-debug stanza, so it uses add throughout the entire stanza. \u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;d:domain xmlns:d=\u0026#34;http://xmlns.oracle.com/weblogic/domain\u0026#34; xmlns:f=\u0026#34;http://xmlns.oracle.com/weblogic/domain-fragment\u0026#34; xmlns:s=\u0026#34;http://xmlns.oracle.com/weblogic/situational-config\u0026#34; \u0026gt; \u0026lt;d:server\u0026gt; \u0026lt;d:name\u0026gt;admin-server\u0026lt;/d:name\u0026gt; \u0026lt;d:max-message-size f:combine-mode=\u0026#34;add\u0026#34;\u0026gt;78787878\u0026lt;/d:max-message-size\u0026gt; \u0026lt;d:server-debug f:combine-mode=\u0026#34;add\u0026#34;\u0026gt; \u0026lt;d:debug-server-life-cycle f:combine-mode=\u0026#34;add\u0026#34;\u0026gt;true\u0026lt;/d:debug-server-life-cycle\u0026gt; \u0026lt;d:debug-jmx-core f:combine-mode=\u0026#34;add\u0026#34;\u0026gt;true\u0026lt;/d:debug-jmx-core\u0026gt; \u0026lt;/d:server-debug\u0026gt; \u0026lt;d:network-access-point\u0026gt; \u0026lt;d:name\u0026gt;T3Channel\u0026lt;/d:name\u0026gt; \u0026lt;d:public-address f:combine-mode=\u0026#34;replace\u0026#34;\u0026gt;${secret:test-host.hostname}\u0026lt;/d:public-address\u0026gt; \u0026lt;d:public-port f:combine-mode=\u0026#34;replace\u0026#34;\u0026gt;${secret:test-host.port}\u0026lt;/d:public-port\u0026gt; \u0026lt;/d:network-access-point\u0026gt; \u0026lt;/d:server\u0026gt; \u0026lt;/d:domain\u0026gt; Overriding a data source module The following jdbc-testDS.xml override template demonstrates setting the URL, user name, and password-encrypted fields of a JDBC module named testDS by using secret macros. The generated configuration overrides that replaces the macros with secret values will be located in the DOMAIN_HOME/optconfig/jdbc directory. The password-encrypted field will be populated with an encrypted value because it uses a secret macro with an :encrypt suffix. The Secret is named dbsecret and contains three keys: url, username, and password.\nBest practices for data source modules and their overrides:\nA data source module must already exist in your original configuration if you want to override it; it\u0026rsquo;s not possible to add a new module by using overrides. If you need your overrides to set up a new module, then have your original configuration specify \u0026lsquo;skeleton\u0026rsquo; modules that can be overridden. See the next two bulleted items for the typical contents of a skeleton data source module. Set your original (non-overridden) URL, username, and password to invalid values. This helps prevent accidentally starting a server without overrides, and then having the data source successfully connect to a database that\u0026rsquo;s wrong for the current environment. For example, if these attributes are set to reference a QA database in your original configuration, then a mistake configuring overrides in your production Kubernetes Deployment could cause your production applications to use your QA database. Set your original (non-overridden) JDBCConnectionPoolParams MinCapacity and InitialCapacity to 0, and set your original DriverName to a reference an existing JDBC Driver. This ensures that you can still successfully boot a server even when you have configured invalid URL/username/password values, your database isn\u0026rsquo;t running, or you haven\u0026rsquo;t specified your overrides yet. \u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;jdbc:jdbc-data-source xmlns:jdbc=\u0026#34;http://xmlns.oracle.com/weblogic/jdbc-data-source\u0026#34; xmlns:f=\u0026#34;http://xmlns.oracle.com/weblogic/jdbc-data-source-fragment\u0026#34; xmlns:s=\u0026#34;http://xmlns.oracle.com/weblogic/situational-config\u0026#34;\u0026gt; \u0026lt;jdbc:name\u0026gt;testDS\u0026lt;/jdbc:name\u0026gt; \u0026lt;jdbc:jdbc-driver-params\u0026gt; \u0026lt;jdbc:url f:combine-mode=\u0026#34;replace\u0026#34;\u0026gt;${secret:dbsecret.url}\u0026lt;/jdbc:url\u0026gt; \u0026lt;jdbc:properties\u0026gt; \u0026lt;jdbc:property\u0026gt; \u0026lt;jdbc:name\u0026gt;user\u0026lt;/jdbc:name\u0026gt; \u0026lt;jdbc:value f:combine-mode=\u0026#34;replace\u0026#34;\u0026gt;${secret:dbsecret.username}\u0026lt;/jdbc:value\u0026gt; \u0026lt;/jdbc:property\u0026gt; \u0026lt;/jdbc:properties\u0026gt; \u0026lt;jdbc:password-encrypted f:combine-mode=\u0026#34;replace\u0026#34;\u0026gt;${secret:dbsecret.password:encrypt}\u0026lt;/jdbc:password-encrypted\u0026gt; \u0026lt;/jdbc:jdbc-driver-params\u0026gt; \u0026lt;/jdbc:jdbc-data-source\u0026gt; Step-by-step guide Make sure your domain home meets the prerequisites. See Prerequisites. Make sure your overrides are supported. See Typical overrides, Overrides distribution, and Unsupported overrides. Create a directory containing (A) a set of configuration overrides templates for overriding the MBean properties you want to replace and (B) a version.txt file. This directory must not contain any other files. The version.txt file must contain exactly the string 2.0. Note: This version.txt file must stay 2.0 even when you are updating your templates from a previous deployment. Templates must not override the settings listed in Unsupported overrides. Templates must be formatted and named as per Override template names and syntax. Templates can embed macros that reference environment variables or Kubernetes Secrets. See Override template macros. Create a Kubernetes ConfigMap from the directory of templates. The ConfigMap must be in the same Kubernetes Namespace as the domain.\nIf the ConfigMap is going to be used by a single DOMAIN_UID, then we recommend adding the weblogic.domainUID=\u0026lt;mydomainuid\u0026gt; label to help track the resource.\nFor example, assuming ./mydir contains your version.txt and situation configuration template files:\n$ kubectl -n MYNAMESPACE create cm MYCMNAME --from-file ./mydir $ kubectl -n MYNAMESPACE label cm MYCMNAME weblogic.domainUID=DOMAIN_UID Create any Kubernetes Secrets referenced by a template \u0026ldquo;secret macro\u0026rdquo;. Secrets can have multiple keys (files) that can hold either cleartext or base64 values. We recommend that you use base64 values for passwords by using Opaque type secrets in their data field, so that they can\u0026rsquo;t be easily read at a casual glance. For more information, see https://kubernetes.io/docs/concepts/configuration/secret/.\nSecrets must be in the same Kubernetes Namespace as the domain.\nIf a Secret is going to be used by a single DOMAIN_UID, then we recommend adding the weblogic.domainUID=\u0026lt;mydomainuid\u0026gt; label to help track the resource.\nFor example:\n$ kubectl -n MYNAMESPACE create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret $ kubectl -n MYNAMESPACE label secret my-secret weblogic.domainUID=DOMAIN_UID Configure the name of the ConfigMap in the Domain YAML file configuration.overridesConfigMap field. Configure the names of each Secret in Domain YAML file. If the Secret contains the WebLogic admin username and password keys, then set the Domain YAML file webLogicCredentialsSecret field. For all other Secrets, add them to the Domain YAML file configuration.secrets field. Note: This must be in an array format even if you only add one Secret (see the following sample Domain YAML file). Changes to configuration overrides, including the contents of the ConfigMap containing the override templates or the contents of referenced Secrets, do not take effect until the operator runs or repeats its introspection of the WebLogic domain configuration. If your configuration overrides modify non-dynamic MBean attributes and you currently have WebLogic Server instances from this domain running: Decide if the changes you are making to non-dynamic MBean attributes can be applied by rolling the affected clusters or Managed Server instances, or if the change requires a full domain shutdown. (See Overrides distribution) If a full domain shut down is required, stop all running WebLogic Server instance Pods in your domain and then restart them. (See Starting and stopping servers.) Otherwise, simply restart your domain, which includes rolling clusters. (See Restarting servers.) See Debugging for ways to check if the configuration overrides are taking effect or if there are errors. Example Domain YAML:\napiVersion: \u0026#34;weblogic.oracle/v9\u0026#34; kind: Domain metadata: name: domain1 namespace: default labels: weblogic.domainUID: domain1 spec: [ ... ] webLogicCredentialsSecret: name: domain1-wl-credentials-secret configuration: overridesConfigMap: domain1-overrides-config-map secrets: [my-secret, my-other-secret] [ ... ] Debugging Use this information to verify that your overrides are taking effect or if there are errors.\nBackground notes:\nThe WebLogic Server Administration Console will not reflect any override changes.\nYou cannot use the Console to verify that overrides are taking effect. Instead, you can check overrides using WLST; see the following wlst.sh script for details. Incorrect override files may be silently accepted without warnings or errors.\nFor example, WebLogic Server instance Pods may fully start regardless of XML override syntax errors or if the specified name of an MBean is incorrect. So, it is important to make sure that the template files are correct in a QA environment, otherwise, WebLogic Servers may start even though critically required overrides are failing to take effect. Some incorrect overrides may be detected on WebLogic Server versions that support the weblogic.SituationalConfig.failBootOnError system property.\nIf the system property is supported, then, by default, WebLogic Server will fail to boot if it encounters a syntax error while loading configuration overrides files. If you want to start up WebLogic Servers with incorrectly formatted override files, then disable this check by setting the FAIL_BOOT_ON_SITUATIONAL_CONFIG_ERROR environment variable in the Kubernetes containers for the WebLogic Servers to false. Debugging steps:\nMake sure that you\u0026rsquo;ve followed each step in the Step-by-step guide.\nIf WebLogic Server instance Pods do not come up at all, then:\nExamine your Domain resource status: kubectl -n MYDOMAINNAMESPACE describe domain MYDOMAIN\nCheck events for the Domain: kubectl -n MY_NAMESPACE get events --sort-by='.lastTimestamp'. For more information, see Domain events.\nCheck the introspector job and its log.\nIn the domain\u0026rsquo;s namespace, see if you can find a job named DOMAIN_UID-introspector and a corresponding pod named something like DOMAIN_UID-introspector-xxxx. If so, examine: kubectl -n MYDOMAINNAMESPACE describe job INTROSPECTJOBNAME kubectl -n MYDOMAINNAMESPACE logs INTROSPECTPODNAME The introspector log is mirrored to the Domain resource spec.logHome directory when spec.logHome is configured and spec.logHomeEnabled is true. Check the operator log for Warning/Error/Severe messages.\nkubectl -n MYOPERATORNAMESPACE logs OPERATORPODNAME If WebLogic Server instance Pods do start, then:\nSearch your Administration Server Pod\u0026rsquo;s kubectl log for the keyword situational, for example, kubectl logs MYADMINPOD | grep -i situational. The only WebLogic Server log lines that match should look something like: \u0026lt;Dec 14, 2018 12:20:47 PM UTC\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Management\u0026gt; \u0026lt;BEA-141330\u0026gt; \u0026lt;Loading situational configuration file: /shared/domains/domain1/optconfig/custom-situational-config.xml\u0026gt; This line indicates a configuration overrides file has been loaded. If the search yields Warning or Error lines, then the format of the custom configuration overrides template is incorrect, and the Warning or Error text should describe the problem. NOTE: The following exception may show up in the server logs when overriding JDBC modules. It is not expected to affect runtime behavior, and can be ignored: java.lang.NullPointerException at weblogic.management.provider.internal.situationalconfig.SituationalConfigManagerImpl.registerListener(SituationalConfigManagerImpl.java:227) at weblogic.management.provider.internal.situationalconfig.SituationalConfigManagerImpl.start(SituationalConfigManagerImpl.java:319) ... at weblogic.management.configuration.DomainMBeanImpl.setJDBCSystemResources(DomainMBeanImpl.java:11444) ... Look in your DOMAIN_HOME/optconfig directory. This directory, or a subdirectory within this directory, should contain each of your custom configuration overrides files. If it doesn\u0026rsquo;t, then this likely indicates that your Domain YAML file configuration.overridesConfigMap was not set to match your custom override ConfigMap name, or that your custom override ConfigMap does not contain your override files. If the Administration Server Pod does start but fails to reach ready state or tries to restart:\nCheck for this message WebLogic Server failed to start due to missing or invalid situational configuration files in the Administration Server Pod\u0026rsquo;s kubectl log This suggests that the Administration Server failure to start may have been caused by errors found in a configuration override file. Lines containing the String situational may be found in the Administration Server Pod log to provide more hints. For example: \u0026lt;Jun 20, 2019 3:48:45 AM GMT\u0026gt; \u0026lt;Warning\u0026gt; \u0026lt;Management\u0026gt; \u0026lt;BEA-141323\u0026gt; \u0026lt;The situational config file has an invalid format, it is being ignored: XMLSituationalConfigFile[/shared/domains/domain1/optconfig/jdbc/testDS-0527-jdbc-situational-config.xml] because org.xml.sax.SAXParseException; lineNumber: 8; columnNumber: 3; The element type \u0026#34;jdbc:jdbc-driver-params\u0026#34; must be terminated by the matching end-tag \u0026#34;\u0026lt;/jdbc:jdbc-driver-params\u0026gt;\u0026#34;. The warning message suggests a syntax error is found in the provided configuration override file for the testDS JDBC data source. Check for pod-related Kubernetes events: kubectl -n MY_NAMESPACE get events --sort-by='.lastTimestamp'. If you\u0026rsquo;d like to verify that the configuration overrides are taking effect in the WebLogic MBean tree, then one way to do this is to compare the server config and domain config MBean tree values.\nThe domain config value should reflect the original value in your domain home configuration. The server config value should reflect the overridden value. For example, assuming your DOMAIN_UID is domain1, and your domain contains a WebLogic Server named admin-server, then: $ kubectl exec -it domain1-admin-server /bin/bash $ wlst.sh \u0026gt; connect(MYADMINUSERNAME, MYADMINPASSWORD, \u0026#39;t3://domain1-admin-server:7001\u0026#39;) \u0026gt; domainConfig() \u0026gt; get(\u0026#39;/Servers/admin-server/MaxMessageSize\u0026#39;) \u0026gt; serverConfig() \u0026gt; get(\u0026#39;/Servers/admin-server/MaxMessageSize\u0026#39;) \u0026gt; exit() To cause the WebLogic configuration overrides feature to produce additional debugging information in the WebLogic Server logs, configure the JAVA_OPTIONS environment variable in your Domain YAML file with:\n-Dweblogic.debug.DebugSituationalConfig=true -Dweblogic.debug.DebugSituationalConfigDumpXml=true Internal design flow The operator generates the final configuration overrides, which include the merging of operator-generated overrides and the processing of any customer-provided configuration overrides templates and Secrets, during its introspection phase. The operator creates a Kubernetes Job for introspection named DOMAIN_UID-introspector. The introspector Job\u0026rsquo;s Pod: Mounts the Kubernetes ConfigMap and Secrets specified by using the operator Domain configuration.overridesConfigMap, webLogicCredentialsSecret, and configuration.secrets fields. Reads the mounted configuration overrides templates from the ConfigMap and expands them to create the actual configuration overrides files for the domain: It expands some fixed replaceable values (for example, ${env:DOMAIN_UID}). It expands referenced Secrets by reading the value from the corresponding mounted secret file (for example, ${secret:mysecret.mykey}). It optionally encrypts secrets using offline WLST to encrypt the value - useful for passwords (for example, ${secret:mysecret.mykey:encrypt}). It returns expanded configuration overrides files to the operator. It reports any errors when attempting expansion to the operator. The operator runtime: Reads the expanded configuration overrides files or errors from the introspector. And, if the introspector reported no errors, it: Puts configuration overrides files in one or more ConfigMaps whose names start with DOMAIN_UID-weblogic-domain-introspect-cm. Mounts these ConfigMaps into the WebLogic Server instance Pods. Otherwise, if the introspector reported errors, it: Logs warning, error, or severe messages. Will not start WebLogic Server instance Pods; however, any already running Pods are preserved. The startServer.sh script in the WebLogic Server instance Pods: Copies the expanded configuration overrides files to a special location where the WebLogic runtime can find them: config.xml overrides are copied to the optconfig directory in its domain home. Module overrides are copied to the optconfig/jdbc, optconfig/jms, or optconfig/diagnostics directory. Deletes any configuration overrides files in the optconfig directory that do not have corresponding template files in the ConfigMap. WebLogic Servers read their overrides from their domain home\u0026rsquo;s optconfig directory. If WebLogic Server instance Pods are already running when introspection is repeated and this new introspection generates different configuration overrides then: After the operator updates the ConfigMap, Kubernetes modifies the mounted files in running containers to match the new contents of the ConfigMap. The rate of this periodic sync of ConfigMap data by kubelet is configurable, but defaults to 10 seconds. If overridesDistributionStrategy is Dynamic, then the livenessProbe.sh script, which is already periodically invoked by Kubernetes, will perform the same actions as startServer.sh to update the files in optconfig. WebLogic Server instances monitor the files in optconfig and dynamically update the active configuration based on the current contents of the configuration overrides files. Otherwise, if the overridesDistributionStrategy is OnRestart, then the updated files at the ConfigMap\u0026rsquo;s mount point are not copied to optconfig while the WebLogic Server instance is running and, therefore, don\u0026rsquo;t affect the active configuration. Changes to configuration overrides distributed to running WebLogic Server instances can only take effect if the corresponding WebLogic configuration MBean attribute is \u0026ldquo;dynamic\u0026rdquo;. For instance, the Data Source passwordEncrypted attribute is dynamic while the Url attribute is non-dynamic.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/model-in-image/overview/",
	"title": "Overview",
	"tags": [],
	"description": "Introduction to Model in Image, description of its runtime behavior, and references.",
	"content": "Contents Introduction WebLogic Deploy Tooling models Runtime behavior Using demo SSL certificates in v14.1.2.0.0 or later Runtime updates Continuous integration and delivery (CI/CD) References Introduction Model in Image is an alternative to the operator\u0026rsquo;s Domain in Image and Domain on PV domain home source types. For a comparison, see Choose a domain home source type. Unlike Domain on PV and Domain in Image, Model in Image eliminates the need to pre-create your WebLogic domain home prior to deploying your Domain YAML file.\nIt enables:\nDefining a WebLogic domain home configuration using WebLogic Deploy Tooling (WDT) model files and application archives. Embedding these files in a single image that also contains a WebLogic installation, and using the WebLogic Image Tool (WIT) to generate this image. Or, alternatively, embedding the files in one or more application-specific images. Optionally, supplying additional model files using a Kubernetes ConfigMap. Supplying Kubernetes Secrets that resolve macro references within the models. For example, a secret can be used to supply a database credential. Updating WDT model files at runtime. The WDT models are considered the source of truth and match the domain configuration at all times. For example, you can add a data source to a running domain. See Runtime updates for details. This feature is supported for standard WLS domains. For JRF domains, use Domain on PV.\nWebLogic Deploy Tooling models WDT models are a convenient and simple alternative to WebLogic Scripting Tool (WLST) configuration scripts. They compactly define a WebLogic domain using YAML files and support including application archives in a ZIP file. For a description of the model format and its integration with Model in Image, see Usage and Model files. The WDT model format is fully described in the open source, WebLogic Deploy Tooling GitHub project.\nRuntime behavior When you deploy a Model in Image domain resource YAML file:\nThe operator will run a Kubernetes Job called the \u0026lsquo;introspector job\u0026rsquo; that:\nFor an Auxiliary Image deployment, an init container is used to copy and set up the WDT installer in the main container, and all the WDT models are also copied to the main container. Sets up the call parameters for WDT to create the domain. The ordering of the models follow the pattern Model files naming and ordering. Runs WDT tooling to generate a domain home using the parameters from the previous step. Encrypts the domain salt key SerializedSystemIni.dat. Packages the domain home and passes it to the operator. The packaged domain has two parts. The first part primordial domain contains the basic configuration including the encrypted salt key. The second part domain config contains the rest of the configuration config/**/*.xml. These files are compressed but do not contain any applications, libraries, key stores, and such, because they can be restored from the WDT archives. After the introspector job completes:\nThe operator creates one or more ConfigMaps following the pattern DOMAIN_UID-weblogic-domain-introspect-cm***. These ConfigMaps contain the packaged domains from the introspector job and other information for starting the domain. After completion of the introspector job, the operator will start the domain:\nFor an Auxiliary Image deployment, an init container is used to copy and set up the WDT installer in the main container, and all the WDT models are also copied to the main container first. Restore the packaged domains in the server pod. Restore applications, libraries, key stores, and such, from the WDT archives. Decrypt the domain salt key. Start the domain. Using demo SSL certificates in v14.1.2.0.0 or later Beginning with WebLogic Server version 14.1.2.0.0, when a domain is production mode enabled, it is automatically secure mode enabled, therefore, all communications with the domain are using SSL channels and non-secure listening ports are disabled. If there are no custom certificates configured for the SSL channels, then the server uses the demo SSL certificates. The demo SSL certificates are now domain specific and generated when the domain is first created, unlike previous releases, which were distributed with the WebLogic product installation. Oracle recommends using custom SSL certificates in a production environment.\nThe certificates are created under the domain home security folder.\n-rw-r----- 1 oracle oracle 1275 Feb 15 15:55 democakey.der\r-rw-r----- 1 oracle oracle 1070 Feb 15 15:55 democacert.der\r-rw-r----- 1 oracle oracle 1478 Feb 15 15:55 DemoTrust.p12\r-rw-r----- 1 oracle oracle 1267 Feb 15 15:55 demokey.der\r-rw-r----- 1 oracle oracle 1099 Feb 15 15:55 democert.der\r-rw-r----- 1 oracle oracle 1144 Feb 15 15:55 DemoCerts.props\r-rw-r----- 1 oracle oracle 2948 Feb 15 15:55 DemoIdentity.p12 For Model in Image domains, whenever you change any security credentials including, but not limited to, the Administration Server credentials, RCU credentials, and such, the domain will be recreated and a new set of demo SSL certificates will be generated. The SSL certificates are valid for 6 months, then they expire.\nThe demo CA certificate expires in 5 years, however, whenever the domain is recreated, the entire set of certificates are regenerated so you must import the demo CA certificate again.\nIf you have any external client that needs to communicate with WebLogic Servers using SSL, then you need to import the current self-signing CA certificate, democacert.der, into your local trust store.\nkeytool -importcert -keystore \u0026lt;keystore path\u0026gt; -alias wlscacert -file $HOME/Downloads/democacer.der If you are using the WebLogic Scripting Tool, before starting the WLST session, you can set the following system properties.\nexport WLST_PROPERTIES=\u0026#34;-Dweblogic.security.TrustKeyStore=DemoTrust -Dweblogic.security.SSL.ignoreHostnameVerification=true\u0026#34; Runtime updates Model updates can be applied at runtime by changing an image, secrets, a domain resource, or a WDT model ConfigMap after initial deployment.\nSome updates may be applied to a running domain without requiring any WebLogic pod restarts (an online update), but others may require rolling the pods to propagate the update\u0026rsquo;s changes (an offline update), and still others may require shutting down the entire domain before applying the update (a full domain restart update). It is the administrator\u0026rsquo;s responsibility to make the necessary changes to a domain resource to initiate the correct type of update.\nSee Runtime updates.\nContinuous integration and delivery (CI/CD) To understand how Model in Image works with CI/CD, see CI/CD considerations.\nReferences Model in Image sample WebLogic Deploy Tooling (WDT) WebLogic Image Tool (WIT) Domain schema, documentation HTTP load balancers: Ingress documentation, sample CI/CD considerations "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/accessing-the-domain/logs/",
	"title": "Log files",
	"tags": [],
	"description": "Configure WebLogic Server and domain log settings.",
	"content": "Contents Overview WebLogic Server log file location WebLogic Server log file rotation and size Overview The operator can automatically override WebLogic Server, domain, and introspector .log and .out locations. This occurs if the Domain logHomeEnabled field is explicitly set to true, or if logHomeEnabled isn\u0026rsquo;t set and domainHomeSourceType is set to PersistentVolume. When overriding, the log location will be the location specified by the logHome setting.\nWebLogic Server .out files contain a subset of WebLogic Server .log files. The operator, by default, echoes these .out files to each server pod log. To disable this behavior, set the Domain includeServerOutInPodLog to false.\nOptionally, you can monitor a WebLogic Server and its log using an Elastic Stack (previously referred to as the ELK Stack, after Elasticsearch, Logstash, and Kibana). For an example, see the WebLogic Server Elastic Stack sample.\nKubernetes stores pod logs on each of its nodes, and, depending on the Kubernetes implementation, extra steps may be necessary to limit their disk space usage. For more information, see Kubernetes Logging Architecture.\nWebLogic Server log file location When logHomeEnabled is false, WebLogic Server log files are placed in a subdirectory \u0026lt;domain.spec.domainHome\u0026gt;/servers/\u0026lt;server name\u0026gt;/logs.\nWhen logHomeEnabled is true, WebLogic Server log files are placed in a subdirectory \u0026lt;domain.spec.logHome\u0026gt;/servers/\u0026lt;server name\u0026gt;/logs by default, or alternatively placed in subdirectory \u0026lt;domain.spec.logHome\u0026gt; when logHomeLayout is set to Flat.\nFor example, here is the default layout of the log files under the logHome root:\n/shared/logs/domain1$ ls -aRtl -rw-r----- 1 docker root 291340 Apr 27 10:26 sample-domain1.log -rw-r--r-- 1 docker root 24772 Apr 26 12:50 introspector_script.out drwxr-xr-x 1 docker root 108 Apr 25 13:49 servers ./servers/managed-server2/logs: -rw-r----- 1 docker root 921385 Apr 27 18:20 managed-server2.log -rw-r----- 1 docker root 25421 Apr 27 10:26 managed-server2.out -rw-r----- 1 docker root 14711 Apr 27 10:25 managed-server2_nodemanager.log -rw-r--r-- 1 docker root 16829 Apr 27 10:25 managed-server2_nodemanager.out -rw-r----- 1 docker root 5 Apr 27 10:25 managed-server2.pid ./servers/admin-server/logs: -rw-r----- 1 docker root 903878 Apr 27 18:19 admin-server.log -rw-r----- 1 docker root 16516 Apr 27 10:25 admin-server_nodemanager.log -rw-r--r-- 1 docker root 18610 Apr 27 10:25 admin-server_nodemanager.out -rw-r----- 1 docker root 25514 Apr 27 10:25 admin-server.out -rw-r----- 1 docker root 5 Apr 27 10:25 admin-server.pid WebLogic Server log file rotation and size If you want to fine tune the .log and .out rotation behavior for WebLogic Servers and domains, then you can update the related Log MBean in your WebLogic configuration. Alternatively, for WebLogic Servers, you can set corresponding system properties in JAVA_OPTIONS:\nHere are some WLST offline examples for creating and accessing commonly tuned Log MBeans:\n# domain log cd(\u0026#39;/\u0026#39;) create(dname,\u0026#39;Log\u0026#39;) cd(\u0026#39;/Log/\u0026#39; + dname); # configured server log for a server named \u0026#39;sname\u0026#39; cd(\u0026#39;/Servers/\u0026#39; + sname) create(sname, \u0026#39;Log\u0026#39;) cd(\u0026#39;/Servers/\u0026#39; + sname + \u0026#39;/Log/\u0026#39; + sname) # templated (dynamic) server log for a template named \u0026#39;tname\u0026#39; cd(\u0026#39;/ServerTemplates/\u0026#39; + tname) create(tname,\u0026#39;Log\u0026#39;) cd(\u0026#39;/ServerTemplates/\u0026#39; + tname + \u0026#39;/Log/\u0026#39; + tname) Here is sample WLST offline code for commonly tuned Log MBean attributes:\n# minimum log file size before rotation in kilobytes set(\u0026#39;FileMinSize\u0026#39;, 1000) # maximum number of rotated files set(\u0026#39;FileCount\u0026#39;, 10) # set to true to rotate file every time on startup (instead of append) set(\u0026#39;RotateLogOnStartup\u0026#39;, \u0026#39;true\u0026#39;) Here are the defaults for commonly tuned Log MBean attributes:\nLog MBean Attribute Production Mode Default Development Mode Default FileMinSize (in kilobytes) 5000 500 FileCount 100 7 RotateLogOnStartup false true For WebLogic Server .log and .out files (including both dynamic and configured servers), you can alternatively set logging attributes using system properties that start with weblogic.log. and that end with the corresponding Log MBean attribute name.\nFor example, you can include -Dweblogic.log.FileMinSize=1000 -Dweblogic.log.FileCount=10 -Dweblogic.log.RotateLogOnStartup=true in domain.spec.serverPod.env.name.JAVA_OPTIONS to set the behavior for all WebLogic Servers in your domain. For information about setting JAVA_OPTIONS, see Domain resource.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/persistent-storage/",
	"title": "Persistent storage",
	"tags": [],
	"description": "Use a Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC) to store WebLogic domain homes and log files.",
	"content": "This section provides general information about setting up persistent storage, which can be used for WebLogic domain homes and log files.\nPersistentVolumes and PersistentVolumeClaims Use a Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC) to store WebLogic domain homes and log files.\nUse OCI File Storage (FSS) for persistent volumes If you are running your Kubernetes cluster on Oracle Container Engine for Kubernetes (OKE), and you use Oracle Cloud Infrastructure File Storage (FSS) for persistent volumes to store the WebLogic domain home, then the file system handling, as demonstrated in the operator persistent volume sample, will require an update to properly initialize the file ownership on the persistent volume when the domain is initially created.\nProvide access to a PersistentVolumeClaim Provide an instance with access to a PersistentVolumeClaim.\nProvide access to a ConfigMap Provide an instance with access to a ConfigMap.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-operators/k8s-setup/",
	"title": "Set up Kubernetes",
	"tags": [],
	"description": "Get help for setting up a Kubernetes environment",
	"content": "Contents Cheat sheet for setting up Kubernetes Set up Kubernetes on bare compute resources in a cloud Prerequisites Quick start Install Kubernetes on your own compute resources Install Docker for Mac with Kubernetes Important note about persistent volumes Cheat sheet for setting up Kubernetes If you need some help setting up a Kubernetes environment to experiment with the operator, please read on! The supported environments are either an on-premises installation of Kubernetes, for example, on bare metal, or on a cloud provider like Oracle Cloud, Microsoft Azure, Google, or Amazon. Cloud providers allow you to provision a managed Kubernetes environment from their management consoles. You could also set up Kubernetes manually using compute resources on a cloud. There are also a number of ways to run a Kubernetes single-node cluster that are suitable for development or testing purposes. Your options include:\n\u0026ldquo;Production\u0026rdquo; options:\nSet up your own Kubernetes environment on bare compute resources on a cloud. Use your cloud provider\u0026rsquo;s management console to provision a managed Kubernetes environment. Install Kubernetes on your own compute resources (for example, \u0026ldquo;real\u0026rdquo; computers, outside a cloud). \u0026ldquo;Development/test\u0026rdquo; options:\nInstall Docker for Mac or Docker for Windows and enable its embedded Kubernetes cluster. We do not recommend or support other development/test options like Minikube, Minishift, kind, and so on. We have provided our hints and tips for several of these options in the following sections.\nSet up Kubernetes on bare compute resources in a cloud Follow the basic steps from the Terraform OKE Module Installer for Oracle Cloud Infrastructure.\nPrerequisites Download and install the Terraform OKE Module Installer for Oracle Cloud Infrastructure. Create directory for the Terraform module: $ mkdir terraformmodule $ cd terraformmodule Ensure that you have kubectl installed if you plan to interact with the cluster locally. Quick start The quick start uses the sample provided in Multi-region service mesh with Istio and OKE.\nDo a git clone of the Terraform Kubernetes Installer project:\n$ git clone https://github.com/oracle-terraform-modules/terraform-oci-oke.git Run the following commands:\n$ cd terraform-oci-oke/examples $ mkdir okewko $ cp -rf istio-mc okewko $ cd okewko Edit c1.tf and c2.tf to add:\nallow_bastion_cluster_access = true bastion_is_public = true control_plane_is_public = true $ cp terraform.tfvars.example terraform.tfvars In the terraform.tfvars file, update all values with the correct paths to the keys and IDs.\nRun the commands:\n$ terraform init $ terraform plan $ terraform apply --auto-approve This will create two OKE clusters.\nLog in to the OCI dashboard.\na. Go to Developer Services \u0026gt; OKE clusters.\nb. Select c1 cluster \u0026gt; Access Cluster.\nc. Copy and paste this command to create the kubeconfig, for example:\n$ oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.oc1...... --file $HOME/.kube/config --region us-phoenix-1 --token-version 2.0.0 --kube-endpoint PUBLIC_ENDPOINT $ export KUBECONFIG= $HOME/.kube/config Verify that the cluster is accessible:\n$ kubectl get nodes Install Kubernetes on your own compute resources For example, on Oracle Linux servers outside a cloud.\nThese instructions are for Oracle Linux 7u2+. If you are using a different flavor of Linux, you will need to adjust them accordingly.\nThese steps must be run with the root user, until specified otherwise! Any time you see YOUR_USERID in a command, you should replace it with your actual userid.\nChoose the directories where your Docker and Kubernetes files will be stored. The Docker directory should be on a disk with a lot of free space (more than 100GB) because it will be used for the /var/lib/docker file system, which contains all of your images and containers. The Kubernetes directory will be used for the /var/lib/kubelet file system and persistent volume storage.\n$ export docker_dir=/scratch/docker $ export k8s_dir=/scratch/k8s_dir Create a shell script that sets up the necessary environment variables. You should probably just append this to the user\u0026rsquo;s .bashrc so that it will get executed at login. You will also need to configure your proxy settings here if you are behind an HTTP proxy:\n#!/bin/bash export PATH=$PATH:/sbin:/usr/sbin pod_network_cidr=\u0026#34;10.244.0.0/16\u0026#34; k8s_dir=$k8s_dir ## grab my IP address to pass into kubeadm init, and to add to no_proxy vars # assume ipv4 and eth0 ip_addr=`ip -f inet addr show eth0 | egrep inet | awk \u0026#39;{print $2}\u0026#39; | awk -F/ \u0026#39;{print $1}\u0026#39;\\` export HTTPS_PROXY=http://proxy:80 export https_proxy=http://proxy:80 export NO_PROXY=localhost,127.0.0.1,.my.domain.com,/var/run/docker.sock,$pod_network_cidr,$ip_addr export no_proxy=localhost,127.0.0.1,.my.domain.com,/var/run/docker.sock,$pod_network_cidr,$ip_addr export HTTP_PROXY=http://proxy:80 export http_proxy=http://proxy:80 export KUBECONFIG=$k8s_dir/admin.conf Source that script to set up your environment variables:\n$ . ~/.bashrc If you want command completion, you can add the following to the script:\n[ -f /usr/share/bash-completion/bash_completion ] \u0026amp;\u0026amp; . /usr/share/bash-completion/bash_completion source \u0026lt;(kubectl completion bash) Create the directories you need:\n$ mkdir -p $docker_dir $k8s_dir/kubelet $ ln -s $k8s_dir/kubelet /var/lib/kubelet Set an environment variable with the Docker version you want to install:\n$ docker_version=\u0026#34;18.09.1.ol\u0026#34; Install Docker, removing any previously installed version:\n### install docker and curl-devel (for git if needed) $ yum-config-manager --enable ol7_addons ol7_latest # we are going to just uninstall any docker-engine that is installed $ yum -y erase docker-engine docker-engine-selinux # now install the docker-engine at our specified version $ yum -y install docker-engine-$docker_version curl-devel Update the Docker options:\n# edit /etc/sysconfig/docker to add custom OPTIONS $ cat /etc/sysconfig/docker | sed \u0026#34;s#^OPTIONS=.*#OPTIONS=\u0026#39;--selinux-enabled --group=docker -g $docker_dir\u0026#39;#g\u0026#34; \u0026gt; /tmp/docker.out $ diff /etc/sysconfig/docker /tmp/docker.out $ mv /tmp/docker.out /etc/sysconfig/docker Set up the Docker network, including the HTTP proxy configuration, if you need it:\n# generate a custom /setc/sysconfig/docker-network $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysconfig/docker-network # /etc/sysconfig/docker-network DOCKER_NETWORK_OPTIONS=\u0026#34;-H tcp://0.0.0.0:4243 -H unix:///var/run/docker.sock\u0026#34; HTTP_PROXY=\u0026#34;http://proxy:80\u0026#34; HTTPS_PROXY=\u0026#34;http://proxy:80\u0026#34; NO_PROXY=\u0026#34;localhost,127.0.0.0/8,.my.domain.com,/var/run/docker.sock\u0026#34; EOF Add your user to the docker group:\n$ usermod -aG docker YOUR_USERID Enable and start the Docker service that you just installed and configured:\n$ systemctl enable docker \u0026amp;\u0026amp; systemctl start docker Install the Kubernetes packages:\n#!/bin/bash # generate the yum repo config cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF setenforce 0 # install kube* packages v=${1:-1.17.0-0} old_ver=`echo $v | egrep \u0026#34;^1.7\u0026#34;` yum install -y kubelet-$v kubeadm-$v kubectl-$v kubernetes-cni # change the cgroup-driver to match what docker is using cgroup=`docker info 2\u0026gt;\u0026amp;1 | egrep Cgroup | awk \u0026#39;{print $NF}\u0026#39;` [ \u0026#34;$cgroup\u0026#34; == \u0026#34;\u0026#34; ] \u0026amp;\u0026amp; echo \u0026#34;cgroup not detected!\u0026#34; \u0026amp;\u0026amp; exit 1 cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf | sed \u0026#34;s#KUBELET_CGROUP_ARGS=--cgroup-driver=.*#KUBELET_CGROUP_ARGS=--cgroup-driver=$cgroup\\\u0026#34;#\u0026#34;\u0026gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.out diff /etc/systemd/system/kubelet.service.d/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.out mv /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.out /etc/systemd/system/kubelet.service.d/10-kubeadm.conf if [ \u0026#34;$old_ver\u0026#34; = \u0026#34;\u0026#34; ] ; then # run with swap if not in version 1.7* (starting in 1.8, kubelet # fails to start with swap enabled) # cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/kubelet.service.d/90-local-extras.conf [Service] Environment=\u0026#34;KUBELET_EXTRA_ARGS=--fail-swap-on=false\u0026#34; EOF fi Enable and start the Kubernetes Service:\n$ systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet Install and use Flannel for CNI:\n#!/bin/bash # run kubeadm init as root echo Running kubeadm init --skip-preflight-checks --apiserver-advertise-address=$ip_addr --pod-network-cidr=$pod_network_cidr echo \u0026#34; see /tmp/kubeadm-init.out for output\u0026#34; kubeadm init --skip-preflight-checks --apiserver-advertise-address=$ip_addr --pod-network-cidr=$pod_network_cidr \u0026gt; /tmp/kubeadm-init.out 2\u0026gt;\u0026amp;1 if [ $? -ne 0 ] ; then echo \u0026#34;ERROR: kubeadm init returned non 0\u0026#34; chmod a+r /tmp/kubeadm-init.out exit 1 else echo; echo \u0026#34;kubeadm init complete\u0026#34; ; echo # tail the log to get the \u0026#34;join\u0026#34; token tail -6 /tmp/kubeadm-init.out fi cp /etc/kubernetes/admin.conf $KUBECONFIG chown YOUR_USERID:YOUR_GROUP $KUBECONFIG chmod 644 $KUBECONFIG The following steps should be run with your normal (non-root) user.\nConfigure CNI:\n$ sudo -u YOUR_USERID kubectl create clusterrolebinding permissive-binding --clusterrole=cluster-admin --user=admin --user=kubelet --group=system:serviceaccounts $ sudo -u YOUR_USERID kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml Wait for kubectl get nodes to show Ready for this host:\n#!/bin/bash host=`hostname | awk -F. \u0026#39;{print $1}\u0026#39;` status=\u0026#34;NotReady\u0026#34; max=10 count=1 while [ ${status:=Error} != \u0026#34;Ready\u0026#34; -a $count -lt $max ] ; do sleep 30 status=`sudo -u YOUR_USERID kubectl get nodes | egrep $host | awk \u0026#39;{print $2}\u0026#39;` echo \u0026#34;kubectl status is ${status:=Error}, iteration $count of $max\u0026#34; count=`expr $count + 1` done status=`sudo -u YOUR_USERID kubectl get nodes | egrep $host | awk \u0026#39;{print $2}\u0026#39;` if [ ${status:=Error} != \u0026#34;Ready\u0026#34; ] ; then echo \u0026#34;ERROR: kubectl get nodes reports status=${status:=Error} after configuration, exiting!\u0026#34; exit 1 fi Taint the nodes:\n$ sudo -u YOUR_USERID kubectl taint nodes --all node-role.kubernetes.io/master- $ sudo -u YOUR_USERID kubectl get nodes $ sudo -u YOUR_USERID kubeadm version Congratulations! Docker and Kubernetes are installed and configured!\nInstall Docker for Mac with Kubernetes Docker for Mac 18+ provides an embedded Kubernetes environment that is a quick and easy way to get a simple test environment set up on your Mac. To set it up, follow these instructions:\nInstall \u0026ldquo;Docker for Mac\u0026rdquo; https://download.docker.com/mac/edge/Docker.dmg. Then start up the Docker application (press Command-Space bar, type in Docker and run it). After it is running you will see the Docker icon appear in your status bar:\nClick the Docker icon and select \u0026ldquo;Preferences\u0026hellip;\u0026rdquo; from the drop down menu. Go to the \u0026ldquo;Advanced\u0026rdquo; tab and give Docker a bit more memory if you have enough to spare:\nGo to the \u0026ldquo;Kubernetes\u0026rdquo; tab and click on the option to enable Kubernetes:\nIf you are behind an HTTP proxy, then you should also go to the \u0026ldquo;Proxies\u0026rdquo; tab and enter your proxy details.\nDocker will download the Kubernetes components and start them up for you. When it is done, you will see the Kubernetes status go to green/running in the menu:\nEnsure that kubectl on your Mac, is pointing to the correct cluster and context.\n$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE docker-for-desktop docker-for-desktop-cluster docker-for-desktop kubernetes-admin@kubernetes kubernetes kubernetes-admin $ kubectl config use-context docker-for-desktop Switched to context \u0026#34;docker-for-desktop\u0026#34;. $ kubectl config get-clusters NAME kubernetes docker-for-desktop-cluster $ kubectl config set-cluster docker-for-desktop-cluster Cluster \u0026#34;docker-for-desktop-cluster\u0026#34; set. You should add docker-for-desktop to your /etc/hosts file entry for 127.0.0.1, as shown in this example, and you must be an admin user to edit this file:\n## # Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 127.0.0.1\tlocalhost docker-for-desktop 255.255.255.255\tbroadcasthost ::1 localhost You may also have to tell kubectl to ignore the certificate by entering this command:\n$ kubectl config set-cluster docker-for-desktop --insecure-skip-tls-verify=true Then validate you are talking to the Kubernetes in Docker by entering these commands:\n$ kubectl cluster-info Kubernetes master is running at https://docker-for-desktop:6443 To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. Important note about persistent volumes Docker for Mac has some restrictions on where you can place a directory that can be used as a HostPath for a persistent volume. To keep it simple, place your directory somewhere under /Users.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/faq/",
	"title": "Frequently asked questions",
	"tags": [],
	"description": "",
	"content": "This section provides answers to frequently asked questions.\nAnswers for newcomers Answers to commonly asked newcomer questions.\nCannot pull image My domain will not start and I see errors like `ImagePullBackoff` or `Cannot pull image`.\nBoot identity not valid One or more WebLogic Server instances in my domain will not start and I see errors in the server log like this: `Boot identity not valid`.\nDomain secret mismatch One or more WebLogic Server instances in my domain will not start and the domain resource `status` or the pod log reports errors like this: Domain secret mismatch.\nNode heating problem The operator creates a Pod for each WebLogic Server instance that is started. The Kubernetes Scheduler then selects a Node for each Pod. Because the default scheduling algorithm gives substantial weight to selecting a Node where the necessary container images have already been pulled, this often results in Kubernetes running many of the Pods for WebLogic Server instances on the same Node while other Nodes are not fairly utilized. This is commonly known as the Node heating problem.\nDisabling Fast Application Notifications To support Fast Application Notifications (FAN), Oracle databases configure GRID (Oracle Grid Infrastructure). GRID is typically associated with (and required by) Oracle RAC databases but can also be used in other configurations. Oracle Autonomous Database-Serverless (ATP-S) does not provide GRID.\nCoherence requirements If you are running Coherence on Kubernetes, either inside a WebLogic domain or standalone, then there are some additional requirements to make sure that Coherence can form clusters.\nPod memory and CPU resources Tune container memory and CPU usage by configuring Kubernetes resource requests and limits, and tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your Domain YAML file.\nHandling security validations Why am I seeing these security warnings?\nScheduling pods to particular nodes How do I constrain scheduling WebLogic Server pods to particular nodes?\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/accessing-the-domain/monitoring-exporter/",
	"title": "Monitoring exporter",
	"tags": [],
	"description": "Use the WebLogic Monitoring Exporter to export Prometheus-compatible metrics.",
	"content": "The operator can export Prometheus-compatible metrics by embedding a WebLogic Monitoring Exporter configuration in its domain specification. For more details, see the WebLogic Monitoring Exporter document, Use the Monitoring Exporter with WebLogic Kubernetes Operator.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/accessing-the-domain/",
	"title": "Access and monitor domains",
	"tags": [],
	"description": "Choose among several options for accessing and monitoring domains.",
	"content": " Use the Administration Console Use the WebLogic Server Administration Console with domains running in Kubernetes.\nUse the Remote Console Use the WebLogic Remote Console with domains running in Kubernetes.\nUse WLST Use the WebLogic Scripting Tool (WLST) with domains running in Kubernetes.\nUse port forwarding Use port forwarding to access WebLogic Server administration consoles and WLST.\nIngress Configure load balancers with ingresses.\nUse an OCI load balancer If you are running your Kubernetes cluster on Oracle Container Engine for Kubernetes (OKE), then you can have Oracle Cloud Infrastructure automatically provision load balancers for you.\nIstio support Run the operator and WebLogic domains managed by the operator when Istio sidecar injection is enabled.\nExternal WebLogic clients Give WebLogic applications access to WebLogic JMS or EJB resources when either the applications or their resources are located in Kubernetes.\nDomain events Monitor domain resources using operator-generated events about resources that it manages.\nStatus conditions Monitor Domain and Cluster resources using operator-generated status conditions.\nLog files Configure WebLogic Server and domain log settings.\nMonitoring exporter Use the WebLogic Monitoring Exporter to export Prometheus-compatible metrics.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-operators/common-mistakes/",
	"title": "Common mistakes and solutions",
	"tags": [],
	"description": "Help for common installing and upgrading mistakes.",
	"content": "Contents Namespace related Changes in default Helm chart values from WebLogic Kubernetes Operator version 3.4 to 4.0 Deleting and recreating a namespace that an operator manages without informing the operator Forgetting to configure the operator to monitor a namespace Installing the operator a second time into the same namespace Installing an operator and having it manage a domain namespace that another operator is already managing Upgrading an operator and having it manage a domain namespace that another operator is already managing Installing an operator and having it manage a domain namespace that doesn\u0026rsquo;t exist Upgrading an operator and having it manage a domain namespace that doesn\u0026rsquo;t exist REST port conflict Installing an operator and assigning it the same external REST port number as another operator Upgrading an operator and assigning it the same external REST port number as another operator Missing service account Installing an operator and assigning it a service account that doesn\u0026rsquo;t exist Upgrading an operator and assigning it a service account that doesn\u0026rsquo;t exist Namespace related Common namespace-related mistakes.\nChanges in default Helm chart values from WebLogic Kubernetes Operator version 3.4 to 4.0 The default for the domainNamespaceSelectionStrategy Helm chart value, which specifies how the operator will select the namespaces that it will manage, was changed between version 3.4 and 4.0. In version 3.4, the default was List and in version 4.0, the default is LabelSelector. This means that the operator used to default to managing the set of namespaces listed in the domainNamespaces Helm chart value and the updated default is that the operator searches for namespaces that have the label specified in the domainNamespaceLabelSelector Helm chart value, which defaults to weblogic-operator=enabled.\nWhen upgrading the operator from 3.4 to 4.0, you can use the --reuse-values option, such as helm upgrade --reuse-values, to install the upgraded chart with the values that were used during the original installation, including default values. If you do not use --reuse-values, then the upgraded chart will be installed with the updated default values, which could result in an unanticipated change in the set of namespaces that the operator is managing. Alternatively, you can also set the value of domainNamespaceSelectionStragegy (or any other value) using a values file or the --set option.\nDeleting and recreating a namespace that an operator manages without informing the operator If you create a new domain in a namespace that is deleted and recreated, the domain does not start up until you notify the operator. For more details about the problem and solutions, see Namespace management.\nForgetting to configure the operator to monitor a namespace If it appears that an operator is not managing a domain resource, for example:\nA domain YAML file is deployed and no introspector or WebLogic Server pods start. The operator log contains no mention of the domain. No events are generated for the domain in the domain\u0026rsquo;s namespace. The domain resource\u0026rsquo;s domain.status fields do not contain updated information about the status of the domain. Then check to make sure that the Domain\u0026rsquo;s namespace has been set up to be monitored by an operator. For more information, see Namespace management.\nInstalling the operator a second time into the same namespace A new FAILED Helm release is created.\n$ helm install --no-hooks --name op2 --namespace myuser-op-ns --values custom-values.yaml weblogic-operator/weblogic-operator Error: release op2 failed: secrets \u0026#34;weblogic-operator-secrets\u0026#34; already exists Both the previous and new release own the resources created by the previous operator.\nYou can\u0026rsquo;t modify it to change the namespace (because helm upgrade does not let you change the namespace). You can\u0026rsquo;t fix it by deleting this release because it removes your previous operator\u0026rsquo;s resources. You can\u0026rsquo;t fix it by rolling back this release because it is not in the DEPLOYED state. You can\u0026rsquo;t fix it by deleting the previous release because it removes the operator\u0026rsquo;s resources too. All you can do is delete both operator releases and reinstall the original operator. See https://github.com/helm/helm/issues/2349.\nInstalling an operator and having it manage a domain namespace that another operator is already managing A new FAILED Helm release is created.\n$ helm install --no-hooks --name op2 --namespace myuser-op2-ns --values custom-values.yaml weblogic-operator/weblogic-operator Error: release op2 failed: rolebindings.rbac.authorization.k8s.io \u0026#34;weblogic-operator-rolebinding-namespace\u0026#34; already exists To recover:\nhelm delete --purge the failed release. NOTE: This deletes the role binding in the domain namespace that was created by the first operator release, to give the operator access to the domain namespace. helm upgrade \u0026lt;old op release\u0026gt; weblogic-operator/weblogic-operator --values \u0026lt;old op custom-values.yaml\u0026gt; This recreates the role binding. There might be intermittent failures in the operator for the period of time when the role binding was deleted. Upgrading an operator and having it manage a domain namespace that another operator is already managing The helm upgrade succeeds, and silently adopts the resources the first operator\u0026rsquo;s Helm chart created in the domain namespace (for example, rolebinding), and, if you also instructed it to stop managing another domain namespace, then it abandons the role binding it created in that namespace.\nFor example, if you delete this release, then the first operator will end up without the role binding it needs. The problem is that you don\u0026rsquo;t get a warning, so you don\u0026rsquo;t know that there\u0026rsquo;s a problem to fix.\nThis can be fixed by just upgrading the Helm release. This may also be fixed by rolling back the Helm release. Installing an operator and having it manage a domain namespace that doesn\u0026rsquo;t exist A new FAILED Helm release is created.\n$ helm install --no-hooks --name op2 --namespace myuser-op2-ns --values o.yaml weblogic-operator/weblogic-operator Error: release op2 failed: namespaces \u0026#34;myuser-d2-ns\u0026#34; not found To recover:\nhelm delete --purge the failed release. Create the domain namespace. helm install again. Upgrading an operator and having it manage a domain namespace that doesn\u0026rsquo;t exist The helm upgrade fails and moves the release to the FAILED state.\n$ helm upgrade myuser-op weblogic-operator/weblogic-operator --values o.yaml --no-hooks Error: UPGRADE FAILED: failed to create resource: namespaces \u0026#34;myuser-d2-ns\u0026#34; not found To recover:\nhelm rollback. Create the domain namespace. helm upgrade again. REST port conflict REST port conflict-related mistakes.\nInstalling an operator and assigning it the same external REST port number as another operator A new FAILED Helm release is created.\n$ helm install --no-hooks --name op2 --namespace myuser-op2-ns --values o.yaml weblogic-operator/weblogic-operator Error: release op2 failed: Service \u0026#34;external-weblogic-operator-svc\u0026#34; is invalid: spec.ports[0].nodePort: Invalid value: 31023: provided port is already allocated To recover:\n$ helm delete --purge the failed release. Change the port number and helm install the second operator again. Upgrading an operator and assigning it the same external REST port number as another operator The helm upgrade fails and moves the release to the FAILED state.\n$ helm upgrade --no-hooks --values o23.yaml op2 weblogic-operator/weblogic-operator --wait Error: UPGRADE FAILED: Service \u0026#34;external-weblogic-operator-svc\u0026#34; is invalid: spec.ports[0].nodePort: Invalid value: 31023: provided port is already allocated You can fix this by upgrading the Helm release (to fix the port number). You can also fix this by rolling back the Helm release. Missing service account Missing service account-related mistakes.\nInstalling an operator and assigning it a service account that doesn\u0026rsquo;t exist The following helm install command fails because it tries to install an operator release with a non-existing service account op2-sa.\n$ helm install op2 weblogic-operator/weblogic-operator --namespace myuser-op2-ns --set serviceAccount=op2-sa --wait --no-hooks The output contains the following error message.\nServiceAccount op2-sa not found in namespace myuser-op2-ns To recover:\nCreate the service account. helm install again. Upgrading an operator and assigning it a service account that doesn\u0026rsquo;t exist The helm upgrade with a non-existing service account fails with the same error message as mentioned in the previous section, and the existing operator deployment stays unchanged.\nTo recover:\nCreate the service account. helm upgrade again. "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/deprecated-functionality/",
	"title": "Deprecated functionality",
	"tags": [],
	"description": "",
	"content": "The following functionality has been deprecated and no longer is supported in WebLogic Kubernetes Operator.\nDomain in Image The Domain in Image domain home source type is deprecated in WebLogic Kubernetes Operator version 4.0. Oracle recommends that you choose either Domain on PV or Model in Image, depending on your needs.\nModel in Image without auxiliary images The Model in Image domain home source type without auxiliary images (the WDT model and installation files are included in the same image with the WebLogic Server installation) is deprecated in WebLogic Kubernetes Operator version 4.0.7. Oracle recommends that you use Model in Image with auxiliary images.\nModel in Image for JRF domains The Model in Image domain home source type for JRF domains is deprecated in WebLogic Kubernetes Operator version 4.1.0. For JRF domains, use Domain on PV instead.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/faq/coherence-requirements/",
	"title": "Coherence requirements",
	"tags": [],
	"description": "If you are running Coherence on Kubernetes, either inside a WebLogic domain or standalone, then there are some additional requirements to make sure that Coherence can form clusters.",
	"content": "If you are running Coherence on Kubernetes, either inside a WebLogic domain or standalone, then there are some additional requirements to make sure that Coherence can form clusters.\nNote that some Fusion Middleware products, like SOA Suite, use Coherence and so these requirements apply to them.\nUnicast and Well Known Address When the first Coherence process starts, it will form a cluster. The next Coherence process to start (for example, in a different pod), will use UDP to try to contact the senior member.\nIf you create a WebLogic domain which contains a Coherence cluster using the samples provided in this project, then that cluster will be configured correctly so that it is able to form; you do not need to do any additional manual configuration.\nIf you are running Coherence standalone (outside a WebLogic domain), then you should configure Coherence to use unicast and provide a \u0026ldquo;well known address (WKA)\u0026rdquo; so that all members can find the senior member. Most Kubernetes overlay network providers do not support multicast.\nThis is done by specifying Coherence well known addresses in a variable named coherence.wka as shown in the following example:\n-Dcoherence.wka=my-cluster-service In this example my-cluster-service should be the name of the Kubernetes service that is pointing to all of the members of that Coherence cluster.\nFor more information about running Coherence in Kubernetes outside of a WebLogic domain, refer to the Coherence operator documentation.\nOperating system library requirements In order for Coherence clusters to form correctly, the conntrack library must be installed. Most Kubernetes distributions will do this for you. If you have issues with clusters not forming, then you should check that conntrack is installed using this command (or equivalent):\n$ rpm -qa | grep conntrack libnetfilter_conntrack-1.0.6-1.el7_3.x86_64 conntrack-tools-1.4.4-4.el7.x86_64 You should see output similar to that shown previously. If you do not, then you should install conntrack using your operating system tools.\nFirewall (iptables) requirements Some Kubernetes distributions create iptables rules that block some types of traffic that Coherence requires to form clusters. If you are not able to form clusters, then you can check for this issue using the following command:\n$ iptables -t nat -v -L POST_public_allow -n Chain POST_public_allow (1 references) pkts bytes target prot opt in out source destination 164K 11M MASQUERADE all -- * !lo 0.0.0.0/0 0.0.0.0/0 0 0 MASQUERADE all -- * !lo 0.0.0.0/0 0.0.0.0/0 If you see output similar to the previous example, for example, if you see any entries in this chain, then you need to remove them. You can remove the entries using this command:\n$ iptables -t nat -v -D POST_public_allow 1 Note that you will need to run that command for each line. So in the previous example, you would need to run it twice.\nAfter you are done, you can run the previous command again and verify that the output is now an empty list.\nAfter making this change, restart your domains and the Coherence cluster should now form correctly.\nMake iptables updates permanent across reboots The recommended way to make iptables updates permanent across reboots is to create a systemd service that applies the necessary updates during the startup process.\nHere is an example; you may need to adjust this to suit your own environment:\nCreate a systemd service: $ echo \u0026#39;Set up systemd service to fix iptables nat chain at each reboot (so Coherence will work)...\u0026#39; $ mkdir -p /etc/systemd/system/ $ cat \u0026gt; /etc/systemd/system/fix-iptables.service \u0026lt;\u0026lt; EOF [Unit] Description=Fix iptables After=firewalld.service After=docker.service [Service] ExecStart=/sbin/fix-iptables.sh [Install] WantedBy=multi-user.target EOF Create the script to update iptables: $ cat \u0026gt; /sbin/fix-iptables.sh \u0026lt;\u0026lt; EOF #!/bin/bash echo \u0026#39;Fixing iptables rules for Coherence issue...\u0026#39; TIMES=$((`iptables -t nat -v -L POST_public_allow -n --line-number | wc -l` - 2)) COUNTER=1 while [ $COUNTER -le $TIMES ]; do iptables -t nat -v -D POST_public_allow 1 ((COUNTER++)) done EOF Start the service (or just reboot): $ echo \u0026#39;Start the systemd service to fix iptables nat chain...\u0026#39; $ systemctl enable --now fix-iptables "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/domain-lifecycle/",
	"title": "Domain life cycle",
	"tags": [],
	"description": "Learn how to start, stop, restart, and scale the WebLogic Server instances in your domain.",
	"content": "Learn how to start, stop, restart, and scale the WebLogic Server instances in your domain. Also learn how to manage failures and retries in your domain.\nStartup and shutdown There are fields on the Domain that specify which WebLogic Server instances should be running, started, or restarted. To start, stop, or restart servers, modify these fields on the Domain.\nRestarting This document describes when WebLogic Server instances should and will be restarted in the Kubernetes environment.\nScaling The operator provides several ways to initiate scaling of WebLogic clusters.\nDomain introspection This document describes domain introspection in the Oracle WebLogic Server in Kubernetes environment.\nLiveness and readiness probes customization This document describes how to customize the liveness and readiness probes for WebLogic Server instance Pods.\nDomain failure retry processing This document describes domain failure retry processing in the Oracle WebLogic Server in Kubernetes environment.\nLifecycle scripts A collection of useful domain lifecycle sample scripts.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-operators/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "General advice for debugging and monitoring the operator.",
	"content": "Contents Troubleshooting a particular domain resource Check Helm status Ensure the operator CRDs are installed Check the operator deployment Check the conversion webhook deployment Check common operator issues Check for operator events Check for conversion webhook events Check the operator log Check the conversion webhook log Operator ConfigMap Force the operator to restart Operator and conversion webhook logging level Troubleshooting the conversion webhook Ensure the conversion webhook is deployed and running X509: Certificate signed by unknown authority error from the webhook Webhook errors in older operator versions Webhook errors in operator dedicated Mode Check for runtime errors during conversion See also Troubleshooting a particular domain resource After you have an installed and running operator, it is rarely but sometimes necessary to debug the operator itself. If you are having problems with a particular domain resource, then first see Domain debugging.\nCheck Helm status An operator runtime is installed into a Kubernetes cluster and maintained using a Helm release. For information about how to list your installed Helm releases and get each release\u0026rsquo;s configuration, see Useful Helm operations.\nEnsure the operator CRDs are installed When you install and run an operator, the installation should have deployed a domain custom resource and a cluster custom resource to the cluster. To check, verify that the following command lists a CRD with the name domains.weblogic.oracle and another CRD with the name clusters.weblogic.oracle:\n$ kubectl get crd The command output should look something like the following:\nNAME CREATED AT clusters.weblogic.oracle 2022-10-15T03:45:27Z domains.weblogic.oracle 2022-10-15T03:45:27Z When a domain or cluster CRD is not installed, the operator runtimes will not be able to monitor domains or clusters, and commands like kubectl get domains will fail.\nTypically, the operator automatically installs each CRD when the operator first starts. However, if a CRD was not installed, for example, if the operator lacked sufficient permission to install it, then refer to the operator Prepare for installation documentation.\nCheck the operator deployment Verify that the operator\u0026rsquo;s deployment is deployed and running by listing all deployments with the weblogic.operatorName label.\n$ kubectl get deployment --all-namespaces=true -l weblogic.operatorName Check the operator deployment\u0026rsquo;s detailed status:\n$ kubectl -n OP_NAMESPACE get deployments/weblogic-operator -o yaml And/or:\n$ kubectl -n OP_NAMESPACE describe deployments/weblogic-operator Each operator deployment will have a corresponding Kubernetes pod with a name that has a prefix that matches the deployment name, plus a unique suffix that changes every time the deployment restarts.\nTo find operator pods and check their high-level status:\n$ kubectl get pods --all-namespaces=true -l weblogic.operatorName To check the details for a given pod:\n$ kubectl -n OP_NAMESPACE get pod weblogic-operator-UNIQUESUFFIX -o yaml $ kubectl -n OP_NAMESPACE describe pod weblogic-operator-UNIQUESUFFIX A pod describe usefully includes any events that might be associated with the operator.\nCheck the conversion webhook deployment All operators in a Kubernetes cluster share a single conversion webhook deployment. Verify that the conversion webhook is deployed and running by listing all deployments with the weblogic.webhookName label.\n$ kubectl get deployment --all-namespaces=true -l weblogic.webhookName Check the conversion webhook deployment\u0026rsquo;s detailed status:\n$ kubectl -n WH_NAMESPACE get deployments/weblogic-operator-webhook -o yaml And/or:\n$ kubectl -n WH_NAMESPACE describe deployments/weblogic-operator-webhook Each conversion webhook deployment will have a corresponding Kubernetes pod with a name that has a prefix that matches the deployment name, plus a unique suffix that changes every time the deployment restarts.\nTo find conversion webhook pods and check their high-level status:\n$ kubectl get pods --all-namespaces=true -l weblogic.webhookName To check the details for a given pod:\n$ kubectl -n WH_NAMESPACE get pod weblogic-operator-webhook-UNIQUESUFFIX -o yaml $ kubectl -n WH_NAMESPACE describe pod weblogic-operator-webhook-UNIQUESUFFIX A pod describe usefully includes any events that might be associated with the conversion webhook. For information about installing and uninstalling the webhook, see WebLogic Domain resource conversion webhook.\nCheck common operator issues See Common mistakes and solutions. Check the FAQs. Check for operator events To check for Kubernetes events that may have been logged to the operator\u0026rsquo;s namespace:\n$ kubectl -n OP_NAMESPACE get events --sort-by=\u0026#39;.lastTimestamp\u0026#39; Check for conversion webhook events To check for Kubernetes events that may have been logged to the conversion webhook\u0026rsquo;s namespace:\n$ kubectl -n WH_NAMESPACE get events --sort-by=\u0026#39;.lastTimestamp\u0026#39; Check the operator log Look for SEVERE and ERROR level messages in your operator logs. For example:\nFind your operator.\n$ kubectl get deployment --all-namespaces=true -l weblogic.operatorName NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE sample-weblogic-operator-ns weblogic-operator 1 1 1 1 20h Use grep on the operator log; look for SEVERE and WARNING level messages.\n$ kubectl logs deployment/weblogic-operator -n sample-weblogic-operator-ns \\ | egrep -e \u0026#34;level...(SEVERE|WARNING)\u0026#34; {\u0026#34;timestamp\u0026#34;:\u0026#34;03-18-2020T20:42:21.702+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;WARNING\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;createAndValidateKubernetesVersion\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584564141702,\u0026#34;message\u0026#34;:\u0026#34;Kubernetes minimum version check failed. Supported versions are 1.13.5+,1.14.8+,1.15.7+, but found version v1.12.3\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} You can filter out operator log messages specific to your domainUID by piping the previous logs command through grep \u0026quot;domainUID...MY_DOMAINUID\u0026quot;. For example, assuming your operator is running in namespace sample-weblogic-operator-ns and your domain UID is sample-domain1:\n$ kubectl logs deployment/weblogic-operator -n sample-weblogic-operator-ns \\ | egrep -e \u0026#34;level...(SEVERE|WARNING)\u0026#34; \\ | grep \u0026#34;domainUID...sample-domain1\u0026#34; Check the conversion webhook log To check the conversion webhook deployment\u0026rsquo;s log (especially look for SEVERE and ERROR level messages):\n$ kubectl logs -n YOUR_CONVERSION_WEBHOOK_NS -c weblogic-operator-webhook deployments/weblogic-operator-webhook Operator ConfigMap An operator\u0026rsquo;s settings are automatically maintained by Helm in a Kubernetes ConfigMap named weblogic-operator-cm in the same namespace as the operator. To view the contents of this ConfigMap, call kubectl -n sample-weblogic-operator-ns get cm weblogic-operator-cm -o yaml.\nForce the operator to restart An operator is designed to robustly handle thousands of domains even in the event of failures, so it should not normally be necessary to force an operator to restart, even after an upgrade. Accordingly, if you encounter a problem that you think requires an operator restart to resolve, then please make sure that the operator development team is aware of the issue (see Get Help).\nWhen you restart an operator:\nThe operator is temporarily unavailable for managing its namespaces. For example, a domain that is created while the operator is restarting will not be started until the operator pod is fully up again. This will not shut down your current domains or affect their resources. The restarted operator will rediscover existing domains and manage them. There are several approaches for restarting an operator:\nMost simply, use the helm upgrade command: helm upgrade \u0026lt;release-name\u0026gt; --reuse-values --recreate-pods\n$ helm upgrade weblogic-operator --reuse-values --recreate-pods Delete the operator pod, and let Kubernetes restart it.\na. First, find the operator pod you wish to delete:\n$ kubectl get pods --all-namespaces=true -l weblogic.operatorName b. Second, delete the pod. For example:\n$ kubectl delete pod/weblogic-operator-65b95bc5b5-jw4hh -n OP_NAMESPACE Scale the operator deployment to 0, and then back to 1, by changing the value of the replicas.\na. First, find the namespace of the operator deployment you wish to restart:\n$ kubectl get deployment --all-namespaces=true -l weblogic.operatorName b. Second, scale the deployment down to zero replicas:\n$ kubectl scale deployment.apps/weblogic-operator -n OP_NAMESPACE --replicas=0 c. Finally, scale the deployment back up to one replica:\n$ kubectl scale deployment.apps/weblogic-operator -n OP_NAMESPACE --replicas=1 Operator and conversion webhook logging level It should rarely be necessary to change the operator and conversion webhook to use a finer-grained logging level, but, in rare situations, the operator support team may direct you to do so. If you change the logging level, then be aware that FINE or finer-grained logging levels can be extremely verbose and quickly use up gigabytes of disk space in the span of hours, or, at the finest levels, during heavy activity, in even minutes. Consequently, the logging level should only be increased for as long as is needed to help get debugging data for a particular problem.\nTo set the operator javaLoggingLevel to FINE (default is INFO) assuming the operator Helm release is named sample-weblogic-operator its namespace is sample-weblogic-operator-ns, and you have locally downloaded the operator src to /tmp/weblogic-kubernetes-operator:\n$ cd /tmp/weblogic-kubernetes-operator $ helm upgrade \\ sample-weblogic-operator \\ weblogic-operator/weblogic-operator \\ --namespace sample-weblogic-operator-ns \\ --reuse-values \\ --set \u0026#34;javaLoggingLevel=FINE\u0026#34; \\ --wait To set the operator javaLoggingLevel back to INFO:\n$ helm upgrade \\ sample-weblogic-operator \\ weblogic-operator/weblogic-operator \\ --namespace sample-weblogic-operator-ns \\ --reuse-values \\ --set \u0026#34;javaLoggingLevel=INFO\u0026#34; \\ --wait For more information, see the javaLoggingLevel documentation.\nTroubleshooting the conversion webhook The following are some common mistakes and solutions for the conversion webhook.\nEnsure the conversion webhook is deployed and running Verify that the conversion webhook is deployed and running by following the steps in check the conversion webhook deployment. If it is not deployed, then you will see the following conversion webhook not found error when creating a Domain with weblogic.oracle/v8 schema Domain resource.\nError from server: error when creating \u0026#34;k8s-domain.yaml\u0026#34;: conversion webhook for weblogic.oracle/v9, Kind=Domain failed: Post \u0026#34;https://weblogic-operator-webhook-svc.sample-weblogic-operator-ns.svc:8084/webhook?timeout=30s\u0026#34;: service \u0026#34;weblogic-operator-webhook-svc\u0026#34; not found The conversion webhook can be deployed standalone or as part of an operator installation. Note that if the conversion webhook was installed as part of an operator installation, then it is implicitly removed by default when the operator is uninstalled. If the conversion webhook is not deployed or running, then reinstall it by following the steps in Installing the conversion webhook.\nIf the conversion webhook Deployment is deployed but is not in the ready status, then you will see a connection refused error when creating a Domain using the weblogic.oracle/v8 schema Domain resource.\nThe POST URL in the error message has the name of the conversion webhook service and the namespace. For example, if the POST URL is https://weblogic-operator-webhook-svc.sample-weblogic-operator-ns.svc:8084/webhook?timeout=30s, then the service name is weblogic-operator-webhook-svc and the namespace is sample-weblogic-operator-ns. In this case, run the following commands to ensure that the Deployment is running and the webhook service exists in the sample-weblogic-operator-ns namespace.\n$ kubectl get deployment weblogic-operator-webhook -n sample-weblogic-operator-ns NAME READY UP-TO-DATE AVAILABLE AGE weblogic-operator-webhook 1/1 1 1 87m $ kubectl get service weblogic-operator-webhook-svc -n sample-weblogic-operator-ns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE weblogic-operator-webhook-svc ClusterIP 10.106.89.198 \u0026lt;none\u0026gt; 8084/TCP 88m If the conversion webhook Deployment status is not ready, then check the conversion webhook log and the conversion webhook events in the conversion webhook namespace. If the conversion webhook service doesn\u0026rsquo;t exist, make sure that the conversion webhook was installed correctly and reinstall the conversion webhook to see if it resolves the issue.\nX509: Certificate signed by unknown authority error from the webhook The following x509: certificate signed by unknown authority error from the conversion webhook can be due to the incorrect proxy configuration of the Kubernetes API server in your environment or incorrect self-signed certificate in the conversion webhook configuration in the Domain CRD.\nError from server (InternalError): error when creating \u0026#34;./weblogic-domains/sample-domain1/domain.yaml\u0026#34;: Internal error occurred: conversion webhook for weblogic.oracle/v8, Kind=Domain failed: Post \u0026#34;https://weblogic-operator-webhook-svc.sample-weblogic-operator-ns.svc:8084/webhook?timeout=30s\u0026#34;: x509: certificate signed by unknown authority If your environment uses a PROXY server, then ensure that the NO_PROXY settings of the Kubernetes API server include the .svc value. The Kubernetes API server makes a REST request to the conversion webhook REST endpoint using the host name weblogic-operator-webhook-svc.${NAMESPACE}.svc in the POST URL. If the REST request is routed through a PROXY server, then you will see an \u0026ldquo;x509: certificate signed by unknown authority\u0026rdquo; error. Because this REST request is internal to your Kubernetes cluster, ensure that it doesn\u0026rsquo;t get routed through a PROXY server by adding .svc to the NO_PROXY settings.\nIf, for some reason your Domain CRD conversion webhook configuration has an incorrect self-signed certificate, then you can patch the Domain CRD to remove the existing conversion webhook configuration. The operator will re-create the conversion webhook configuration with the correct self-signed certificate in the Domain CRD. Use the following patch command to remove the conversion webhook configuration in the Domain CRD to see if it resolves the error.\nkubectl patch crd domains.weblogic.oracle --type=merge --patch \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;conversion\u0026#34;: {\u0026#34;strategy\u0026#34;: \u0026#34;None\u0026#34;, \u0026#34;webhook\u0026#34;: null}}}\u0026#39; Webhook errors in older operator versions When you install operator version 4.x or upgrade to operator 4.x, a conversion webhook configuration is added to your Domain CRD. If you downgrade or switch back to the operator version 3.x, the conversion webhook configuration is not removed from the CRD. This is to support environments with multiple operator installations potentially with different versions. For environments having a single operator installation, use the following patch command to manually remove the conversion webhook configuration from Domain CRD.\nkubectl patch crd domains.weblogic.oracle --type=merge --patch \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;conversion\u0026#34;: {\u0026#34;strategy\u0026#34;: \u0026#34;None\u0026#34;, \u0026#34;webhook\u0026#34;: null}}}\u0026#39; Webhook errors in operator dedicated Mode If the operator is running in the Dedicated mode, the operator\u0026rsquo;s service account will not have the permission to read or update the CRD. If you need to convert the domain resources with weblogic.oracle/v8 schema to weblogic.oracle/v9 schema using the conversion webhook in Dedicated mode, then you can manually add the conversion webhook configuration to the Domain CRD. Use the following patch command to add the conversion webhook configuration to the Domain CRD.\nNOTE: Substitute YOUR_OPERATOR_NS in the below command with the namespace where the operator is installed.\nexport OPERATOR_NS=YOUR_OPERATOR_NS kubectl patch crd domains.weblogic.oracle --type=merge --patch \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;conversion\u0026#34;: {\u0026#34;strategy\u0026#34;: \u0026#34;Webhook\u0026#34;, \u0026#34;webhook\u0026#34;: {\u0026#34;clientConfig\u0026#34;: { \u0026#34;caBundle\u0026#34;: \u0026#34;\u0026#39;$(kubectl get secret weblogic-webhook-secrets -n ${OPERATOR_NS} -o=jsonpath=\u0026#34;{.data.webhookCert}\u0026#34;| base64 --decode)\u0026#39;\u0026#34;, \u0026#34;service\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;weblogic-operator-webhook-svc\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;\u0026#39;${OPERATOR_NS}\u0026#39;\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/webhook\u0026#34;, \u0026#34;port\u0026#34;: 8084}}, \u0026#34;conversionReviewVersions\u0026#34;: [\u0026#34;v1\u0026#34;]}}}}\u0026#39; Check for runtime errors during conversion If you see a WebLogic Domain custom resource conversion webhook failed error when creating a Domain with a weblogic.oracle/v8 schema domain resource, then check the conversion webhook runtime Pod logs and check for the generated events in the conversion webhook namespace. Assuming that the conversion webhook is deployed in the sample-weblogic-operator-ns namespace, run the following commands to check for logs and events.\n$ kubectl logs -n sample-weblogic-operator-ns -c weblogic-operator-webhook deployments/weblogic-operator-webhook $ kubectl get events -n sample-weblogic-operator-ns See also If you have set up either of the following, then these documents may be helpful in debugging:\nOperator REST HTTPS interface Elastic Stack (Elasticsearch, Logstash, and Kibana) integration "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/known-limitations/",
	"title": "Known limitations",
	"tags": [],
	"description": "",
	"content": "The following sections describe known limitations for WebLogic Kubernetes Operator. Each issue may contain a workaround or an associated issue number.\nNGINX SSL passthrough ingress service does not work with Kubernetes headless service ISSUE: When installing NGINX ingress controller with SSL passthrough enabled --set \u0026quot;controller.extraArgs.enable-ssl-passthrough=true\u0026quot;, any ingress rule created subsequently, using SSL passthrough to the individual server service, will fail.\n$ kubectl -n nginx get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-operator-ingress-nginx-controller-admission ClusterIP 10.43.234.82 \u0026lt;none\u0026gt; 443/TCP 3m3s nginx-operator-ingress-nginx-controller LoadBalancer 10.43.193.149 192.168.106.2 80:32315/TCP,443:31710/TCP 3m3s For example, after creating the domain, the operator creates a headless Kubernetes service for each server and a headed service for the cluster. The individual service for each server is headless as the CLUSTER-IP is None; the cluster service is headed as the CLUSTER-IP has a valid IP address.\n$ kubectl -n sample-domain1-ns get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE sample-domain1-admin-server ClusterIP None \u0026lt;none\u0026gt; 7001/TCP,7002/TCP 23h sample-domain1-cluster-cluster-1 ClusterIP 10.43.108.163 \u0026lt;none\u0026gt; 8001/TCP,7002/TCP 23h sample-domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP,7002/TCP 23h If you create a passthrough ingress rule to use SSL passthrough to access the admin server, for example:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: console-ssl-passthru namespace: sample-domain1-ns annotations: nginx.ingress.kubernetes.io/ssl-passthrough: \u0026#39;true\u0026#39; spec: ingressClassName: nginx rules: - http: paths: - backend: service: name: sample-domain1-admin-server port: number: 7002 path: / pathType: Prefix host: localk8s.com Accessing the WebLogic Console on the admin server, through the ingress controller, will result in an error.\ncurl -k -v -L https://localk8s.com:31710/console * Trying 192.168.106.2:31710... * Connected to localk8s.com (192.168.106.2) port 31710 (#0) * ALPN: offers h2,http/1.1 * (304) (OUT), TLS handshake, Client hello (1): * LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to localk8s.com:31710 * Closing connection 0 curl: (35) LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to localk8s.com:31710 This is currently reported as an NGINX bug in https://github.com/kubernetes/ingress-nginx/issues/1718\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/faq/resource-settings/",
	"title": "Pod memory and CPU resources",
	"tags": [],
	"description": "Tune container memory and CPU usage by configuring Kubernetes resource requests and limits, and tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your Domain YAML file.",
	"content": "Contents Introduction Setting resource requests and limits in a Domain or Cluster resource Determining Pod Quality Of Service Java heap size and memory resource considerations Importance of setting heap size and memory resources Default heap sizes Configuring heap size CPU resource considerations Operator sample heap and resource configuration Burstable pods and JDK active processor count calculation Configuring CPU affinity Measuring JVM heap, Pod CPU, and Pod memory References Introduction The CPU and memory requests and limits for WebLogic Server Pods usually need to be tuned where the optimal values depend on your workload, applications, and the Kubernetes environment. Requests and limits should be configured based on the expected traffic during peak usage. For example:\nTune CPU and memory high enough to handle expected peak workloads for applications that require large amounts of in-memory processing or very CPU intensive calculations. Tune memory high enough so that WebLogic JMS messaging applications that generate large backlogs of unprocessed persistent or non-persistent messages can expect JMS to efficiently cache the backlogs in memory. CPU requirements are sometimes significantly higher when a WebLogic Server is starting. This means that a low CPU allocation that might be suitable for light runtime workloads risks causing unacceptably slow startup times. Requirements vary considerably between use cases. You may need to experiment and make adjustments based on monitoring resource usage in your environment.\nThe operator creates a container in its own Pod for each domain\u0026rsquo;s WebLogic Server instances and for the short-lived introspector job that is automatically launched before WebLogic Server Pods are launched. You can tune container memory and CPU usage by configuring Kubernetes resource requests and limits, and you can tune a WebLogic JVM heap usage using the USER_MEM_ARGS environment variable in your Domain YAML file. By default, the introspector job pod uses the same CPU and memory settings as the domain\u0026rsquo;s WebLogic Administration Server pod. Similarly, the operator created init containers in the introspector job pod for the Auxiliary Images based domains use the same CPU and memory settings as the domain\u0026rsquo;s WebLogic Administration Server pod. You can override the settings of the introspector job pod using the domain.spec.introspector.serverPod element. A resource request sets the minimum amount of a resource that a container requires. A resource limit is the maximum amount of a resource a container is given and prevents a container from using more than its share of a resource. Additionally, resource requests and limits determine a Pod\u0026rsquo;s quality of service.\nThis FAQ discusses tuning these parameters so WebLogic Server instances run efficiently.\nSetting resource requests and limits in a Domain or Cluster resource You can set Kubernetes memory and CPU requests and limits in a Domain or Cluster YAML file using its domain.spec.serverPod.resources stanza, and you can override the setting for individual WebLogic Server instances using the serverPod.resources element in domain.spec.adminServer, or domain.spec.managedServers. You can override the setting for member servers of a cluster using the cluster.spec.serverPod element. Note that the introspector job pod uses the same settings as the WebLogic Administration Server pod. You can override the settings of the introspector job pod using the domain.spec.introspector.serverPod element.\nValues set in the .serverPod stanzas for a more specific type of pod, override the same values if they are also set for a more general type of pod, and inherit any other values set in the more general pod. The domain.spec.adminServer.serverPod, domain.spec.managedServers.serverPod, and cluster.spec.serverPod stanzas all inherit from and override the domain.spec.serverPod stanza. When a domain.spec.managedServers.serverPod stanza refers to a pod that is part of a cluster, it inherits from and overrides from its cluster\u0026rsquo;s cluster.spec.serverPod setting (if any), which in turn inherits from and overrides the domain\u0026rsquo;s domain.spec.serverPod setting.\nspec: serverPod: resources: requests: cpu: \u0026#34;250m\u0026#34; memory: \u0026#34;768Mi\u0026#34; limits: cpu: \u0026#34;2\u0026#34; memory: \u0026#34;2Gi\u0026#34; Limits and requests for CPU resources are measured in CPU units. One CPU, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An m suffix in a CPU attribute indicates \u0026lsquo;milli-CPU\u0026rsquo;, so 250m is 25% of a CPU.\nMemory can be expressed in various units, where one Mi is one IEC unit mega-byte (1024^2), and one Gi is one IEC unit giga-byte (1024^3).\nSee also Managing Resources for Containers, Assign Memory Resources to Containers and Pods and Assign CPU Resources to Containers and Pods in the Kubernetes documentation.\nDetermining Pod Quality Of Service A Pod\u0026rsquo;s Quality of Service (QoS) is based on whether it\u0026rsquo;s configured with resource requests and limits:\nBest Effort QoS (lowest priority): If you don\u0026rsquo;t configure requests and limits for a Pod, then the Pod is given a best-effort QoS. In cases where a Node runs out of non-shareable resources, the default out-of-resource eviction policy evicts running Pods with the best-effort QoS first.\nBurstable QoS (medium priority): If you configure both resource requests and limits for a Pod, and set the requests to be less than their respective limits, then the Pod will be given a burstable QoS. Similarly, if you only configure resource requests (without limits) for a Pod, then the Pod QoS is also burstable. If a Node runs out of non-shareable resources, the Node\u0026rsquo;s kubelet will evict burstable Pods only when there are no more running best-effort Pods.\nGuaranteed QoS (highest priority): If you set a Pod\u0026rsquo;s requests and the limits to equal values, then the Pod will have a guaranteed QoS. These settings indicate that your Pod will consume a fixed amount of memory and CPU. With this configuration, if a Node runs out of shareable resources, then the Node\u0026rsquo;s kubelet will evict best-effort and burstable QoS Pods before terminating guaranteed QoS Pods.\nFor most use cases, Oracle recommends configuring WebLogic Pods with memory and CPU requests and limits, and furthermore, setting requests equal to their respective limits to ensure a guaranteed QoS.\nIn later versions of Kubernetes, it is possible to fine tune scheduling and eviction policies using Pod Priority Preemption in combination with the serverPod.priorityClassName Domain field. Note that Kubernetes already ships with two PriorityClasses: system-cluster-critical and system-node-critical. These are common classes and are used to ensure that critical components are always scheduled first.\nJava heap size and memory resource considerations Oracle recommends configuring Java heap sizes for WebLogic JVMs instead of relying on the defaults. For detailed information about memory settings when running WLST from the pod where the WLS server is running, see Use kubectl exec.\nImportance of setting heap size and memory resources It\u0026rsquo;s extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods.\nA WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\nA Pod memory limit must be sufficiently sized to accommodate the configured heap and native memory requirements, but should not be sized too large so as not to waste memory resources. If a JVM\u0026rsquo;s memory usage (sum of heap and native memory) exceeds its Pod\u0026rsquo;s limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.\nOracle recommends setting minimum and maximum heap (or heap percentages) and at least a container memory request.\nIf resource requests and resource limits are set too high, then your Pods may not be scheduled due to a lack of Node resources. It will unnecessarily use up CPU shared resources that could be used by other Pods, or may prevent other Pods from running.\nDefault heap sizes With the latest Java versions, Java 8 update 191 and later, or Java 11, if you don\u0026rsquo;t configure a heap size (no -Xms or -Xms), the default heap size is dynamically determined:\nIf you configure the memory limit for a container, then the JVM default maximum heap size will be 25% (1/4th) of container memory limit and the default minimum heap size will be 1.56% (1/64th) of the limit value.\nIn this case, the default JVM heap settings are often too conservative because the WebLogic JVM is the only major process running in the container.\nIf no memory limit is configured, then the JVM default maximum heap size will be 25% (1/4th) of its Node\u0026rsquo;s machine RAM and the default minimum heap size will be 1.56% (1/64th) of the RAM.\nIn this case, the default JVM heap settings can have undesirable behavior, including using unnecessary amounts of memory to the point where it might affect other Pods that run on the same Node.\nConfiguring heap size If you specify Pod memory limits, Oracle recommends configuring WebLogic Server heap sizes as a percentage. The JVM will interpret the percentage as a fraction of the limit. This is done using the JVM -XX:InitialRAMPercentage and -XX:MaxRAMPercentage options in the USER_MEM_ARGS Domain environment variable. For example:\nspec: resources: env: - name: USER_MEM_ARGS value: \u0026#34;-XX:InitialRAMPercentage=25.0 -XX:MaxRAMPercentage=50.0 -Djava.security.egd=file:/dev/./urandom\u0026#34; Additionally, there\u0026rsquo;s a node-manager process that\u0026rsquo;s running in the same container as the WebLogic Server, which has its own heap and native memory requirements. Its heap is tuned by using -Xms and -Xmx in the NODEMGR_MEM_ARGS environment variable. Oracle recommends setting the Node Manager heap memory to fixed sizes, instead of percentages, where the default tuning is usually sufficient.\nNotice that the NODEMGR_MEM_ARGS, USER_MEM_ARGS, and WLST_EXTRA_PROPERTIES environment variables all include -Djava.security.egd=file:/dev/./urandom by default. This helps to speed up the Node Manager and WebLogic Server startup on systems with low entropy, plus similarly helps to speed up introspection job usage of the WLST encrypt command. We have included this property in the previous example for specifying a custom USER_MEM_ARGS value to preserve this speedup. See the environment variable defaults documentation for more information.\nIn some cases, you might only want to configure memory resource requests but not configure memory resource limits. In such scenarios, you can use the traditional fixed heap size settings (-Xms and -Xmx) in your WebLogic Server USER_MEM_ARGS instead of the percentage settings (-XX:InitialRAMPercentage and -XX:MaxRAMPercentage).\nCPU resource considerations It\u0026rsquo;s important to set both a CPU request and a limit for WebLogic Server Pods. This ensures that all WebLogic Server Pods have enough CPU resources, and, as discussed earlier, if the request and limit are set to the same value, then they get a guaranteed QoS. A guaranteed QoS ensures that the Pods are handled with a higher priority during scheduling and as such, are the least likely to be evicted.\nIf a CPU request and limit are not configured for a WebLogic Server Pod:\nThe Pod can end up using all the CPU resources available on its Node and starve other containers from using shareable CPU cycles.\nThe WebLogic Server JVM may choose an unsuitable garbage collection (GC) strategy.\nA WebLogic Server self-tuning work-manager may incorrectly optimize the number of threads it allocates for the default thread pool.\nIt\u0026rsquo;s also important to keep in mind that if you set a value of CPU core count that\u0026rsquo;s larger than the core count of your biggest Node, then the Pod will never be scheduled. Let\u0026rsquo;s say you have a Pod that needs 4 cores but you have a Kubernetes cluster that\u0026rsquo;s comprised of 2 core VMs. In this case, your Pod will never be scheduled and will have Pending status. For example:\n$ kubectl get pod sample-domain1-managed-server1 -n sample-domain1-ns NAME READY STATUS RESTARTS AGE sample-domain1-managed-server1 0/1 Pending 0 65s $ kubectl describe pod sample-domain1-managed-server1 -n sample-domain1-ns Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 16s (x3 over 26s) default-scheduler 0/2 nodes are available: 2 Insufficient cpu. Operator sample heap and resource configuration The operator samples configure non-default minimum and maximum heap sizes for WebLogic Server JVMs of at least 256MB and 512MB respectively. You can edit a sample\u0026rsquo;s template or Domain or Cluster YAML file resources.env USER_MEM_ARGS to have different values. See Configuring heap size.\nSimilarly, the operator samples configure CPU and memory resource requests to at least 250m and 768Mi respectively.\nThere\u0026rsquo;s no memory or CPU limit configured by default in samples and so the default QoS for sample WebLogic Server Pod\u0026rsquo;s is burstable.\nIf you wish to set resource requests or limits differently on a sample Domain or Cluster YAML file or template, see Setting resource requests and limits in a Domain or Cluster resource. Or, for samples that generate their Domain resource using an \u0026ldquo;inputs\u0026rdquo; YAML file, see the serverPodMemoryRequest, serverPodMemoryLimit, serverPodCpuRequest, and serverPodCpuLimit parameters in the sample\u0026rsquo;s create-domain.sh inputs file.\nBurstable pods and JDK active processor count calculation If you have Burstable Pods that configure the CPU resource requests but have no CPU limits, then the JDK can incorrectly calculate the active processor count. The JDK interprets the value of the --cpu-shares parameter (which maps to spec.containers[].resources.requests.cpu) to limit how many CPUs the current process can use, as explained in JDK-8288367. This might cause the JVM to use fewer CPUs than available, leading to an under utilization of CPU resources when running in a Kubernetes environment. Updating the JDK to newer versions, JDK 8u371 or JDK 11.0.17, fixes this.\nTo override the number of CPUs that the JVM automatically detects and uses when creating threads for various subsystems, use the -XX:ActiveProcessorCount Java option.\nConfiguring CPU affinity A Kubernetes hosted WebLogic Server may exhibit high lock contention in comparison to an on-premises deployment. This lock contention may be due to a lack of CPU cache affinity or scheduling latency when workloads move between different CPU cores.\nIn an on-premises deployment, CPU cache affinity, and therefore reduced lock contention, can be achieved by binding WLS Java process to a particular CPU core(s) (using the taskset command).\nIn a Kubernetes deployment, similar cache affinity can be achieved by doing the following:\nEnsuring a Pod\u0026rsquo;s CPU resource request and limit are set and equal (to ensure a guaranteed QoS). Configuring the kubelet CPU manager policy to be static (the default is none). See Control CPU Management Policies on the Node. Note that some Kubernetes environments may not allow changing the CPU management policy. Measuring JVM heap, Pod CPU, and Pod memory You can monitor JVM heap, Pod CPU, and Pod memory using Prometheus and Grafana. Also, see Tools for Monitoring Resources in the Kubernetes documentation.\nReferences Managing Resources for Containers in the Kubernetes documentation. Assign Memory Resources to Containers and Pods in the Kubernetes documentation. Assign CPU Resources to Containers and Pods in the Kubernetes documentation. Pod Priority Preemption in the Kubernetes documentation. GCP Kubernetes best practices: Resource requests and limits Tools for Monitoring Resources in the Kubernetes documentation. Blog \u0026ndash; Docker support in Java 8. (Discusses Java container support in general.) Blog \u0026ndash; Kubernetes Patterns : Capacity Planning "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/cicd/",
	"title": "CI/CD considerations",
	"tags": [],
	"description": "Learn about managing domain images with continuous integration and continuous delivery (CI/CD).",
	"content": "Overview In this section, we will discuss the recommended techniques for managing the evolution and mutation of container images to run WebLogic Server in Kubernetes. There are several approaches and techniques available, and the choice of which to use depends very much on your particular requirements. We will start with a review of the \u0026ldquo;problem space,\u0026rdquo; and then talk about the considerations that would lead us to choose various approaches. We will provide details about several approaches to implementing CI/CD and links to examples.\nReview of the problem space Kubernetes makes a fundamental assumption that images are immutable, that they contain no state, and that updating them is as simple as throwing away a pod/container and replacing it with a new one that uses a newer version of the image. These assumptions work very well for microservices applications, but for more traditional workloads, we need to do some extra thinking and some extra work to get the behavior we want.\nCI/CD is an area where the standard assumptions aren\u0026rsquo;t always suitable. In the microservices architecture, you typically minimize dependencies and build images from scratch with all of the dependencies in them. You also typically keep all of the configuration outside of the image, for example, in Kubernetes config maps or secrets, and all of the state outside of the image too. This makes it very easy to update running pods with a new image.\nLet\u0026rsquo;s consider how a WebLogic image is different. There will, of course, be a base layer with the operating system; let\u0026rsquo;s assume it is Oracle Linux \u0026ldquo;slim\u0026rdquo;. Then you need a JDK and this is very commonly in another layer. Many people will use the officially supported JDK images from the Docker Store, like the Server JRE image, for example. On top of this, you need the WebLogic Server binaries (the \u0026ldquo;Oracle Home\u0026rdquo;). On top of that, you may wish to have some patches or updates installed. And then you need your domain, that is the configuration.\nThere is also other information associated with a domain that needs to live somewhere, for example leasing tables, message and transaction stores, and so on. We recommend that these be kept in a database to take advantage of built-in database server HA, and the fact that disaster recovery of sites across all but the shortest distances, almost always requires using a single database server to consolidate and replicate data (DataGuard).\nThere are three common approaches on how to structure these components:\nThe first, \u0026ldquo;domain on a persistent volume\u0026rdquo; or Domain on PV, places the JDK and WebLogic binaries in the image, but the domain home is kept on a separate persistent storage outside of the image. The second, Domain in Image, puts the JDK, WebLogic Server binaries, and the domain home all in the image. NOTE: The Domain in Image domain home source type is deprecated in WebLogic Kubernetes Operator version 4.0. Oracle recommends that you choose either Domain on PV or Model in Image, depending on your needs. The third approach, Model in Image, puts the JDK, WebLogic Server binaries, and a domain model in the image, and generates the domain home at runtime from the domain model. All of these approaches are perfectly valid (and fully supported) and they have various advantages and disadvantages. We have listed the relative advantages of these approaches here.\nOne of the key differences between these approaches is how many images you have, and therefore, how you build and maintain them - your image CI/CD process. Let\u0026rsquo;s take a short detour and talk about image layering.\nContainer image layering Learn about container image layering and why it is important.\nWhy layering matters Learn why container image layering affects CI/CD processes.\nChoose an approach How to choose an approach.\nMutate the domain layer How to mutate the domain layer.\nCopy domains How to copy domains.\nTools Tools that are available to build CI/CD pipelines.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/faq/security-validation/",
	"title": "Handling security validations",
	"tags": [],
	"description": "Why am I seeing these security warnings?",
	"content": " After applying the July2021 PSU, I\u0026rsquo;m now seeing security warnings, such as:\nDescription: Production Mode is enabled but user lockout settings are not secure in realm: myrealm, i.e. LockoutThreshold should not be greater than 5, LockoutDuration should not be less than 30.\nSOLUTION: Update the user lockout settings (LockoutThreshold, LockoutDuration) to be secure.\nWebLogic Server has a new, important feature to ensure and help you secure your WLS domains when running in production. With the July 2021 PSU applied, WebLogic Server regularly validates your domain configuration settings against a set of security configuration guidelines to determine whether the domain meets key security guidelines recommended by Oracle. For more information and additional details, see MOS Doc 2788605.1 \u0026ldquo;WebLogic Server Security Warnings Displayed Through the Admin Console\u0026rdquo; and Review Potential Security Issues in Securing a Production Environment for Oracle WebLogic Server.\nWarnings may be at the level of the JDK, or that SSL is not enabled. Some warnings may recommend updating your WebLogic configuration. You can make the recommended configuration changes using an approach that depends on your domain home source type:\nFor Domain on PV, use the WebLogic Scripting Tool (WLST), WebLogic Server Administration Console, WebLogic Deploy Tooling (WDT), or configuration overrides.\nFor Domain in Image, create a new image with the recommended changes or use configuration overrides.\nFor Model in Image, supply model files with the recommended changes in its image\u0026rsquo;s modelHome directory or use runtime updates.\nMsg ID: 090985\nDescription: Production Mode is enabled but the the file or directory /u01/oracle/user_projects/domains/domain/bin/setDomainEnv.sh is insecure since its permission is not a minimum of umask 027.\nSOLUTION: Change the file or directory permission to at most allow only write by owner, read by group.\nDescription: The file or directory SerializedSystemIni.dat is insecure since its permission is not a minimum of umask 027.\nSOLUTION: Change the file or directory permission to at most allow only write by owner, read by group.\nWhen the WebLogic Image Tool (WIT) creates a Domain Home in Image, you can specify the --target OpenShift option so that when WIT creates the domain, it sets the correct permissions in the domain home. When no --target option is specified, then the domain home directory has a umask of 027.\nFor information about handling file permission warnings on the OpenShift Kubernetes Platform, see the OpenShift documentation.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/managing-fmw-domains/",
	"title": "Manage FMW Infrastructure domains",
	"tags": [],
	"description": "FMW Infrastructure domains contain the Java Required Files (JRF) feature and are the prerequisite for upper stack products like Oracle SOA Suite.",
	"content": "Contents Limitations Obtaining the FMW Infrastructure image Creating an FMW Infrastructure image Configuring access to your database Running the database inside Kubernetes Running the database outside Kubernetes Running the Repository Creation Utility to set up your database schema Creating schemas Dropping schemas Create a Kubernetes Secret with the RCU credentials Creating an FMW Infrastructure domain Patching the FMW Infrastructure image Additional considerations for Coherence The operator supports FMW Infrastructure domains, that is, domains that are created with the FMW Infrastructure installer rather than the WebLogic Server installer. These domains contain Java Required Files (JRF), which are a prerequisite for \u0026ldquo;upper stack\u0026rdquo; products like Oracle SOA Suite, for example. These domains also require a database and the use of the Repository Creation Utility (RCU).\nThis document provides details about the special considerations for running FMW Infrastructure domains with the operator. Other than those considerations listed here, FMW Infrastructure domains work in the same way as WebLogic Server domains. The remainder of the documentation in this site applies equally to FMW Infrastructure domains and WebLogic Server domains.\nNOTE: FMW Infrastructure domains are supported using the Domain on PV domain home source type only.\nFor more information about the deployment of Oracle Fusion Middleware products on Kubernetes, see https://oracle.github.io/fmw-kubernetes/.\nLimitations Compared to running a WebLogic Server domain in Kubernetes using the operator, the following limitations currently exist for FMW Infrastructure domains:\nThe WebLogic Monitoring Exporter currently supports the WebLogic MBean trees only. Support for JRF MBeans has not been added yet. Only configured clusters are supported. Dynamic clusters are not supported for FMW Infrastructure domains. Note that you can still use all of the scaling features, you just need to define the maximum size of your cluster at domain creation time. FMW Infrastructure domains are not supported with any version of the operator before version 2.2.0. FMW Infrastructure domains are not supported using the Model in Image or Domain in Image domain home source types. Obtaining the FMW Infrastructure image The WebLogic Kubernetes Operator requires patch 29135930. The standard pre-built FMW Infrastructure General Availability image, container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.3, already has this patch applied. The FMW Infrastructure 12.2.1.4.0 images do not require this patch. For detailed instructions on how to log in to the Oracle Container Registry and accept license agreement, see this document.\nNOTE: As of December, 2022, Fusion Middleware 12.2.1.3 is no longer supported. The last Critical Patch Updates (CPU) images for FMW Infrastructure 12.2.1.3 were published in October, 2022. As of June, 2023, Oracle WebLogic Server 12.2.1.3 is no longer supported. The last CPU images for WebLogic Server 12.2.1.3 were published in April, 2023.\nThis sample uses General Availability (GA) images. GA images are suitable for demonstration and development purposes only where the environments are not available from the public Internet; they are not acceptable for production use. In production, you should always use CPU (patched) images from OCR or create your images using the WebLogic Image Tool (WIT) with the --recommendedPatches option. For more guidance, see Apply the Latest Patches and Updates in Securing a Production Environment for Oracle WebLogic Server.\nTo pull an image from the Oracle Container Registry, in a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms is stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nFirst, you will need to log in to the Oracle Container Registry:\n$ docker login container-registry.oracle.com Then, you can pull the image with this command:\n$ docker pull container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 If desired, you can:\nCheck the WLS version with docker run container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 sh -c 'source $ORACLE_HOME/wlserver/server/bin/setWLSEnv.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; java weblogic.version'\nCheck the WLS patches with docker run container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 sh -c '$ORACLE_HOME/OPatch/opatch lspatches'\nAdditional information about using this image is available in the Oracle Container Registry.\nCreating an FMW Infrastructure image You can also create an image containing the FMW Infrastructure binaries. We provide a sample in the Oracle GitHub account that demonstrates how to create an image to run the FMW Infrastructure. Please consult the README file associated with this sample for important prerequisite steps, such as building or pulling the Server JRE image and downloading the Fusion Middleware Infrastructure installer binary.\nAfter cloning the repository and downloading the installer from Oracle Technology Network or e-delivery, you create your image by running the provided script:\n$ cd docker-images/OracleFMWInfrastructure/dockerfiles $ ./buildDockerImage.sh -v 12.2.1.4 -s The image produced will be named oracle/fmw-infrastructure:12.2.1.4.\nYou must also install the required patch to use this image with the operator. We provide a sample that demonstrates how to create an image with the necessary patch installed.\nAfter downloading the patch from My Oracle Support, you create the patched image by running the provided script:\n$ cd docker-images/OracleFMWInfrastructure/samples/12213-patch-fmw-for-k8s $ ./build.sh This will produce an image named oracle/fmw-infrastructure:12213-update-k8s.\nAll samples and instructions reference the pre-built image, container-registry.oracle.com/middleware/fmw_infrastructure:12.2.1.4. Because these samples build an image based on WebLogic Server 12.2.1.3 and use the tag, oracle/fmw-infrastructure:12213-update-k8s, be sure to update your sample inputs to use this image value.\nThese samples allow you to create an image containing the FMW Infrastructure binaries and the necessary patch. You can use this image to run the Repository Creation Utility and to run your domain using the \u0026ldquo;domain on a persistent volume\u0026rdquo; model. If you want to use the \u0026ldquo;domain in an image\u0026rdquo; model, you will need to go one step further and add another layer with your domain in it. You can use WLST or WDT to create your domain.\nBefore creating a domain, you will need to set up the necessary schemas in your database.\nConfiguring access to your database FMW Infrastructure domains require a database with the necessary schemas installed in them. We provide a utility, called the Repository Creation Utility (RCU), which allows you to create those schemas. You must set up the database before you create your domain. There are no additional requirements added by running FMW Infrastructure in Kubernetes; the same existing requirements apply.\nFor testing and development, you may choose to run your database inside Kubernetes or outside of Kubernetes.\nThe Oracle Database images are supported only for non-production use. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker Doc ID 2216342.1.\nRunning the database inside Kubernetes If you wish to run the database inside Kubernetes, you can use the official container image from the Oracle Container Registry. Please note that there is a Slim Variant (12.2.0.1-slim tag) of EE that has reduced disk space (4GB) requirements and a quicker container startup.\nRunning the database inside the Kubernetes cluster is possibly more relevant or desirable in test/development or CI/CD scenarios.\nHere is an example of a Kubernetes YAML file to define a deployment of the Oracle database:\napiVersion: apps/v1 kind: Deployment metadata: name: oracle-db namespace: default spec: replicas: 1 selector: matchLabels: app.kubernetes.io/instance: dev app.kubernetes.io/name: oracle-db strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: creationTimestamp: null labels: app.kubernetes.io/instance: dev app.kubernetes.io/name: oracle-db spec: containers: - env: - name: DB_SID value: devcdb - name: DB_PDB value: devpdb - name: DB_DOMAIN value: k8s image: container-registry.oracle.com/database/enterprise:12.2.0.1-slim imagePullPolicy: IfNotPresent name: oracle-db ports: - containerPort: 1521 name: tns protocol: TCP resources: limits: cpu: \u0026#34;1\u0026#34; memory: 2Gi requests: cpu: 200m terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 Notice that you can pass in environment variables to set the SID, the name of the PDB, and so on. The documentation describes the other variables that are available.\nFollow the instructions in the documentation to set the sys password.\nYou should also create a service to make the database available within the Kubernetes cluster with a well known name. Here is an example:\napiVersion: v1 kind: Service metadata: name: oracle-db namespace: default spec: ports: - name: tns port: 1521 protocol: TCP targetPort: 1521 selector: app.kubernetes.io/instance: dev app.kubernetes.io/name: oracle-db sessionAffinity: None type: ClusterIP In the previous example, the database would be visible in the cluster using the address oracle-db.default.svc.cluster.local:1521/devpdb.k8s.\nWhen you run the database in the Kubernetes cluster, you will probably want to also run RCU from a pod inside your network, though this is not strictly necessary. You could create a NodePort to expose your database outside the Kubernetes cluster and run RCU on another machine with access to the cluster.\nRunning the database outside Kubernetes If you wish to run the database outside Kubernetes, you need to create a way for containers running in pods in Kubernetes to see the database. This can be done by defining a Kubernetes Service with no selector and associating it with an endpoint definition, as shown in the following example:\nkind: Service apiVersion: v1 metadata: name: database spec: type: ClusterIP ports: - port: 1521 targetPort: 1521 --- kind: Endpoints apiVersion: v1 metadata: name: database subsets: - addresses: - ip: 129.123.1.4 ports: - port: 1521 This creates a DNS name database in the current namespace, ordefault if no namespace is specified, as in the previous example. In this example, the fully qualified name would be database.default.svc.cluster.local. The second part is the namespace. If you looked up the ClusterIP for such a service, it would have an IP address on the overlay network, that is the network inside the Kubernetes cluster. If you are using flannel, for example, the address might be something like 10.0.1.25. Note that this is usually a non-routed address.\nFrom a container in a pod running in Kubernetes, you can make a connection to that address and port 1521. Kubernetes will route the connection to the address provided in the endpoint definition, in this example, 129.123.1.4:1521. This IP address (or name) is resolved from the point of view of the Kubernetes Node\u0026rsquo;s IP stack, not the overlay network inside the Kubernetes cluster. Note that this is a \u0026ldquo;real\u0026rdquo; routed IP address.\nWhen you create your data sources, you would use the internal address, for example, database:1521/some.service.\nBecause your database is externally accessible, you can run RCU in the normal way, from any machine on your network.\nRunning the Repository Creation Utility to set up your database schema If you want to run RCU from a pod inside the Kubernetes cluster, you can use the container image that you built earlier as a \u0026ldquo;service\u0026rdquo; pod to run RCU. To do this, start up a pod using that image as follows:\n$ kubectl run rcu --generator=run-pod/v1 --image container-registry.oracle.com/middleware/fmw_infrastructure:12.2.1.4 -- sleep infinity This will create a Kubernetes Deployment called rcu containing a pod running a container created from the container-registry.oracle.com/middleware/fmw_infrastructure:12.2.1.4 image which will just run sleep infinity, which essentially creates a pod that we can \u0026ldquo;exec\u0026rdquo; into and use to run whatever commands we need to run.\nTo get inside this container and run commands, use this command:\n$ kubectl exec -ti rcu /bin/bash When you are finished with this pod, you can remove it with this command:\n$ kubectl delete pod rcu You can use the same approach to get a temporary pod to run other utilities like WLST.\nCreating schemas Inside this pod, you can use the following command to run RCU in command-line (no GUI) mode to create your FMW schemas. You will need to provide the right prefix and connect string. You will be prompted to enter the password for the sys user, and then the password to use for the regular schema users:\n$ /u01/oracle/oracle_common/bin/rcu \\ -silent \\ -createRepository \\ -databaseType ORACLE \\ -connectString oracle-db.default:1521/devpdb.k8s \\ -dbUser sys \\ -dbRole sysdba \\ -useSamePasswordForAllSchemaUsers true \\ -selectDependentsForComponents true \\ -schemaPrefix FMW1 \\ -component MDS \\ -component IAU \\ -component IAU_APPEND \\ -component IAU_VIEWER \\ -component OPSS \\ -component WLS \\ -component STB You need to make sure that you maintain the association between the database schemas and the matching domain just like you did in a non-Kubernetes environment. There is no specific functionality provided to help with this. We recommend that you consider making the RCU prefix (value of the schemaPrefix argument) the same as your domainUID to help maintain this association.\nDropping schemas If you want to drop the schema, you can use a command like this:\n$ /u01/oracle/oracle_common/bin/rcu \\ -silent \\ -dropRepository \\ -databaseType ORACLE \\ -connectString oracle-db.default:1521/devpdb.k8s \\ -dbUser sys \\ -dbRole sysdba \\ -selectDependentsForComponents true \\ -schemaPrefix FMW1 \\ -component MDS \\ -component IAU \\ -component IAU_APPEND \\ -component IAU_VIEWER \\ -component OPSS \\ -component WLS \\ -component STB Again, you will need to set the right prefix and connection string, and you will be prompted to enter the sys user password.\nCreate a Kubernetes Secret with the RCU credentials You also need to create a Kubernetes Secret containing the credentials for the database schemas. When you create your domain using the following sample, it will obtain the RCU credentials from this secret.\nWe provide a sample that demonstrates how to create the secret. The schema owner user name required will be the schemaPrefix value followed by an underscore and a component name, such as FMW1_STB. The schema owner password will be the password you provided for regular schema users during RCU creation.\nCreating an FMW Infrastructure domain Now that you have your images and you have created your RCU schemas, you are ready to create your domain. We provide a sample that demonstrates how to create an FMW Infrastructure domain.\nPatching the FMW Infrastructure image There are two kinds of patches that can be applied to the FMW Infrastructure binaries:\nPatches which are eligible for Zero Downtime Patching (ZDP), meaning that they can be applied with a rolling restart. Non-ZDP eligible compliant patches, meaning that the domain must be shut down and restarted. You can find out whether or not a patch is eligible for Zero Downtime Patching by consulting the README file that accompanies the patch.\nIf you wish to apply a ZDP compliant patch which can be applied with a rolling restart, after you have patched the FMW Infrastructure image as shown in this sample, you can edit the domain custom resource with the name of the new image and the operator will initiate a rolling restart of the domain.\nIf you wish to apply a non-ZDP compliant patch to the FMW Infrastructure binary image, you must shut down the entire domain before applying the patch. Please see the documentation on domain lifecycle operations for more information.\nAn example of a non-ZDP compliant patch is one that includes a schema change that can not be applied dynamically.\nAdditional considerations for Coherence If you are running a domain which contains Coherence, please refer to Coherence requirements for more information.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/aks/",
	"title": "Azure Kubernetes Service (AKS)",
	"tags": [],
	"description": "Deploy WebLogic Server on Azure Kubernetes Service.",
	"content": "Contents Introduction Basics Project details Instance details Credentials for WebLogic Optional Basic Configuration AKS Azure Kubernetes Service Image selection Application Advanced TLS/SSL Upload existing KeyStores Load balancing Application Gateway Ingress Controller Standard Load Balancer service DNS Database Autoscaling Kubernetes Metrics Server (simple autoscaling) WebLogic Monitoring Exporter (advanced autoscaling) Tags Review + create Template outputs Introduction This document is the reference for the Azure Marketplace offer for WebLogic Server on Azure Kubernetes Service. The offer makes it easy to get started with WebLogic Server on Azure. The offer handles all the initial setup, creating the AKS cluster, container registry, load-balancer, WebLogic Kubernetes Operator installation, and domain creation using the model-in-image domain home source type.\nTo deploy the offer from the Azure portal, see WebLogic Server on Azure.\nWith the exception of the resource group, and the username and password for the Oracle Single Sign-On fields, each panel of the offer has been configured with sensible defaults. If you were referred to this guidance by another guidance, it is appropriate to accept the defaults for each panel.\nAzure Kubernetes Service (AKS) makes it simple to deploy a managed Kubernetes cluster in Azure. AKS reduces the complexity and operational overhead of managing Kubernetes by offloading much of that responsibility to Azure. As a hosted Kubernetes service, Azure handles critical tasks like health monitoring and maintenance for you. The Kubernetes masters are managed by Azure. You manage and maintain only the agent nodes. As a managed Kubernetes service, AKS is free - you pay for only the agent nodes within your clusters, not for the masters.\nTo learn more, see What is Azure Kubernetes Service?\nFor complete details on domain home source types, see Choose a domain home source type.\nIt is also possible to run the WebLogic Kubernetes Operator manually, without the aid of the Azure Marketplace offer. The steps for doing so are documented in the Azure Kubernetes Service sample.\nThe remaining steps on this page document the user experience for the Azure Marketplace offer for WebLogic Server on Azure Kubernetes Service.\nBasics Use the Basics blade to provide the basic configuration details for deploying an Oracle WebLogic Server configured cluster. To do this, enter the values for the fields listed in the following tables.\nProject details Field Description Subscription Select a subscription to use for the charges accrued by this offer. You must have a valid active subscription associated with the Azure account that is currently logged in. If you don’t have it already, follow the steps described in Associate or add an Azure subscription to your Azure Active Directory tenant. Resource group A resource group is a container that holds related resources for an Azure solution. The resource group includes those resources that you want to manage as a group. You decide which resources belong in a resource group based on what makes the most sense for your organization. If you have an existing resource group into which you want to deploy this solution, you can enter its name here. Note deploying this solution multiple times into the same resource group is not supported. Alternatively, you can click the Create new, and enter the name so that Azure creates a new resource group before provisioning the resources. For more information about resource groups, see the Azure documentation. Instance details Field Description Region Select an Azure region from the drop-down list. Credentials for WebLogic Field Description Username for WebLogic Administrator Enter a user name to access the WebLogic Server Administration Console which is started automatically after the provisioning. For more information about the WebLogic Server Administration Console, see Overview of Administration Consoles in Understanding Oracle WebLogic Server. Password for WebLogic Administrator Enter a password to access the WebLogic Server Administration Console. Confirm password Re-enter the value of the preceding field. Password for WebLogic Model encryption Model in Image requires a runtime encryption secret with a secure password key. This secret is used by the operator to encrypt model and domain home artifacts before it adds them to a runtime ConfigMap or log. For more information, see Required runtime encryption secret. Confirm password Re-enter the value of the preceding field. Optional Basic Configuration Field Description Accept defaults for optional configuration? If you want to retain the default values for the optional configuration, such as Name prefix for Managed Server, WebLogic Domain Name and others, set the toggle button to Yes, and click Next to configure AKS cluster. If you want to specify different values for the optional configuration, set the toggle button to No, and enter the following details. Name prefix for Managed Server Enter a prefix for the Managed Server name. WebLogic Domain Name Enter the name of the domain that will be created by the offer. WebLogic Domain UID Enter the UID of the domain that will be created by the offer. Maximum dynamic cluster size The maximum size of the dynamic WebLogic cluster created. Custom Java Options to start WebLogic Server Java VM arguments passed to the invocation of WebLogic Server. For more information, see the FAQ. When you are satisfied with your selections, select Next and open AKS blade.\nAKS Use the AKS blade to configure fundamental details of how Oracle WebLogic Server runs on AKS. To do this, enter the values for the fields listed in the following tables.\nAzure Kubernetes Service In this section, you can configure some options about the AKS which will run WebLogic Server.\nField Description Create a new AKS cluster If set to Yes, the deployment will create a new AKS cluster resource in the specified resource group. If set to No, you have the opportunity to select an existing AKS cluster, into which the deployment is configured. Note: the offer assumes the existing AKS cluster has no WebLogic related deployments. Node size The default VM size is 2x Standard DSv2, 2 vcpus, 7 GB memory. If you want to select a different VM size, select Change Size, select the size from the list (for example, A3) on the Select a VM size page, and select Select. For more information about sizing the virtual machine, see the Azure documentation on Sizes. Minimum node count The minimum node count in the AKS cluster. This value can be changed after deployment. For information, see AKS autoscaler. Maximum node count The maximum node count in the AKS cluster. This value can be changed after deployment. For information, see AKS autoscaler. Select AKS cluster This option is shown if Create a new AKS cluster? is set to No. If visible, select an existing Azure Kubernetes Service instance. Image selection In this section, you can configure the image that is deployed using the model-in-image domain home source type. There are several options for the WebLogic image and the application image deployed therein.\nField Description Create a new Azure Container Registry to store application images? If set to Yes, the offer will create a new Azure Container Registry (ACR) to hold the images for use in the deployment. If set to No, you must specify an existing ACR. In this case, you must be sure the selected ACR has the admin account enabled. For details, please see Admin account. Select ACR instance This option is shown if Create a new Azure Container Registry to store application images? is set to No. If visible, select an existing Azure Container Registry instance. Username for Oracle Single Sign-On authentication The Oracle Single Sign-on account user name for which the Terms and Restrictions for the selected WebLogic Server image have been accepted. Password for Oracle Single Sign-On authentication The password for that account. Confirm password Re-enter the value of the preceding field. Select the type of WebLogic Server Images. If set to Patched WebLogic Server Images, you must accept the license agreement in the middleware/weblogic_cpu repository. If set to General WebLogic Server Images, you must accept the license agreement in the middleware/weblogic. Steps to accept the license agreement: log in to the Oracle Container Registry; navigate to the middleware/weblogic_cpu and middleware/weblogic repository; accept license agreement. See this document for more information. Select desired combination of WebLogic Server, JDK and Operating System or fully qualified Docker tag Select one of the supported images. WebLogic Docker tag This option is shown if Select desired combination of WebLogic Server, JDK and Operating System or fully qualified Docker tag is set to Others. If visible, input a fully qualified Docker tag. You can find the available tags from WebLogic Server Images page of Oracle Container Registry Application In this section, you can deploy an application along with WebLogic Server.\nField Description Deploy an application? If set to Yes, you must specify a WAR, EAR, or JAR file suitable for deployment with the selected version of WebLogic Server. If set to No, no application is deployed. Application package (.war,.ear,.jar) With the Browse button, you can select a file from a pre-existing Azure Storage Account and Storage Container within that account. To learn how to create a Storage Account and Container, see Create a storage account and Create a Storage Container and upload application files. Number of WebLogic Managed Server replicas The initial value of the replicas field of the Domain. For information, see Scaling. Advanced Field Description Show advanced configuration? If you want to retain the default values for the optional configuration, such as Enable Container insights, Create Persistent Volume using Azure File share service and others, set the toggle button to No, and click Next to configure TLS/SSL. If you want to specify different values for the optional configuration, set the toggle button to Yes, and enter the following details. Enable Container insights If selected, causes the deployment to create an Azure Monitoring workspace and connect it to the AKS cluster as the Azure Monitoring Agent. Azure Monitoring Agent is a tool that collects data and sends it to Azure Container Insights. Container insights gives you performance visibility by collecting memory and processor metrics from controllers, nodes, and containers that are available in Kubernetes through the Metrics API. Container logs are also collected. Metrics are written to the metrics store and log data is written to the logs store associated with your Log Analytics workspace. For more information, see Azure Monitor Agent overview and Container insights overview. Create Persistent Volume using Azure File share service If selected, an Azure Storage Account and an Azure Files share will be provisioned. The file system type is NFS. The name of the Azure Files share is weblogic. The mount point is /shared as a persistent volume in the nodes of the AKS cluster. For more information, see Oracle WebLogic Server persistent storage and persistent volume with Azure Files share on AKS. Bring your own WebLogic Server Docker image from Azure Container Registry? If the checkbox is selected, you can use your own WebLogic Server Docker image from a pre-existing Azure Container Registry instance. Select existing ACR instance This option is shown only if Bring your own WebLogic Server Docker image from Azure Container Registry? is checked. If visible, select an existing Azure Container Registry instance. Please provide the image path This option is shown only if Bring your own WebLogic Server Docker image from Azure Container Registry? is checked. If visible, the value must be a fully qualified Docker tag of an image within the specified ACR. When you are satisfied with your selections, select Next and open TLS/SSL blade.\nTLS/SSL With the TLS/SSL blade, you can configure Oracle WebLogic Server Administration Console on a secure HTTPS port, with your own SSL certificate provided by a Certifying Authority (CA). See Oracle WebLogic Server Keystores configuration for more information.\nSelect Yes or No for the option Configure WebLogic Server Administration Console, Remote Console, and cluster to use HTTPS (Secure) ports, with your own TLS/SSL certificate. If you select No, you don\u0026rsquo;t have to provide any details, and can proceed by selecting Next. If you select Yes, you must upload your existing keystores.\nEnter the values for the fields listed in the following table.\nUpload existing KeyStores Field Description Identity KeyStore Data file(.jks,.p12) Upload a custom identity keystore data file by doing the following: 1. Click on the file icon. 2. Navigate to the folder where the identity keystore file resides, and select the file. 3. Click Open. Password Enter the passphrase for the custom identity keystore. Confirm password Re-enter the value of the preceding field. The Identity KeyStore type (JKS,PKCS12) Select the type of custom identity keystore. The supported values are JKS and PKCS12. The alias of the server\u0026rsquo;s private key within the Identity KeyStore Enter the alias for the private key. The passphrase for the server\u0026rsquo;s private key within the Identity KeyStore Enter the passphrase for the private key. Confirm passphrase Re-enter the value of the preceding field. Trust KeyStore Data file(.jks,.p12) Upload a custom trust keystore data file by doing the following: 1. Click on the file icon. 2. Navigate to the folder where the identity keystore file resides, and select the file. 3. Click Open. Password Enter the password for the custom trust keystore. Confirm password Re-enter the value of the preceding field. The Trust KeyStore type (JKS,PKCS12) Select the type of custom trust keystore. The supported values are JKS and PKCS12. When you are satisfied with your selections, select Next and open Load balancing blade.\nLoad balancing Use this blade to configure load balancing. There are three options:\nApplication Gateway Ingress Controller. Standard Load Balancer service. No Load Balancer. Application Gateway Ingress Controller In this section, you can create an Azure Application Gateway instance as the ingress controller of your WebLogic Server. This Application Gateway is pre-configured for end-to-end-SSL with TLS termination at the gateway using the provided SSL certificate and load balances across your cluster.\nSelect Application Gateway Ingress Controller for the option Load Balancing Options based on your preference. You must specify the details required for the Application Gateway integration by entering the values for the fields as described next.\nYou can specify a virtual network for the application gateway. To do this, enter the values for the fields listed in the following tables.\nConfigure virtual networks\nField Description Virtual network Select a virtual network in which to place the application gateway. Make sure your virtual network meets the requirements in Application Gateway virtual network and dedicated subnet. Subnet An application gateway is a dedicated deployment in your virtual network. Within your virtual network, a dedicated subnet is required for the application gateway. See Application Gateway virtual network and dedicated subnet. You must select one of the following three options, each described in turn.\nGenerate a self-signed front-end certificate: Generate a self-signed front-end certificate and apply it during deployment. Upload a TLS/SSL certificate: Upload the pre-signed certificate now. Generate a self-signed frontend certificate\nField Description Trusted root certificate(.cer, .cert) A trusted root certificate is required to allow back-end instances in the application gateway. The root certificate is a Base-64 encoded X.509(.CER) format root certificate. Upload a TLS/SSL certificate\nField Description Frontend TLS/SSL certificate(.pfx) For information on how to create a certificate in PFX format, see Overview of TLS termination and end to end TLS with Application Gateway. Password The password for the certificate Confirm password Re-enter the value of the preceding field. Trusted root certificate(.cer, .cert) A trusted root certificate is required to allow back-end instances in the application gateway. The root certificate is a Base-64 encoded X.509(.CER) format root certificate. Regardless of how you provide the certificates, there are several other options when configuring the Application Gateway, as described next.\nField Description Disable cookie based affinity Select this box to disable cookie based affinity (sometimes called \u0026ldquo;sticky sessions\u0026rdquo;). For more information, see Enable Cookie based affinity with an Application Gateway. Create ingress for Administration Console. Select Yes to create an ingress for the Administration Console with the path /console. Create ingress for WebLogic Remote Console. Select Yes to create an ingress for the Remote Console with the path /remoteconsole. Standard Load Balancer service Selecting Yes here will cause the offer to provision the Azure Load Balancer as a Kubernetes load balancer service. For more information on the Standard Load Balancer see Use a public Standard Load Balancer in Azure Kubernetes Service (AKS). You can still deploy an Azure Application Gateway even if you select No here.\nIf you select Yes, you have the option of configuring the Load Balancer as an internal Load Balancer. For more information on Azure internal load balancers see Use an internal load balancer with Azure Kubernetes Service (AKS).\nIf you select Yes, you must fill in the following table to map the services to load balancer ports.\nService name prefix column:\nYou can fill in any valid value in this column.\nTarget and Port column:\nFor the ports, the recommended values are the usual 7001 for admin-server and 8001 for cluster-1.\nWhen you are satisfied with your selections, select Next and open DNS blade.\nDNS With the DNS blade, you can provision the Oracle WebLogic Server Administration Console using a custom DNS name.\nSelect Yes or No for the option Custom DNS Alias? based on your preference. If you select No, you don\u0026rsquo;t have to provide any details, and can proceed by selecting Next to configure database. If you select Yes, you must choose either to configure a custom DNS alias based on an existing Azure DNS zone, or create an Azure DNS zone and a custom DNS alias. This can be done by selecting Yes or No for the option Use an existing Azure DNS Zone.\nFor more information about the DNS zones, see Overview of DNS zones and records.\nIf you choose to configure a custom DNS alias based on an existing Azure DNS zone, by selecting Yes for the option Use an existing Azure DNS Zone, you must specify the DNS configuration details by entering the values for the fields listed in the following table.\nField Description DNS Zone Name Enter the DNS zone name. Name of the resource group contains the DNS Zone in current subscription Enter the name of the resource group that contains the DNS zone in the current subscription. Label for Oracle WebLogic Server Administration Console Enter a label to generate a sub-domain of the Oracle WebLogic Server Administration Console. For example, if the domain is mycompany.com and the sub-domain is admin, then the WebLogic Server Administration Console URL will be admin.mycompany.com. Label for WebLogic Cluster Specify a label to generate subdomain of WebLogic Cluster. If you choose to create an Azure DNS zone and a custom DNS alias, by selecting No for the option Use an existing Azure DNS Zone, you must specify the values for the following fields:\nDNS Zone Name Label for Oracle WebLogic Server Administration Console Label for WebLogic Cluster See the preceding table for the description of these fields.\nIn the case of creating an Azure DNS zone and a custom DNS alias, you must perform the DNS domain delegation at your DNS registry post deployment. See Delegation of DNS zones with Azure DNS.\nWhen you are satisfied with your selections, select Next and open Database blade.\nDatabase Use the Database blade to configure Oracle WebLogic Server to connect to an existing database. Select Yes or No for the option Connect to Database? based on your preference. If you select No, you don\u0026rsquo;t have to provide any details, and can proceed by clicking Next. If you select Yes, you must specify the details of your database by entering the values for the fields listed in the following table.\nField Description Choose database type From the drop-down menu, select an existing database to which you want Oracle WebLogic Server to connect. The available options are:\n• Azure Database for PostgreSQL (with support for passwordless connection) • Oracle Database • Azure SQL (with support for passwordless connection) • MySQL (with support for passwordless connection) • Other JNDI Name Enter the JNDI name for your database JDBC connection. DataSource Connection String Enter the JDBC connection string for your database. For information about obtaining the JDBC connection string, see Obtain the JDBC Connection String for Your Database. Global transactions protocol Determines the transaction protocol (global transaction processing behavior) for the data source. For more information, see JDBC Data Source Transaction Options. IMPORTANT: The correct value for this parameter depends on the selected database type. For PostgreSQL, select EmulateTwoPhaseCommit. Use passwordless datasource connection If you select a database type that supports passwordless connection, this check box will appear. If selected, a passwordless connection to the data source will be configured. For more information, see Passwordless connections for Azure services. Database Username Enter the user name of your database. Database Password Enter the password for the database user. Confirm password Re-enter the value of the preceding field. User assigned managed identity Select a user assigned identity that is able to connect to your database. For how to create a database user for your managed identity, see https://aka.ms/javaee-db-identity. If you select Other as the database type, there are some additional values you must provide. WebLogic Server provides support for application data access to any database using a JDBC-compliant driver. Refer to the documentation for driver requirements.\nField Description DataSource driver (.jar) Use the Browse button to upload the JAR file for the JDBC driver to a storage container. To learn how to create a Storage Account and Container, see Create a storage account. DataSource driver name The fully qualified Java class name of the JDBC driver. Test table name The name of the database table to use when testing physical database connections. This value depends on the specified database. Some suggested values include the following. • For Oracle, use SQL ISVALID. • For PostgreSQL, SQL Server and MariaDB use SQL SELECT 1. • For Informix use SYSTABLES. When you are satisfied with your selections, select Next and open Autoscaling blade.\nAutoscaling Use the Autoscaling blade to configure metrics that scale the WebLogic cluster. Select Yes or No for the option Provision resources for horizontal autoscaling?, based on your preference. If you select No, you don\u0026rsquo;t have to provide any details, and can proceed by clicking Review + create. If you select Yes, you must specify the details of your autoscaling settings.\nYou must select one of the following two options, each described in turn.\nKubernetes Metrics Server (simple autoscaling): this option configures and runs Kubernetes Horizontal Pod Autoscaler (HPA) to scale a WebLogic cluster, based on the CPU or memory utilization. WebLogic Monitoring Exporter (advanced autoscaling): this option allows you to create Java metric aware KEDA scaling rules. Kubernetes Metrics Server (simple autoscaling) Field Description Select metric There are two options:\n• Average CPU Utilization • Average Memory Utilization Average CPU Utilization Pick average CPU utilization in percent. The HPA autoscales WebLogic Server instances from a minimum of 1 cluster member up to the maximum of cluster members, and the scale up or down action occurs when the average CPU is consistently over/under the utilization. Average Memory Utilization Pick average memory utilization in percent. The HPA autoscales WebLogic Server instances from a minimum of 1 cluster member up to the maximum of cluster members, and the scale up or down action occurs when the average memory is consistently over/under the utilization. WebLogic Monitoring Exporter (advanced autoscaling) This option installs all the software necessary to allow you to create Java metric aware KEDA scaling rules. The offer provisions the following software. Right-click and select Open Link in New Tab to follow links and learn more:\nInstall WebLogic Monitoring Exporter to scrape WebLogic Server metrics. Install AKS Prometheus metrics addon. Feed WebLogic Server metrics to Azure Monitor Managed Service for Prometheus. Integrate KEDA with AKS cluster. After the provisioning is complete, you can create KEDA scaling rules. A sample rule is provided in the deployment outputs. The following steps show how to see the sample rule.\nView the resource group for this deployment in the Azure portal. In the Settings section, select Deployments. Select the oldest deployment. The name of the deployment looks similar to oracle.20210620-wls-on-aks. Select Outputs. The shellCmdtoOutputKedaScalerSample value is the base64 string of a scaler sample. Copy the value and run it in your terminal. For guidance on how to complete the configuration, see Tutorial: Migrate Oracle WebLogic Server to AKS with KEDA scaler based on Prometheus Metrics. When you are satisfied with your selections, select Next and open the Tags blade.\nTags Use the Tags blade to provide tags for resources and resource groups. Tags are name/value pairs that allow you to categorize resources and consolidate billing by applying the same tag to multiple resources and resource groups. Learn more about tags.\nEnter the desired name/value pairs for resources in the table, filling out each of the following fields.\nName Value Resource Tag name. Tag value. Resources available for tagging in this offer. When you are satisfied with your selections, select Review + create.\nReview + create In the Review + create blade, review the details you provided for deploying Oracle WebLogic Server on AKS. If you want to make changes to any of the fields, click \u0026lt; previous or click on the respective blade and update the details.\nIf you want to use the underlying template to further customize it yourself (for example, as part of your CI/CD pipeline), download it by selecting Download a template for automation.\nClick Create to start the deployment. This process may take 30 to 60 minutes.\nTemplate outputs After clicking Create, you will go to the Deployment is in progress page. When the deployment is complete, the page shows Your deployment is complete. In the left panel, select Outputs. These are the outputs from the deployment. The following table is a reference to the deployment outputs.\nField Description aksClusterName Name of your AKS cluster that is running the WLS cluster. Sample value: wlsonaksiyiql2i2o2u2i. adminConsoleInternalUrl The fully qualified, private link to the Administration Console portal. You can access it only inside the AKS cluster. Sample value: http://contoso.com:7001/console. adminConsoleExternalUrl This output is not always present:\nYou must configure Networking to enable the Azure Load Balancer service or Azure Application Gateway Ingress Controller for the Administration Console.\nThis is a fully qualified, public link to the Administration Console portal. You can access it from the public Internet. Sample value: http://contoso.com/console. adminConsoleExternalSecuredUrl This output is not always present:\n1. You must configure Networking to enable the Azure Load Balancer service or Azure Application Gateway Ingress Controller for the Administration Console.\n2. You must configure a custom DNS name by filling out DNS Configuration.\n3. The TLS/SSL certificate used is configured by filling out TLS/SSL configuration.\nThis is a fully qualified, secure, public link to the Administration Console portal. You can access it from the public Internet. Sample value: https://contoso.com/console. adminRemoteConsoleUrl This output is not always present:\nYou must configure Networking to enable the Azure Load Balancer service or Azure Application Gateway Ingress Controller for the Administration Console.\nThis is a fully qualified, public link to the WebLogic Server Remote Console. You can access it from the public Internet.\nSample value: http://contoso.azure.com/remoteconsole. adminRemoteConsoleSecuredUrl This output is not always present:\n1. You must configure Networking to enable the Azure Load Balancer service or Azure Application Gateway Ingress Controller for the Administration Console.\n2. You must configure a custom DNS name following DNS Configuration.\n3. The TLS/SSL certificate used is configured by filling out TLS/SSL configuration.\nThis is a fully qualified, public link to the WebLogic Server Remote Console. You can access it from the public Internet.\nSample value: https://contoso.com/remoteconsole. adminServerT3InternalUrl This output is not always present:\n1. You must create/update the WLS cluster with advanced configuration.\n2. You must enable custom T3 channel by setting enableAdminT3Tunneling=true.\nThis is a fully qualified, private link to custom T3 channel of the Administration Server.\nSample value: http://contoso.com:7005/console. adminServerT3ExternalUrl This output is not always present:\n1. You must create/update the WLS cluster with advanced configuration.\n2. You must enable custom T3 channel by setting enableAdminT3Tunneling=true.\n3. You must configure Networking to enable the Azure Load Balancer service for the Administration Server.\nThis is a fully qualified, public link to custom T3 channel of the Administration Server.\nSample value: http://20.4.56.3:7005/console/ clusterInternalUrl The fully qualified, private link to the WLS cluster. You are able to access your application with ${clusterInternalUrl}\u0026lt;your-app-path\u0026gt; inside AKS cluster.\nSample value: http://contoso.com:8001/. clusterExternalUrl This output is not always present:\nYou must configure Networking to enable the Azure Load Balancer service or Azure Application Gateway Ingress Controller for the WLS cluster.\nThis is a fully qualified, public link to the WLS cluster. You can access your application with ${clusterExternalUrl}\u0026lt;your-app-path\u0026gt; from the public Internet.\nSample value: http://contoso.azure.com/. clusterExternalSecuredUrl This output is not always present:\n1. You must configure Networking to enable the Azure Load Balancer service or Azure Application Gateway Ingress Controller for the WLS cluster.\n2. The TLS/SSL certificate used is configured by filling out TLS/SSL configuration.\nThis is a fully qualified, public link to the WLS cluster. You can access your application with ${clusterExternalUrl}\u0026lt;your-app-path\u0026gt; from the public Internet.\nSample value: https://contoso.azure.com/. clusterT3InternalUrl This output is not always present:\n1. You must create/update the WLS cluster with advanced configuration.\n2. You must enable custom T3 channel by setting enableClusterT3Tunneling=true.\nThis is a fully qualified, private link to custom T3 channel of the WLS cluster. clusterT3ExternalEndpoint This output is not always present:\n1. You must create/update the WLS cluster with advanced configuration.\n2. You must enable custom T3 channel by setting enableClusterT3Tunneling=true.\n3. You must configure Networking to enable the Azure Load Balancer service for the WLS cluster.\nThis is a fully qualified, public link to custom T3 channel of the WLS cluster.\nSample value:http://20.4.56.3:8005/ kedaScalerServerAddress This output is the server address of that saves the WLS metrics. KEDA is able to access and retrieve metric from the address. shellCmdtoConnectAks AZ CLI command to connect to the AKS cluster.\nSample value: az account set\n--subscription \u0026lt;id\u0026gt;;\naz aks get-credentials --resource-group contoso-rg\n--name contosoakscluster shellCmdtoOutputKedaScalerSample Sell command to display the the base64 string of a scaler sample.\nSample value: echo -e YXBpVm...XV0aAo= | base64 -d \u0026gt; scaler.yaml shellCmdtoOutputWlsDomainYaml Shell command to display the base64 encoded string of the WLS domain resource definition.\nSample value: echo -e YXBpV...mVCg== | base64 -d \u0026gt; domain.yaml shellCmdtoOutputWlsImageModelYaml Shell command to display the base64 encoded string of the WLS image model.\nSample value:\necho -e IyBDb...3EnC | base64 -d \u0026gt; model.yaml shellCmdtoOutputWlsImageProperties Shell command to display the base64 encoded string of the model properties.\nSample value:\necho -e IyBDF...PTUK | base64 -d \u0026gt; model.properties shellCmdtoOutputWlsVersionsandPatches Shell command to display the base64 encoded string of the WLS version and patches.\nSample value:\necho -e CldlY...gMS4= | base64 -d \u0026gt; version.info "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/faq/node-selector/",
	"title": "Scheduling pods to particular nodes",
	"tags": [],
	"description": "How do I constrain scheduling WebLogic Server pods to particular nodes?",
	"content": " How do I constrain scheduling WebLogic Server pods to a particular set of nodes?\nTo do this:\nFirst, set a label on the nodes on which the WebLogic Server pods will run. For example:\n$ kubectl label nodes name=weblogic-pods In the Domain CR, set a nodeSelector: a selector which must match a node\u0026rsquo;s labels for the pod to be scheduled on that node. See kubectl explain pods.spec.nodeSelector.\nYou can set nodeSelector labels for WebLogic Server pods, all server pods in a cluster, or all server pods in a domain. nodeSelector is a field under the serverPod element, which occurs at several points in the Domain CR schema:\nAt the top-level spec.severPod for the entire domain At spec.adminServer.serverPod for the Administration Server At spec.clusters[*].serverPod for each cluster At spec.managedServers[*].serverPod for individual Managed Servers spec: serverPod: nodeSelector: Under that level, you specify labels and values that match the labels on the nodes you want to select. For example:\nnodeSelector: name: weblogic-pods For more details, see Assign Pods to Nodes in the Kubernetes documentation.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/debugging/",
	"title": "Domain debugging",
	"tags": [],
	"description": "Debug deployed domains.",
	"content": "Here are some suggestions for debugging problems with a domain after your Domain or Cluster YAML files are deployed.\nContents Understand failure types, severities, and tuning Check the Domain status Check the Cluster status Check the Domain events Check the introspector job Check the WebLogic Server pods Check the docs Check the operator Understand failure types, severities, and tuning When debugging, it helps to understand failure types, failure severities, retry behavior, and retry tuning, see Domain failure retry processing. These apply to failures reported in the resource status, events, introspector jobs, and pods.\nCheck the Domain status To check the Domain status: kubectl -n MY_NAMESPACE describe domain MY_DOMAIN_RESOURCE_NAME. For more information, see Domain conditions.\nNOTE: If .status.observedGeneration does not equal .metadata.generation, then this is an indication that the status is not up-to-date with respect to the latest changes to the .spec or .metadata. Either the operator is in the process of updating of the status or the operator is not running.\nCheck the Cluster status If you have deployed cluster resources, then you can optionally check the status of each using kubectl -n MY_NAMESPACE describe cluster MY_CLUSTER_NAME.\nThe same information is reported in the Domain resource status under domain.status.clusters. For more information, see Cluster conditions.\nNOTE: If .observedGeneration for a particular cluster status does not equal .metadata.generation for the corresponding cluster resource, then this is an indication that the status is not up-to-date with respect to the latest changes to the .spec or .metadata. Either the operator is in the process of updating of the status or the operator is not running.\nCheck the Domain events To check events for the Domain: kubectl -n MY_NAMESPACE get events --sort-by='.lastTimestamp'.\nFor more information, see Domain events.\nCheck the introspector job If your introspector job failed, then examine the kubectl describe of the job and its pod, and also examine its log, if one exists.\nTo prevent the introspector job from retrying while you are debugging a failure, configure domain.spec.failureRetryLimitMinutes to 0. For more information, see Domain failure retry processing.\nFor example, assuming your domain UID is sample-domain1 and your domain namespace is sample-domain1-ns.\nHere we see a failed introspector job pod among the domain\u0026rsquo;s pods:\n$ kubectl -n sample-domain1-ns get pods -l weblogic.domainUID=sample-domain1 NAME READY STATUS RESTARTS AGE sample-domain1-admin-server 1/1 Running 0 19h sample-domain1-introspector-v2l7k 0/1 Error 0 75m sample-domain1-managed-server1 1/1 Running 0 19h sample-domain1-managed-server2 1/1 Running 0 19h First, look at the output from the job\u0026rsquo;s describe command.\n$ kubectl -n sample-domain1-ns describe job/sample-domain1-introspector Now, look at the job\u0026rsquo;s pod describe output; in particular look at its events.\n$ kubectl -n sample-domain1-ns describe pod/sample-domain1-introspector-v2l7k Last, look at the job\u0026rsquo;s pod\u0026rsquo;s log.\n$ kubectl -n sample-domain1-ns logs job/sample-domain1-introspector Here\u0026rsquo;s an alternative log command that will have same output as shown in the previous command. $ kubectl -n sample-domain1-ns logs pod/sample-domain1-introspector-v2l7k\nA common reason for the introspector job to fail in a Model in Image domain is because of an error in a model file. Here\u0026rsquo;s some sample log output from an introspector job that shows such a failure:\n... SEVERE Messages: 1. WLSDPLY-05007: Model file /u01/wdt/models/model1.yaml,/weblogic-operator/wdt-config-map/..2020_03_19_15_43_05.993607882/datasource.yaml contains an unrecognized section: TYPOresources. The recognized sections are domainInfo, topology, resources, appDeployments, kubernetes The introspector log is mirrored to the Domain resource spec.logHome directory when spec.logHome is configured and spec.logHomeEnabled is true.\nIf a model file error references a model file in your spec.configuration.model.configMap, then you can correct the error by redeploying the ConfigMap with a corrected model file and then initiating a domain restart or roll. Similarly, if a model file error references a model file in your model image, then you can correct the error by deploying a corrected image, modifying your Domain YAML file to reference the new image, and then initiating a domain restart or roll.\nCheck the WebLogic Server pods If your introspector job succeeded, then there will be no introspector job or pod, the operator will create a MY_DOMAIN_UID-weblogic-domain-introspect-cm ConfigMap for your domain, and the operator will then run the domain\u0026rsquo;s WebLogic Server pods.\nIf kubectl -n MY_NAMESPACE get pods reveals that your WebLogic Server pods have errors, then use kubectl -n MY_NAMESPACE describe pod POD_NAME, kubectl -n MY_NAMESPACE logs POD_NAME, and/or kubectl -n MY_NAMESPACE get events --sort-by='.lastTimestamp' to debug.\nIf you are performing an online update to a running domain\u0026rsquo;s WebLogic configuration, then see Online update status and labels.\nCheck the docs Common issues that have corresponding documentation include:\nWhen a Domain or Cluster YAML file is deployed and no introspector or WebLogic Server pods start, plus the operator log contains no mention of the domain, then check to make sure that the Domain\u0026rsquo;s namespace has been set up to be monitored by an operator. See the operator Namespace management and operator Common mistakes and solutions documentation. If a describe of an introspector job or WebLogic Server pod reveals image access errors, see the Cannot pull image FAQ. Check the operator If the problem is specific to the operator itself, or its namespace management, then consult the operator Troubleshooting documentation.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/model-in-image/usage/",
	"title": "Usage",
	"tags": [],
	"description": "Steps for creating and deploying Model in Image images and their associated Domain YAML files.",
	"content": "This document describes what\u0026rsquo;s needed to create and deploy a typical Model in Image domain.\nContents WebLogic Kubernetes Operator WebLogic Server image Directory structure Supplying initial WDT model files and WDT Home Optional WDT model ConfigMap Required runtime encryption secret Secrets for model macros Domain fields Always use external state WebLogic Kubernetes Operator Deploy the operator and ensure that it is monitoring the desired namespace for your Model in Image domain. See Manage operators and Quick Start.\nWebLogic Server image Model in Image requires an image with a WebLogic Server installation.\nYou can start with WebLogic Server 12.2.1.4 or later, an Oracle Container Registry pre-built base image, such as container-registry.oracle.com/middleware/weblogic:12.2.1.4. The images in container-registry.oracle.com/middleware/weblogic are unpatched images. You should always either use patched images from container-registry.oracle.com/middleware/weblogic_cpu or build your own patched images (see Create a custom image with patches applied).\nThe example base images are GA images and are suitable for demonstration and development purposes only where the environments are not available from the public Internet; they are not acceptable for production use. In production, you should always use CPU (patched) images from OCR or create your images using the WebLogic Image Tool (WIT) with the --recommendedPatches option. For more guidance, see Apply the Latest Patches and Updates in Securing a Production Environment for Oracle WebLogic Server.\nFor an example of this approach, see the Model in Image sample. For detailed instructions on how to log in to the Oracle Container Registry and accept the license agreement for an image (required to allow pulling an Oracle Container Registry image), see this document.\nOr, you can manually build your own base image, as described in Create a custom image with patches applied. This is useful if you want your base images to include additional patches.\nNOTE: As of June, 2023, Oracle WebLogic Server 12.2.1.3 is no longer supported. The last Critical Patch Updates (CPU) images for WebLogic Server 12.2.1.3 were published in April, 2023.\nDirectory structure Model in Image requires the following directory structure in its pods for its (optional) WDT model files and (required) WDT Home :\nDomain resource attribute Contents Default directory domain.spec.configuration.model.modelHome Zero or more model .yaml, .properties, and/or archive .zip files. Optional. Location of the WDT model home, which can include model YAML files, .properties files, and application .zip archives. Defaults to /u01/wdt/models if no Auxiliary Images are configured, and to /aux/models otherwise. domain.spec.configuration.model.wdtInstallHome Unzipped WDT installation (required). Optional. Location of the WDT Home. Defaults to /u01/wdt/weblogic-deploy if no Auxiliary Images are configured, and to /aux/weblogic-deploy otherwise. If you set modelHome and wdtInstallHome to a non-default value, then the operator will ignore WDT model files and WDT Home that are copied from Auxiliary Images.\nSupplying initial WDT model files and WDT Home Model in Image minimally requires an image with a WebLogic installation (see WebLogic Server image), plus access to:\nA WDT installation in domain.spec.configuration.model.wdtInstallHome. One or more WDT model .yaml files that configure your domain in the domain.spec.configuration.model.modelHome directory or in the optional WDT model ConfigMap. Zero or more WDT model .properties files in the domain.spec.configuration.model.modelHome directory or in the optional WDT model ConfigMap. Zero or more WDT model application .zip archives in the domain.spec.configuration.model.modelHome directory. Archives must be supplied in the model home because application archives are not supported in the optional WDT model ConfigMap. There are multiple methods for supplying Model in Image WDT models files, WDT variables files, and WDT archive files (collectively known as WDT model files):\nUse auxiliary images: Use auxiliary images to create one or more small images that contain the desired files.\nThis is the recommended best approach. It automatically copies files from each of the small images into the /aux/models and /aux/weblogic-deploy directories in each pod\u0026rsquo;s file system so that the introspection job can find them.\nInclude in the main image: You can include the WDT model files in your domain resource domain.spec.image in its domain.spec.configuration.model.modelHome and domain.spec.configuration.model.wdtInstallHome directories as a layer on top of your base image (where the base image includes your WebLogic installation).\nNOTE: Model in Image without auxiliary images (the WDT model and installation files are included in the same image with the WebLogic Server installation) is deprecated in WebLogic Kubernetes Operator version 4.0.7. Oracle recommends that you use Model in Image with auxiliary images. See Auxiliary images.\nUse either of the following methods.\nManual image creation uses Docker commands to layer the WDT models files, described in the previous table, on top of your base image into a new image. The WebLogic Image Tool (WIT) has built-in options for layering WDT model files, WDT installation, WebLogic Server installation, and WebLogic Server patches in an image. See Create a custom image with your model inside the image. Use a Persistent Volume Claim (PVC): This method is for advanced use cases only. Supply WDT model YAML, variables, or archive files in a Persistent Volume Claim and modify configuration.model.modelHome and configuration.model.wdtInstallHome to the corresponding directory within the PVC\u0026rsquo;s mount location.\nUse a WDT model ConfigMap: Use the Optional WDT model ConfigMap for WDT model YAML and .properties files. This can be combined with any of the previously mentioned methods and is most often used to facilitate runtime updates to models supplied by one of these methods.\nFor more information about model file syntax, see Working with WDT model files.\nOptional WDT model ConfigMap You can create a WDT model ConfigMap that defines additional model .yaml and .properties files beyond what you\u0026rsquo;ve already supplied in your image, and then reference this ConfigMap using your Domain YAML file\u0026rsquo;s configuration.model.configMap attribute. This is optional if the supplied image already fully defines your model.\nWDT model ConfigMap files will be merged with the WDT files defined in your image at runtime before your domain home is created. The ConfigMap files can add to, remove from, or alter the model configuration that you supplied within your image.\nFor example, place additional .yaml and .properties files in a directory called /home/acmeuser/wdtoverride and run the following commands:\n$ kubectl -n MY-DOMAIN-NAMESPACE \\ create configmap MY-DOMAINUID-my-wdt-config-map \\ --from-file /home/acmeuser/wdtoverride $ kubectl -n MY-DOMAIN-NAMESPACE \\ label configmap MY-DOMAINUID-my-wdt-config-map \\ weblogic.domainUID=MY-DOMAINUID See Working with WDT model files for a description of model file syntax and loading order, and see Runtime updates for a description of using WDT model ConfigMaps to update the model configuration of a running domain.\nRequired runtime encryption secret Model in Image requires a runtime encryption secret with a secure password key. This secret is used by the operator to encrypt model and domain home files before it adds them to a runtime ConfigMap or log. You can safely change the password, at any time after you\u0026rsquo;ve fully shut down a domain, but it must remain the same for the life of a running domain. The runtime encryption secret that you create can be named anything, but note that it is a best practice to name and label secrets with their domain UID to help ensure that cleanup scripts can find and delete them.\nNOTE: Because the runtime encryption password does not need to be shared and needs to exist only for the life of a domain, you may want to use a password generator.\nExample:\n$ kubectl -n MY-DOMAIN-NAMESPACE \\ create secret generic MY-DOMAINUID-runtime-encrypt-secret \\ --from-literal=password=MY-RUNTIME-PASSWORD $ kubectl -n MY-DOMAIN-NAMESPACE \\ label secret MY-DOMAINUID-runtime-encrypt-secret \\ weblogic.domainUID=MY-DOMAINUID Corresponding Domain YAML file snippet:\nconfiguration: model: runtimeEncryptionSecret: MY-DOMAINUID-runtime-encrypt-secret Secrets for model macros Create additional secrets as needed by macros in your model files. For example, these can store database URLs and credentials that are accessed using @@SECRET macros in your model that reference the secrets. For a description of model macros, see Model files.\nDomain fields The following Domain fields are specific to Model in Image domains.\nDomain Resource Attribute Notes domainHomeSourceType Required. Set to FromModel. domainHome Must reference an empty or non-existent directory within your image. Do not include the mount path of any persistent volume. Note that Model in Image recreates the domain home for a WebLogic Server pod every time the pod restarts. configuration.model.configMap Optional. Set if you have stored additional models in a ConfigMap as per Optional WDT model ConfigMap. configuration.secrets Optional. Set this array if your image or ConfigMap models contain macros that reference custom Kubernetes Secrets. For example, if your macros depend on secrets my-secret and my-other-secret, then set to [my-secret, my-other-secret]. configuration.model.runtimeEncryptionSecret Required. All Model in Image domains must specify a runtime encryption secret. See Required runtime encryption secret. configuration.model.domainType Set the type of domain. WLS is the default. See WDT Domain Types. configuration.model.runtimeEncryptionSecret Required. All Model in Image domains must specify a runtime encryption secret. See Required runtime encryption secret. configuration.model.modelHome Optional. Location of the WDT model home, which can include model YAML files, .properties files, and application .zip archives. Defaults to /u01/wdt/models if no Auxiliary Images are configured, and to /aux/models otherwise. configuration.model.wdtInstallHome Optional. Location of the WDT Home . Defaults to /aux/weblogic-deploy when Auxiliary Images are configured, otherwise to /u01/wdt/weblogic-deploy. If you set modelHome and wdtInstallHome to a non-default value, then the operator will ignore WDT model files and WDT Home that are copied from Auxiliary Images.\nNOTES:\nThere are additional attributes that are common to all domain home source types, such as the image or clusters field. See the Domain Resource schema and documentation for a full list of Domain fields.\nFor fully specified Model in Image Domain YAML file examples, see the kubernetes/samples/scripts/create-weblogic-domain/model-in-image/domain-resources GitHub directory for the Model in Image sample.\nAlways use external state Regardless of the domain home source type, we recommend that you always keep state outside the image. This includes cluster database leasing tables, JMS and transaction stores, EJB timers, and so on. This ensures that data will not be lost when a container is destroyed.\nWe recommend that state be kept in a database to take advantage of built-in database server high availability features, and the fact that disaster recovery of sites across all but the shortest distances, almost always requires using a single database server to consolidate and replicate data (DataGuard).\nFor more information see:\nTuning JDBC Stores in Tuning Performance of Oracle WebLogic Server. Using a JDBC Store in Administering the WebLogic Persistent Store. High Availability Best Practices in Administering JMS Resources for Oracle WebLogic Server. Leasing in Administering Clusters for Oracle WebLogic Server. "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/model-in-image/auxiliary-images/",
	"title": "Auxiliary images",
	"tags": [],
	"description": "Auxiliary images are an alternative approach for supplying a domain&#39;s model files or other types of files.",
	"content": "Contents Introduction References Configuration Source locations Multiple auxiliary images Model and WDT installation homes Configuration examples Example 1: Basic configuration Example 2: Source locations Example 3: Multiple images Sample Using Docker to create an auxiliary image Automated upgrade of the weblogic.oracle/v8 schema auxiliary images configuration Sample weblogic.oracle/v8 schema auxiliary image configuration Compatibility weblogic.oracle/v9 schema auxiliary image configuration generated by conversion webhook in operator 4.0 Domain upgrade tool to manually upgrade the weblogic.oracle/v8 schema domain resource Introduction Auxiliary images are the recommended best approach for including Model in Image model files, application archive files, and the WebLogic Deploy Tooling installation, in your pods. This feature eliminates the need to provide these files in the image specified in domain.spec.image.\nInstead:\nThe domain resource\u0026rsquo;s domain.spec.image directly references a base image that needs to include only a WebLogic installation and a Java installation. The domain resource\u0026rsquo;s auxiliary image related fields reference one or more smaller images that contain the desired Model in Image files. The advantages of auxiliary images for Model In Image domains are:\nUse or patch a WebLogic installation image without needing to include a WDT installation, application archive, or model artifacts within the image. Share one WebLogic installation image with multiple different model configurations that are supplied in specific images. Distribute or update model files, application archives, and the WebLogic Deploy Tooling executable using specific images that do not contain a WebLogic installation. Auxiliary images internally use a Kubernetes emptyDir volume and Kubernetes init containers to share files from additional images.\nReferences Run the kubectl explain domain.spec.configuration.model.auxiliaryImages command.\nSee the model.auxiliaryImages section in the domain resource schema.\nConfiguration Beginning with operator version 4.0, you can configure one or more auxiliary images in a domain resource configuration.model.auxiliaryImages array. Each array entry must define an image which is the name of an auxiliary image. Optionally, you can set the imagePullPolicy, which defaults to Always if the image ends in :latest and IfNotPresent, otherwise. If image pull secrets are required for pulling auxiliary images, then the secrets must be referenced using domain.spec.imagePullSecrets.\nAlso, optionally, you can configure the source locations of the WebLogic Deploy Tooling model and the directory where the WebLogic Deploy Tooling software is installed (known as the WDT Home) in the auxiliary image using the sourceModelHome and sourceWDTInstallHome fields, described in the following section.\nFor details about each field, see the schema.\nFor a basic configuration example, see Configuration example 1.\nSource locations Use the optional attributes configuration.model.auxiliaryImages[].sourceModelHome and configuration.model.auxiliaryImages[].sourceWdtInstallHome to specify non-default locations of WebLogic Deploy Tooling model and WDT Home in your auxiliary image(s). Allowed values for sourceModelHome and sourceWdtInstallHome:\nUnset - Defaults to /auxiliary/models and /auxiliary/weblogic-deploy, respectively. Set to a path - Must point to an existing location containing WDT model files and WDT Home, respectively. None - Indicates that the image has no WDT models or WDT Home, respectively. If you set the sourceModelHome or sourceWDTInstallHome to None or, the source attributes are left unset and there are no files at the default locations, then the operator will ignore the source directories. Otherwise, note that if you set a source directory attribute to a specific value and there are no files in the specified directory in the auxiliary image, then the domain deployment will fail.\nThe files in sourceModelHome and sourceWDTInstallHome directories will be made available in /aux/models and /aux/weblogic-deploy directories of the WebLogic Server container in all pods, respectively.\nFor example source locations, see Configuration example 2.\nMultiple auxiliary images If specifying multiple auxiliary images with model files in their respective configuration.model.auxiliaryImages[].sourceModelHome directories, then model files are merged. The operator will merge the model files from multiple auxiliary images in the same order in which images appear under model.auxiliaryImages. Files from later images in the merge overwrite same-named files from earlier images.\nWhen specifying multiple auxiliary images, ensure that only one of the images supplies a WDT Home using configuration.model.auxiliaryImages[].sourceWDTInstallHome. If you provide more than one WDT Home among multiple auxiliary images, then the domain deployment will fail. Set sourceWDTInstallHome to None, or make sure there are no files in /auxiliary/weblogic-deploy, for all but one of your specified auxililary images.\nFor an example of configuring multiple auxiliary images, see Configuration example 3.\nModel and WDT installation homes If you are using auxiliary images, typically, it should not be necessary to set domain.spec.configuration.models.modelHome and domain.spec.configuration.models.wdtInstallHome. The model and WDT installation you supply in the auxiliary image (see source locations) are always placed in the /aux/models and /aux/weblogic-deploy directories, respectively, in all WebLogic Server pods. When auxiliary image(s) are configured, the operator automatically changes the default for modelHome and wdtInstallHome to match.\nIf you set modelHome and wdtInstallHome to a non-default value, then the domain will ignore the WDT model files and WDT Home in its auxiliary image(s).\nConfiguration examples The following configuration examples illustrate each of the previously described sections.\nExample 1: Basic configuration This example specifies the required image parameter for the auxiliary image(s); all other fields are at default values.\nspec: configuration: model: auxiliaryImages: - image: model-in-image:v1 Example 2: Source locations This example is same as Example 1 except that it specifies the source locations for the WebLogic Deploy Tooling model and WDT Home.\nspec: configuration: model: auxiliaryImages: - image: model-in-image:v1 sourceModelHome: /foo/models sourceWDTInstallHome: /bar/weblogic-deploy Example 3: Multiple images This example is the same as Example 1, except it configures multiple auxiliary images and sets the sourceWDTInstallHome for the second image to None. In this case, the source location of the WebLogic Deploy Tooling installation from the second image new-model-in-image:v1 will be ignored.\nspec: configuration: model: auxiliaryImages: - image: model-in-image:v1 - image: new-model-in-image:v1 sourceWDTInstallHome: None Sample The Model in Image Sample demonstrates deploying a Model in Image domain that uses auxiliary images to supply the domain\u0026rsquo;s WDT model files, application archive ZIP files, and WDT installation in a small, separate container image.\nUsing Docker to create an auxiliary image The Model in Image Sample initial use case describes using the WebLogic Image Tool as a convenient way to create the auxiliary image, which is the recommended best approach. Alternatively, you can \u0026ldquo;manually\u0026rdquo; build the image. For example, the following steps modify the Model in Image sample\u0026rsquo;s initial use case to use Docker to build its auxiliary image:\nDownload the Model in Image sample source and WebLogic Deploy Tooling by following the corresponding steps in the Model in Image Sample prerequisites.\nCreate a /tmp/mystaging/models directory as a staging directory and copy the model YAML file, properties, and archive into it:\n$ mkdir -p /tmp/mystaging/models $ cp /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/model.10.yaml ./models $ cp /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/model.10.properties ./models $ cp /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1/archive.zip ./models If the archive.zip file is missing, then repeat the step to create this file in the Model in Image sample initial use case while using /tmp/sample/wdt-artifacts/wdt-model-files/WLS-v1 as the target directory.\nInstall WDT in the staging directory and remove its weblogic-deploy/bin/*.cmd files, which are not used in UNIX environments:\n$ cd /tmp/mystaging $ unzip /tmp/mii-sample/model-images/weblogic-deploy.zip -d . $ rm ./weblogic-deploy/bin/*.cmd If the weblogic-deploy.zip file is missing, then repeat the step to download the latest WebLogic Deploy Tooling (WDT) in the Model in Image sample prerequisites.\nRun the docker build command using /tmp/mii-sample/ai-docker-file/Dockerfile.\n$ cd /tmp/mystaging $ docker build -f /tmp/mii-sample/ai-docker-file/Dockerfile \\ --build-arg AUXILIARY_IMAGE_PATH=/auxiliary \\ --tag model-in-image:WLS-v1 . See ./Dockerfile for an explanation of each build argument.\nClick here to view the Dockerfile. # Copyright (c) 2021, 2022, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # This is a sample Dockerfile for supplying Model in Image model files # and a WDT installation in a small separate auxiliary image # image. This is an alternative to supplying the files directly # in the domain resource `domain.spec.image` image. # AUXILIARY_IMAGE_PATH arg: # Parent location for Model in Image model and WDT Home. # The default is \u0026#39;/auxiliary\u0026#39;, which matches the parent directory in the default values for # \u0026#39;domain.spec.configuration.model.auxiliaryImages.sourceModelHome\u0026#39; and # \u0026#39;domain.spec.configuration.model.auxiliaryImages.sourceWDTInstallHome\u0026#39;, respectively. # FROM busybox ARG AUXILIARY_IMAGE_PATH=/auxiliary ARG USER=oracle ARG USERID=1000 ARG GROUP=root ENV AUXILIARY_IMAGE_PATH=${AUXILIARY_IMAGE_PATH} RUN adduser -D -u ${USERID} -G $GROUP $USER # ARG expansion in COPY command\u0026#39;s --chown is available in docker version 19.03.1+. # For older docker versions, change the Dockerfile to use separate COPY and \u0026#39;RUN chown\u0026#39; commands. COPY --chown=$USER:$GROUP ./ ${AUXILIARY_IMAGE_PATH}/ USER $USER If you have successfully created the image, then it should now be in your local machine\u0026rsquo;s Docker repository. For example:\n$ docker images model-in-image:WLS-v1 REPOSITORY TAG IMAGE ID CREATED SIZE model-in-image WLS-v1 eac9030a1f41 1 minute ago 4.04MB After the image is created, it should have the WDT executables in /auxiliary/weblogic-deploy, and WDT model, property, and archive files in /auxiliary/models. You can run ls in the Docker image to verify this:\n$ docker run -it --rm model-in-image:WLS-v1 ls -l /auxiliary total 8 drwxr-xr-x 1 oracle root 4096 Jun 1 21:53 models drwxr-xr-x 1 oracle root 4096 May 26 22:29 weblogic-deploy $ docker run -it --rm model-in-image:WLS-v1 ls -l /auxiliary/models total 16 -rw-rw-r-- 1 oracle root 5112 Jun 1 21:52 archive.zip -rw-rw-r-- 1 oracle root 173 Jun 1 21:59 model.10.properties -rw-rw-r-- 1 oracle root 1515 Jun 1 21:59 model.10.yaml $ docker run -it --rm model-in-image:WLS-v1 ls -l /auxiliary/weblogic-deploy total 28 -rw-r----- 1 oracle root 4673 Oct 22 2019 LICENSE.txt -rw-r----- 1 oracle root 30 May 25 11:40 VERSION.txt drwxr-x--- 1 oracle root 4096 May 26 22:29 bin drwxr-x--- 1 oracle root 4096 May 25 11:40 etc drwxr-x--- 1 oracle root 4096 May 25 11:40 lib drwxr-x--- 1 oracle root 4096 Jan 22 2019 samples Automated upgrade of the weblogic.oracle/v8 schema auxiliary images configuration The automated upgrade described in this section converts weblogic.oracle/v8 schema auxiliary image configuration into low-level Kubernetes schema, for example, init containers and volumes. Instead of relying on the generated low-level schema, Oracle recommends using a simplified weblogic.oracle/v9 schema configuration for auxiliary images, as documented in the Configuration section.\nIn operator version 4.0, we have enhanced auxiliary images to improve ease of use; also, its configuration has changed from operator 3.x releases.\nOperator 4.0 provides a seamless upgrade of Domains with weblogic.oracle/v8 schema auxiliary images configuration. When you create a Domain with auxiliary images using weblogic.oracle/v8 schema in a namespace managed by the 4.0 operator, the WebLogic Domain resource conversion webhook performs an automated upgrade of the domain resource to the weblogic.oracle/v9 schema. The conversion webhook runtime converts the weblogic.oracle/v8 auxiliary image configuration to the equivalent configuration using init containers, volume and volume mounts under the serverPod spec in weblogic.oracle/v9. Similarly, when upgrading the operator, Domains with weblogic.oracle/v8 schema auxiliary images are seamlessly upgraded.\nThe following is a sample weblogic.oracle/v8 schema auxiliary image configuration in operator 3.x and the equivalent weblogic.oracle/v9 schema configuration generated by the conversion webhook in operator 4.0.\nSample weblogic.oracle/v8 schema auxiliary image configuration spec: auxiliaryImageVolumes: - name: auxiliaryImageVolume1 mountPath: \u0026#34;/auxiliary\u0026#34; serverPod: auxiliaryImages: - image: \u0026#34;model-in-image:WLS-v1\u0026#34; imagePullPolicy: IfNotPresent volume: auxiliaryImageVolume1 Compatibility weblogic.oracle/v9 schema auxiliary image configuration generated by conversion webhook in operator 4.0 serverPod: initContainers: - command: - /weblogic-operator/scripts/auxImage.sh env: - name: AUXILIARY_IMAGE_PATH value: /auxiliary - name: AUXILIARY_IMAGE_TARGET_PATH value: /tmpAuxiliaryImage - name: AUXILIARY_IMAGE_COMMAND value: cp -R $AUXILIARY_IMAGE_PATH/* $AUXILIARY_IMAGE_TARGET_PATH - name: AUXILIARY_IMAGE_CONTAINER_IMAGE value: model-in-image:WLS-v1 - name: AUXILIARY_IMAGE_CONTAINER_NAME value: compat-operator-aux-container1 image: model-in-image:WLS-v1 imagePullPolicy: IfNotPresent name: compat-operator-aux-container1 volumeMounts: - mountPath: /tmpAuxiliaryImage name: compat-ai-vol-auxiliaryimagevolume1 - mountPath: /weblogic-operator/scripts name: weblogic-scripts-cm-volume volumeMounts: - mountPath: /auxiliary name: compat-ai-vol-auxiliaryimagevolume1 volumes: - emptyDir: {} name: compat-ai-vol-auxiliaryimagevolume1 The conversion webhook runtime creates init containers with names prefixed with compat- when converting the auxiliary image configuration of the weblogic.oracle/v8 schema. The operator generates only init containers with names starting with either compat- or wls-shared- in the introspector job pod. To alter the generated init container\u0026rsquo;s name, the new name must start with either compat- or wls-shared-.\nDomain upgrade tool to manually upgrade the weblogic.oracle/v8 schema domain resource To manually upgrade the domain resource from the weblogic.oracle/v8 schema to the weblogic.oracle/v9 schema, see Upgrade the weblogic.oracle/v8 schema domain resource manually.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/model-in-image/model-files/",
	"title": "Model files",
	"tags": [],
	"description": "Model file requirements, macros, and loading order.",
	"content": "Contents Introduction Sample model file Important notes about Model in Image model files Model file naming and loading order Model file macros Using secrets in model files Using environment variables in model files Combining secrets and environment variables in model files Introduction This document describes basic Model in Image model file syntax, naming, and macros. For additional information, see the WebLogic Deploy Tooling documentation.\nThe WDT Discover Domain Tool is particularly useful for generating model files from an existing domain home.\nSample model file Here\u0026rsquo;s an example of a model YAML file that defines a WebLogic Server Administration Server and dynamic cluster.\ndomainInfo: AdminUserName: \u0026#39;@@SECRET:__weblogic-credentials__:username@@\u0026#39; AdminPassword: \u0026#39;@@SECRET:__weblogic-credentials__:password@@\u0026#39; ServerStartMode: \u0026#39;prod\u0026#39; topology: Name: \u0026#39;@@ENV:DOMAIN_UID@@\u0026#39; AdminServerName: \u0026#34;admin-server\u0026#34; Cluster: \u0026#34;cluster-1\u0026#34;: DynamicServers: ServerTemplate: \u0026#34;cluster-1-template\u0026#34; ServerNamePrefix: \u0026#34;managed-server\u0026#34; DynamicClusterSize: 5 MaxDynamicClusterSize: 5 CalculatedListenPorts: false Server: \u0026#34;admin-server\u0026#34;: ListenPort: 7001 ServerTemplate: \u0026#34;cluster-1-template\u0026#34;: Cluster: \u0026#34;cluster-1\u0026#34; ListenPort: 8001 This sample model file:\nIncludes a WebLogic credentials stanza that is required by Model in Image. Derives its domain name from the predefined environment variable DOMAIN_UID, but note that this is not required. For a description of model file macro references to secrets and environment variables, see Model file macros.\nImportant notes about Model in Image model files Using model file macros\nYou can use model macros to reference arbitrary secrets from model files. This is recommended for handling mutable values such as database user names, passwords, and URLs. See Using secrets in model files.\nAll password fields in a model should use a secret macro. Passwords should not be directly included in property or model files because the files may appear in logs or debugging.\nModel files encrypted with the WDT Encrypt Model Tool are not supported. Use secrets instead.\nYou can use model macros to reference arbitrary environment variables from model files. This is useful for handling plain text mutable values that you can define using an env stanza in your Domain YAML file, and is also useful for accessing the built in DOMAIN_UID environment variable. See Using environment variables in model files.\nFor most models, it\u0026rsquo;s useful to minimize or eliminate the usage of model variable files (also known as property files) and use secrets or environment variables instead.\nA model must contain a domainInfo stanza that references your WebLogic administrative credentials. You can use the @@SECRET macro with the reserved secret name __weblogic-credentials__ to reference your Domain YAML file\u0026rsquo;s WebLogic credentials secret for this purpose. For example:\ndomainInfo: AdminUserName: \u0026#39;@@SECRET:__weblogic-credentials__:username@@\u0026#39; AdminPassword: \u0026#39;@@SECRET:__weblogic-credentials__:password@@\u0026#39; A JRF domain type model must contain a domainInfo.RCUDbInfo stanza. NOTE: JRF support in Model in Image domains is deprecated in operator version 4.1.0; use the Domain on PV domain home source type instead.\nYou can control the order that WDT uses to load your model files, see Model file naming and loading order.\nModel file naming and loading order Refer to this section if you need to control the order in which your model files are loaded. The order is important when two or more model files refer to the same configuration, because the last model that\u0026rsquo;s loaded has the highest precedence.\nDuring domain home creation, model and property files are first loaded from the configuration.models.modelHome directory within a pod. After the modelHome files are all loaded, the domain home creation then loads files from the optional WDT ConfigMap, described in Optional WDT model ConfigMap. If a modelHome file and ConfigMap file both have the same name, then both files are loaded.\nThe loading order within each of these locations is first determined using the convention filename.##.yaml and filename.##.properties, where ## are digits that specify the desired order when sorted numerically. Additional details:\nEmbedding a .##. in a file name is optional. When present, it must be placed just before the properties or yaml extension in order for it to take precedence over alphabetical precedence. The precedence of file names that include more than one .##. is undefined. The number can be any integer greater than or equal to zero. File names that don\u0026rsquo;t include .##. sort before other files as if they implicitly have the lowest possible .##. If two files share the same number, the loading order is determined alphabetically as a tie-breaker. NOTE: If configuration.models.modelHome files are supplied by combining multiple Auxiliary images, then the files in this directory are populated according to their Auxiliary image merge order before the loading order is determined.\nFor example, if you have these files in the model home directory:\njdbc.20.yaml main-model.10.yaml my-model.10.yaml y.yaml And you have these files in the ConfigMap:\njdbc-dev-urlprops.10.yaml z.yaml Then the combined model files list is passed to WebLogic Deploy Tooling as:\ny.yaml,main-model.10.yaml,my-model.10.yaml,jdbc.20.yaml,z.yaml,jdbc-dev-urlprops.10.yaml Property files (ending in .properties) use the same sorting algorithm, but they are appended together into a single file prior to passing them to the WebLogic Deploy Tooling.\nModel file macros WDT models can have macros that reference secrets or environment variables.\nUsing secrets in model files You can use WDT model @@SECRET macros to reference the WebLogic administrator username and password keys that are stored in a Kubernetes Secret and to optionally reference additional secrets. Here is the macro pattern for accessing these secrets:\nDomain Resource Attribute Corresponding WDT Model @@SECRET Macro webLogicCredentialsSecret @@SECRET:__weblogic-credentials__:username@@ and @@SECRET:__weblogic-credentials__:password@@ configuration.secrets @@SECRET:mysecret:mykey@@ For example, you can reference the WebLogic credential user name using @@SECRET:__weblogic-credentials__:username@@, and you can reference a custom secret mysecret with key mykey using @@SECRET:mysecret:mykey@@.\nAny secrets that are referenced by an @@SECRET macro must be deployed to the same namespace as your Domain, and must be referenced in your Domain YAML file using the weblogicCredentialsSecret and configuration.secrets fields.\nHere\u0026rsquo;s a sample snippet from a Domain YAML file that sets a webLogicCredentialsSecret and two custom secrets my-custom-secret1 and my-custom-secret2.\n... spec: webLogicCredentialsSecret: name: my-weblogic-credentials-secret configuration: secrets: [ my-custom-secret1,my-custom-secret2 ] ... Using environment variables in model files You can reference operator environment variables in model files. This includes any that you define yourself in your Domain YAML file using domain.spec.serverPod.env or domain.spec.adminServer.serverPod.env, or the built-in DOMAIN_UID environment variable.\nFor example, the @@ENV:DOMAIN_UID@@ macro resolves to the current domain\u0026rsquo;s domain UID.\nCombining secrets and environment variables in model files You can embed an environment variable macro in a secret macro. This is useful for referencing secrets that you\u0026rsquo;ve named based on your domain\u0026rsquo;s domainUID.\nFor example, if your domainUID is domain1, then the macro @@SECRET:@@ENV:DOMAIN_UID@@-super-double-secret:mykey@@ resolves to the value stored in mykey for secret domain1-super-double-secret.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/model-in-image/runtime-updates/",
	"title": "Runtime updates",
	"tags": [],
	"description": "Updating a running Model in Image domain&#39;s images and model files.",
	"content": "Contents Overview Updating an existing model Offline updates Offline update sample Online updates Online update scenarios Online update status and labels Online update handling of non-dynamic WebLogic configuration changes Online update handling of deletes MBean type section deletion Deleting cross-referenced MBeans Online update sample Appendices Supported updates Unsupported updates Using the WDT Discover Domain and Compare Model Tools Changing a Domain restartVersion or introspectVersion Overview If you want to make a WebLogic domain home configuration update to a running Model in Image domain, and you want the update to survive WebLogic Server pod restarts, then you must modify your existing model and instruct the WebLogic Kubernetes Operator to propagate the change.\nIf instead you make a direct runtime WebLogic configuration update of a Model in Image domain using the WebLogic Server Administration Console or WLST scripts, then the update will be ephemeral. This is because a Model in Image domain home is regenerated from the model on every pod restart.\nThere are two approaches for propagating model updates to a running Model in Image domain without first shutting down the domain:\nOffline updates: Offline updates are propagated to WebLogic pods by updating your model and then initiating a domain roll, which generates a new domain configuration, restarts the domain\u0026rsquo;s WebLogic Server Administration Server with the updated configuration, and then restarts other running servers.\nOnline updates: If model changes are configured to fully dynamic configuration MBean attributes, then you can optionally propagate changes to WebLogic pods without a roll using an online update. If an online update request includes non-dynamic model updates that can only be achieved using an offline update, then the resulting behavior is controlled by the domain resource YAML domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges attribute, which is described in detail in Online update handling of non-dynamic WebLogic configuration changes.\nThe operator does not support all types of WebLogic configuration changes while a domain is still running. If a change is unsupported for an online or offline update, then propagating the change requires entirely shutting domain the domain, applying the change, and finally restarting the domain. Full domain restarts are described in Full domain restarts.\nNOTE: If you are using WebLogic Server 14.1.2.0.0 or later, see Using demo SSL certificates in v14.1.2.0.0 or later.\nNOTE: Supported and unsupported changes are described in these sections: Supported updates and Unsupported updates. It is the administrator\u0026rsquo;s responsibility to make the necessary changes to a domain resource to initiate the correct approach for an update.\nCustom configuration overrides, which are WebLogic configuration overrides specified using a domain resource YAML file configuration.overridesConfigMap, as described in Configuration overrides, are not supported in combination with Model in Image. Model in Image will generate an error if custom overrides are specified. This should not be a concern because model file, secret, or model image updates are simpler and more flexible than custom configuration override updates. Unlike configuration overrides, the syntax for a model file update exactly matches the syntax for specifying your model file originally.\nUpdating an existing model If you have verified your proposed model updates to a running Model in Image domain are supported by consulting Supported updates and Unsupported updates, then you can use the following approaches.\nFor online or offline updates:\nSpecify a new or changed WDT ConfigMap that contains model files and use your domain resource YAML file configuration.model.configMap field to reference the map. The model files in the ConfigMap will be merged with any model files in the image. Ensure the ConfigMap is deployed to the same namespace as your domain.\nChange, add, or delete secrets that are referenced by macros in your model files and use your domain resource YAML file configuration.secrets field to reference the secrets. Ensure the secrets are deployed to the same namespace as your domain.\nFor offline updates only, there are two additional options:\nSupply a new image with new or changed model files.\nIf the files are located in the image specified in the domain resource YAML file spec.image, then change this field to reference the image. If you are using auxiliary images to supply model files in an image, then change the corresponding serverPod.auxiliaryImages.image field value to reference the new image or add a new serverPod.auxiliaryImages mount for the new image. Change, add, or delete environment variables that are referenced by macros in your model files. Environment variables are specified in the domain resource YAML file spec.serverPod.env or spec.serverPod.adminServer.env attributes.\nIt is advisable to defer the last two modification options, or similar domain resource YAML file changes to fields that cause servers to be restarted, until all of your other modifications are ready. This is because such changes automatically and immediately result in a rerun of your introspector job, and, if the job succeeds, then a roll of the domain, plus, an offline update, if there are any accompanying model changes.\nModel updates can include additions, changes, and deletions. For help generating model changes:\nFor a description of model file syntax, see the WebLogic Deploy Tooling documentation and Model in Image Model files documentation.\nFor a description of helper tooling that you can use to generate model change YAML, see Using the WDT Discover and Compare Model Tools.\nIf you specify multiple model files in your image, volumes (including those based on images from the auxiliary images feature), or WDT ConfigMap, then the order in which they\u0026rsquo;re loaded and merged is determined as described in Model file naming and loading order.\nIf you are performing an online update and the update includes deletes, then see Online update handling of deletes.\nAfter your model updates are prepared, you can instruct the operator to propagate the changed model to a running domain by following the steps in Offline updates or Online updates.\nOffline updates Use the following steps to initiate an offline configuration update to your model:\nEnsure your updates are supported by checking Supported and Unsupported updates. Modify, add, or delete your model resources as per Updating an existing model. Modify your domain resource YAML file: If you have updated your image: If the files are located in the image specified in the domain resource YAML file spec.image, then change this field to reference the image. If you are using auxiliary images to supply model files in an image, then change the corresponding serverPod.auxiliaryImages.image field value to reference the new image or add a new serverPod.auxiliaryImages mount for the new image. If you are updating environment variables, change domain.spec.serverPod.env or domain.spec.adminServer.serverPod.env accordingly. If you are specifying a WDT ConfigMap, then set domain.spec.configuration.model.configMap to the name of the ConfigMap. If you are adding or deleting secrets as part of your change, then ensure the domain.spec.configuration.secrets array reflects all current secrets. If you have modified your image or environment variables, then no more domain resource YAML file changes are needed; otherwise, change an attribute that instructs the operator to roll the domain. For examples, see change the domain spec.restartVersion or change any of the other Domain resource YAML fields that cause servers to be restarted. The operator will subsequently rerun the domain\u0026rsquo;s introspector job. This job will reload all of your secrets and environment variables, merge all of your model files, and generate a new domain home.\nIf the job succeeds, then the operator will make the updated domain home available to pods using a ConfigMap named DOMAIN_UID-weblogic-domain-introspect-cm and the operator will subsequently roll (restart) each running WebLogic Server pod in the domain so that it can load the new configuration. A domain roll begins by restarting the domain\u0026rsquo;s Administration Server and then proceeds to restart each Managed Server in the domain.\nIf the job reports a failure, see Debugging for advice.\nOffline update sample For an offline update sample which adds a data source, see the Update 1 use case in the Model in Image sample.\nOnline updates Use the following steps to initiate an online configuration update to your model:\nEnsure your updates are supported by checking Supported and Unsupported updates. Modify, add, or delete your model secrets or WDT ConfigMap as per Updating an existing model. Modify your domain resource YAML file: Do not change domain.spec.image, domain.spec.serverPod.env, or any other domain resource YAML fields that cause servers to be restarted; this will automatically and immediately result in a rerun of your introspector job, a roll if the job succeeds, plus an offline update if there are any accompanying model changes.\nIf you are specifying a WDT ConfigMap, then set domain.spec.configuration.model.configMap to the name of the ConfigMap.\nIf you are adding or deleting secrets as part of your change, then ensure the domain.spec.configuration.secrets array reflects all current secrets.\nSet domain.spec.configuration.model.onlineUpdate.enabled to true (default is false).\nSet domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges to one of CommitUpdateOnly (default), and CommitUpdateAndRoll. For details, see online update handling of non-dynamic WebLogic configuration changes.\nOptionally, tune the WDT timeouts in domain.spec.configuration.model.onlineUpdate.wdtTimeouts.\nThis is only necessary in the rare case when an introspector job\u0026rsquo;s WDT online update command timeout results in an error in the introspector job log or operator log. All timeouts are specified in milliseconds and default to two or three minutes. For a full list of timeouts, you can call kubectl explain domain.spec.configuration.model.onlineUpdate.wdtTimeouts. Change domain.spec.introspectVersion to a different value. For examples, see change the domain spec.introspectVersion.\nAfter you\u0026rsquo;ve completed these steps, the operator will subsequently run an introspector Job which generates a new merged model, compares the new merged model to the previously deployed merged model, and runs WebLogic Deploy Tooling to process the differences:\nIf the introspector job WDT determines that the differences are confined to fully dynamic WebLogic configuration MBean changes, then the operator will send delta online updates to the running WebLogic pods.\nIf WDT detects non-dynamic WebLogic configuration MBean changes, then the operator may ignore the updates, honor only the online updates, or initiate an offline update (roll) depending on whether you have configured domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges to CommitUpdateOnly (default), or CommitUpdateAndRoll. For details, see online update handling of non-dynamic WebLogic configuration changes.\nIf the introspector job reports a failure or any other failure occurs, then see Debugging for advice. When recovering from a failure, please keep the following points in mind:\nThe operator cannot automatically revert changes to resources that are under user control (just like with offline updates). For example, it is the administrator\u0026rsquo;s responsibility to revert problem changes to an image, configMap, secrets, and domain resource YAML file.\nIf there is any failure during an online update, then no WebLogic configuration changes are made to the running domain and the introspector job retries up to the failure retry time limit specified in domain.spec.failureRetryLimitMinutes. To correct the problem, modify and reapply your model resources (ConfigMap and/or secrets), plus, if the introspector job has stopped retrying, you must also change your domain resource domain.spec.introspectVersion again. For more information, see Domain failure retry processing.\nSample domain resource YAML file for an online update:\n... kind: Domain metadata: name: sample-domain1 namespace: sample-domain1-ns ... spec: ... introspectVersion: 5 configuration: ... model: domainType: \u0026#34;WLS\u0026#34; configMap: sample-domain1-wdt-config-map runtimeEncryptionSecret: sample-domain1-runtime-encryption-secret onlineUpdate: enabled: true onNonDynamicChanges: \u0026#34;CommitUpdateAndRoll\u0026#34; secrets: - sample-domain1-datasource-secret - sample-domain1-another-secret Online update scenarios Successful online update that includes only dynamic WebLogic MBean changes.\nExample dynamic WebLogic MBean changes: Changing data source connection pool capacity, password, and targets. Changing application targets. Deleting or adding a data source. Deleting or adding an application. The MBean changes are committed in the running domain and effective immediately. Expected outcome after the introspector job completes: The domain Completed condition status is set to True. For more information, see Domain conditions. The weblogic.introspectVersion label on all pods will be set to match the domain.spec.introspectVersion. Actions required: None. Successful online update that includes non-dynamic WebLogic MBean attribute changes when domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges is CommitUpdateOnly (the default).\nExample non-dynamic WebLogic MBean change: Changing a data source driver parameter property (such as username). Expected outcome after the introspector job completes: Any dynamic WebLogic configuration changes are committed in the running domain and effective immediately. Non-dynamic WebLogic configuration changes will not take effect on already running WebLogic Server pods until an administrator subsequently rolls the pod. The domain status Available condition will have a Status of True. The domain status ConfigChangesPendingRestart condition will have a Status of True until an administrator subsequently rolls all WebLogic Server pods that are already running. Each WebLogic Server pod\u0026rsquo;s weblogic.introspectVersion label will match domain.spec.introspectVersion. Each WebLogic Server pod that is already running will be given a weblogic.configChangesPendingRestart=true label until an administrator subsequently rolls the pod. Actions required: If you want the non-dynamic changes to take effect, then restart the pod(s) with the weblogic.configChangesPendingRestart=true label (such as by initiating a domain roll). See Online update handling of non-dynamic WebLogic configuration changes. Successful online update that includes non-dynamic WebLogic MBean attribute changes when domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges is CommitUpdateAndRoll.\nExpected outcome after the introspector job completes: Any dynamic WebLogic configuration changes are committed in the running domain and effective immediately. The operator will initiate a domain roll. Non-dynamic WebLogic configuration changes will take effect on each pod when the pod is rolled. Each WebLogic Server pod\u0026rsquo;s weblogic.introspectVersion label will match domain.spec.introspectVersion after it is rolled. The domain status Available condition will have a Status of True after the roll completes. Actions required: None. All changes will complete after the operator initiated domain roll completes. See Online update handling of non-dynamic WebLogic configuration changes. Changing any of the domain resource fields that cause servers to be restarted in addition to domain.spec.introspectVersion, spec.configuration.secrets, spec.configuration.model.onlineUpdate, or spec.configuration.model.configMap.\nExpected outcome after the introspector job completes: No online update was attempted by the introspector job. All model changes are treated the same as offline updates (which may result in restarts/roll after job success). Actions required: None. Changing any model attribute that is unsupported.\nExpected outcome: The expected behavior is often undefined, but in some cases there will be helpful error in the introspector job, events, and/or domain status, and the job will periodically retry until the error is corrected or its maximum error count exceeded. Actions required: Use offline updates if they are supported, or, if not, shutdown the entire domain and restart it. See Debugging. Errors in the model; for example, a syntax error.\nExpected outcome after the introspector job completes: Error in the introspector job\u0026rsquo;s pod log, domain events, and domain status. The domain status Failed condition will have a Status of True. Periodic job retries until the error is corrected or until a maximum error count is exceeded. Actions required: Correct the model. If retries have halted, then alter the spec.introspectVersion. Other errors while updating the domain.\nExpected outcome: Error in the introspector job, domain events, and/or domain status. The domain status Failed condition will have a Status of True. If there\u0026rsquo;s a failed introspector job, the job will retry periodically until the error is corrected or until it exceeds its maximum error count. Other types of errors will also usually incur periodic retries. Actions required: See Debugging. Make corrections to the domain resource and/or model. If retries have halted, then alter the spec.introspectVersion. Online update status and labels During an online update, the operator will rerun the introspector job, which in turn attempts online WebLogic configuration changes to the running domain. You can monitor an update\u0026rsquo;s status using its domain resource\u0026rsquo;s status conditions and its WebLogic Server pod labels.\nFor example, for the domain status you can check the domain resource domain.status stanza using kubectl -n MY_NAMESPACE get domain MY_DOMAINUID -o yaml, and for the WebLogic pod labels you can use kubectl -n MY_NAMESPACE get pods --show-labels plus optionally add --watch to watch the pods as they change over time.\nThe ConfigChangesPendingRestart condition in domain.status contains information about the progress of the online update. See ConfigChangesPendingRestart condition for details.\nHere are some of the expected WebLogic pod labels after an online update success:\nEach WebLogic Server pod\u0026rsquo;s weblogic.introspectVersion label value will eventually match the domain.spec.introspectVersion value that you defined.\nIf the domain resource attribute domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges is CommitUpdateOnly (the default), then the introspect version label on all pods is immediately updated after the introspection job successfully completes. If the domain resource attribute domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges is CommitUpdateAndRoll and there are no non-dynamic configuration changes to the model, then the introspect version label on all pods is immediately updated after the introspection job successfully completes. If the domain resource attribute domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges is CommitUpdateAndRoll and there are non-dynamic clabel onfiguration changes to the model, then the introspect version label on each pod is updated after the pod is rolled. There will be a weblogic.configChangesPendingRestart=true label on each WebLogic Server pod until the pod is restarted (rolled) by an administrator if all of the following are true:\nThe domain resource domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges attribute is CommitUpdateOnly (the default). Non-dynamic WebLogic configuration changes were included in a successful online model update. Online update handling of non-dynamic WebLogic configuration changes The domain resource YAML domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges attribute controls behavior when non-dynamic WebLogic configuration changes are detected during an online update introspector job. Non-dynamic changes are changes that require a domain restart to take effect. Valid values are CommitUpdateOnly (default), or CommitUpdateAndRoll:\nIf set to CommitUpdateOnly (the default) and any non-dynamic changes are detected, then all changes will be committed, dynamic changes will take effect immediately, the domain will not automatically restart (roll), and any non-dynamic changes will become effective on a pod only when the pod is later restarted.\nIf set to CommitUpdateAndRoll and any non-dynamic changes are detected, then all changes will be committed, dynamic changes will take effect immediately, the domain will automatically restart (roll), and non-dynamic changes will take effect on each pod after the pod restarts.\nWhen updating a domain with non-dynamic MBean changes with\r`domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges=CommitUpdateOnly` (the default),\rthe non-dynamic changes are not effective on a WebLogic pod until the pod is restarted.\rHowever, if you scale up a cluster or otherwise start any new servers in the domain,\rthen the new servers will start with the new non-dynamic changes\rand the domain will then be running in an inconsistent state until its older servers are restarted.\rOnline update handling of deletes The primary use case for online updates is to make small additions, deletions of single resources or MBeans that have no dependencies, or changes to non-dynamic MBean attributes.\nDeletion can be problematic for online updates in two cases:\nDeleting multiple resources that have cross dependencies. Deleting the parent type section in an MBean hierarchy. In general, complex deletion should be handled by offline updates to avoid these problems.\nNOTE: Implicitly removing a model\u0026rsquo;s parent type section may sometimes work depending on the type of the section. For example, if you have an application in the model under appDeployments: in a model.configMap and you subsequently update the ConfigMap using an online update so that it no longer includes the appDeployment section, then the online update will delete the application from the domain.\nMBean type section deletion For an example of an MBean deletion, consider a WDT ConfigMap that starts with:\nresources: SelfTuning: WorkManager: wm1: Target: \u0026#39;cluster-1\u0026#39; wm2: Target: \u0026#39;cluster-1\u0026#39; JDBCSystemResource: ... If you want to online update to a new model without work-managers, then change the ConfigMap to the following:\nresources: SelfTuning: WorkManager: JDBCSystemResource: ... Or, supply an additional ConfigMap:\nresources: SelfTuning: WorkManager: \u0026#39;!wm1\u0026#39;: \u0026#39;!wm2\u0026#39;: The online update will fail if you try replace the ConfigMap with the SelfTuning section omitted:\nresources: JDBCSystemResource: ... This fails because it implicitly removes the MBean types SelfTuning and WorkManager.\nDeleting cross-referenced MBeans For an example of an unsupported online update delete of MBeans with cross references, consider the case of a Work Manager configured with constraints where you want to delete the entire Work Manager:\nresources: SelfTuning: WorkManager: newWM: Target: \u0026#39;cluster-1\u0026#39; MinThreadsConstraint: \u0026#39;SampleMinThreads\u0026#39; MaxThreadsConstraint: \u0026#39;SampleMaxThreads\u0026#39; MinThreadsConstraint: SampleMinThreads: Count: 1 MaxThreadsConstraint: SampleMaxThreads: Count: 10 If you try to specify the updated model in the ConfigMap as:\nresources: SelfTuning: WorkManager: MinThreadsConstraint: MaxThreadsConstraint: Then, the operator will try use this delta to online update the domain:\nresources: SelfTuning: MaxThreadsConstraint: \u0026#39;!SampleMaxThreads\u0026#39;: WorkManager: \u0026#39;!newWM\u0026#39;: MinThreadsConstraint: \u0026#39;!SampleMinThreads\u0026#39;: This can fail because an online update might not delete all the referenced Constraints first before deleting the WorkManager.\nTo work around problems with online updates to objects with cross dependencies, you can use a series of online updates to make the change in stages. For example, continuing the previous Work Manager example, first perform an online update to omit the Work Manager but not the constraints:\nresources: SelfTuning: WorkManager: MinThreadsConstraint: SampleMinThreads: Count: 1 MaxThreadsConstraint: SampleMaxThreads: Count: 10 After that update completes, then perform another online update:\nresources: SelfTuning: WorkManager: MinThreadsConstraint: MaxThreadsConstraint: Online update sample For an online update sample which alters a data source and Work Manager, see the Update 4 use case in the Model in Image sample.\nAppendices Review the following appendices for additional, important information.\nSupported updates The following updates are supported for offline or online updates, except when they reference an area that is specifically documented as unsupported:\nYou can add a new WebLogic cluster or standalone server.\nYou can increase the size of a dynamic WebLogic cluster.\nYou can add new MBeans or resources by specifying their corresponding model YAML file snippet along with their parent bean hierarchy. For example, you can add a data source.\nYou can change or add MBean attributes by specifying a YAML file snippet along with its parent bean hierarchy that references an existing MBean and the attribute. For example, to add or alter the maximum capacity of a data source named mynewdatasource:\nresources: JDBCSystemResource: mynewdatasource: JdbcResource: JDBCConnectionPoolParams: MaxCapacity: 5 For more information, see Using Multiple Models in the WebLogic Deploy Tooling documentation.\nYou can change or add secrets that your model macros reference (macros that use the @@SECRET:secretname:secretkey@@ syntax). For example, you can change a database password secret.\nFor offline updates only, you can change or add environment variables that your model macros reference (macros that use the @@ENV:myenvvar@@ syntax).\nYou can remove an MBean, application deployment, or resource by omitting any reference to it in your image model files and WDT config map. You can also remove a named MBean, application deployment, or resource by specifying an additional model file with an exclamation point (!) just before its name plus ensuring the new model file is loaded after the original model file that contains the original named configuration. For example, if you have a data source named mynewdatasource defined in your model, then it can be removed by specifying a small model file that loads after the model file that defines the data source, where the small model file looks like this:\nresources: JDBCSystemResource: !mynewdatasource: There are some exceptions for online updates.\nFor more information, see Declaring Named MBeans to Delete in the WebLogic Deploying Tooling documentation.\nUnsupported updates It is important to avoid applying unsupported model updates to a running domain. An attempt to use an unsupported update may not always result in a clear error message, and the expected behavior may be undefined. If you need to make an unsupported update and no workaround is documented, then shut down your domain entirely before making the change. See Full domain restarts.\nThe following summarizes the types of runtime update configuration that are not supported in Model in Image unless a workaround or alternative is documented:\nAltering cluster size: You have a limited ability to change an existing WebLogic cluster\u0026rsquo;s membership. Specifically, do not apply runtime updates for: Adding WebLogic Servers to a configured cluster. As an alternative, consider using dynamic clusters instead of configured clusters. Removing WebLogic Servers from a configured cluster. As an alternative, you can lower your cluster\u0026rsquo;s domain resource YAML replicas attribute. Decreasing the size of a dynamic cluster. As an alternative, you can lower your cluster\u0026rsquo;s domain resource YAML replicas attribute. You cannot change, add, or remove network listen address, port, protocol, and enabled configuration for existing clusters or servers at runtime. Specifically, do not apply runtime updates for: A Default, SSL, Admin channel Enabled, listen address, or port. A Network Access Point (custom channel) Enabled, listen address, protocol, or port. Note that it is permitted to override network access point public or external addresses and ports. External access to JMX (MBean) or online WLST requires that the network access point internal port and external port match (external T3 or HTTP tunneling access to JMS, RMI, or EJBs don\u0026rsquo;t require port matching). Due to security considerations, we strongly recommend that T3 or any RMI protocol should not be exposed outside the cluster.\nNode Manager related configuration. Log related settings. This applies to changing, adding, or removing server and domain log related settings in an MBean at runtime when the domain resource is configured to override the same MBeans using the spec.logHome, spec.logHomeEnabled, or spec.httpAccessLogInLogHome attributes. Changing the domain name. You cannot change the domain name at runtime. Deleting an MBean attribute: There is no way to directly delete an attribute from an MBean that\u0026rsquo;s already been specified by a model file. The workaround is to do this using two model files: Add a model file that deletes the named bean/resource that is a parent to the attribute you want to delete using the ! syntax as described in Supported Updates. Add another model file that will be loaded after the first one, which fully defines the named bean/resource but without the attribute you want to delete. Changing any existing MBean name: There is no way to directly change the MBean name of an attribute. Instead, you can remove a named MBean using the ! syntax as described in Supported Updates. Then, you add a new one as a replacement. Embedded LDAP entries: Embedded LDAP security entries for users, groups, roles, and credential mappings. For example, you cannot add a user to the default security realm. Online update attempts in this area will fail during the introspector job, and offline update attempts may result in inconsistent security checks during the offline update\u0026rsquo;s rolling cycle. If you need to make these kinds of updates, then shut down your domain entirely before making the change, or switch to an external security provider. Any Model YAML topology: stanza changes: For example, ConsoleEnabled, RootDirectory, AdminServerName, and such. For a complete list, run /u01/wdt/weblogic-deploy/bin/modelHelp.sh -oracle_home $ORACLE_HOME topology (this assumes you have installed WDT in the /u01/wdt/weblogic-deploy directory). Dependency deletion in combination with online updates. Deleting Model entries by type: For example, you cannot delete an entire SelfTuning type stanza by omitting the stanza in an online update or by specifying an additional model with !SelfTuning in either an offline or an online update. Instead, you can delete the specific MBean by omitting the MBean itself while leaving its SelfTuning parent in place or by specifying an additional model using the ! syntax in combination with the name of the specific MBean. For details, see Online update handling of deletes. Deleting multiple resources that have cross-references in combination with online updates: For example, concurrently deleting a persistent store and a data source referenced by the persistent store. For this type of failure, the introspection job will fail and log an error describing the failed reference, and the job will automatically retry up to its maximum retries. For details, see Online update handling of deletes. Security related changes in combination with online updates: Such changes included security changes in domainInfo.Admin*, domainInfo.RCUDbinfo.*, topology.Security.*, and topology.SecurityConfiguration.*. Any online update changes in these sections will result in a failure. Using the WDT Discover Domain and Compare Model Tools Optionally, you can use the WDT Discover Domain and Compare Domain Tools to help generate your model file updates. The WebLogic Deploy Tooling Discover Domain Tool generates model files from an existing domain home, and its Compare Model Tool compares two domain models and generates the YAML file for updating the first domain to the second domain.\nFor example, assuming you\u0026rsquo;ve installed WDT in /u01/wdt/weblogic-deploy and assuming your domain type is WLS:\nRun discover for your existing domain home.\n$ /u01/wdt/weblogic-deploy/bin/discoverDomain.sh \\ -oracle_home $ORACLE_HOME \\ -domain_home $DOMAIN_HOME \\ -domain_type WLS \\ -archive_file old.zip \\ -model_file old.yaml \\ -variable_file old.properties Now make some WebLogic config changes using the console or WLST.\nRun discover for your changed domain home.\n$ /u01/wdt/weblogic-deploy/bin/discoverDomain.sh \\ -oracle_home $ORACLE_HOME \\ -domain_home $DOMAIN_HOME \\ -domain_type WLS \\ -archive_file new.zip \\ -model_file new.yaml \\ -variable_file new.properties Compare your old and new YAML using diff.\n$ diff new.yaml old.yaml Compare your old and new YAML using compareDomain to generate the YAML update file you can use for transforming the old to new.\n$ /u01/wdt/weblogic-deploy/bin/compareModel.sh \\\r-oracle_home $ORACLE_HOME \\\r-output_dir /tmp \\\r-variable_file old.properties \\\rold.yaml \\\rnew.yaml The compareModel will generate these files:\n# /tmp/diffed_model.json\r# /tmp/diffed_model.yaml, and\r# /tmp/compare_model_stdout Changing a Domain restartVersion or introspectVersion As was mentioned in Offline updates, one way to tell the operator to apply offline configuration changes to a running domain is by altering the Domain spec.restartVersion. Similarly, an online update is initiated by altering the Domain spec.introspectVersion. Here are some common ways to alter either of these fields:\nYou can alter restartVersion or introspectVersion interactively using kubectl edit -n MY_NAMESPACE domain MY_DOMAINUID.\nIf you have your domain\u0026rsquo;s resource file, then you can alter this file and call kubectl apply -f on the file.\nYou can use the Kubernetes get and patch commands.\nHere\u0026rsquo;s a sample automation script for restartVersion that takes a namespace as the first parameter (default sample-domain1-ns) and a domainUID as the second parameter (default sample-domain1):\n#!/bin/bash NAMESPACE=${1:-sample-domain1-ns} DOMAINUID=${2:-sample-domain1} currentRV=$(kubectl -n ${NAMESPACE} get domain ${DOMAINUID} -o=jsonpath=\u0026#39;{.spec.restartVersion}\u0026#39;) if [ $? = 0 ]; then # we enter here only if the previous command succeeded nextRV=$((currentRV + 1)) echo \u0026#34;@@ Info: Rolling domain \u0026#39;${DOMAINUID}\u0026#39; in namespace \u0026#39;${NAMESPACE}\u0026#39; from restartVersion=\u0026#39;${currentRV}\u0026#39; to restartVersion=\u0026#39;${nextRV}\u0026#39;.\u0026#34; kubectl -n ${NAMESPACE} patch domain ${DOMAINUID} --type=\u0026#39;json\u0026#39; \\ -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/restartVersion\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\u0026#39;${nextRV}\u0026#39;\u0026#34; }]\u0026#39; fi Here\u0026rsquo;s a similar sample script for introspectVersion:\n#!/bin/bash NAMESPACE=${1:-sample-domain1-ns} DOMAINUID=${2:-sample-domain1} currentIV=$(kubectl -n ${NAMESPACE} get domain ${DOMAINUID} -o=jsonpath=\u0026#39;{.spec.introspectVersion}\u0026#39;) if [ $? = 0 ]; then # we enter here only if the previous command succeeded nextIV=$((currentIV + 1)) echo \u0026#34;@@ Info: Rolling domain \u0026#39;${DOMAINUID}\u0026#39; in namespace \u0026#39;${NAMESPACE}\u0026#39; from introspectVersion=\u0026#39;${currentIV}\u0026#39; to introspectVersion=\u0026#39;${nextIV}\u0026#39;.\u0026#34; kubectl -n ${NAMESPACE} patch domain ${DOMAINUID} --type=\u0026#39;json\u0026#39; \\ -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/introspectVersion\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\u0026#39;${nextIV}\u0026#39;\u0026#34; }]\u0026#39; fi You can use a WebLogic Kubernetes Operator sample script that invokes the same commands that are described in the previous bulleted item.\nSee patch-restart-version.sh and patch-introspect-version.sh in the kubernetes/samples/scripts/create-weblogic-domain/model-in-image/utils/ directory. Or, see the more advanced introspectDomain.sh and rollDomain.sh among the Domain lifecycle sample scripts. "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/managing-domains/model-in-image/move-to-pv/",
	"title": "Move MII/JRF domains to PV",
	"tags": [],
	"description": "Moving an MII/JRF domain to a persistent volume.",
	"content": "FMW/JRF domains using the Model in Image domain home source type has been deprecated since WebLogic Kubernetes Operator 4.1. We recommend moving your domain home to Domain on Persistent Volume (Domain on PV). For more information, see Domain On Persistent Volume.\nIf you cannot move the domain to a persistent volume right now, you can use the following procedure.\nBack up the OPSS wallet and save it in a secret if you have not already done it.\nThe operator provides a helper script, the OPSS wallet utility, for extracting the wallet file and storing it in a Kubernetes walletFileSecret. In addition, you should save the wallet file in a safely backed-up location, outside of Kubernetes. For example, the following command saves the OPSS wallet for the sample-domain1 domain in the sample-ns namespace to a file named ewallet.p12 in the /tmp directory and also stores it in the wallet secret named sample-domain1-opss-walletfile-secret.\n$ opss-wallet.sh -n sample-ns -d sample-domain1 -s -r -wf /tmp/ewallet.p12 -ws sample-domain1-opss-walletfile-secret Follow the steps in Upgrade managed domains.\nIf you are using an auxiliary image in your MII/JRF domain, then it will be used as a domain creation image. If you are not using an auxiliary image in your MII/JRF domain, then create a Domain creation image.\nYou can delete the old domain resource YAML file by using this command: $ kubectl delete -f \u0026lt;original domain resource YAML\u0026gt;.\nThen, create a new domain resource YAML file. You should have at least the following changes:\n# Change type to PersistentVolume domainHomeSourceType: PersistentVolume image: \u0026lt;Fusion Middleware Infrastructure base image\u0026gt; ... serverPod: ... # specify the volume and volume mount information volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: sample-domain1-pvc-rwm1 volumeMounts: - mountPath: /share name: weblogic-domain-storage-volume # specify a new configuration section, remove the old configuration section. configuration: # secrets that are referenced by model yaml macros # sample-domain1-rcu-access is used for JRF domains secrets: [ sample-domain1-rcu-access ] initializeDomainOnPV: persistentVolumeClaim: metadata: name: sample-domain1-pvc-rwm1 spec: storageClassName: my-storage-class resources: requests: storage: 10Gi domain: createIfNotExists: Domain domainCreationImages: - image: \u0026#39;myaux:v6\u0026#39; domainType: JRF domainCreationConfigMap: sample-domain1-wdt-config-map opss: # Make sure you have already saved the wallet file secret. This allows the domain to use # an existing JRF database schemas. walletFileSecret: sample-domain1-opss-walletfile-secret walletPasswordSecret: sample-domain1-opss-wallet-password-secret Deploy the domain. If it is successful, then the domain has been migrated to a persistent volume.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "WebLogic Kubernetes Operator The WebLogic Kubernetes Operator (the “operator”) supports running your WebLogic Server and Fusion Middleware Infrastructure domains on Kubernetes, an industry standard, cloud neutral deployment platform. It lets you encapsulate your entire WebLogic Server installation and layered applications into a portable set of cloud neutral images and simple resource description files. You can run them on any on-premises or public cloud that supports Kubernetes where you\u0026rsquo;ve deployed the operator.\nFurthermore, the operator is well suited to CI/CD processes. You can easily inject changes when moving between environments, such as from test to production. For example, you can externally inject database URLs and credentials during deployment, or you can inject arbitrary changes to most WebLogic configurations.\nThe operator takes advantage of the Kubernetes operator pattern, which means that it uses Kubernetes APIs to provide support for operations, such as: provisioning, lifecycle management, application versioning, product patching, scaling, and security. The operator also enables the use of tooling that is native to this infrastructure for monitoring, logging, tracing, and security.\nYou can:\nDeploy an operator that manages all WebLogic domains in all namespaces in a Kubernetes cluster, or that only manages domains in a specific subset of the namespaces, or that manages only domains that are located in the same namespace as the operator. At most, a namespace can be managed by one operator. Supply WebLogic domain configuration using: Model in Image: Includes WebLogic Deploy Tooling models and archives in a container image. NOTE: Model in Image without auxiliary images (the WDT model and installation files are included in the same image with the WebLogic Server installation) is deprecated in WebLogic Kubernetes Operator version 4.0.7. Oracle recommends that you use Model in Image with auxiliary images. Domain in Image: Includes a WebLogic domain home in a container image. NOTE: The Domain in Image domain home source type is deprecated in WebLogic Kubernetes Operator version 4.0. Oracle recommends that you choose either Domain on PV or Model in Image, depending on your needs. Domain on PV: Locates WebLogic domain homes in a Kubernetes PersistentVolume (PV). This PV can reside in an NFS file system or other Kubernetes volume types. Configure deployment of WebLogic domains as Kubernetes resources (using Kubernetes custom resource definitions). Override certain aspects of the WebLogic domain configuration; for example, use a different database password for different deployments. Start and stop servers and clusters in the domain based on declarative startup parameters and desired states. Scale WebLogic domains by starting and stopping Managed Servers on demand, Kubernetes scale commands, setting up a Kubernetes Horizontal Pod Autoscaler, or by integrating with a REST API to initiate scaling based on the WebLogic Diagnostics Framework (WLDF), Prometheus, Grafana, or other rules. Expose HTTP paths on a WebLogic domain outside the Kubernetes domain with load balancing, and automatically update the load balancer when Managed Servers in the WebLogic domain are started or stopped. Expose the WebLogic Server Administration Console outside the Kubernetes cluster, if desired. Expose T3 channels outside the Kubernetes domain, if desired. Publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana. The fastest way to experience the operator is to follow the Quick Start guide, or you can peruse our documentation, read our blogs, or try out the samples. Also, you can step through the Tutorial using the operator to deploy and run a WebLogic domain container-packaged web application on an Oracle Container Engine for Kubernetes (OKE) cluster.\nCurrent production release This is the current release of the operator. See the operator prerequisites and supported environments.\nRecent changes and known issues See the Release Notes for recent changes to the operator and Known Limitations for the current set of known issues.\nOperator earlier versions As of November 2024, version 3.4.x of the WebLogic Kubernetes Operator is no longer supported.\nDocumentation for prior releases of the operator: 3.4, 4.0, and 4.1.\nBackward compatibility guidelines Starting from the 2.0.1 release, operator releases are backward compatible with respect to the domain resource schema, operator Helm chart input values, configuration overrides template, Kubernetes resources created by the operator Helm chart, Kubernetes resources created by the operator, and the operator REST interface. We intend to maintain compatibility for three releases, except in the case of a clearly communicated deprecated feature, which will be maintained for one release after a replacement is available.\nGetting help See Get help.\nRelated projects Oracle Fusion Middleware on Kubernetes WebLogic Deploy Tooling WebLogic Image Tool WebLogic Monitoring Exporter WebLogic Remote Console WebLogic Kubernetes Toolkit UI "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/table_of_contents/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Contents {{ .Page.TableOfContents }}"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/4.2/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]