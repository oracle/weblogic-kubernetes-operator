[
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/cicd/layering/",
	"title": "Docker image layering",
	"tags": [],
	"description": "Learn about Docker image layering and why it is important.",
	"content": " Docker images are composed of layers, as shown in the diagram below. If you download the standard weblogic:12.2.1.3 image from the Docker Store, then you can see these layers using the command docker inspect store/oracle/weblogic:12.2.1.3 (the domain layer will not be there). You are not required to use layers, but efficient use of layers is considered a best practice.\nWhy is it important to maintain the layering of images? Layering is an important technique in Docker images. Layers are important because they are shared between images. Let\u0026rsquo;s consider an example. In the diagram below, we have two domains that we have built using layers. The second domain has some additional patches that we needed on top of those provided in the standard WebLogic image. Those are installed in their own layer, and then the second domain is created in another layer on top of that.\nLet\u0026rsquo;s assume we have a three-node Kubernetes cluster and we are running both domains in this cluster. Sooner or later, we will end up with servers in each domain running on each node, so eventually all of the image layers are going to be needed on all of the nodes. Using the approach shown below (that is, standard Docker layering techniques) we are going to need to store all six of these layers on each node. If you add up the sizes, then you will see that it comes out to about 1.5GB per node.\nNow, let\u0026rsquo;s consider the alternative, where we do not use layers, but instead, build images for each domain and put everything in one big layer (this is often called \u0026ldquo;squashing\u0026rdquo; the layers). In this case, we have the same content, but if you add up the size of the images, you get 2.9GB per node. That’s almost twice the size!\nWith only two domains, you start to see the problem. In the layered approach, each new domain is adding only a relatively very small increment. In the non-layered approach, each new domain is essentially adding the entire stack over again. Imagine if we had ten domains, now the calculation looks like this:\n    With Layers Without Layers     Shared Layers 1.4GB 0GB   Dedicated/different layers 10 x 10MB = 100MB 10 x 1.5GB = 15GB   Total per node 1.5GB 15GB    You can see how the amount of storage for images really starts to add up, and it is not just a question of storage. When Kubernetes creates a container from an image, the size of the image has an impact on how long it takes to create and start the container.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/security/domain-security/image-protection/",
	"title": "Docker image protection",
	"tags": [],
	"description": "WebLogic domain in Docker image protection",
	"content": " WebLogic domain in Docker image protection Oracle strongly recommends storing the Docker images that contain a WebLogic domain home as private in the Docker registry. In addition to any local registry, public Docker registries include Docker Hub and the Oracle Cloud Infrastructure Registry (OCIR).\n The WebLogic domain home that is part of a Docker image contains sensitive information about the domain including keys and credentials that are used to access external resources (for example, data source password). In addition, the Docker image may be used to create a running server that further exposes the WebLogic domain outside of the Kubernetes cluster.\nThere are two main options to pull images from a private registry:\n Specify the image pull secret on the WebLogic Domain resource. Set up the ServiceAccount in the domain namespace with an image pull secret.  1. Use imagePullSecrets with the Domain resource. In order to access a Docker image that is protected by a private registry, the imagePullSecrets should be specified on the Kubernetes Domain resource definition:\napiVersion: \u0026quot;weblogic.oracle/v2\u0026quot; kind: Domain metadata: name: domain1 namespace: domain1-ns labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: domainHomeInImage: true image: \u0026quot;my-domain-home-in-image\u0026quot; imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; imagePullSecrets: - name: \u0026quot;my-registry-pull-secret\u0026quot; webLogicCredentialsSecret: name: \u0026quot;domain1-weblogic-credentials\u0026quot;  To create the Kubernetes secret called my-registry-pull-secret in the namespace where the domain will be running, domain1-ns, the following command can be used:\n$ kubectl create secret docker-registry my-registry-pull-secret \\ -n domain1-ns \\ --docker-server=\u0026lt;registry-server\u0026gt; \\ --docker-username=\u0026lt;name\u0026gt; \\ --docker-password=\u0026lt;password\u0026gt; \\ --docker-email=\u0026lt;email\u0026gt;  For more information about creating Kubernetes secrets for accessing the registry, see the Kubernetes documentation about pulling an image from a private registry.\n2. Set up the Kubernetes ServiceAccount with imagePullSecrets. An additional option for accessing a Docker image protected by a private registry is to set up the Kubernetes ServiceAccount in the namespace running the WebLogic domain with a set of image pull secrets thus avoiding the need to set imagePullSecrets for each Domain resource being created (as each resource instance represents a WebLogic domain that the operator is managing).\nThe Kubernetes secret would be created in the same manner as shown above and then the ServiceAccount would be updated to include this image pull secret:\n$ kubectl patch serviceaccount default -n domain1-ns \\ -p '{\u0026quot;imagePullSecrets\u0026quot;: [{\u0026quot;name\u0026quot;: \u0026quot;my-registry-pull-secret\u0026quot;}]}'  For more information about updating a Kubernetes ServiceAccount for accessing the registry, see the Kubernetes documentation about configuring service accounts.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/security/certificates/",
	"title": "Certificates",
	"tags": [],
	"description": "SSL/TLS certificate handling for the WebLogic operator.",
	"content": " Updating operator external certificate If the operator needs to update the external certificate and key currently being used or was installed without an external REST API SSL/TLS identity, the helm upgrade command is used to re-start the operator with the new or updated kubernetes tls secret that contains the desired certificate(s).\nThe operator requires a re-start in order to begin using the new or udpated external certificate. The Helm --recreate-pods flag is used to cause the existing kubernetes pod to be terminated and a new pod to be started with the updated configuration.\nFor example, if the operator was installed with the Helm release name weblogic-operator in the namespace weblogic-operator-ns and the kubernetes tls secret is named weblogic-operator-cert, the following commands can be used to update the operator certificate(s) and key:\n$ kubectl create secret tls weblogic-operator-cert -n weblogic-operator-ns \\ --cert=\u0026lt;path-to-certificate\u0026gt; --key=\u0026lt;path-to-private-key\u0026gt; $ helm get values weblogic-operator $ helm upgrade --wait --recreate-pods --reuse-values \\ --set externalRestEnabled=true \\ --set externalRestIdentitySecret=weblogic-operator-cert \\ weblogic-operator kubernetes/charts/weblogic-operator  Additional reading  Configure the external REST interface SSL/TLS identity REST interface configuration settings Sample to create external certificate and key  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/samples/simple/credentials/",
	"title": "Credentials",
	"tags": [],
	"description": "Sample for creating a Kubernetes secret that contains the Administration Server credentials. This secret can be used in creating a WebLogic domain resource.",
	"content": " Creating credentials for a WebLogic domain This sample demonstrates how to create a Kubernetes secret containing the credentials for a WebLogic domain. The operator expects this secret to be named following the pattern domainUID-weblogic-credentials, where domainUID is the unique identifier of the domain. It must be in the same namespace that the domain will run in.\nTo use the sample, run the command:\n$ ./create-weblogic-credentials.sh -u username -p password -d domainUID -n namespace -s secretName  The parameters are as follows:\n -u user name, must be specified. -p password, must be specified. -d domainUID, optional. The default value is domain1. If specified, the secret will be labeled with the domainUID unless the given value is an empty string. -n namespace, optional. Use the default namespace if not specified. -s secretName, optional. If not specified, the secret name will be determined based on the domainUID value.  This creates a generic secret containing the user name and password as literal values.\nYou can check the secret with the kubectl get secret command. An example is shown below, including the output:\n$ kubectl -n domain-namespace-1 get secret domain1-weblogic-credentials -o yaml apiVersion: v1 data: password: d2VsY29tZTE= username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: 2018-12-12T20:25:20Z labels: weblogic.domainName: domain1 weblogic.domainUID: domain1 name: domain1-weblogic-credentials namespace: domain-namespace-1 resourceVersion: \u0026quot;5680\u0026quot; selfLink: /api/v1/namespaces/domain-namespace-1/secrets/domain1-weblogic-credentials uid: 0c2b3510-fe4c-11e8-994d-00001700101d type: Opaque  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/samples/simple/domains/manually-create-domain/",
	"title": "Manually",
	"tags": [],
	"description": "Sample for creating the domain custom resource manually.",
	"content": "In some circumstances you may wish to manually create your domain custom resource. If you have created your own Docker image containing your domain and the specific patches that you require, then this approach will probably be most suitable for your needs.\nTo create the domain custom resource, just make a copy of the sample domain.yaml, and then edit it as per the instructions provided in the comments in that file. When it is ready, you can create the domain in your Kubernetes cluster using the command:\n$ kubectl apply -f domain.yaml  You can verify the domain custom resource was created using this command:\n$ kubectl -n YOUR_NAMESPACE get domains  You can view details of the domain using this command:\n$ kubectl -n YOUR_NAMESPACE describe domain YOUR_DOMAIN  In both of these commands, replace YOUR_NAMESPACE with the namespace that you created the domain in, and replace YOUR_DOMAIN with the domainUID you chose.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/samples/simple/storage/",
	"title": "Storage",
	"tags": [],
	"description": "Sample for creating a PV or PVC that can be used by a domain resource as the persistent storage for the WebLogic domain home or log files.",
	"content": " Sample persistent volume and persistent volume claim The sample scripts demonstrate the creation of a Kubernetes persistent volume (PV) and persistent volume claim (PVC), which can then be used in a domain resource as a persistent storage for the WebLogic domain home or log files.\nA PV and PVC can be shared by multiple WebLogic domains or dedicated to a particular domain.\nPrerequisites Before you begin, read this document, Persistent storage.\nUsing the scripts to create a PV and PVC Prior to running the create-pv-pvc.sh script, make a copy of the create-pv-pvc-inputs.yaml file, and uncomment and explicitly configure the weblogicDomainStoragePath property in the inputs file.\nRun the create script, pointing it at your inputs file and an output directory:\n$ ./create-pv-pvc.sh \\ -i create-pv-pvc-inputs.yaml \\ -o /path/to/output-directory  The create-pv-pvc.sh script will create a subdirectory pv-pvcs under the given /path/to/output-directory directory. By default, the script generates two YAML files, namely weblogic-sample-pv.yaml and weblogic-sample-pvc.yaml, in the /path/to/output-directory/pv-pvcs. These two YAML files can be used to create the Kubernetes resources using the kubectl create -f command.\n$ kubectl create -f weblogic-sample-pv.yaml $ kubectl create -f weblogic-sample-pvc.yaml  As a convenience, the script can optionally create the PV and PVC resources using the -e option.\nThe usage of the create script is as follows:\n$ sh create-pv-pvc.sh -h usage: create-pv-pvc.sh -i file -o dir [-e] [-h] -i Parameter inputs file, must be specified. -o Output directory for the generated yaml files, must be specified. -e Also create the Kubernetes objects using the generated yaml files -h Help  If you copy the sample scripts to a different location, make sure that you copy everything in the \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts directory together into the target directory, maintaining the original directory hierarchy.\nConfiguration parameters The PV and PVC creation inputs can be customized by editing the create-pv-pvc-inputs.yaml file.\n   Parameter Definition Default     domainUID ID of the domain resource to which the generated PV and PVC will be dedicated. Leave it empty if the PV and PVC are going to be shared by multiple domains. no default   namespace Kubernetes namespace to create the PVC. default   baseName Base name of the PV and PVC. The generated PV and PVC will be \u0026lt;baseName\u0026gt;-pv and \u0026lt;baseName\u0026gt;-pvc respectively. weblogic-sample   weblogicDomainStoragePath Physical path of the storage for the PV. When weblogicDomainStorageType is set to HOST_PATH, this value should be set the to path to the domain storage on the Kubernetes host. When weblogicDomainStorageType is set to NFS, then weblogicDomainStorageNFSServer should be set to the IP address or name of the DNS server, and this value should be set to the exported path on that server. Note that the path where the domain is mounted in the WebLogic containers is not affected by this setting; that is determined when you create your domain. no default   weblogicDomainStorageReclaimPolicy Kubernetes PVC policy for the persistent storage. The valid values are: Retain, Delete, and Recycle. Retain   weblogicDomainStorageSize Total storage allocated for the PVC. 10Gi   weblogicDomainStorageType Type of storage. Legal values are NFS and HOST_PATH. If using NFS, weblogicDomainStorageNFSServer must be specified. HOST_PATH   weblogicDomainStorageNFSServer Name or IP address of the NFS server. This setting only applies if weblogicDomainStorateType is NFS. no default    Shared versus dedicated PVC By default, the domainUID is left empty in the inputs file, which means the generated PV and PVC will not be associated with a particular domain, but can be shared by multiple domain resources in the same Kubernetes namespaces as the PV and PVC.\nFor the use cases where dedicated PV and PVC are desired for a particular domain, the domainUID needs to be set in the create-pv-pvc-inputs.yaml file. The presence of a non-empty domainUID in the inputs file will cause the generated PV and PVC to be associated with the specified domainUID. The association includes that the names of the generated YAML files and the Kubernetes PV and PVC objects are decorated with the domainUID, and the PV and PVC objects are also labeled with the domainUID.\nVerify the results The create script will verify that the PV and PVC were created, and will report a failure if there was any error. However, it may be desirable to manually verify the PV and PVC, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nGenerated YAML files with the default inputs The content of the generated weblogic-sample-pvc.yaml:\n# Copyright 2018, Oracle Corporation and/or its affiliates. All rights reserved. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: weblogic-sample-pvc namespace: default labels: weblogic.resourceVersion: domain-v2 storageClassName: weblogic-sample-storage-class accessModes: - ReadWriteMany resources: requests: storage: 10Gi  The content of the generated weblogic-sample-pv.yaml:\n# Copyright 2018, Oracle Corporation and/or its affiliates. All rights reserved. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. apiVersion: v1 kind: PersistentVolume metadata: name: weblogic-sample-pv labels: weblogic.resourceVersion: domain-v2 # weblogic.domainUID: spec: storageClassName: weblogic-sample-storage-class capacity: storage: 10Gi accessModes: - ReadWriteMany # Valid values are Retain, Delete or Recycle persistentVolumeReclaimPolicy: Retain hostPath: # nfs: # server: %SAMPLE_STORAGE_NFS_SERVER% path: \u0026quot;/scratch/k8s_dir\u0026quot;  Generated YAML files for dedicated PV and PVC The content of the generated domain1-weblogic-sample-pvc.yaml when domainUID is set to domain1:\n# Copyright 2018, Oracle Corporation and/or its affiliates. All rights reserved. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: domain1-weblogic-sample-pvc namespace: default labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: storageClassName: domain1-weblogic-sample-storage-class accessModes: - ReadWriteMany resources: requests: storage: 10Gi  The content of the generated domain1-weblogic-sample-pv.yaml when domainUID is set to domain1:\n# Copyright 2018, Oracle Corporation and/or its affiliates. All rights reserved. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. apiVersion: v1 kind: PersistentVolume metadata: name: domain1-weblogic-sample-pv labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: storageClassName: domain1-weblogic-sample-storage-class capacity: storage: 10Gi accessModes: - ReadWriteMany # Valid values are Retain, Delete or Recycle persistentVolumeReclaimPolicy: Retain hostPath: # nfs: # server: %SAMPLE_STORAGE_NFS_SERVER% path: \u0026quot;/scratch/k8s_dir\u0026quot;  Verify the PV and PVC objects You can use this command to verify the persistent volume was created. Note that the Status field should have the value Bound, indicating the that persistent volume has been claimed:\n$ kubectl describe pv weblogic-sample-pv Name: weblogic-sample-pv Labels: weblogic.resourceVersion=domain-v2 Annotations: pv.kubernetes.io/bound-by-controller=yes StorageClass: weblogic-sample-storage-class Status: Bound Claim: default/weblogic-sample-pvc Reclaim Policy: Retain Access Modes: RWX Capacity: 10Gi Message: Source: Type: HostPath (bare host directory volume) Path: /scratch/k8s_dir HostPathType: Events: \u0026lt;none\u0026gt;  You can use this command to verify the persistent volume claim was created:\n$ kubectl describe pvc weblogic-sample-pvc Name: weblogic-sample-pvc Namespace: default StorageClass: weblogic-sample-storage-class Status: Bound Volume: weblogic-sample-pv Labels: weblogic.resourceVersion=domain-v2 Annotations: pv.kubernetes.io/bind-completed=yes pv.kubernetes.io/bound-by-controller=yes Finalizers: [] Capacity: 10Gi Access Modes: RWX Events: \u0026lt;none\u0026gt;  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/developerguide/requirements/",
	"title": "Requirements",
	"tags": [],
	"description": "",
	"content": " In addition to the requirements listed in the User guide, the following software is also required to obtain and build the operator:\n Git (1.8 or later recommended) Java Developer Kit (1.8u131 or later recommended; please use 1.8, tests will not work on 1.9 or later versions) Apache Maven (3.3 or later recommended)  The operator is written primarily in Java, BASH shell scripts, and WLST scripts. The Java code uses features introduced in Java 1.8 \u0026ndash; for example, closures \u0026ndash; but does not use any Java 1.9 features.\nBecause the target runtime environment for the operator is Oracle Linux, no particular effort has been made to ensure the build or tests run on any other operating system. Please be aware that Oracle will not provide support, or accept pull requests to add support, for other operating systems.\nObtaining the operator source code The operator source code is published on GitHub at https://github.com/oracle/weblogic-kubernetes-operator. Developers may clone this repository to a local machine or, if desired, create a fork in their personal namespace and clone the fork. Developers who are planning to submit a pull request are advised to create a fork.\nTo clone the repository from GitHub, issue this command:\n$ git clone https://github.com/oracle/weblogic-kubernetes-operator.git  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/samples/simple/",
	"title": "Simple samples",
	"tags": [],
	"description": "",
	"content": "This section provides samples for individual tasks. The samples in this section are intended to be modified before production use.\n Credentials  Sample for creating a Kubernetes secret that contains the Administration Server credentials. This secret can be used in creating a WebLogic domain resource.\n Storage  Sample for creating a PV or PVC that can be used by a domain resource as the persistent storage for the WebLogic domain home or log files.\n Domains  These samples show various choices for working with domains.\n REST APIs  Sample for generating a self-signed certificate and private key that can be used for the operator\u0026#39;s external REST API.\n Ingress  Load balancer sample scripts.\n Elastic Stack  Sample for configuring the Elasticsearch and Kibana deployments and services for the operator\u0026#39;s logs.\n "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-operators/using-the-operator/using-helm/",
	"title": "Use Helm",
	"tags": [],
	"description": "Useful Helm operations.",
	"content": " Contents  Useful Helm operations Operator Helm configuration values  Overall operator information Creating the operator pod WebLogic domain management Elastic Stack integration REST interface configuration Debugging options  Common mistakes and solutions  Note that the operator Helm chart is available from the GitHub chart repository, see Alternatively, install the operator Helm chart from GitHub chart repository.\nUseful Helm operations Show the available operator configuration parameters and their default values:\n$ helm inspect values kubernetes/charts/weblogic-operator  Show the custom values you configured for the operator Helm release:\n$ helm get values weblogic-operator  Show all of the values your operator Helm release is using:\n$ helm get values --all weblogic-operator  List the Helm releases that have been installed in this Kubernetes cluster:\n$ helm list  Get the status of the operator Helm release:\n$ helm status weblogic-operator  Show the history of the operator Helm release:\n$ helm history weblogic-operator  Roll back to a previous version of this operator Helm release, in this case, the first version:\n$ helm rollback weblogic-operator 1  Change one or more values in the operator Helm release. In this example, the --reuse-values flag indicates that previous overrides of other values should be retained:\n$ helm upgrade \\ --reuse-values \\ --set \u0026quot;domainNamespaces={sample-domains-ns1}\u0026quot; \\ --set \u0026quot;javaLoggingLevel=FINE\u0026quot; \\ --wait \\ weblogic-operator \\ kubernetes/charts/weblogic-operator  Operator Helm configuration values This section describes the details of the operator Helm chart\u0026rsquo;s available configuration values.\nOverall operator information serviceAccount Specifies the name of the service account in the operator\u0026rsquo;s namespace that the operator will use to make requests to the Kubernetes API server. You are responsible for creating the service account.\nDefaults to default.\nExample:\nserviceAccount: \u0026quot;weblogic-operator\u0026quot;  javaLoggingLevel Specifies the level of Java logging that should be enabled in the operator. Valid values are: SEVERE, WARNING, INFO, CONFIG, FINE, FINER, and FINEST.\nDefaults to INFO.\nExample:\njavaLoggingLevel: \u0026quot;FINE\u0026quot;  Creating the operator pod image Specifies the Docker image containing the operator code.\nDefaults to weblogic-kubernetes-operator:2.1.\nExample:\nimage: \u0026quot;weblogic-kubernetes-operator:LATEST\u0026quot;  imagePullPolicy Specifies the image pull policy for the operator Docker image.\nDefaults to IfNotPresent.\nExample:\nimage: \u0026quot;Always\u0026quot;  imagePullSecrets Contains an optional list of Kubernetes secrets, in the operator\u0026rsquo;s namepace, that are needed to access the registry containing the operator Docker image. You are responsible for creating the secret. If no secrets are required, then omit this property.\nExample:\nimagePullSecrets: - name: \u0026quot;my-image-pull-secret\u0026quot;  WebLogic domain management domainNamespaces Specifies a list of WebLogic domain namespaces which the operator manages. The names must be lower case. You are responsible for creating these namespaces.\nThis property is required.\nExample 1: In the configuration below, the operator will monitor the default Kubernetes namespace:\ndomainNamespaces: - \u0026quot;default\u0026quot;  Example 2: In the configuration below, the Helm installation will manage namespace1 and namespace2:\ndomainNamespaces: [ \u0026quot;namespace1\u0026quot;, \u0026quot;namespace2\u0026quot; ]  These examples show two valid YAML syntax options for arrays.\n You must include the default namespace in the list if you want the operator to monitor both the default namespace and some other namespaces.\n Elastic Stack integration elkIntegrationEnabled Specifies whether or not Elastic Stack integration is enabled.\nDefaults to false.\nExample:\nelkIntegrationEnabled: true  logStashImage Specifies the Docker image containing Logstash. This parameter is ignored if elkIntegrationEnabled is false.\nDefaults to logstash:6.6.0.\nExample:\nlogStashImage: \u0026quot;logstash:6.2\u0026quot;  elasticSearchHost Specifies the hostname where Elasticsearch is running. This parameter is ignored if elkIntegrationEnabled is false.\nDefaults to elasticsearch.default.svc.cluster.local.\nExample:\nelasticSearchHost: \u0026quot;elasticsearch2.default.svc.cluster.local\u0026quot;  elasticSearchPort Specifies the port number where Elasticsearch is running. This parameter is ignored if elkIntegrationEnabled is false.\nDefaults to 9200.\nExample:\nelasticSearchPort: 9201  REST interface configuration externalRestEnabled Determines whether the operator\u0026rsquo;s REST interface will be exposed outside the Kubernetes cluster.\nDefaults to false.\nIf set to true, you must provide the externalRestIdentitySecret property that contains the name of the Kubernetes secret which contains the SSL certificate and private key for the operator\u0026rsquo;s external REST interface.\nExample:\nexternalRestEnabled: true  externalRestHttpsPort Specif`ies the node port that should be allocated for the external operator REST HTTPS interface.\nOnly used when externalRestEnabled is true, otherwise ignored.\nDefaults to 31001.\nExample:\nexternalRestHttpsPort: 32009  externalRestIdentitySecret Specifies the user supplied secret that contains the SSL/TLS certificate and private key for the external operator REST HTTPS interface. The value must be the name of the Kubernetes tls secret previously created in the namespace where the WebLogic operator is deployed. This parameter is required if externalRestEnabled is true, otherwise, it is ignored. In order to create the Kubernetes tls secret you can use the following command:\n$ kubectl create secret tls \u0026lt;secret-name\u0026gt; \\ -n \u0026lt;operator-namespace\u0026gt; \\ --cert=\u0026lt;path-to-certificate\u0026gt; \\ --key=\u0026lt;path-to-private-key\u0026gt;  There is no default value.\nThe Helm installation will produce an error, similar to the following, if externalRestIdentitySecret is not specified (left blank) and externalRestEnabled is true:\nError: render error in \u0026quot;weblogic-operator/templates/main.yaml\u0026quot;: template: weblogic-operator/templates/main.yaml:9:3: executing \u0026quot;weblogic-operator/templates/main.yaml\u0026quot; at \u0026lt;include \u0026quot;operator.va...\u0026gt;: error calling include: template: weblogic-operator/templates/_validate-inputs.tpl:42:14: executing \u0026quot;operator.validateInputs\u0026quot; at \u0026lt;include \u0026quot;utils.endVa...\u0026gt;: error calling include: template: weblogic-operator/templates/_utils.tpl:22:6: executing \u0026quot;utils.endValidation\u0026quot; at \u0026lt;fail $scope.validati...\u0026gt;: error calling fail: string externalRestIdentitySecret must be specified  Example:\nexternalRestIdentitySecret: weblogic-operator-external-rest-identity  externalOperatorCert (Deprecated) Use externalRestIdentitySecret instead\n Specifies the user supplied certificate to use for the external operator REST HTTPS interface. The value must be a string containing a Base64 encoded PEM certificate. This parameter is required if externalRestEnabled is true, otherwise, it is ignored.\nThere is no default value.\nThe Helm installation will produce an error, similar to the following, if externalOperatorCert is not specified (left blank) and externalRestEnabled is true:\nError: render error in \u0026quot;weblogic-operator/templates/main.yaml\u0026quot;: template: weblogic-operator/templates/main.yaml:4:3: executing \u0026quot;weblogic-operator/templates/main.yaml\u0026quot; at \u0026lt;include \u0026quot;operator.va...\u0026gt;: error calling include: template: weblogic-operator/templates/_validate-inputs.tpl:53:4: executing \u0026quot;operator.validateInputs\u0026quot; at \u0026lt;include \u0026quot;operator.re...\u0026gt;: error calling include: template: weblogic-operator/templates/_utils.tpl:137:6: executing \u0026quot;operator.reportValidationErrors\u0026quot; at \u0026lt;fail .validationErro...\u0026gt;: error calling fail: The string property externalOperatorCert must be specified.  Example:\nexternalOperatorCert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQwakNDQXJxZ0F3S ...  externalOperatorKey (Deprecated) Use externalRestIdentitySecret instead\n Specifies user supplied private key to use for the external operator REST HTTPS interface. The value must be a string containing a Base64 encoded PEM key. This parameter is required if externalRestEnabled is true, otherwise, it is ignored.\nThere is no default value.\nThe Helm installation will produce an error, similar to the following, if externalOperatorKey is not specified (left blank) and externalRestEnabled is true:\nError: render error in \u0026quot;weblogic-operator/templates/main.yaml\u0026quot;: template: weblogic-operator/templates/main.yaml:4:3: executing \u0026quot;weblogic-operator/templates/main.yaml\u0026quot; at \u0026lt;include \u0026quot;operator.va...\u0026gt;: error calling include: template: weblogic-operator/templates/_validate-inputs.tpl:53:4: executing \u0026quot;operator.validateInputs\u0026quot; at \u0026lt;include \u0026quot;operator.re...\u0026gt;: error calling include: template: weblogic-operator/templates/_utils.tpl:137:6: executing \u0026quot;operator.reportValidationErrors\u0026quot; at \u0026lt;fail .validationErro...\u0026gt;: error calling fail: The string property externalOperatorKey must be specified.  Example:\nexternalOperatorKey: QmFnIEF0dHJpYnV0ZXMKICAgIGZyaWVuZGx5TmFtZTogd2VibG9naWMtb3B ...  Debugging options remoteDebugNodePortEnabled Specifies whether or not the operator will start a Java remote debug server on the provided port and suspend execution until a remote debugger has attached.\nDefaults to false.\nExample:\nremoteDebugNodePortEnabled: true  internalDebugHttpPort Specifies the port number inside the Kubernetes cluster for the operator\u0026rsquo;s Java remote debug server.\nThis parameter is required if remoteDebugNodePortEnabled is true. Otherwise, it is ignored.\nDefaults to 30999.\nExample:\ninternalDebugHttpPort: 30888  externalDebugHttpPort Specifies the node port that should be allocated for the Kubernetes cluster for the operator\u0026rsquo;s Java remote debug server.\nThis parameter is required if remoteDebugNodePortEnabled is true. Otherwise, it is ignored.\nDefaults to 30999.\nExample:\nexternalDebugHttpPort: 30777  Common mistakes and solutions Installing the operator a second time into the same namespace A new FAILED Helm release is created.\n$ helm install --no-hooks --name op2 --namespace myuser-op-ns --values custom-values.yaml kubernetes/charts/weblogic-operator Error: release op2 failed: secrets \u0026quot;weblogic-operator-secrets\u0026quot; already exists  Both the previous and new release own the resources created by the previous operator.\n You can\u0026rsquo;t modify it to change the namespace (because helm upgrade doesn\u0026rsquo;t let you change the namespace). You can\u0026rsquo;t fix it by deleting this release because it removes your previous operator\u0026rsquo;s resources. You can\u0026rsquo;t fix it by rolling back this release because it is not in the DEPLOYED state. You can\u0026rsquo;t fix it by deleting the previous release because it removes the operator\u0026rsquo;s resources too. All you can do is delete both operator releases and reinstall the original operator. See https://github.com/helm/helm/issues/2349  Installing an operator and telling it to manage a domain namespace that another operator is already managing A new FAILED Helm release is created.\n$ helm install --no-hooks --name op2 --namespace myuser-op2-ns --values custom-values.yaml kubernetes/charts/weblogic-operator Error: release op2 failed: rolebindings.rbac.authorization.k8s.io \u0026quot;weblogic-operator-rolebinding-namespace\u0026quot; already exists  To recover:\n helm delete --purge the failed release.  NOTE: This deletes the role binding in the domain namespace that was created by the first operator release to give the operator access to the domain namespace.  helm upgrade \u0026lt;old op release\u0026gt; kubernetes/charts/weblogic-operator --values \u0026lt;old op custom-values.yaml\u0026gt;  This recreates the role binding. There might be intermittent failures in the operator for the period of time when the role binding was deleted.   Upgrading an operator and telling it to manage a domain namespace that another operator is already managing The helm upgrade succeeds, and silently adopts the resources the first operator\u0026rsquo;s Helm chart created in the domain namespace (for example, rolebinding), and, if you also told it to stop managing another domain namespace, it abandons the role binding it created in that namespace.\nFor example, if you delete this release, then the first operator will get messed up because the role binding it needs is gone. The big problem is that you don\u0026rsquo;t get a warning, so you don\u0026rsquo;t know that there\u0026rsquo;s a problem to fix.\n This can be fixed by just upgrading the Helm release. This may also be fixed by rolling back the Helm release.  Installing an operator and telling it to use the same external REST port number as another operator A new FAILED Helm release is created.\n$ helm install --no-hooks --name op2 --namespace myuser-op2-ns --values o.yaml kubernetes/charts/weblogic-operator Error: release op2 failed: Service \u0026quot;external-weblogic-operator-svc\u0026quot; is invalid: spec.ports[0].nodePort: Invalid value: 31023: provided port is already allocated  To recover:\n $ helm delete --purge the failed release. Change the port number and helm install the second operator again.  Upgrading an operator and telling it to use the same external REST port number as another operator The helm upgrade fails and moves the release to the FAILED state.\n$ helm upgrade --no-hooks --values o23.yaml op2 kubernetes/charts/weblogic-operator --wait Error: UPGRADE FAILED: Service \u0026quot;external-weblogic-operator-svc\u0026quot; is invalid: spec.ports[0].nodePort: Invalid value: 31023: provided port is already allocated   You can fix this by upgrading the Helm release (to fix the port number). You can also fix this by rolling back the Helm release.  Installing an operator and telling it to use a service account that doesn\u0026rsquo;t exist The helm install eventually times out and creates a failed release.\n$ helm install kubernetes/charts/weblogic-operator --name op2 --namespace myuser-op2-ns --values o24.yaml --wait --no-hooks Error: release op2 failed: timed out waiting for the condition kubectl logs -n kube-system tiller-deploy-f9b8476d-mht6v ... [kube] 2018/12/06 21:16:54 Deployment is not ready: myuser-op2-ns/weblogic-operator ...  To recover:\n helm delete --purge the failed release. Create the service account. helm install again.  Upgrading an operator and telling it to use a service account that doesn\u0026rsquo;t exist The helm upgrade succeeds and changes the service account on the existing operator deployment, but the existing deployment\u0026rsquo;s pod doesn\u0026rsquo;t get modified, so it keeps running. If the pod is deleted, the deployment creates another one using the OLD service account. However, there\u0026rsquo;s an error in the deployment\u0026rsquo;s status section saying that the service account doesn\u0026rsquo;t exist.\nlastTransitionTime: 2018-12-06T23:19:26Z lastUpdateTime: 2018-12-06T23:19:26Z message: 'pods \u0026quot;weblogic-operator-88bbb5896-\u0026quot; is forbidden: error looking up service account myuser-op2-ns/no-such-sa2: serviceaccount \u0026quot;no-such-sa2\u0026quot; not found' reason: FailedCreate status: \u0026quot;True\u0026quot; type: ReplicaFailure  To recover:\n Create the service account. helm rollback helm upgrade again.  Installing an operator and telling it to manage a domain namespace that doesn\u0026rsquo;t exist A new FAILED Helm release is created.\n$ helm install --no-hooks --name op2 --namespace myuser-op2-ns --values o.yaml kubernetes/charts/weblogic-operator Error: release op2 failed: namespaces \u0026quot;myuser-d2-ns\u0026quot; not found  To recover:\n helm delete --purge the failed release. Create the domain namespace. helm install again.  Upgrading an operator and telling it to manage a domain namespace that doesn\u0026rsquo;t exist The helm upgrade fails and moves the release to the FAILED state.\n$ helm upgrade myuser-op kubernetes/charts/weblogic-operator --values o.yaml --no-hooks Error: UPGRADE FAILED: failed to create resource: namespaces \u0026quot;myuser-d2-ns\u0026quot; not found  To recover:\n helm rollback Create the domain namespace. helm upgrade again.  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-domains/domain-lifecycle/startup/",
	"title": "Startup and shutdown",
	"tags": [],
	"description": "There are properties on the domain resource that specify which servers should be running and which servers should be restarted. To start, stop, or restart servers, modify these properties on the domain resource.",
	"content": " Contents  Starting and stopping servers  Common starting and stopping scenarios  Restarting servers  Rolling restarts Common restarting scenarios   There are properties on the domain resource that specify which servers should be running and which servers should be restarted. To start, stop, or restart servers, modify these properties on the domain resource (for example, by using kubectl or the Kubernetes REST API). The operator will notice the changes and apply them.\nStarting and stopping servers The serverStartPolicy property on the domain resource controls which servers should be running. The operator runtime monitors this property and creates or deletes the corresponding server pods.\nserverStartPolicy rules You can specify the serverStartPolicy property at the domain, cluster, and server levels. Each level supports a different set of values.\nAvailable serverStartPolicy values    Level Default Value Supported Values     Domain IF_NEEDED IF_NEEDED, ADMIN_ONLY, NEVER   Cluster IF_NEEDED IF_NEEDED, NEVER   Server IF_NEEDED IF_NEEDED, ALWAYS, NEVER    Administration Server start and stop rules    Domain Admin Server Started / Stopped     NEVER any value Stopped   ADMIN_ONLY, IF_NEEDED NEVER Stopped   ADMIN_ONLY, IF_NEEDED IF_NEEDED, ALWAYS Started    Standalone Managed Server start and stop rules    Domain Standalone Server Started / Stopped     ADMIN_ONLY, NEVER any value Stopped   IF_NEEDED NEVER Stopped   IF_NEEDED IF_NEEDED, ALWAYS Started    Clustered Managed Server start and stop rules    Domain Cluster Clustered Server Started / Stopped     ADMIN_ONLY, NEVER any value any value Stopped   IF_NEEDED NEVER any value Stopped   IF_NEEDED IF_NEEDED NEVER Stopped   IF_NEEDED IF_NEEDED ALWAYS Started   IF_NEEDED IF_NEEDED IF_NEEDED Started if needed to get to the cluster\u0026rsquo;s replicas count    Servers configured as ALWAYS count toward the cluster\u0026rsquo;s replicas count.\n If more servers are configured as ALWAYS than the cluster\u0026rsquo;s replicas count, they will all be started and the replicas count will be ignored.\n Common starting and stopping scenarios Normal running state Normally, the Administration Server, all of the standalone Managed Servers, and enough Managed Servers in each cluster to satisfy its replicas count, should be started. In this case, the domain resource does not need to specify serverStartPolicy, or list any clusters or servers, but it does need to specify a replicas count.\nFor example:\n domain: spec: image: ... replicas: 10  Shut down all the servers Sometimes you need to completely shut down the domain (for example, take it out of service).\n domain: spec: serverStartPolicy: \u0026quot;NEVER\u0026quot; ...  Only start the Administration Server Sometimes you want to start the Administration Server only, that is, take the domain out of service but leave the Administration Server running so that you can administer the domain.\n domain: spec: serverStartPolicy: \u0026quot;ADMIN_ONLY\u0026quot; ...  Shut down a cluster To shut down a cluster (for example, take it out of service), add it to the domain resource and set its serverStartPolicy to NEVER.\n domain: spec: clusters: - clusterName: \u0026quot;cluster1\u0026quot; serverStartPolicy: \u0026quot;NEVER\u0026quot; ...  Shut down a specific standalone server To shut down a specific standalone server, add it to the domain resource and set its serverStartPolicy to NEVER.\n domain: spec: managedServers: - serverName: \u0026quot;server1\u0026quot; serverStartPolicy: \u0026quot;NEVER\u0026quot; ...  Force a specific clustered Managed Server to start Normally, all of the Managed Servers in a cluster are identical and it doesn\u0026rsquo;t matter which ones are running as long as the operator starts enough of them to get to the cluster\u0026rsquo;s replicas count. However, sometimes some of the Managed Servers are different (for example, support some extra services that the other servers in the cluster use) and need to always be started.\nThis is done by adding the server to the domain resource and setting its serverStartPolicy to ALWAYS.\n domain: spec: managedServers: - serverName: \u0026quot;cluster1_server1\u0026quot; serverStartPolicy: \u0026quot;ALWAYS\u0026quot; ...  The server will count toward the cluster\u0026rsquo;s replicas count. Also, if you configure more than the replicas servers count to ALWAYS, they will all be started, even though the replicas count will be exceeded.\n Restarting servers The operator runtime automatically recreates (restarts) server pods when properties on the domain resource that affect server pods change (such as image, volumes, and env). The restartVersion property on the domain resource lets you force the operator to restart a set of server pods.\nThe operator runtime does rolling restarts of clustered servers so that service is maintained.\nProperties that cause servers to be restarted The operator will restart servers when any of the follow properties on the domain resource that affect the server are changed:\n containerSecurityContext domainHome domainHomeInImage env image imagePullPolicy imagePullSecrets includeServerOutInPodLog logHomeEnabled logHome livenessProbe nodeSelector podSecurityContext readinessProbe resources restartVersion serverStartState volumes volumeMounts  If the only change detected is the addition or modification of a domain-specified label or annotation, the operator will patch the server\u0026rsquo;s pod rather than restarting it. Removing a label or annotation from the domain resource will cause neither a restart nor a patch. It is possible to force a restart to remove such a label or annotation by modifying the restartVersion.\n Rolling restarts Clustered servers that need to be restarted are gradually restarted (for example, rolling restarted) so that the cluster is not taken out of service and in-flight work can be migrated to other servers in the cluster.\nThe maxUnavailable property on the domain resource determines how many of the cluster\u0026rsquo;s servers may be taken out of service at a time when doing a rolling restart. It can be specified at the domain and cluster levels and defaults to 1 (that is, by default, clustered servers are restarted one at a time).\nWhen using in-memory session replication, Oracle WebLogic Server employs a primary-secondary session replication model to provide high availability of application session state (that is, HTTP and EJB sessions). The primary server creates a primary session state on the server to which the client first connects, and a secondary replica on another WebLogic Server instance in the cluster. Specifying a maxUnavailable property value of 1 protects against inadvertent session state loss which could occur if both the primary and secondary servers are shut down at the same time during the rolling restart process.\nUsing restartVersion to force the operator to restart servers The restartVersion property lets you force the operator to restart servers.\nIt\u0026rsquo;s basically a user-specified string that gets added to new server pods (as a label) so that the operator can tell which servers need to be restarted. If the value is different, then the server pod is old and needs to be restarted. If the value matches, then the server pod has already been restarted.\nEach time you want to restart some servers, you need to set restartVersion to a different string (the particular value doesn\u0026rsquo;t matter).\nThe operator will notice the new value and restart the affected servers (using the same mechanisms as when other properties that affect the server pods are changed, including doing rolling restarts of clustered servers).\nThe restartVersion property can be specified at the domain, cluster, and server levels. A server will be restarted if any of these three values change.\nThe servers will also be restarted if restartVersion is removed from the domain resource (for example, if you had previously specified a value to cause a restart, then you remove that value after the previous restart has completed).\n Common restarting scenarios Restart all the servers in the domain Set restartVersion at the domain level to a new value.\n domain: spec: restartVersion: \u0026quot;domainV1\u0026quot; ...  Restart all the servers in the cluster Set restartVersion at the cluster level to a new value.\n domain: spec: clusters: - clusterName : \u0026quot;cluster1\u0026quot; restartVersion: \u0026quot;cluster1V1\u0026quot; maxUnavailable: 2 ...  Restart the Administration Server Set restartVersion at the adminServer level to a new value.\n domain: spec: adminServer: restartVersion: \u0026quot;adminV1\u0026quot; ...  Restart a standalone or clustered Managed Server Set restartVersion at the managedServer level to a new value.\n domain: spec: managedServers: - serverName: \u0026quot;standalone_server1\u0026quot; restartVersion: \u0026quot;v1\u0026quot; - serverName: \u0026quot;cluster1_server1\u0026quot; restartVersion: \u0026quot;v1\u0026quot; ...  Full domain restarts To do a full domain restart, first shut down all of the domain\u0026rsquo;s servers (Administration and Managed Servers), taking the domain out of service, then restart them. Unlike rolling restarts, the operator cannot detect and initiate a full domain restart; you must always manually initiate it.\nTo manually initiate a full domain restart:\n Change the domain level serverStartPolicy on the domain resource to NEVER.\n domain: spec: serverStartPolicy: \u0026quot;NEVER\u0026quot; ...  Wait for the operator to stop ALL the servers for that domain.\n To restart the domain, set the domain level serverStartPolicy back to IF_NEEDED. Alternatively, you do not have to specify the serverStartPolicy as the default value is IF_NEEDED.\n domain: spec: serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; ...  The operator will restart all the servers in the domain.\n  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-operators/installation/",
	"title": "Install the operator",
	"tags": [],
	"description": "",
	"content": " The operator uses Helm to create and deploy the necessary resources and then run the operator in a Kubernetes cluster.\nInstall the operator Helm chart Use the helm install command to install the operator Helm chart. As part of this, you must specify a \u0026ldquo;release\u0026rdquo; name for the operator.\nYou can override default configuration values in the operator Helm chart by doing one of the following:\n Creating a custom YAML file containing the values to be overridden, and specifying the --value option on the Helm command line. Overriding individual values directly on the Helm command line, using the --set option.  You supply the –namespace argument from the helm install command line to specify the namespace in which the operator should be installed. If not specified, then it defaults to default. If the namespace does not already exist, then Helm will automatically create it (and create a default service account in the new namespace), but will not remove it when the release is deleted. If the namespace already exists, then Helm will re-use it. These are standard Helm behaviors.\nSimilarly, you may override the default serviceAccount configuration value to specify which service account in the operator\u0026rsquo;s namespace, the operator should use. If not specified, then it defaults to default (for example, the namespace\u0026rsquo;s default service account). If you want to use a different service account, then you must create the operator\u0026rsquo;s namespace and the service account before installing the operator Helm chart.\nFor example:\n$ helm install kubernetes/charts/weblogic-operator \\ --name weblogic-operator --namespace weblogic-operator-namespace \\ --values custom-values.yaml --wait  or:\n$ helm install kubernetes/charts/weblogic-operator \\ --name weblogic-operator --namespace weblogic-operator-namespace \\ --set \u0026quot;javaLoggingLevel=FINE\u0026quot; --wait  This creates a Helm release, named weblogic-operator in the weblogic-operator-namespace namespace, and configures a deployment and supporting resources for the operator.\nIf weblogic-operator-namespace exists, then it will be used. If it does not exist, then Helm will create it.\nYou can verify the operator installation by examining the output from the helm install command.\nWhen the operator image is stored in a private registry, see WebLogic operator image pull secret for more information on specifying the registry credentials.\n Alternatively, install the operator Helm chart from GitHub chart repository Add this repository to the Helm installation:\n$ helm repo add weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts  Verify that the repository was added correctly:\n$ helm repo list NAME URL weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts  Update with the latest information about charts from the chart repositories:\n$ helm repo update  Install the operator from the repository:\n$ helm install weblogic-operator/weblogic-operator --name weblogic-operator  Removing the operator The helm delete command is used to remove an operator release and its associated resources from the Kubernetes cluster. The release name used with the helm delete command is the same release name used with the helm install command (see Install the Helm chart). For example:\n$ helm delete --purge weblogic-operator  If the operator\u0026rsquo;s namespace did not exist before the Helm chart was installed, then Helm will create it, however, helm delete will not remove it.\n After removing the operator deployment, you should also remove the domain custom resource definition:\n$ kubectl delete customresourcedefinition domains.weblogic.oracle  Note that the domain custom resource definition is shared if there are multiple operators in the same cluster.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-domains/domain-in-image/base-images/",
	"title": "Base images",
	"tags": [],
	"description": "Creating or obtaining WebLogic Docker images.",
	"content": " Contents  Creating or obtaining WebLogic Docker images Setting up secrets to access the Docker store Obtaining standard images from the Docker store Creating a custom image with patches applied Creating a custom image with your domain inside the image  Creating or obtaining WebLogic Docker images You will need Docker images to run your WebLogic domains in Kubernetes. There are two main options available:\n Use a Docker image which contains the WebLogic Server binaries but not the domain, or Use a Docker image which contains both the WebLogic Server binaries and the domain directory.  If you want to use the first option, you will need to obtain the standard WebLogic Server image from the Docker store, see here; this image already contains the mandatory patches applied as described in this section. If you want to use additional patches, you can customize that process to include additional patches.\nIf you want to use the second option, which includes the domain directory inside the Docker image, then you will need to build your own Docker images as described in this section.\nSetting up secrets to access the Docker store This version of the operator requires WebLogic Server 12.2.1.3.0 plus patch 29135930; the standard image store/oracle/weblogic:12.2.1.3 already includes this patch pre-applied.\n In order for Kubernetes to obtain the WebLogic Server Docker image from the Docker store, which requires authentication, a Kubernetes secret containing the registry credentials must be created. To create a secret with Docker store credentials, issue the following command:\n$ kubectl create secret docker-registry SECRET_NAME -n NAMESPACE --docker-server=index.docker.io/v1/ --docker-username=YOUR_USERNAME --docker-password=YOUR_PASSWORD --docker-email=YOUR_EMAIL  In this command, replace the uppercase items with the appropriate values. The SECRET_NAME will be needed in later parameter files. The NAMESPACE must match the namespace where the first domain will be deployed, otherwise Kubernetes will not be able to find it.\nIt may be preferable to manually pull the image in advance, on each Kubernetes worker node, as described in the next section. If you choose this approach, you do not require the Kubernetes secret.\nObtaining standard images from the Docker store Oracle provides a WebLogic Server 12.2.1.3.0 Docker image in the Docker store which already has the necessary patches applied. To obtain that image, you must have a Docker store account, log on to the Docker store, navigate to that image and click the \u0026ldquo;Proceed to Checkout\u0026rdquo; button which will prompt you to read and accept the license agreement for the image. After you have accepted the license agreement, you will be able to pull the image using your Docker store credentials.\nFirst, you will need to log in to the Docker store:\n$ docker login  Then, you can pull the image with this command:\n$ docker pull store/oracle/weblogic:12.2.1.3  If desired, you can:\n Check the WLS version with docker run store/oracle/weblogic:12.2.1.3 sh -c 'source $ORACLE_HOME/wlserver/server/bin/setWLSEnv.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; java weblogic.version'\n Check the WLS patches with docker run store/oracle/weblogic:12.2.1.3 sh -c '$ORACLE_HOME/OPatch/opatch lspatches'\n  Additional information about using this image is available on the Docker store.\nCreating a custom image with patches applied The Oracle WebLogic Server Kubernetes Operator requires patch 29135930. This patch does have some prerequisite patches that will also need to be applied. The standard image store/oracle/weblogic:12.2.1.3 already has all of these patches applied.\nThis sample in the Oracle GitHub Docker images repository demonstrates how to create an image with arbitrary patches, starting from an unpatched WebLogic Server 12.2.1.3 image (not the standard store/oracle/weblogic:12.2.1.3 pre-patched image). You can customize that sample to apply a different set of patches, if you require additional patches or PSUs.\nWhen using that sample, you will need to download the required patch and also some prerequisite patches. To find the correct version of the patch, you should use the \u0026ldquo;Product or Family (Advanced)\u0026rdquo; option, then choose \u0026ldquo;Oracle WebLogic Server\u0026rdquo; as the product, and set the release to \u0026ldquo;Oracle WebLogic Server 12.2.1.3.181016\u0026rdquo; as shown in the image below:\nThe Dockerfile in that sample lists the base image as follows:\nFROM oracle/weblogic:12.2.1.3-developer  You can change this to use the standard WebLogic Server image you downloaded from the Docker store by updating the FROM statement as follows:\nFROM store/oracle/weblogic:12.2.1.3  After running docker build as described in the sample, you will have created a Docker image with the necessary patches to run WebLogic 12.2.1.3 in Kubernetes using the operator.\nCreating a custom image with your domain inside the image You can also create a Docker image with the WebLogic domain inside the image. Samples are provided that demonstrate how to create the image using:\n WLST to define the domain, or WebLogic Deploy Tooling to define the domain.  In these samples, you will see a reference to a \u0026ldquo;base\u0026rdquo; or FROM image. You should use an image with the mandatory patches installed as this base image. This image could be either the standard store/oracle/weblogic:12.2.1.3 pre-patched image or an image you created using the instructions above.\nOracle recommends that Docker images containing WebLogic domains be kept in a private repository.\n "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-domains/choosing-a-model/",
	"title": "Choose a model",
	"tags": [],
	"description": "",
	"content": "When using the operator, a WebLogic domain can be located either in a persistent volume (PV) or in a Docker image. There are advantages to both approaches, and there are sometimes technical limitations of various cloud providers that may make one approach better suited to your needs. You can also mix and match on a domain-by-domain basis.\n   Domain on a persistent volume Domain in a Docker image     Let\u0026rsquo;s you use the same standard read-only Docker image for every server in every domain. Requires a different image for each domain, but all servers in that domain use the same image.   No state is kept in Docker images making them completely throw away (cattle not pets). Runtime state should not be kept in the images, but applications and configuration are.   The domain is long-lived, so you can mutate the configuration or deploy new applications using standard methods (Administration Console, WLST, and such). If you want to mutate the domain configuration or deploy application updates, you must create a new image.   Logs are automatically placed on persistent storage. Logs are kept in the images, and sent to the pod\u0026rsquo;s log (stdout) unless you manually place them on persistent storage.   Patches can be applied by simply changing the image and rolling the domain. To apply patches, you must create a new domain-specific image and then roll the domain.   Many cloud providers do not provide persistent volumes that are shared across availability zones, so you may not be able to use a single persistent volume. You may need to use some kind of volume replication technology or a clustered file system. You do not have to worry about volume replication across availability zones since each pod has its own copy of the domain. WebLogic replication will handle propagation of any online configuration changes.   CI/CD pipelines may be more complicated because you would probably need to run WLST against the live domain directory to effect changes. CI/CD pipelines are simpler because you can create the whole domain in the image and don\u0026rsquo;t have to worry about a persistent copy of the domain.   There are less images to manage and store, which could provide significant storage and network savings. There are more images to manage and store in this approach.   You may be able to use standard Oracle-provided images or, at least, a very small number of self-built images, for example, with patches installed. You may need to do more work to set up processes to build and maintain your images.    "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-domains/prepare/",
	"title": "Prepare to run a domain",
	"tags": [],
	"description": "",
	"content": "Perform these steps to prepare your Kubernetes cluster to run a WebLogic domain:\n Create the domain namespace(s). One or more domains can share a namespace. A single instance of the operator can manage multiple namespaces.\n$ kubectl create namespace domain-namespace-1  Replace domain-namespace-1 with name you want to use. The name must follow standard Kubernetes naming conventions, that is, lower case, numbers, and hyphens.\n Create a Kubernetes secret containing the Administration Server boot credentials. You can do this manually or by using the provided sample. To create the secret manually, use this command:\n$ kubectl -n domain-namespace-1 \\ create secret generic domain1-weblogic-credentials \\ --from-literal=username=weblogic \\ --from-literal=password=welcome1   Replace domain-namespace-1 with the namespace that the domain will be in. Replace domain1-weblogic-credentials with the name of the secret. The operator expects the secret name to be the domainUID followed by the literal string -weblogic-credentials and many of the samples assume this name. Replace the string weblogic in the third line with the user name for the administrative user. Replace the string welcome1 in the fourth line with the password.  Optionally, create a PV \u0026amp; persistent volume claim (PVC) which can hold the domain home, logs, and application binaries. Even if you put your domain in a Docker image, you may want to put the logs on a persistent volume so that they are available after the pods terminate. This may be instead of, or as well as, other approaches like streaming logs into Elasticsearch.\n Optionally, configure load balancer(s) to manage access to any WebLogic clusters.\n  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/overview/prepare/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "",
	"content": " Set up your Kubernetes cluster If you need help setting up a Kubernetes environment, check our cheat sheet.\nAfter creating Kubernetes clusters, you can optionally:\n Create load balancers to direct traffic to backend domains. Configure Kibana and Elasticsearch for your operator logs.  Load balance with an Ingress controller or a web server You can choose a load balancer provider for your WebLogic domains running in a Kubernetes cluster. Please refer to the WebLogic Operator Load Balancer Samples for information about the current capabilities and setup instructions for each of the supported load balancers.\nConfigure Kibana and Elasticsearch You can send the operator logs to Elasticsearch, to be displayed in Kibana. Use this sample script to configure Elasticsearch and Kibana deployments and services.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/introduction/introduction/",
	"title": "Get started",
	"tags": [],
	"description": "Learn about the Oracle WebLogic Server Kubernetes Operator, how it works and how to use it to manage WebLogic domains.",
	"content": " An operator is an application-specific controller that extends Kubernetes to create, configure, and manage instances of complex applications. The Oracle WebLogic Server Kubernetes Operator follows the standard Kubernetes operator pattern, and simplifies the management and operation of WebLogic domains and deployments.\nYou can have one or more operators in your Kubernetes cluster that manage one or more WebLogic domains each. We provide a Helm chart to manage the installation and configuration of the operator. Detailed instructions are available here.\nPrerequisites  Kubernetes 1.10.11+, 1.11.5+, 1.12.3+, and 1.13.0+ (check with kubectl version). Flannel networking v0.9.1-amd64 (check with docker images | grep flannel). Docker 18.03.1.ce (check with docker version). Helm 2.8.2+ (check with helm version). Oracle WebLogic Server 12.2.1.3.0 with patch 29135930.  The existing WebLogic Docker image, store/oracle/weblogic:12.2.1.3, was updated on January 17, 2019, and has all the necessary patches applied. A docker pull is required if you pulled the image prior to that date. Check the WLS version with docker run store/oracle/weblogic:12.2.1.3 sh -c 'source $ORACLE_HOME/wlserver/server/bin/setWLSEnv.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; java weblogic.version'. Check the WLS patches with docker run store/oracle/weblogic:12.2.1.3 sh -c '$ORACLE_HOME/OPatch/opatch lspatches'.  You must have the cluster-admin role to install the operator.  OpenShift Operator 2.0.1+ is certified for use on OpenShift 3.11.43+, with Kubernetes 1.11.5+\nWhen using the operator in OpenShift, the anyuid security context constraint is required to ensure that WebLogic containers run with a UNIX UID that has the correct permissions on the domain filesystem.\nOperator Docker image You can find the operator image in Docker Hub.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/reference/javadoc/",
	"title": "Javadoc",
	"tags": [],
	"description": "Java API documentation.",
	"content": "View the Java API documentation here.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/quickstart/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Use this Quick Start guide to create a WebLogic deployment in a Kubernetes cluster with the Oracle WebLogic Kubernetes Operator. Please note that this walk-through is for demonstration purposes only, not for use in production. These instructions assume that you are already familiar with Kubernetes. If you need more detailed instructions, please refer to the User guide.\nImportant note for users of operator releases before 2.0   Click here to expand   If you have an older version of the operator installed on your cluster, for example, a 1.x version or one of the 2.0 release candidates, then you must remove it before installing this version. This includes the 2.0-rc1 version; it must be completely removed. You should remove the deployment (for example, kubectl delete deploy weblogic-operator -n your-namespace) and the custom resource definition (for example, kubectl delete crd domain). If you do not remove the custom resource definition you may see errors like this:\nError from server (BadRequest): error when creating \u0026quot;/scratch/output/uidomain/weblogic-domains/uidomain/domain.yaml\u0026quot;: the API version in the data (weblogic.oracle/v2) does not match the expected API version (weblogic.oracle/v1    \n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/quickstart/",
	"title": "Quick Start",
	"tags": [],
	"description": "",
	"content": " Quick Start The Quick Start guide provides a simple tutorial to help you get the operator up and running quickly.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/recent-changes/",
	"title": "Recent changes",
	"tags": [],
	"description": "",
	"content": "This document tracks recent changes to the operator, especially ones that introduce backward incompatibilities.\n   Date Version Introduces backward incompatibilities Change     April 4, 2019 v2.1 no Customers can add init and sidecar containers to generated pods.   March 4, 2019 v2.0.1 no OpenShift support is now certified. Many bug fixes, including fixes for configuration overrides, cluster services, and domain status processing.   January 24, 2019 v2.0 yes; not compatible with 1.x releases, but is compatible with 2.0-rc2. Final version numbers and documentation updates.   January 16, 2019 v2.0-rc2 yes Schema updates are completed, and various bugs fixed.   December 20, 2018 v2.0-rc1 yes Operator is now installed via Helm charts, replacing the earlier scripts. The operator now supports the domain home on persistent volume or in Docker image use cases, which required a redesign of the domain schema. You can override the domain configuration using configuration override templates. Now load balancers and Ingresses can be independently configured. You can direct WebLogic logs to a persistent volume or to the pod\u0026rsquo;s log. Added lifecycle support for servers and significantly enhanced configurability for generated pods. The final v2.0 release will be the initial release where the operator team intends to provide backward compatibility as part of future releases.   March 20, 2018 v1.1 yes Several files and input parameters have been renamed. This affects how operators and domains are created. It also changes generated Kubernetes artifacts, therefore customers must recreate their operators and domains.   April 4, 2018 v1.0 yes Many Kubernetes artifact names and labels have changed. Also, the names of generated YAML files for creating a domain\u0026rsquo;s PV and PVC have changed. Because of these changes, customers must recreate their operators and domains.   May 7, 2018  no Added support for dynamic clusters, the Apache HTTP Server, the Voyager Ingress Controller, and for PV in NFS storage for multi-node environments.    "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/cicd/why-layering-matters/",
	"title": "Why layering matters",
	"tags": [],
	"description": "Learn why Docker image layering affects CI/CD processes.",
	"content": " How does layering affect our CI/CD process? Now that we know more about layering, let’s talk about why it is important to our CI/CD process. Let\u0026rsquo;s consider the kinds of updates we might want to make to our domain:\nYou might want to update the domain by:\n Installing a patch on the operating system or a library. Updating the version of the JDK you are using. Picking up a new version of WebLogic Server. Installing patches on WebLogic Server. Updating the domain configuration, for example:  Adding or changing a resource like a data source or queue. Installing or updating applications. Changing various settings in the domain configuration.   If we just want to update the domain configuration itself, that is the top layer, then it is pretty easy. We can make the necessary changes and save a new version of that layer, and then roll the domain. We could also choose to just build another layer on top of the existing top layer that contains our delta. If the change is small, then we will just end up with another small layer, and as we have seen, the small layers are no problem.\nBut consider a more complicated scenario - let\u0026rsquo;s take updating the JDK as an example to understand the impact of layers. Say we want to update from JDK 8u201 to 8u202 as shown in the example above. If we took the \u0026ldquo;your first domain\u0026rdquo; image and updated the JDK, then we would end up with a new layer on top containing JDK 8u202. That other layer with JDK 8u201 is still there; even if we \u0026ldquo;delete\u0026rdquo; the directory, we don\u0026rsquo;t get that space back. So now our 1.5GB \u0026ldquo;image\u0026rdquo; has grown to 1.75GB. This is not ideal, and the more often we try to change lower layers, the worse it gets.\nYou might be asking, \u0026ldquo;Can\u0026rsquo;t we just swap out the JDK layer for a new one?\u0026rdquo; That is an excellent question, but the unfortunate reality today is that there is no reliable way to do that. There are various attempts to create a \u0026ldquo;rebasing\u0026rdquo; capability for Docker that would enable such an action, but some research will show you that they are mostly abandoned due to limited documentation of how the layering works at the level of detail needed to implement something like this.\nNext you might think, \u0026ldquo;Oh, that’s ok, we can just rebuild the layers above the JDK on top of this new layer.\u0026rdquo; That is very true, we can. But there is a big caveat here. When you create a WebLogic domain, a domain encryption key is created. This key is stored in the security/SerializedSystemIni.dat file in your domain and it is used to encrypt several other things in your domain configuration, like passwords, for example. Today (in WebLogic Server 12.2.1.3.0) there is no way to conveniently \u0026ldquo;extract\u0026rdquo; or \u0026ldquo;reuse\u0026rdquo; this encryption key. So what does this mean in practice?\n If you recreate the domain in your CI/CD process, even though you may end up with a domain that is for all intents and purposes identical to the previous domain, it will have a different encryption key.\n This means that technically it is a \u0026ldquo;different\u0026rdquo; domain. Does this matter? Maybe, maybe not. It depends. If you want to do a rolling restart of your domain, then yes, it matters. First of all, the \u0026ldquo;new\u0026rdquo; servers will fail to start because the operator will be trying to inject credentials to start the server which were encrypted with the \u0026ldquo;old\u0026rdquo; domain encryption key.\nBut even if this did not prevent them from starting, there would still be a problem.\nYou cannot have members of a domain with different encryption keys. If WebLogic saw a new member trying to join the domain with a different key, it would consider it to be an intruder and refuse to accept it into the domain. Client HTTP sessions would not work across the two different sets of servers, so clients could see errors and need to retry. Worse, if these two different sets of servers tried to access the same resources this could lead to data corruption.\nSo what can we do? Well, we could not roll the domain, but instead completely shut down the old version first, and then start up the new one. This way we avoid any issues with incompatibilities, but we do introduce a brief outage. This may be acceptable, or it may not.\nAnother option is to find a way to keep the \u0026ldquo;same\u0026rdquo; domain, that is, the same domain encryption key, so that we can still roll the domain and there will be no conflicts.\nMutating the domain configuration without losing the encryption keys If we want to make a change in a lower layer without losing our domain encryption keys, then we need to find a way to \u0026ldquo;save\u0026rdquo; the domain and then put it back into a new layer, later, on top of the other new (lower) layers, as depicted in the image below:\nThe process looks like this:\n From our existing image (left), we extract the domain into some kind of archive. Then we start with the new JDK image which was built on top of the same base image (or we build it ourselves, if needed). We build a new WebLogic layer (or grab the one that Oracle built for us) on top of this new JDK. Then we need to “restore” our domain from the archive into a new layer.  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/security/domain-security/",
	"title": "Domain security",
	"tags": [],
	"description": "WebLogic domain security and the WebLogic operator",
	"content": "  Docker image protection  WebLogic domain in Docker image protection\n Channels  WebLogic channels\n "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/security/domain-security/weblogic-channels/",
	"title": "Channels",
	"tags": [],
	"description": "WebLogic channels",
	"content": " WebLogic T3 channels Oracle recommends not to expose any administrative, RMI, or T3 channels outside the Kubernetes cluster unless absolutely necessary. If exposing any of these channels, limit access using controls like security lists or set up a Bastion to provide access.\n When accessing T3/RMI based channels, the preferred approach is to kubectl exec into the Kubernetes pod and then run wlst or set up Bastion access and then run wlst from the Bastion host to connect to the Kubernetes cluster.\nAlso, consider a private VPN if you need use cross-domain T3 access between clouds, data centers, and such.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-domains/domain-lifecycle/restarting/",
	"title": "Restarting",
	"tags": [],
	"description": "This document describes when to restart servers in the Oracle WebLogic Server in Kubernetes environment.",
	"content": " This document describes when to restart servers in the Oracle WebLogic Server in Kubernetes environment.\nOverview There are many situations where changes to the Oracle WebLogic Server in Kubernetes environment require that all the servers in a domain or cluster be restarted, for example, when applying a WebLogic Server patch or when upgrading an application.\nOne of the operator\u0026rsquo;s most important jobs is to start and stop WebLogic Servers by creating and deleting their corresponding Kubernetes pods. Sometimes, you need to make changes that make the pods obsolete, therefore the pods need to be deleted and recreated. Depending on the change, sometimes the pods can be gradually recreated, without taking the entire domain out of service (for example, rolling restarts) and sometimes all the pods need to be deleted then recreated, taking the entire domain out of service for a while (for example, full restarts).\nThe following types of server restarts are supported in Oracle WebLogic Server in Kubernetes:\n Rolling restarts - a coordinated and controlled shut down of all of the servers in a domain or cluster while ensuring that service to the end user is not interrupted.\n Operator initiated - where the WebLogic Kubernetes Operator can detect some types of changes and will automatically initiate rolling restarts of server pods in a WebLogic domain.\n Manually initiated - required when certain changes in the Oracle WebLogic Server in Kubernetes environment cannot be detected by the operator, so a rolling restart must be manually initiated.\n  Full domain restarts - the Administration Server and all the Managed Servers in a domain are shutdown, impacting service availability to the end user, and then restarted. Unlike a rolling restart, the operator cannot detect and initiate a full domain restart; it must always be manually initiated.\n  For detailed information on how to restart servers in a Oracle WebLogic Server in Kubernetes environment, see Starting, stopping, and restarting servers.\nCommon restart scenarios This document describes what actions you need to take to properly restart your servers for a number of common scenarios:\n Modifying the WebLogic configuration Changing the custom domain configuration overrides (also called situational configuration) Changing the WebLogic Server credentials (the user name and password) Changing properties on the domain resource that affect server pods (such as image, volumes, and env) Applying WebLogic Server patches Updating deployed applications for domain home in image  Use cases Modifying the WebLogic configuration Changes to the Oracle WebLogic Server configuration may require either a rolling or full domain restart depending on the domain home location and the type of configuration change.\n Domain home in image: For domain home in image, any changes (dynamic or non-dynamic) to the WebLogic configuration requires a full domain restart.\n If you create a new image with a new name, then you must avoid a rolling restart, which can cause unexpected behavior for the running domain due to configuration inconsistencies as seen by the various servers, by following the steps in Avoiding a rolling restart when changing image property on a domain resource. If you create a new image with the same name, then you must manually initiate a full domain restart. See Full domain restarts in Starting, stopping, and restarting servers.  Domain home on PV: For domain home on PV, the type of restart needed to apply the changes, depends on the nature of the WebLogic configuration change:\n Changes to parts of the WebLogic configuration that the operator introspects, require a full restart, even if the changes are dynamic. The following are the types of changes to the WebLogic Server configuration that the operator introspects:  Adding or removing a cluster, server, dynamic server, or network access point Changing a cluster, server, dynamic server, or network access point name Enabling or disabling the listen port, SSL port, or admin port Changing any port numbers Changing a network access point\u0026rsquo;s public address  Other dynamic WebLogic configuration changes do not require a restart. For example, a change to a server\u0026rsquo;s connection timeout property is dynamic and does not require a restart. Other non-dynamic WebLogic configuration changes require either a manually initiated rolling restart or a full domain restart, depending on the nature of the change. For example, a rolling restart is applicable when changing a WebLogic Server\u0026rsquo;s stuck thread timer interval property. See Restart all the servers in the domain in Starting, stopping, and restarting servers.   Changing the custom domain configuration overrides Any change to domain configuration overrides requires a full domain restart. This includes:\n Changing the domain resource\u0026rsquo;s configOverides to point to a different configuration map Changing the domain resource\u0026rsquo;s configOverridesSecrets to point to a different list of secrets Changing the contents of the configuration map referenced by configOverrides Changing the contents to any of the secrets referenced by configOverridesSecrets  Changing the WebLogic Server credentials A change to the WebLogic Server credentials (the user name and password), contained in the Kubernetes secret for the domain, requires a full domain restart. The Kubernetes secret can be updated directly or a new secret can be created and then referenced by the webLogicCredentialsSecret property in the domain resource.\nChanging properties on the domain resource that affect server pods The operator will initiate a rolling restart of the domain when you modify any of the domain resource properties that affect the server pods configuration, such as image, volumes, and env. For a complete list, see Properties that cause servers to be restarted in Starting, stopping, and restarting servers.\nYou can modify these properties using the kubectl command-line tool\u0026rsquo;s edit and patch commands or through the Kubernetes REST API.\nFor example, to edit the domain resource directly using the kubectl command-line tool:\nkubectl edit domain \u0026lt;domain name\u0026gt; -n \u0026lt;domain namespace\u0026gt;  The edit command opens a text editor which lets you edit the domain resource in place.\nTypically, it\u0026rsquo;s better to edit the domain resource directly; otherwise, if you scaled the domain, and you edit only the original domain.yaml file and reapply it, you could go back to your old replicas count.\n Applying WebLogic Server patches Oracle provides different types of patches for WebLogic Server, such as Patch Set Updates, Security Patch Updates, and One-Off patches. Information on whether a patch is rolling compatible or requires a manual full domain restart usually can be found in the patch\u0026rsquo;s documentation, such as the README file.\nWebLogic Server patches can be applied to either a domain home in image or a domain home on PV.\nWith rolling compatible patches:\n If you update the image property with a new image name, then the operator will initiate a rolling restart. If you keep the same image name, then you must manually initiate a rolling restart. See Restart all the servers in the domain in Starting, stopping, and restarting servers.  With patches that are not rolling compatible:\n If you keep the same image name, then you must manually initiate a full domain restart. See Full domain restarts in Starting, stopping, and restarting servers. If you update the image property with a new image name, then you must avoid the rolling restart by following the steps in Avoiding a rolling restart when changing image property on a domain resource.  Updating deployed applications for domain home in image Frequent updates of deployed applications using a continuous integration/continuous delivery (CI/CD) process is a very common use case. The process for applying an updated application is different for domain home in image than it is for domain home on PV. A rolling compatible application update is where some servers are running the old version and some are running the new version of the application during the rolling restart process. On the other hand, an application update that is not rolling compatible requires that all the servers in the domain be shutdown and restarted.\nIf the application update is rolling compatible:\n If you update the image property with a new image name, then the operator will initiate a rolling restart. If you keep the same image name, then you must manually initiate a rolling restart. See Restart all the servers in the domain in Starting, stopping, and restarting servers.  If the application update is not rolling compatible:\n If you keep the same image name, then you must manually initiate a full domain restart. See Full domain restarts in Starting, stopping, and restarting servers. If you update the image property with a new image name, then you must avoid the rolling restart by following the steps in Avoiding a rolling restart when changing image property on a domain resource.  Rolling out an updated domain home in image Follow these steps to create new rolling compatible image if you only need to patch your WebLogic Server domain or update application deployment files:\na. Select a different name for the new image.\nb. Using the same domain home-in-image Docker image as a base, create a new Docker image by copying (COPY command in a Dockerfile) the updated application deployment files or WebLogic Server patches into the Docker image during the Docker image build.\nThe key here is to make sure that you do not re-run WLST or WDT to create a new domain home even though it will have the same configuration. Creating a new domain will change the domain secret and you won\u0026rsquo;t be able to do a rolling restart.\n c. Deploy the new Docker image to your Docker repository with the new name.\nd. Update the image property of the domain resource specifying the new image name.\nFor example:\n ``` domain: spec: image: oracle/weblogic-updated:2.1 ```  e. The operator will now initiate a rolling restart, which will apply the updated image, for all the server pods in the domain.\nAvoiding a rolling restart when changing image property on a domain resource If you\u0026rsquo;ve created a new image that is not rolling compatible, and you\u0026rsquo;ve changed the image name, then:\n Bring the domain down (stopping all the server pods) by setting the serverStartPolicy to NEVER. See Shut down all the servers in Starting, stopping, and restarting servers.\n Update the image property with a new image name.\n Start up the domain (starting all the server pods) by setting the serverStartPolicy to IF_NEEDED.\n  Other considerations for restarting a domain  Consider the order of changes:\nIf you need to make multiple changes to your domain at the same time, you\u0026rsquo;ll want to be careful about the order in which you do your changes, so that servers aren\u0026rsquo;t restarted prematurely or restarted needlessly. For example, if you want to change the readiness probe\u0026rsquo;s tuning parameters and the Java options (both of which are rolling compatible), then you should update the domain resource once, changing both values, so that the operator rolling restarts the servers once. Or, if you want to change the readiness probe\u0026rsquo;s tuning parameters (which is rolling compatible) and change the domain customizations (which require a full restart), then you should do a full shutdown first, then make the changes, and then restart the servers.\nAlternatively, if you know that your set of changes are not rolling compatible, then you must avoiding a rolling restart by:\n Bringing the domain down (stopping all the server pods) by setting the serverStartPolicy to NEVER. See Shut down all the servers in Starting, stopping, and restarting servers.\n Make all your changes to the Oracle WebLogic Server in Kubernetes environment.\n Starting up the domain (starting all the server pods) by setting the serverStartPolicy to IF_NEEDED.\n  Changes that require domain knowledge.\nSometimes you need to make changes that require server restarts, yet the changes are not to the WebLogic configuration, the image, or the Kubernetes resources that register your domain with the operator. For example, your servers are caching information from an external database and you\u0026rsquo;ve modified the contents of the database.\nIn these cases, you must manually initiate a restart.\n  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/introduction/demo/",
	"title": "Demo",
	"tags": [],
	"description": "Watch a video demonstration of the WebLogic Server Kubernetes Operator.",
	"content": "This video provides a demonstration of the WebLogic Server Kubernetes Operator.\n   This video provides a demonstration of the WebLogic Server Kubernetes Operator.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/samples/simple/domains/domain-home-on-pv/",
	"title": "Domain home on a PV",
	"tags": [],
	"description": "Sample for creating a WebLogic domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated WebLogic domain.",
	"content": " The sample scripts demonstrate the creation of a WebLogic domain home on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain. Optionally, the scripts start up the domain, and WebLogic Server pods and services.\nPrerequisites Before you begin, read this document, Domain resource.\nThe following prerequisites must be handled prior to running the create domain script:\n Make sure the WebLogic operator is running. The operator requires WebLogic Server 12.2.1.3.0 with patch 29135930 applied. The existing WebLogic Docker image, store/oracle/weblogic:12.2.1.3, was updated on January 17, 2019, and has all the necessary patches applied; a docker pull is required if you pulled the image prior to that date. Refer to WebLogic Docker images for details on how to obtain or create the image. Create a Kubernetes namespace for the domain unless the intention is to use the default namespace. In the same Kubernetes namespace, create the Kubernetes persistent volume (PV) where the domain home will be hosted, and the Kubernetes persistent volume claim (PVC) for the domain. For samples to create a PV and PVC, see Create sample PV and PVC. By default, the create-domain.sh script creates a domain with the domainUID set to domain1 and expects the PVC domain1-weblogic-sample-pvc to be present. You can create domain1-weblogic-sample-pvc using create-pv-pvc.sh with an inputs file that has the domainUID set to domain1. Create the Kubernetes secrets username and password of the admin account in the same Kubernetes namespace as the domain.  Use the script to create a domain Make a copy of the create-domain-inputs.yaml file, and run the create script, pointing it at your inputs file and an output directory:\n$ ./create-domain.sh \\ -i create-domain-inputs.yaml \\ -o /path/to/output-directory  The script will perform the following steps:\n Create a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The pathname is /path/to/weblogic-operator-output-directory/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, its contents must be removed before using this script. Create a Kubernetes job that will start up a utility WebLogic Server container and run offline WLST scripts, or WebLogic Deploy Tool (WDT) scripts, to create the domain on the shared storage. Run and wait for the job to finish. Create a Kubernetes domain YAML file, domain.yaml, in the directory that is created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command:\n$ kubectl apply -f /path/to/output-directory/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml  Create a convenient utility script, delete-domain-job.yaml, to clean up the domain home created by the create script.\n  As a convenience, using the -e option, the script can optionally create the domain object, which in turn results in the creation of the corresponding WebLogic Server pods and services as well.\nThe usage of the create script is as follows:\n$ sh create-domain.sh -h usage: create-domain.sh -o dir -i file [-e] [-v] [-h] -i Parameter inputs file, must be specified. -o Output directory for the generated YAML files, must be specified. -e Also create the resources in the generated YAML files, optional. -v Validate the existence of persistentVolumeClaim, optional. -h Help  If you copy the sample scripts to a different location, make sure that you copy everything in the \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts directory together into the target directory, maintaining the original directory hierarchy.\nThe default domain created by the script has the following characteristics:\n An Administration Server named admin-server listening on port 7001. A dynamic cluster named cluster-1 of size 5. Two Managed Servers, named managed-server1 and managed-server2, listening on port 8001. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;. No applications deployed. No data sources or JMS resources. A T3 channel.  The domain creation inputs can be customized by editing create-domain-inputs.yaml.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Definition Default     adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. admin-server   clusterName Name of the WebLogic cluster instance to generate for the domain. cluster-1   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. It can also be set to the relative path wdt, and then the built-in WDT scripts will be used instead. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes config map, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes job to run the script (specified in the createDomainScriptName property) in a Kubernetes pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the WebLogic domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /shared/domains/domain1   domainPVMountPath Mount path of the domain persistent volume. /shared   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. domain1   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   image WebLogic Docker image. The operator requires WebLogic Server 12.2.1.3.0 with patch 29135930 applied. The existing WebLogic Docker image, store/oracle/weblogic:12.2.1.3, was updated on January 17, 2019, and has all the necessary patches applied; a docker pull is required if you pulled the image prior to that date. Refer to WebLogic Docker images for details on how to obtain or create the image. store/oracle/weblogic:12.2.1.3   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified    includeServerOutInPodLog Boolean indicating whether to include the server .out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to initially start for the domain. 2   javaOptions Java options for starting the Administration and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for domain log, server logs, server out, and Node Manager log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /shared/logs/domain1   managedServerNameBase Base string used to generate Managed Server names. managed-server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes namespace in which to create the domain. default   persistentVolumeClaimName Name of the persistent volume claim. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc domain1-weblogic-sample-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Servers will be started up. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would normally be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. domain1-weblogic-credentials   weblogicImagePullSecretName Name of the Kubernetes secret for the Docker Store, used to pull the WebLogic Server image. docker-store-secret    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create a WebLogic domain home and associated Kubernetes resources for a domain that only has one cluster. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain YAML file could also be modified to cover more use cases.\nVerify the results The create script will verify that the domain was created, and will report failure if there was any error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nNote that the example results below use the default Kubernetes namespace. If you are using a different namespace, you need to replace NAMESPACE in the example kubectl commands with the actual Kubernetes namespace.\nGenerated YAML files with the default inputs The content of the generated domain.yaml:\n# Copyright 2017, 2019, Oracle Corporation and/or its affiliates. All rights reserved. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026quot;weblogic.oracle/v2\u0026quot; kind: Domain metadata: name: domain1 namespace: default labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: # The WebLogic Domain Home domainHome: /shared/domains/domain1 # If the domain home is in the image domainHomeInImage: false # The WebLogic Server Docker image that the operator uses to start the domain image: \u0026quot;store/oracle/weblogic:12.2.1.3\u0026quot; # imagePullPolicy defaults to \u0026quot;Always\u0026quot; if image version is :latest imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: domain1-weblogic-credentials # Whether to include the server out file into the pod's stdout, default is true includeServerOutInPodLog: true # Whether to enable log home logHomeEnabled: true # The in-pod name location for domain log, server logs, server out, and Node Manager log files logHome: /shared/logs/domain1 # serverStartPolicy legal values are \u0026quot;NEVER\u0026quot;, \u0026quot;IF_NEEDED\u0026quot;, or \u0026quot;ADMIN_ONLY\u0026quot; # This determines which WebLogic Servers the operator will start up when it discovers this Domain # - \u0026quot;NEVER\u0026quot; will not start any server in the domain # - \u0026quot;ADMIN_ONLY\u0026quot; will start up only the administration server (no managed servers will be started) # - \u0026quot;IF_NEEDED\u0026quot; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom -Xms64m -Xmx256m \u0026quot; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: domain1-weblogic-sample-pvc volumeMounts: - mountPath: /shared name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026quot;RUNNING\u0026quot; or \u0026quot;ADMIN\u0026quot; # \u0026quot;RUNNING\u0026quot; means the listed server will be started up to \u0026quot;RUNNING\u0026quot; mode # \u0026quot;ADMIN\u0026quot; means the listed server will be start up to \u0026quot;ADMIN\u0026quot; mode serverStartState: \u0026quot;RUNNING\u0026quot; # adminService: # channels: # The Admin Server's NodePort # - channelName: default # nodePort: 30701 # Uncomment to export the T3Channel as a service # - channelName: T3Channel # clusters is used to configure the desired behavior for starting member servers of a cluster. # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: cluster-1 serverStartState: \u0026quot;RUNNING\u0026quot; replicas: 2 # The number of managed servers to start for unlisted clusters # replicas: 1  Verify the domain To confirm that the domain was created, use this command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE  Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\nHere is an example of the output of this command:\n$ kubectl describe domain domain1 Name: domain1 Namespace: default Labels: weblogic.domainUID=domain1 weblogic.resourceVersion=domain-v2 Annotations: kubectl.kubernetes.io/last-applied-configuration={\u0026quot;apiVersion\u0026quot;:\u0026quot;weblogic.oracle/v2\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Domain\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;labels\u0026quot;:{\u0026quot;weblogic.domainUID\u0026quot;:\u0026quot;domain1\u0026quot;,\u0026quot;weblogic.resourceVersion\u0026quot;:\u0026quot;do... API Version: weblogic.oracle/v2 Kind: Domain Metadata: Cluster Name: Creation Timestamp: 2019-01-10T14:50:52Z Generation: 1 Resource Version: 3700284 Self Link: /apis/weblogic.oracle/v2/namespaces/default/domains/domain1 UID: 2023ae0a-14e7-11e9-b751-fa163e855ac8 Spec: Admin Server: Server Pod: Annotations: Container Security Context: Env: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Clusters: Cluster Name: cluster-1 Cluster Service: Annotations: Labels: Replicas: 2 Server Pod: Annotations: Container Security Context: Env: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Domain Home: /shared/domains/domain1 Domain Home In Image: false Image: store/oracle/weblogic:12.2.1.3 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Log Home: /shared/logs/domain1 Log Home Enabled: true Managed Servers: Server Pod: Annotations: Container Security Context: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Xms64m -Xmx256m Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Mount Path: /shared Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: domain1-weblogic-sample-pvc Server Service: Annotations: Labels: Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: domain1-weblogic-credentials Status: Conditions: Last Transition Time: 2019-01-10T14:52:33.878Z Reason: ServersReady Status: True Type: Available Servers: Health: Activation Time: 2019-01-10T14:52:07.351Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: admin-server State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-01-10T14:53:30.352Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: managed-server1 State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-01-10T14:53:26.503Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: managed-server2 State: RUNNING Start Time: 2019-01-10T14:50:52.104Z Events: \u0026lt;none\u0026gt;  In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued very soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. The operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Use the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE  Here is an example of the output of this command:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE domain1-admin-server 1/1 Running 0 1m domain1-managed-server1 1/1 Running 0 8m domain1-managed-server2 1/1 Running 0 8m  Verify the services Use the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE  Here is an example of the output of this command:\n$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE domain1-admin-server ClusterIP 10.96.206.134 \u0026lt;none\u0026gt; 7001/TCP 23m domain1-admin-server-external NodePort 10.107.164.241 \u0026lt;none\u0026gt; 30012:30012/TCP 22m domain1-cluster-cluster-1 ClusterIP 10.109.133.168 \u0026lt;none\u0026gt; 8001/TCP 22m domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 22m domain1-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 22m  Delete the generated domain home Sometimes in production, but most likely in testing environments, you might want to remove the domain home that is generated using the create-domain.sh script. Do this by running the generated delete domain job script in the /path/to/weblogic-operator-output-directory/weblogic-domains/\u0026lt;domainUID\u0026gt; directory.\n$ kubectl create -f delete-domain-job.yaml  Troubleshooting  Message: status on iteration 20 of 20 pod domain1-create-weblogic-sample-domain-job-4qwt2 status is Pending The create domain job is not showing status completed after waiting 300 seconds.\nThe most likely cause is related to the value of persistentVolumeClaimName, defined in domain-home-on-pv/create-domain-inputs.yaml. To determine if this is the problem:\n Execute kubectl get all --all-namespaces to find the name of the create-weblogic-sample-domain-job. Execute kubectl describe pod \u0026lt;name-of-create-weblogic-sample-domain-job\u0026gt; to see if there is an event that has text similar to persistentvolumeclaim \u0026quot;domain1-weblogic-sample-pvc\u0026quot; not found. Find the name of the PVC that was created by executing create-pv-pvc.sh, using kubectl describe pvc. It is likely to be weblogic-sample-pvc. Change the value of persistentVolumeClaimName to match the name created when you executed create-pv-pvc.sh. Rerun the create-domain.sh script with the same arguments as you did before. Verify that the operator is deployed. Use the command:\nkubectl get all --all-namespaces  Look for lines similar to:\nweblogic-operator1 pod/weblogic-operator-  If you do not find something similar in the output, the WebLogic Operator for Kubernetes may not have been installed completely. Review the operator installation instructions.\n  Message: ERROR: Unable to create folder /shared/domains\nThe most common cause is a poor choice of value for weblogicDomainStoragePath in the input file used when you executed:\ncreate-pv-pvc.sh  You should delete the resources for your sample domain, correct the value in that file, and rerun the commands to create the PV/PVC and the credential before you attempt to rerun:\ncreate-domain.sh  A correct value for weblogicDomainStoragePath will meet the following requirements:\n Must be the name of a directory. The directory must be world writable.\n  Optionally, follow these steps to tighten permissions on the named directory after you run the sample the first time:\n Become the root user. ls -nd $value-of-weblogicDomainStoragePath Note the values of the third and fourth field of the output. chown $third-field:$fourth-field $value-of-weblogicDomainStoragePath chmod 755 $value-of-weblogicDomainStoragePath Return to your normal user ID.  Message: ERROR: The create domain job will not overwrite an existing domain. The domain folder /shared/domains/domain1 already exists\nYou will see this message if the directory domains/domain1 exists in the directory named as the value of weblogicDomainStoragePath in create-pv-pvc-inputs.yaml. For example, if the value of weblogicDomainStoragePath is /tmp/wls-op-4-k8s, you would need to remove (or move) /tmp/wls-op-4-k8s/domains/domain1.\n  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-operators/using-the-operator/",
	"title": "Use the operator",
	"tags": [],
	"description": "",
	"content": "  Use Helm  Useful Helm operations.\n The REST API  Use the operator\u0026#39;s REST services.\n "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-domains/domain-resource/",
	"title": "Domain resource",
	"tags": [],
	"description": "",
	"content": " Use this document to set up and configure your own domain resource which can be used to configure your WLS domain. Then, you can use the domain resource to start the Kubernetes artifacts of the corresponding domain.\nSwagger documentation is available here.\nPrerequisites The following prerequisites must be fulfilled before proceeding with the creation of the resource:\n Make sure the WebLogic operator is running. Create a Kubernetes namespace for the domain resource unless the intention is to use the default namespace. Create the Kubernetes secrets username and password of the administrative account in the same Kubernetes namespace as the domain resource.  YAML files Domain resources are defined using the domain resource YAML files. For each WLS domain you want to create and configure, you should create one domain resource YAML file and apply it. In the example referenced below, the sample script, create-domain.sh, generates a domain resource YAML file that you can use as a basis. Copy the file and override the default settings so that it matches all the WLS domain parameters that define your WLS domain.\nSee the WebLogic sample domain home on a persistent volume README.\nKubernetes resources After you have written your YAML files, you use them to create your WLS domain artifacts using the kubectl apply -f command.\n$ kubectl apply -f domain-resource.yaml  Verify the results To confirm that the domain resource was created, use this command:\n$ kubectl describe domain [domain name] -n [namespace]  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "Learn about the operator&#39;s design, architecture, important terms, and prerequisites.",
	"content": "  Get started  Learn about the Oracle WebLogic Server Kubernetes Operator, how it works and how to use it to manage WebLogic domains.\n Demo  Watch a video demonstration of the WebLogic Server Kubernetes Operator.\n Architecture  The operator consists of several parts: the operator runtime, the model for a Kubernetes custom resource definition (CRD), a Helm chart for installing the operator, a variety of sample shell scripts for preparing or packaging WebLogic domains for running in Kubernetes, and sample Helm charts or shell scripts for conditionally exposing WebLogic endpoints outside the Kubernetes cluster.\n Design philosophy  The Oracle WebLogic Server Kubernetes Operator (the “operator”) is designed to fulfill a similar role to that which a human operator would fill in a traditional data center deployment. It contains a set of useful built-in knowledge about how to perform various life cycle operations on a domain correctly.\n This guide provides detailed user information for the Oracle WebLogic Server Kubernetes Operator. It provides instructions on how to install the operator in your Kubernetes cluster and how to use it to manage WebLogic domains. If you are looking for information about how the operator is designed, implemented, built, and such, then you should refer to the Developer guide.\nImportant terms This documentation uses several important terms which are intended to have a specific meaning.\n   Term Definition     Cluster Because this term is ambiguous, it will be prefixed to indicate which type of cluster is meant. A WebLogic cluster is a group of Managed Servers that together host some application or component and which are able to share load and state between them. A Kubernetes cluster is a group of machines (“nodes”) that all host Kubernetes resources, like pods and services, and which appear to the external user as a single entity. If the term “cluster” is not prefixed, it should be assumed to mean a Kubernetes cluster.   Domain A WebLogic domain is a group of related applications and resources along with the configuration information necessary to run them.   Ingress A Kubernetes Ingress provides access to applications and services in a Kubernetes environment to external clients. An Ingress may also provide additional features like load balancing.   Namespace A Kubernetes namespace is a named entity that can be used to group together related objects, for example, pods and services.   Operator A Kubernetes operator is software that performs management of complex applications.   Pod A Kubernetes pod contains one or more containers and is the object that provides the execution environment for an instance of an application component, such as a web server or database.   Job A Kubernetes job is a type of controller that creates one or more pods that run to completion to complete a specific task.   Secret A Kubernetes secret is a named object that can store secret information like user names, passwords, X.509 certificates, or any other arbitrary data.   Service A Kubernetes service exposes application endpoints inside a pod to other pods, or outside the Kubernetes cluster. A service may also provide additional features like load balancing.    Additional reading Before using the operator, you might want to read the design philosophy to develop an understanding of the operator\u0026rsquo;s design, and the architectural overview to understand its architecture, including how WebLogic domains are deployed in Kubernetes using the operator. Also, worth reading are the details of the Kubernetes RBAC definitions required by the operator.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/overview/",
	"title": "Overview",
	"tags": [],
	"description": "",
	"content": "The information in the following sections is organized in the order that you would most likely need to use it. If you want to set up an operator and use it to create and manage WebLogic domains, you should follow the User Guide sections from top to bottom, and the necessary information will be presented in the correct order.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/overview/k8s-setup/",
	"title": "Set up Kubernetes",
	"tags": [],
	"description": "",
	"content": " Cheat sheet for setting up Kubernetes If you need some help setting up a Kubernetes environment to experiment with the operator, please read on! The supported environments are either an on-premises installation of Kubernetes, for example, on bare metal, or on a cloud provider like Oracle Cloud, Google, or Amazon. Cloud providers allow you to provision a managed Kubernetes environment from their management consoles. You could also set up Kubernetes manually using compute resources on a cloud. There are also a number of ways to run a Kubernetes single-node cluster that are suitable for development or testing purposes. Your options look like this:\n\u0026ldquo;Production\u0026rdquo; options:\n Set up your own Kubernetes environment on bare compute resources on a cloud. Use your cloud provider\u0026rsquo;s management console to provision a managed Kubernetes environment. Install Kubernetes on your own compute resources (for example, \u0026ldquo;real\u0026rdquo; computers, outside a cloud).  \u0026ldquo;Development/test\u0026rdquo; options:\n Install Docker for Mac and enable its embedded Kubernetes cluster (or register for the Docker for Windows beta and wait until Kubernetes is available there). Install Minikube on your Windows/Linux/Mac computer.  We have provided our hints and tips for several of these options in the sections below.\nSet up Kubernetes on bare compute resources in a cloud Follow the basic steps from the Terraform Kubernetes installer for Oracle Cloud Infrastructure.\nPrerequisites  Download and install Terraform (v0.10.3 or later). Download and install the OCI Terraform Provider (v2.0.0 or later). Create an Terraform configuration file at ~/.terraformrc that specifies the path to the OCI provider:\nproviders { oci = \u0026quot;\u0026lt;path_to_provider_binary\u0026gt;/terraform-provider-oci\u0026quot; }  Ensure that you have kubectl installed if you plan to interact with the cluster locally.\n  Quick Start  Do a git clone of the Terraform Kubernetes installer project:\ngit clone https://github.com/oracle/terraform-kubernetes-installer.git  Initialize your project:\ncd terraform-kubernetes-installer terraform init  Copy the example terraform.tvfars:\ncp terraform.example.tfvars terraform.tfvars  Edit the terraform.tvfars file to include values for your tenancy, user, and compartment. Optionally, edit the variables to change the Shape of the VMs for your Kubernetes master and workers, and your etcd cluster. For example:\n #give a label to your cluster to help identify it if you have multiple label_prefix=\u0026quot;weblogic-operator-1-\u0026quot; #identification/authorization info tenancy_ocid = \u0026quot;ocid1.tenancy....\u0026quot; compartment_ocid = \u0026quot;ocid1.compartment....\u0026quot; fingerprint = \u0026quot;...\u0026quot; private_key_path = \u0026quot;/Users/username/.oci/oci_api_key.pem\u0026quot; user_ocid = \u0026quot;ocid1.user...\u0026quot; #shapes for your VMs etcdShape = \u0026quot;VM.Standard1.2\u0026quot; k8sMasterShape = \u0026quot;VM.Standard1.8\u0026quot; k8sWorkerShape = \u0026quot;VM.Standard1.8\u0026quot; k8sMasterAd1Count = \u0026quot;1\u0026quot; k8sWorkerAd1Count = \u0026quot;2\u0026quot; #this ingress is set to wide-open for testing **not secure** etcd_ssh_ingress = \u0026quot;0.0.0.0/0\u0026quot; master_ssh_ingress = \u0026quot;0.0.0.0/0\u0026quot; worker_ssh_ingress = \u0026quot;0.0.0.0/0\u0026quot; master_https_ingress = \u0026quot;0.0.0.0/0\u0026quot; worker_nodeport_ingress = \u0026quot;0.0.0.0/0\u0026quot; #create iscsi volumes to store your etcd and /var/lib/docker info worker_iscsi_volume_create = true worker_iscsi_volume_size = 100 etcd_iscsi_volume_create = true etcd_iscsi_volume_size = 50  Test and apply your changes:\nterraform plan terraform apply  Test your cluster using the built-in script scripts/cluster-check.sh:\nscripts/cluster-check.sh  Output the SSH private key:\n# output the ssh private key for use later $ rm -f generated/instances_id_rsa \u0026amp;\u0026amp; terraform output ssh_private_key \u0026gt; generated/instances_id_rsa \u0026amp;\u0026amp; chmod 600 generated/instances_id_rsa  If you need shared storage between your Kubernetes worker nodes, enable and configure NFS:\n  In the current GA version, the OCI Container Engine for Kubernetes supports network block storage that can be shared across nodes with access permission RWOnce (meaning that only one can write, others can read only). If you choose to place your domain in a persistent volume, you must use a shared file system to store the WebLogic domain configuration, which MUST be accessible from all the pods across the nodes. Oracle recommends that you use the Oracle Cloud Infrastructure File Storage Service (or equivalent on other cloud providers). Alternatively, you may install an NFS server on one node and share the file system across all the nodes.\nCurrently, we recommend that you use NFS version 3.0 for running WebLogic Server on OCI Container Engine for Kubernetes. During certification, we found that when using NFS 4.0, the servers in the WebLogic domain went into a failed state intermittently. Because multiple threads use NFS (default store, diagnostics store, Node Manager, logging, and domain_home), there are issues when accessing the file store. These issues are removed by changing the NFS to version 3.0.\n $ terraform output worker_public_ips IP1, IP2 $ terraform output worker_private_ips PRIVATE_IP1, PRIVATE_IP2 $ ssh -i `pwd`/generated/instances_id_rsa opc@IP1 worker-1$ sudo su - worker-1# yum install -y nfs-utils worker-1# mkdir /scratch worker-1# echo \u0026quot;/scratch PRIVATE_IP2(rw)\u0026quot; \u0026gt;\u0026gt; /etc/exports worker-1# systemctl restart nfs worker-1# exit worker-1$ exit # configure worker-2 to mount the share from worker-1 $ ssh -i `pwd`/generated/instances_id_rsa opc@IP2 worker-2$ sudo su - worker-2# yum install -y nfs-utils worker-2# mkdir /scratch worker-2# echo \u0026quot;PRIVATE_IP1:/scratch /scratch nfs nfsvers=3 0 0\u0026quot; \u0026gt;\u0026gt; /etc/fstab worker-2# mount /scratch worker-2# exit worker-2$ exit $  Install Kubernetes on your own compute resources (for example, Oracle Linux servers outside a cloud) These instructions are for Oracle Linux 7u2+. If you are using a different flavor of Linux, you will need to adjust them accordingly.\nThese steps must be run with the root user, until specified otherwise! Any time you see YOUR_USERID in a command, you should replace it with your actual userid.\n  Choose the directories where your Docker and Kubernetes files will be stored. The Docker directory should be on a disk with a lot of free space (more than 100GB) because it will be used for the /var/lib/docker file system, which contains all of your images and containers. The Kubernetes directory will be used for the /var/lib/kubelet file system and persistent volume storage.\nexport docker_dir=/scratch/docker export k8s_dir=/scratch/k8s_dir  Create a shell script that sets up the necessary environment variables. You should probably just append this to the user\u0026rsquo;s .bashrc so that it will get executed at login. You will also need to configure your proxy settings here if you are behind an HTTP proxy:\nexport PATH=$PATH:/sbin:/usr/sbin pod_network_cidr=\u0026quot;10.244.0.0/16\u0026quot; k8s_dir=$k8s_dir ## grab my IP address to pass into kubeadm init, and to add to no_proxy vars # assume ipv4 and eth0 ip_addr=`ip -f inet addr show eth0 | egrep inet | awk '{print $2}' | awk -F/ '{print $1}'\\` export HTTPS_PROXY=http://proxy:80 export https_proxy=http://proxy:80 export NO_PROXY=localhost,127.0.0.1,.my.domain.com,/var/run/docker.sock,$pod_network_cidr,$ip_addr export no_proxy=localhost,127.0.0.1,.my.domain.com,/var/run/docker.sock,$pod_network_cidr,$ip_addr export HTTP_PROXY=http://proxy:80 export http_proxy=http://proxy:80 export KUBECONFIG=$k8s_dir/admin.conf  Source that script to set up your environment variables:\n. ~/.bashrc  If you want command completion, you can add the following to the script:\n[ -f /usr/share/bash-completion/bash_completion ] \u0026amp;\u0026amp; . /usr/share/bash-completion/bash_completion source \u0026lt;(kubectl completion bash)  Create the directories you need:\nmkdir -p $docker_dir $k8s_dir/kubelet ln -s $k8s_dir/kubelet /var/lib/kubelet  Set an environment variable with the Docker version you want to install:\ndocker_version=\u0026quot;17.03.1.ce\u0026quot;  Install Docker, removing any previously installed version:\n### install docker and curl-devel (for git if needed) yum-config-manager --enable ol7_addons ol7_latest # we are going to just uninstall any docker-engine that is installed yum -y erase docker-engine docker-engine-selinux # now install the docker-engine at our specified version yum -y install docker-engine-$docker_version curl-devel  Update the Docker options:\n# edit /etc/sysconfig/docker to add custom OPTIONS cat /etc/sysconfig/docker | sed \u0026quot;s#^OPTIONS=.*#OPTIONS='--selinux-enabled --group=docker -g $docker_dir'#g\u0026quot; \u0026gt; /tmp/docker.out diff /etc/sysconfig/docker /tmp/docker.out mv /tmp/docker.out /etc/sysconfig/docker  Set up the Docker network, including the HTTP proxy configuration, if you need it:\n# generate a custom /setc/sysconfig/docker-network cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysconfig/docker-network # /etc/sysconfig/docker-network DOCKER_NETWORK_OPTIONS=\u0026quot;-H tcp://0.0.0.0:4243 -H unix:///var/run/docker.sock\u0026quot; HTTP_PROXY=\u0026quot;http://proxy:80\u0026quot; HTTPS_PROXY=\u0026quot;http://proxy:80\u0026quot; NO_PROXY=\u0026quot;localhost,127.0.0.0/8,.my.domain.com,/var/run/docker.sock\u0026quot; EOF  Add your user to the docker group:\nusermod -aG docker YOUR_USERID  Enable and start the Docker service that you just installed and configured:\nsystemctl enable docker \u0026amp;\u0026amp; systemctl start docker  Install the Kubernetes packages:\n# generate the yum repo config cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF setenforce 0 # install kube* packages v=${1:-1.8.4-0} old_ver=`echo $v | egrep \u0026quot;^1.7\u0026quot;` yum install -y kubelet-$v kubeadm-$v kubectl-$v kubernetes-cni # change the cgroup-driver to match what docker is using cgroup=`docker info 2\u0026gt;\u0026amp;1 | egrep Cgroup | awk '{print $NF}'` [ \u0026quot;$cgroup\u0026quot; == \u0026quot;\u0026quot; ] \u0026amp;\u0026amp; echo \u0026quot;cgroup not detected!\u0026quot; \u0026amp;\u0026amp; exit 1 cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf | sed \u0026quot;s#KUBELET_CGROUP_ARGS=--cgroup-driver=.*#KUBELET_CGROUP_ARGS=--cgroup-driver=$cgroup\\\u0026quot;#\u0026quot;\u0026gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.out diff /etc/systemd/system/kubelet.service.d/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.out mv /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.out /etc/systemd/system/kubelet.service.d/10-kubeadm.conf if [ \u0026quot;$old_ver\u0026quot; = \u0026quot;\u0026quot; ] ; then # run with swap if not in version 1.7* (starting in 1.8, kubelet # fails to start with swap enabled) # cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/kubelet.service.d/90-local-extras.conf [Service] Environment=\u0026quot;KUBELET_EXTRA_ARGS=--fail-swap-on=false\u0026quot; EOF fi  Enable and start the Kubernetes service:\nsystemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet  Install and use Flannel for CNI:\n# run kubeadm init as root echo Running kubeadm init --skip-preflight-checks --apiserver-advertise-address=$ip_addr --pod-network-cidr=$pod_network_cidr echo \u0026quot; see /tmp/kubeadm-init.out for output\u0026quot; kubeadm init --skip-preflight-checks --apiserver-advertise-address=$ip_addr --pod-network-cidr=$pod_network_cidr \u0026gt; /tmp/kubeadm-init.out 2\u0026gt;\u0026amp;1 if [ $? -ne 0 ] ; then echo \u0026quot;ERROR: kubeadm init returned non 0\u0026quot; chmod a+r /tmp/kubeadm-init.out exit 1 else echo; echo \u0026quot;kubeadm init complete\u0026quot; ; echo # tail the log to get the \u0026quot;join\u0026quot; token tail -6 /tmp/kubeadm-init.out fi cp /etc/kubernetes/admin.conf $KUBECONFIG chown YOUR_USERID:YOUR_GROUP $KUBECONFIG chmod 644 $KUBECONFIG  The following steps should be run with your normal (non-root) user.\n  Configure CNI:\nsudo -u YOUR_USERID kubectl create clusterrolebinding permissive-binding --clusterrole=cluster-admin --user=admin --user=kubelet --group=system:serviceaccounts sudo -u YOUR_USERID kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml  Wait for kubectl get nodes to show Ready for this host:\nhost=`hostname | awk -F. '{print $1}'` status=\u0026quot;NotReady\u0026quot; max=10 count=1 while [ ${status:=Error} != \u0026quot;Ready\u0026quot; -a $count -lt $max ] ; do sleep 30 status=`sudo -u YOUR_USERID kubectl get nodes | egrep $host | awk '{print $2}'` echo \u0026quot;kubectl status is ${status:=Error}, iteration $count of $max\u0026quot; count=`expr $count + 1` done status=`sudo -u YOUR_USERID kubectl get nodes | egrep $host | awk '{print $2}'` if [ ${status:=Error} != \u0026quot;Ready\u0026quot; ] ; then echo \u0026quot;ERROR: kubectl get nodes reports status=${status:=Error} after configuration, exiting!\u0026quot; exit 1 fi  Taint the nodes:\nsudo -u YOUR_USERID kubectl taint nodes --all node-role.kubernetes.io/master- sudo -u YOUR_USERID kubectl get nodes sudo -u YOUR_USERID kubeadm version  Congratulations! Docker and Kubernetes are installed and configured!\n  Install Docker for Mac with Kubernetes Docker for Mac 18+ provides an embedded Kubernetes environment that is a quick and easy way to get a simple test environment set up on your Mac. To set it up, follow these instructions:\n Install \u0026ldquo;Docker for Mac\u0026rdquo; https://download.docker.com/mac/edge/Docker.dmg. Then start up the Docker application (press Command-Space bar, type in Docker and run it). After it is running you will see the Docker icon appear in your status bar:\n Click the Docker icon and select \u0026ldquo;Preferences\u0026hellip;\u0026rdquo; from the drop down menu. Go to the \u0026ldquo;Advanced\u0026rdquo; tab and give Docker a bit more memory if you have enough to spare:\n Go to the \u0026ldquo;Kubernetes\u0026rdquo; tab and click on the option to enable Kubernetes:\nIf you are behind an HTTP proxy, then you should also go to the \u0026ldquo;Proxies\u0026rdquo; tab and enter your proxy details.\n Docker will download the Kubernetes components and start them up for you. When it is done, you will see the Kubernetes status go to green/running in the menu:\n Ensure that kubectl on your Mac, is pointing to the correct cluster and context.\n$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE docker-for-desktop docker-for-desktop-cluster docker-for-desktop kubernetes-admin@kubernetes kubernetes kubernetes-admin $ kubectl config use-context docker-for-desktop Switched to context \u0026quot;docker-for-desktop\u0026quot;. $ kubectl config get-clusters NAME kubernetes docker-for-desktop-cluster $ kubectl config set-cluster docker-for-desktop-cluster Cluster \u0026quot;docker-for-desktop-cluster\u0026quot; set.  You should add docker-for-desktop to your /etc/hosts file entry for 127.0.0.1, as shown in this example, and you must be an admin user to edit this file:\n## # Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 127.0.0.1 localhost docker-for-desktop 255.255.255.255 broadcasthost ::1 localhost  You may also have to tell kubectl to ignore the certificate by entering this command:\nkubectl config set-cluster docker-for-desktop --insecure-skip-tls-verify=true  Then validate you are talking to the Kubernetes in Docker by entering these commands:\n$ kubectl cluster-info Kubernetes master is running at https://docker-for-desktop:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.  Important note about persistent volumes Docker for Mac has some restrictions on where you can place a directory that can be used as a HostPath for a persistent volume. To keep it simple, place your directory somewhere under /Users.\n   "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/reference/swagger/",
	"title": "Swagger",
	"tags": [],
	"description": "Swagger REST API documentation.",
	"content": "View the Swagger REST API documentation here.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/quickstart/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "For this exercise, you’ll need a Kubernetes cluster. If you need help setting one up, check out our cheat sheet. This guide assumes a single node cluster.\nThe operator uses Helm to create and deploy the necessary resources and then run the operator in a Kubernetes cluster. For Helm installation and usage information, see Install Helm and Tiller.\nYou should clone this repository to your local machine so that you have access to the various sample files mentioned throughout this guide:\n$ git clone https://github.com/oracle/weblogic-kubernetes-operator  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/",
	"title": "User Guide",
	"tags": [],
	"description": "",
	"content": "The User Guide provides detailed information about all aspects of using the operator.\n Introduction  Learn about the operator\u0026#39;s design, architecture, important terms, and prerequisites.\n Overview  The information in the following sections is organized in the order that you would most likely need to use it. If you want to set up an operator and use it to create and manage WebLogic domains, you should follow the User Guide sections from top to bottom, and the necessary information will be presented in the correct order.\n Manage operators  Helm is used to create and deploy necessary operator resources and to run the operator in a Kubernetes cluster. Use the operator\u0026#39;s Helm chart to install and manage the operator.\n Manage domains  Important considerations for WebLogic domains in Kubernetes.\n CI/CD considerations  Learn about managing domain images with continuous integration and continuous delivery (CI/CD).\n "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/cicd/choose-an-approach/",
	"title": "Choose an approach",
	"tags": [],
	"description": "How to choose an approach.",
	"content": "Let\u0026rsquo;s review what we have discussed and talk about when we might want to use various approaches. We can start by asking ourselves questions like these:\n Can you make the desired change with a configuration override?\nThe WebLogic Kubernetes Operator allows you to inject a number of configuration overrides into your pods before starting any servers in the domain. This allows you to use the same image for multiple different configurations. A good example would be changing the settings for a data source, for example. You may wish to have a larger connection pool in your production environment than you do in your development/test environments. You probably also want to have different credentials. You may want to change the service name, and so on. All of these kinds of updates can be made with configuration overrides. These are placed in a Kubernetes config map, that is, they are outside of the image, so they do not require rebuilding the Docker image. If all of your changes fit into this category, it is probably much better to just use configuration overrides instead of building a new image.\n Are you only changing the WebLogic configuration, for example, deploying or updating an application, changing a resource configuration in a way that is not supported by configuration overrides, and such?\nIf your changes fit into this category, and you have used the \u0026ldquo;domain-in-image\u0026rdquo; approach and the Docker layering model, then you only need to update the top layer of your image. This is relatively easy compared to making changes in lower layers. You could create a new layer with the changes, or you could rebuild/replace the existing top layer with a new one. Which approach you choose depends mainly on whether you need to maintain the same domain encryption keys or not. Do you need to be able to do a rolling restart? If you need to do a rolling restart, for example to maintain the availability of your applications, then you need to make sure the new domain layer has the same domain encryption keys. You cannot perform a rolling restart of a domain if the new members have a different encryption key. Do you need to mutate something in a lower layer, for example, patch WebLogic, the JDK, or Linux?\nIf you need to make an update in a lower layer, then you will need to rebuild that layer and all of the layers above it. This means that you will need to rebuild the domain layer. You will need to determine if you need to keep the same domain encryption keys.  The diagram below summarizes these concerns in a decision tree for the “domain in image” case:\nIf you are using the \u0026ldquo;domain on persistent storage\u0026rdquo; approach, many of these concerns become moot because you have an effective separation between your domain and the Docker image. There is still the possibility that an update in the Docker image could affect your domain; for example, if you updated the JDK, you may need to update some of your domain scripts to reflect the new JDK path.\nHowever, in this scenario, your environment is much closer to what you are probably used to in a traditional (non-Kubernetes) environment, and you will probably find that all of the practices you used from that pre-Kubernetes environment are directly applicable here too, with just some small modifications. For example, applying a WebLogic patch would now involve building a new Docker image.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/introduction/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "The operator consists of several parts: the operator runtime, the model for a Kubernetes custom resource definition (CRD), a Helm chart for installing the operator, a variety of sample shell scripts for preparing or packaging WebLogic domains for running in Kubernetes, and sample Helm charts or shell scripts for conditionally exposing WebLogic endpoints outside the Kubernetes cluster.",
	"content": " The operator consists of the following parts:\n The operator runtime, a process that runs in a Docker container deployed into a Kubernetes pod and which performs the actual management tasks. The model for a Kubernetes custom resource definition (CRD) that when installed in a Kubernetes cluster allows the Kubernetes API server to manage instances of this new type representing the operational details and status of WebLogic domains. A Helm chart for installing the operator runtime and related resources. A variety of sample shell scripts for preparing or packaging WebLogic domains for running in Kubernetes. A variety of sample Helm charts or shell scripts for conditionally exposing WebLogic endpoints outside the Kubernetes cluster.  The operator is packaged in a Docker image which you can access using the following docker pull commands:\n$ docker login $ docker pull oracle/weblogic-kubernetes-operator:2.1  For more details on acquiring the operator image and prerequisites for installing the operator, consult the Quick Start guide.\nThe operator registers a Kubernetes custom resource definition called domain.weblogic.oracle (shortname domain, plural domains). More details about the domain resource type defined by this CRD, including its schema, are available here.\nThe diagram below shows the general layout of high-level components, including optional components, in a Kubernetes cluster that is hosting WebLogic domains and the operator:\nThe Kubernetes cluster has several namespaces. Components may be deployed into namespaces as follows:\n The operator is deployed into its own namespace. If the Elastic Stack integration option is configured, then a Logstash pod will also be deployed in the operator’s namespace. WebLogic domains will be deployed into various namespaces. There can be more than one domain in a namespace, if desired. There is no limit on the number of domains or namespaces that an operator can manage. Note that there can be more than one operator in a Kubernetes cluster, but each operator is configured with a list of the specific namespaces that it is responsible for. The operator will not take any action on any domain that is not in one of the namespaces the operator is configured to manage. Customers are responsible for load balancer configuration, which will typically be in the same namespace with domains or in a system, shared namespace such as the kube-system namespace. Customers are responsible for Elasticsearch and Kibana deployment, which are typically deployed in the default namespace.  Domain architecture The diagram below shows how the various parts of a WebLogic domain are manifest in Kubernetes by the operator.\nThis diagram shows the following details:\n An optional, persistent volume is created by the customer using one of the available providers. If the persistent volume is shared across the domain or members of a cluster, then the chosen provider must support “Read Write Many” access mode. The shared state on the persistent volume may include the “domain” directory, the “applications” directory, a directory for storing logs, and a directory for any file-based persistence stores. A pod is created for the WebLogic Administration Server. This pod is labeled with weblogic.domainUID, weblogic.serverName, and weblogic.domainName. One container runs in this pod. WebLogic Node Manager and Administration Server processes are run inside this container. The Node Manager process is used as an internal implementation detail for the liveness probe, for patching, and to provide monitoring and control capabilities to the Administration Console. It is not intended to be used for other purposes, and it may be removed in some future release. A ClusterIP type service is created for the Administration Server pod. This service provides a stable, well-known network (DNS) name for the Administration Server. This name is derived from the domainUID and the Administration Server name, and it is known before starting up any pod. The Administration Server ListenAddress is set to this well-known name. ClusterIP type services are only visible inside the Kubernetes cluster. They are used to provide the well-known names that all of the servers in a domain use to communicate with each other. This service is labeled with weblogic.domainUID and weblogic.domainName. A NodePort type service is optionally created for the Administration Server pod. This service provides HTTP access to the Administration Server to clients that are outside the Kubernetes cluster. This service is intended to be used to access the WebLogic Server Administration Console or for the T3 protocol for WLST connections. This service is labeled with weblogic.domainUID and weblogic.domainName. A pod is created for each WebLogic Managed Server. These pods are labeled with weblogic.domainUID, weblogic.serverName, and weblogic.domainName. One container runs in each pod. WebLogic Node Manager and Managed Server processes are run inside each of these containers. The Node Manager process is used as an internal implementation detail for the liveness probe. It is not intended to be used for other purposes, and it may be removed in some future release. A ClusterIP type service is created for each Managed Server pod that contains a Managed Server that is not part of a WebLogic cluster. These services are intended to be used to access applications running on the Managed Servers. These services are labeled with weblogic.domainUID and weblogic.domainName. Customers must expose these services using a load balancer or NodePort type service to expose these endpoints outside the Kubernetes cluster. An Ingress may optionally be created by the customer for each WebLogic cluster. An Ingress provides load balanced HTTP access to all Managed Servers in that WebLogic cluster. The load balancer updates its routing table for an Ingress every time a Managed Server in the WebLogic cluster becomes “ready” or ceases to be able to service requests, such that the Ingress always points to just those Managed Servers that are able to handle user requests.  The diagram below shows the components inside the containers running WebLogic Server instances:\nThe domain resource specifies a Docker image, defaulting to store/oracle/weblogic:12.2.1.3. All containers running WebLogic Server use this same Docker image. Depending on the use case, this image could contain the WebLogic Server product binaries or also include the domain directory. During a rolling event caused by a change to the domain resource\u0026rsquo;s image field, containers will be using a mix of the updated value of the image field and its previous value.\n Within the container, the following aspects are configured by the operator:\n The ENTRYPOINT is configured by a script that starts up a Node Manager process, and then uses WLST to request that Node Manager start the server. Node Manager is used to start servers so that the socket connection to the server will be available to obtain server status even when the server is unresponsive. This is used by the liveness probe. The liveness probe is configured to check that the server is alive by querying the Node Manager process. The liveness probe is by default configured to check liveness every 15 seconds, and to timeout after 5 seconds. If a pod fails the liveness probe, Kubernetes will restart that container. The readiness probe is configured to use the WebLogic Server ReadyApp framework. The readiness probe is used to determine if the server is ready to accept user requests. The readiness is used to determine when a server should be included in a load balancer\u0026rsquo;s endpoints, when a restarted server is fully started in the case of a rolling restart, and for various other purposes. A shutdown hook is configured that will execute a script that performs a graceful shutdown of the server. This ensures that servers have an opportunity to shut down cleanly before they are killed.  Domain state stored outside Docker images The operator expects (and requires) that all state be stored outside of the Docker images that are used to run the domain. This means either in a persistent file system, or in a database. The WebLogic configuration, that is, the domain directory and the applications directory may come from the Docker image or a persistent volume. However, other state, such as file-based persistent stores, and such, must be stored on a persistent volume or in a database. All of the containers that are participating in the WebLogic domain use the same image, and take on their personality; that is, which server they execute, at startup time. Each pod mounts storage, according to the domain resource, and has access to the state information that it needs to fulfill its role in the domain.\nIt is worth providing some background information on why this approach was adopted, in addition to the fact that this separation is consistent with other existing operators (for other products) and the Kubernetes “cattle, not pets” philosophy when it comes to containers.\nThe external state approach allows the operator to treat the Docker images as essentially immutable, read-only, binary images. This means that the image needs to be pulled only once, and that many domains can share the same image. This helps to minimize the amount of bandwidth and storage needed for WebLogic Server Docker images.\nThis approach also eliminates the need to manage any state created in a running container, because all of the state that needs to be preserved is written into either the persistent volume or a database back end. The containers and pods are completely throwaway and can be replaced with new containers and pods, as necessary. This makes handling failures and rolling restarts much simpler because there is no need to preserve any state inside a running container.\nWhen users wish to apply a binary patch to WebLogic Server, it is necessary to create only a single new, patched Docker image. If desired, any domains that are running may be updated to this new patched image with a rolling restart, because there is no state in the containers.\nIt is envisaged that in some future release of the operator, it will be desirable to be able to “move” or “copy” domains in order to support scenarios like Kubernetes federation, high availability, and disaster recovery. Separating the state from the running containers is seen as a way to greatly simplify this feature, and to minimize the amount of data that would need to be moved over the network, because the configuration is generally much smaller than the size of WebLogic Server Docker images.\nThe team developing the operator felt that these considerations provided adequate justification for adopting the external state approach.\nNetwork name predictability The operator uses services to provide stable, well-known DNS names for each server. These names are known in advance of starting up a pod to run a server, and are used in the ListenAddress fields in the WebLogic Server configuration to ensure that servers will always be able to find each other. This also eliminates the need for pod names or the actual WebLogic Server instance names to be the same as the DNS addresses.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-domains/accessing-the-domain/wlst/",
	"title": "Using WLST",
	"tags": [],
	"description": "You can use the WebLogic Scripting Tool (WLST) to manage a domain running in Kubernetes.",
	"content": "You can use the WebLogic Scripting Tool (WLST) to manage a domain running in Kubernetes. If the Administration Server was configured to expose a T3 channel using the exposeAdminT3Channel setting when creating the domain, then the matching T3 service can be used to connect. For example, if the domainUID is domain1, and the Administration Server name is admin-server, then the service would be called:\ndomain1-admin-server-external  This service will be in the same namespace as the domain. The external port number can be obtained by checking this service’s nodePort:\n$ kubectl get service domain1-admin-server-external -n domain1 -o jsonpath='{.spec.ports[0].nodePort}' 30012  In this example, the nodePort is 30012. If the Kubernetes server’s address was kubernetes001, then WLST can connect to t3://kubernetes001:30012 as shown below:\n$ ~/wls/oracle_common/common/bin/wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt; connect('weblogic','*password*','t3:// kubernetes001:30012') Connecting to t3:// kubernetes001:30012 with userid weblogic ... Successfully connected to Admin Server \u0026quot;admin-server\u0026quot; that belongs to domain \u0026quot;base_domain\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/base_domain/serverConfig/\u0026gt; exit() Exiting WebLogic Scripting Tool.  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/security/encryption/",
	"title": "Encryption",
	"tags": [],
	"description": "WebLogic domain encryption and the WebLogic operator",
	"content": " Contents  WebLogic operator introspector encryption Encryption of Kubernetes secrets Additional reading  WebLogic operator introspector encryption The WebLogic operator has an introspection job that handles WebLogic domain encryption. The introspection also addresses use of Kubernetes secrets for use with configuration overrides. For additional information on the configuration handling, see the configuration overrides documentation.\nThe introspection also creates a boot.properties file that is made available to the pods in the WebLogic domain. The credential used for the WebLogic domain is kept in a Kubernetes Secret which follows the naming pattern \u0026lt;domainUID\u0026gt;-weblogic-credentials, where \u0026lt;domainUID\u0026gt; is the unique identifier of the domain, for example, mydomain-weblogic-credentials.\nFor more information about the WebLogic credentials secret, see Secrets under Security.\n Encryption of Kubernetes secrets To better protect your credentials and private keys, the Kubernetes cluster should be set up with encryption. Please see the Kubernetes documentation about encryption at rest for secret data and using a KMS provider for data encryption.\n Additional reading  Encryption of values for WebLogic configuration overrides  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/samples/simple/domains/domain-home-in-image/",
	"title": "Domain home in image",
	"tags": [],
	"description": "Sample for creating a WebLogic domain home inside a Docker image, and the domain resource YAML file for deploying the generated WebLogic domain.",
	"content": " The sample scripts demonstrate the creation of a WebLogic domain home in a Docker image using one of the domain home in image samples in the WebLogic Server domain Docker image samples GitHub project. The sample scripts have an option of putting the WebLogic domain log, server logs, server output files, and the Node Manager logs on an existing Kubernetes persistent volume (PV) and persistent volume claim (PVC). The scripts also generate the domain YAML file, which can then be used by the scripts or used manually to start the Kubernetes artifacts of the corresponding domain, including the WebLogic Server pods and services.\nPrerequisites Before you begin, read this document, Domain resource.\nThe following prerequisites must be handled prior to running the create domain script:\n The WDT sample requires that JAVA_HOME is set to a Java JDK version 1.8 or later. The operator requires WebLogic Server 12.2.1.3.0 with patch 29135930 applied. The existing WebLogic Docker image, store/oracle/weblogic:12.2.1.3, was updated on January 17, 2019, and has all the necessary patches applied; a docker pull is required if you pulled the image prior to that date. Refer to WebLogic Docker images for details on how to obtain or create the image. Create a Kubernetes namespace for the domain unless the intention is to use the default namespace. If logHomeOnPV is enabled, create the Kubernetes persistent volume where the log home will be hosted, and the Kubernetes persistent volume claim for the domain in the same Kubernates namespace. For samples to create a PV and PVC, see Create sample PV and PVC. Create a Kubernetes secret for the WebLogic administrator credentials that contains the fields username and password, and make sure that the secret name matches the value specified for weblogicCredentialsSecretName (see Configuration table below). For example:\n$ cd ./kubernetes/samples/scripts/create-weblogic-domain-credentials $ create-weblogic-credentials.sh -u weblogic -p welcome1 -d domain1 -n default -s domain1-weblogic-credentials   NOTE: Then make sure to configure weblogicCredentialsSecretName to be domain1-weblogic-credentials.\nUse the script to create a domain The create-domain.sh script generates a new Docker image on each run with a new domain home and a different internal domain secret in it. To prevent having disparate images with different domain secrets in the same domain, we strongly recommend that a new domain uses a domainUID that is different from any of the active domains, or that you delete the existing domain resource using the following command and wait until all the server pods are terminated before you create a domain with the same domainUID:\n $kubectl delete domain [domainUID] -n [domainNamespace]  Make a copy of the create-domain-inputs.yaml file, and run the create script, pointing it at your inputs file and an output directory:\n$ ./create-domain.sh \\ -u \u0026lt;username\u0026gt; \\ -p \u0026lt;password\u0026gt; \\ -i create-domain-inputs.yaml \\ -o /path/to/output-directory  The script will perform the following steps:\n Create a directory for the generated properties and Kubernetes YAML files for this domain if it does not already exist. The pathname is /path/to/weblogic-operator-output-directory/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, its contents will be removed. Create a properties file, domain.properties, in the directory that is created above. This properties file will be used to create a sample WebLogic Server domain. Clone the WebLogic docker-images project into the directory that is derived from the domainHomeImageBuildPath property using git clone https://github.com/oracle/docker-images.git. By default, the script always cleans up the directory and clones it again on every run. You need to specify the -k option if you want to use a previously cloned project. Note that if the specified domainHomeImageBuildPath is empty, the script will still clone the project even if the -k option is specified. Replace the built-in user name and password in the properties/docker-build/domain_security.properties file with the username and password that are supplied on the command line using the -u and -p options. These credentials need to match the WebLogic domain admin credentials in the secret that is specified via the weblogicCredentialsSecretName property in the create-domain-inputs.yaml file. Build a Docker image based on the Docker sample, Example Image with a WebLogic Server Domain using the Oracle WebLogic Scripting Tooling (WLST) or Example Image with a WebLogic Server Domain using the Oracle WebLogic Deploy Tooling (WDT). It will create a sample WebLogic Server domain in the Docker image. Oracle strongly recommends storing the image containing the domain home as private in the registry (for example, Oracle Cloud Infrastructure Registry, Docker Hub, and such) as this image contains sensitive information about the domain including keys and credentials that are used to access external resources (for example, data source password). For more information about domain home in image protection, see the Security section.\n  Create a tag that refers to the generated Docker image. Create a Kubernetes domain YAML file, domain.yaml, in the directory that is created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command.\n$ kubectl apply -f /path/to/output-directory/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml   As a convenience, using the -e option, the script can optionally create the domain object, which in turn results in the creation of the corresponding WebLogic Server pods and services. This option should be used in a single node Kubernetes cluster only.\nFor a multi-node Kubernetes cluster, make sure that the generated image is available on all nodes before creating the domain resource using the kubectl apply -f command.\nThe usage of the create script is as follows:\n$ sh create-domain.sh -h usage: create-domain.sh -o dir -i file -u username -p password [-k] [-e] [-h] -i Parameter inputs file, must be specified. -o Ouput directory for the generated properties and YAML files, must be specified. -u User name used in building the Docker image for WebLogic domain in image. -p Password used in building the Docker image for WebLogic domain in image. -e Also create the resources in the generated YAML files, optional. -v Validate the existence of persistentVolumeClaim, optional. -k Keep what has been previously cloned from https://github.com/oracle/docker-images.git, optional. If not specified, this script will always remove the existing project and clone again. -h Help  If you copy the sample scripts to a different location, make sure that you copy everything in the \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts directory together into the target directory, maintaining the original directory hierarchy.\nThe default domain created by the script has the following characteristics:\n An Administration Server named admin-server listening on port 7001. A dynamic cluster named cluster-1 of size 5. Two Managed Servers, named managed-server1 and managed-server2, listening on port 8001. No applications deployed. A T3 channel.  If you run the sample from a machine that is remote to the Kubernetes cluster, and you need to push the new image to a registry that is local to the cluster, you need to do the following (also see the image property in the Configuration parameters table in the next section):\n Set the image property in the inputs file to the target image name (including the registry hostname/port and the tag, if needed). Run the create-domain.sh script without the -e option. Push the image to the target registry. Run the following command to create the domain:\n$ kubectl apply -f /path/to/output-directory/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml   The domain creation inputs can be customized by editing create-domain-inputs.yaml.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Definition Default     adminPort Port number for the Administration Server inside the Kubernetes cluster. 7001   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. admin-server   clusterName Name of the WebLogic cluster instance to generate for the domain. cluster-1   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   domainHomeImageBase Base WebLogic binary image used to build the WebLogic domain image. The operator requires WebLogic Server 12.2.1.3.0 with patch 29135930 applied. The existing WebLogic Docker image, store/oracle/weblogic:12.2.1.3, was updated on January 17, 2019, and has all the necessary patches applied; a docker pull is required if you pulled the image prior to that date. Refer to WebLogic Docker images for details on how to obtain or create the image. store/oracle/weblogic:12.2.1.3   domainHomeImageBuildPath Location of the WebLogic \u0026ldquo;domain home in image\u0026rdquo; Docker image in https://github.com/oracle/docker-images.git project. If not specified, use \u0026ldquo;./docker-images/OracleWebLogic/samples/12213-domain-home-in-image\u0026rdquo;. Another possible value is \u0026ldquo;./docker-images/OracleWebLogic/samples/12213-domain-home-in-image-wdt\u0026rdquo; which uses WDT, instead of WLST, to generate the domain configuration. ./docker-images/OracleWebLogic/samples/12213-domain-home-in-image   domainPVMountPath Mount path of the domain persistent volume. This parameter is required if logHomeOnPV is true. Otherwise, it is ignored. /shared   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes service name. domain1   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   image WebLogic Server Docker image that the operator uses to start the domain. The create domain scripts generate a WebLogic Server Docker image with a domain home in it. By default, the scripts tag the generated WebLogic server Docker image as either domain-home-in-image or domain-home-in-image-wdt based on the domainHomeImageBuildPath property, and use it plus the tag that is obtained from the domainHomeImageBase to set the image element in the generated domain YAML file. If this property is set, the create domain scripts will use the value specified, instead of the default value, to tag the generated image and set the image in the domain YAML file. A unique value is required for each domain that is created using the scripts. If you are running the sample scripts from a machine that is remote to the Kubernetes cluster where the domain is going to be running, you need to set this property to the image name that is intended to be used in a registry local to that Kubernetes cluster. You also need to push the image to that registry before starting the domain using the kubectl create -f or kubectl apply -f command.    imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never. IfNotPresent   imagePullSecretName Name of the Kubernetes secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include server.out to the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to initially start for the domain. 2   javaOptions Java options for starting the Administration and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHomeOnPV Specifies whether the log home is stored on the persistent volume. If set to true, then you must specify the logHome, persistentVolumeClaimName and domainPVMountPath parameters. false   logHome The in-pod location for domain log, server logs, server out, and Node Manager log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. This parameter is required if logHomeOnPV is true. Otherwise, it is ignored. /shared/logs/domain1   managedServerNameBase Base string used to generate Managed Server names. managed-server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes namespace in which to create the domain. default   persistentVolumeClaimName Name of the persistent volume claim. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. This parameter is required if logHomeOnPV is true. Otherwise, it is ignored. domain1-weblogic-sample-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Servers will be started up. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the NetworkAccessPoint. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would normally be a load balancer address. For development environments only: In a single server (all-in-one) Kubernetes deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the kubernetes cluster   weblogicCredentialsSecretName Name of the Kubernetes secret for the Administration Server\u0026rsquo;s user name and password. domain1-weblogic-credentials    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create a WebLogic domain home and associated Kubernetes resources for a domain that has only one cluster. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. Also, the generated domain YAML file can be modified to cover more use cases.\nVerify the results The create script will verify that the domain was created, and will report failure if there was any error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nNote that the example results below use the default Kubernetes namespace. If you are using a different namespace, you need to replace NAMESPACE in the example kubectl commands with the actual Kubernetes namespace.\nGenerated YAML files with the default inputs The content of the generated domain.yaml:\n# Copyright 2017, 2019, Oracle Corporation and/or its affiliates. All rights reserved. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026quot;weblogic.oracle/v2\u0026quot; kind: Domain metadata: name: domain1 namespace: default labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: # The WebLogic Domain Home domainHome: /u01/oracle/user_projects/domains/domain1 # If the domain home is in the image domainHomeInImage: true # The WebLogic Server Docker image that the operator uses to start the domain image: \u0026quot;domain-home-in-image:12.2.1.3\u0026quot; # imagePullPolicy defaults to \u0026quot;Always\u0026quot; if image version is :latest imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: domain1-weblogic-credentials # Whether to include the server out file into the pod's stdout, default is true includeServerOutInPodLog: true # Whether to enable log home # logHomeEnabled: false # The in-pod location for domain log, server logs, server out, and Node Manager log files # logHome: /shared/logs/domain1 # serverStartPolicy legal values are \u0026quot;NEVER\u0026quot;, \u0026quot;IF_NEEDED\u0026quot;, or \u0026quot;ADMIN_ONLY\u0026quot; # This determines which WebLogic Servers the operator will start up when it discovers this Domain # - \u0026quot;NEVER\u0026quot; will not start any server in the domain # - \u0026quot;ADMIN_ONLY\u0026quot; will start up only the administration server (no managed servers will be started) # - \u0026quot;IF_NEEDED\u0026quot; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom -Xms64m -Xmx256m \u0026quot; # volumes: # - name: weblogic-domain-storage-volume # persistentVolumeClaim: # claimName: domain1-weblogic-sample-pvc # volumeMounts: # - mountPath: /shared # name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026quot;RUNNING\u0026quot; or \u0026quot;ADMIN\u0026quot; # \u0026quot;RUNNING\u0026quot; means the listed server will be started up to \u0026quot;RUNNING\u0026quot; mode # \u0026quot;ADMIN\u0026quot; means the listed server will be start up to \u0026quot;ADMIN\u0026quot; mode serverStartState: \u0026quot;RUNNING\u0026quot; # adminService: # channels: # The Admin Server's NodePort # - channelName: default # nodePort: 30701 # Uncomment to export the T3Channel as a service # - channelName: T3Channel # clusters is used to configure the desired behavior for starting member servers of a cluster. # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: cluster-1 serverStartState: \u0026quot;RUNNING\u0026quot; replicas: 2 # The number of managed servers to start for unlisted clusters # replicas: 1  Verify the domain To confirm that the domain was created, use this command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE  Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\nHere is an example of the output of this command:\n$ kubectl describe domain domain1 Name: domain1 Namespace: default Labels: weblogic.domainUID=domain1 weblogic.resourceVersion=domain-v2 Annotations: \u0026lt;none\u0026gt; API Version: weblogic.oracle/v2 Kind: Domain Metadata: Cluster Name: Creation Timestamp: 2019-01-10T14:29:37Z Generation: 1 Resource Version: 3698533 Self Link: /apis/weblogic.oracle/v2/namespaces/default/domains/domain1 UID: 28655979-14e4-11e9-b751-fa163e855ac8 Spec: Admin Server: Server Pod: Annotations: Container Security Context: Env: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Clusters: Cluster Name: cluster-1 Cluster Service: Annotations: Labels: Replicas: 2 Server Pod: Annotations: Container Security Context: Env: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Domain Home: /u01/oracle/user_projects/domains/domain1 Domain Home In Image: true Image: domain-home-in-image:12.2.1.3 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Managed Servers: Server Pod: Annotations: Container Security Context: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Xms64m -Xmx256m Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: domain1-weblogic-credentials Status: Conditions: Last Transition Time: 2019-01-10T14:31:10.681Z Reason: ServersReady Status: True Type: Available Servers: Health: Activation Time: 2019-01-10T14:30:47.432Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: admin-server State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-01-10T14:32:01.467Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: managed-server1 State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-01-10T14:32:04.532Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: managed-server2 State: RUNNING Start Time: 2019-01-10T14:29:37.455Z Events: \u0026lt;none\u0026gt;  In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued very soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. The operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Use the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE  Here is an example of the output of this command:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE domain1-admin-server 1/1 Running 0 30m domain1-managed-server1 1/1 Running 0 29m domain1-managed-server2 1/1 Running 0 29m  Verify the services Use the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE  Here is an example of the output of this command:\n$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE domain1-admin-server ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 32m domain1-cluster-cluster-1 ClusterIP 10.99.151.142 \u0026lt;none\u0026gt; 8001/TCP 31m domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 31m domain1-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 22m  Delete the domain The generated YAML file in the /path/to/weblogic-operator-output-directory/weblogic-domains/\u0026lt;domainUID\u0026gt; directory can be used to delete the Kubernetes resource. Use the following command to delete the domain:\n$ kubectl delete -f domain.yaml  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/samples/simple/domains/",
	"title": "Domains",
	"tags": [],
	"description": "These samples show various choices for working with domains.",
	"content": "These samples show various choices for working with domains.\n Manually  Sample for creating the domain custom resource manually.\n Domain home on a PV  Sample for creating a WebLogic domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated WebLogic domain.\n Domain home in image  Sample for creating a WebLogic domain home inside a Docker image, and the domain resource YAML file for deploying the generated WebLogic domain.\n Delete domain resources  Delete the domain resources created while executing the samples.\n "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/developerguide/building/",
	"title": "Building",
	"tags": [],
	"description": "",
	"content": " The operator is built using Apache Maven. The build machine will also need to have Docker installed.\nTo build the operator, issue the following command in the project directory:\n$ mvn clean install  This will compile the source files, build JAR files containing the compiled classes and libraries needed to run the operator, and will also execute all of the unit tests.\nContributions must conform to coding and formatting standards. To automatically update local code to conform to formatting standards, issue the following command:\n$ mvn fmt:format  Building Javadoc To build the Javadoc for the operator, issue the following command:\n$ mvn javadoc:javadoc  The Javadoc is also available in the GitHub repository here.\nBuilding the operator Docker image Log in to the Docker Store so that you will be able to pull the base image and create the Docker image as follows. These commands should be executed in the project root directory:\n$ docker login $ docker build --build-arg VERSION=\u0026lt;version\u0026gt; -t weblogic-kubernetes-operator:some-tag --no-cache=true .  Replace \u0026lt;version\u0026gt; with the version of the project found in the pom.xml file in the project root directory.\nWe recommend that you use a tag other than latest, to make it easy to distinguish your image. In the example above, the tag could be the GitHub ID of the developer.\nRunning the operator from an IDE The operator can be run from an IDE, which is useful for debugging. In order to do so, the machine running the IDE must be configured with a Kubernetes configuration file in ~/.kube/config or in a location pointed to by the KUBECONFIG environment variable.\nConfigure the IDE to run the class oracle.kubernetes.operator.Main.\nYou may need to create a directory called /operator on your machine. Please be aware that the operator code is targeted to Linux, and although it will run fine on macOS, it will probably not run on other operating systems. If you develop on another operating system, you should deploy the operator to a Kubernetes cluster and use remote debugging instead.\nRunning the operator in a Kubernetes cluster If you\u0026rsquo;re not running Kubernetes on your development machine, you\u0026rsquo;ll need to make the Docker image available to a registry visible to your Kubernetes cluster. Either docker push the image to a private registry or upload your image to a machine running Docker and Kubernetes as follows:\n# on your build machine $ docker save weblogic-kubernetes-operator:some-tag \u0026gt; operator.tar $ scp operator.tar YOUR_USER@YOUR_SERVER:/some/path/operator.tar # on the Kubernetes server $ docker load \u0026lt; /some/path/operator.tar  Use the Helm charts to install the operator.\nIf the operator\u0026rsquo;s behavior or pod log is insufficient to diagnose and resolve failures, then you can connect a Java debugger to the operator using the debugging options.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-operators/using-the-operator/the-rest-api/",
	"title": "The REST API",
	"tags": [],
	"description": "Use the operator&#39;s REST services.",
	"content": " Use the operator\u0026rsquo;s REST services The operator provides a REST server which you can use to get a list of WebLogic domains and clusters and to initiate scaling operations. Swagger documentation for the REST API is available here.\nYou can access most of the REST services using GET, for example:\n To obtain a list of domains, send a GET request to the URL /operator/latest/domains To obtain a list of clusters in a domain, send a GET request to the URL /operator/latest/domains/\u0026lt;domainUID\u0026gt;/clusters  All of the REST services require authentication. Callers must pass in a valid token header and a CA certificate file. Callers should pass in the Accept:/application/json header.\nTo protect against Cross Site Request Forgery (CSRF) attacks, the operator REST API requires that you send in a X-Requested-By header when you invoke a REST endpoint that makes a change (for example when you POST to the /scale endpoint). The value is an arbitrary name such as MyClient. For example, when using curl:\ncurl ... -H X-Requested-By:MyClient ... -X POST .../scaling  If you do not pass in the X-Requested-By header, then you\u0026rsquo;ll get a 400 (bad request) response without any details explaining why the request is bad. The X-Requested-By header is not needed for requests that only read, for example when you GET any of the operator\u0026rsquo;s REST endpoints.\nBefore using the sample script below, you must:\n Update it to ensure it has the correct service account, namespaces, etc., and it points to the values.yaml that you used to install the operator (so that it can get the certificates). Add your operator\u0026rsquo;s certificate to your operating system\u0026rsquo;s trust store (see below). If you are using a self-signed certificate and your client is macOS, you may need to update the version of curl you have installed. The version of CURL that ships with macOS High Sierra (curl 7.54.0 (x86_64-apple-darwin17.0) libcurl/7.54.0 LibreSSL/2.0.20 zlib/1.2.11 nghttp2/1.24.0) has known issues with self-signed certificates. Oracle recommends curl 7.63.0 (x86_64-apple-darwin17.7.0) libcurl/7.63.0 SecureTransport zlib/1.2.11 which can be installed with brew install curl.  How to add your certificate to your operating system trust store For macOS, find the certificate in Finder, and double-click on it. This will add it to your keystore and open Keychain Access. Find the certificate in Keychain Access and double-click on it to open the details. Open the \u0026ldquo;Trust\u0026rdquo; pull-down menu and set the value of \u0026ldquo;When using this certificate\u0026rdquo; to \u0026ldquo;Always Trust\u0026rdquo;, then close the detail window and enter your password when prompted.\nFor Oracle Linux, run the script below once to copy the certificate into /tmp/operator.cert.pem then run these commands to add the certificate to the trust store:\n$ sudo cp /tmp/operator.cert.pem /etc/pki/ca-trust/source/anchors/ $ sudo update-ca-trust enable; sudo update-ca-trust extract $ openssl x509 -noout -hash -in /tmp/operator.cert.pem $ sudo ln -s /etc/pki/ca-trust/source/anchors/operator.cert.pem /etc/pki/tls/certs/e242d2da.0  In the final command, the filename e242d2da.0 should be the output of the previous command plus the suffix .0.\nPlease consult your operating system\u0026rsquo;s documentation (or Google) for other operating systems.\nSample operator REST client script Here is a small sample BASH script that may help to prepare the necessary token, certificates, and such, to call the operator\u0026rsquo;s REST services. Please read the important caveats above before using this script:\n#!/bin/bash KUBERNETES_SERVER=$1 URL_TAIL=$2 REST_PORT=`kubectl get services -n weblogic-operator -o jsonpath='{.items[?(@.metadata.name == \u0026quot;external-weblogic-operator-svc\u0026quot;)].spec.ports[?(@.name == \u0026quot;rest\u0026quot;)].nodePort}'` REST_ADDR=\u0026quot;https://${KUBERNETES_SERVER}:${REST_PORT}\u0026quot; SECRET=`kubectl get serviceaccount weblogic-operator -n weblogic-operator -o jsonpath='{.secrets[0].name}'` ENCODED_TOKEN=`kubectl get secret ${SECRET} -n weblogic-operator -o jsonpath='{.data.token}'` TOKEN=`echo ${ENCODED_TOKEN} | base64 --decode` OPERATOR_CERT_DATA=`kubectl get secret -n weblogic-operator weblogic-operator-external-rest-identity -o jsonpath='{.data.tls\\.crt}'` OPERATOR_CERT_FILE=\u0026quot;/tmp/operator.cert.pem\u0026quot; echo ${OPERATOR_CERT_DATA} | base64 --decode \u0026gt; ${OPERATOR_CERT_FILE} cat ${OPERATOR_CERT_FILE} echo \u0026quot;Ready to call operator REST APIs\u0026quot; STATUS_CODE=`curl \\ -v \\ --cacert ${OPERATOR_CERT_FILE} \\ -H \u0026quot;Authorization: Bearer ${TOKEN}\u0026quot; \\ -H Accept:application/json \\ -X GET ${REST_ADDR}/${URL_TAIL} \\ -o curl.out \\ --stderr curl.err \\ -w \u0026quot;%{http_code}\u0026quot;` cat curl.err cat curl.out | jq .  You can use the -k option to bypass the check to verify that the operator\u0026rsquo;s certificate is trusted (instead of curl --cacert), but this is insecure.\n To use this script, pass in the Kubernetes server address and then the URL you want to call. The script assumes jq is installed and uses it to format the response. This can be removed if desired. The script also prints out quite a bit of useful debugging information in addition to the response. Here is an example of the output of this script:\n$ ./rest.sh kubernetes001 operator/latest/domains/domain1/clusters Ready to call operator REST APIs Note: Unnecessary use of -X or --request, GET is already inferred. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0* Trying 10.139.151.214... * TCP_NODELAY set * Connected to kubernetes001 (10.1.2.3) port 31001 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH * error setting certificate verify locations, continuing anyway: * CAfile: /tmp/operator.cert.pem CApath: none * TLSv1.2 (OUT), TLS handshake, Client hello (1): } [512 bytes data] * TLSv1.2 (IN), TLS handshake, Server hello (2): { [81 bytes data] * TLSv1.2 (IN), TLS handshake, Certificate (11): { [799 bytes data] * TLSv1.2 (IN), TLS handshake, Server key exchange (12): { [413 bytes data] * TLSv1.2 (IN), TLS handshake, Server finished (14): { [4 bytes data] * TLSv1.2 (OUT), TLS handshake, Client key exchange (16): } [150 bytes data] * TLSv1.2 (OUT), TLS change cipher, Client hello (1): } [1 bytes data] * TLSv1.2 (OUT), TLS handshake, Finished (20): } [16 bytes data] * TLSv1.2 (IN), TLS change cipher, Client hello (1): { [1 bytes data] * TLSv1.2 (IN), TLS handshake, Finished (20): { [16 bytes data] * SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256 * ALPN, server did not agree to a protocol * Server certificate: * subject: CN=weblogic-operator * start date: Jan 18 16:30:01 2018 GMT * expire date: Jan 16 16:30:01 2028 GMT * issuer: CN=weblogic-operator * SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway. \u0026gt; GET /operator/latest/domains/domain1/clusters HTTP/1.1 \u0026gt; Host: kubernetes001:31001 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJ3ZWJsb2dpYy1vcGVyYXRvciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQ (truncated) 1vcGVyYXRvcjp3ZWJsb2dpYy1vcGVyYXRvciJ9.NgaGR0NbzbJpVXguQDjRKyDBnNTqwgwPEXv3NjWwMcaf0OlN54apHubdrIx6KYz9ONGz-QeTLnoMChFY7oWA6CBfbvjt-GQX6JvdoJYxsQo1pt-E6sO2YvqTFE4EG-gpEDaiCE_OjZ_bBpJydhIiFReToA3-mxpDAUK2_rUfkWe5YEaLGMWoYQfXPAykzFiH4vqIi_tzzyzNnGxI2tUcBxNh3tzWFPGXKhzG18HswiwlFU5pe7XEYv4gJbvtV5tlGz7YdmH74Rc0dveV-54qHD_VDC5M7JZVh0ZDlyJMAmWe4YcdwNQQNGs91jqo1-JEM0Wj8iQSDE3cZj6MB0wrdg \u0026gt; Accept:application/json \u0026gt; 0 0 0 0 0 0 0 0 --:--:-- 0:00:01 --:--:-- 0\u0026lt; HTTP/1.1 200 OK \u0026lt; Content-Type: application/json \u0026lt; Content-Length: 463 \u0026lt; { [463 bytes data] 100 463 100 463 0 0 205 0 0:00:02 0:00:02 --:--:-- 205 * Connection #0 to host kubernetes001 left intact { \u0026quot;links\u0026quot;: [ { \u0026quot;rel\u0026quot;: \u0026quot;self\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;href\u0026quot;: \u0026quot;/operator/latest/domains/domain1/clusters\u0026quot; }, { \u0026quot;rel\u0026quot;: \u0026quot;canonical\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;href\u0026quot;: \u0026quot;/operator/latest/domains/domain1/clusters\u0026quot; }, { \u0026quot;rel\u0026quot;: \u0026quot;parent\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;href\u0026quot;: \u0026quot;/operator/latest/domains/domain1\u0026quot; } ], \u0026quot;items\u0026quot;: [ { \u0026quot;links\u0026quot;: [ { \u0026quot;rel\u0026quot;: \u0026quot;self\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;href\u0026quot;: \u0026quot;/operator/latest/domains/domain1/clusters/cluster-1\u0026quot; }, { \u0026quot;rel\u0026quot;: \u0026quot;canonical\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;href\u0026quot;: \u0026quot;/operator/latest/domains/domain1/clusters/cluster-1\u0026quot; } ], \u0026quot;cluster\u0026quot;: \u0026quot;cluster-1\u0026quot; } ] }  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-domains/domain-lifecycle/scaling/",
	"title": "Scaling",
	"tags": [],
	"description": "The operator provides several ways to initiate scaling of WebLogic clusters.",
	"content": " WebLogic Server supports two types of clustering configurations, configured and dynamic. Configured clusters are created by manually configuring each individual Managed Server instance. In dynamic clusters, the Managed Server configurations are generated from a single, shared template. With dynamic clusters, when additional server capacity is needed, new server instances can be added to the cluster without having to manually configure them individually. Also, unlike configured clusters, scaling up of dynamic clusters is not restricted to the set of servers defined in the cluster but can be increased based on runtime demands. For more information on how to create, configure, and use dynamic clusters in WebLogic Server, see Dynamic Clusters.\nThe following blogs provide more in-depth information on support for scaling WebLogic clusters in Kubernetes:\n Automatic Scaling of WebLogic Clusters on Kubernetes WebLogic Dynamic Clusters on Kubernetes  The operator provides several ways to initiate scaling of WebLogic clusters, including:\n On-demand, updating the domain resource directly (using kubectl). Calling the operator\u0026rsquo;s REST scale API, for example, from curl. Using a WLDF policy rule and script action to call the operator\u0026rsquo;s REST scale API. Using a Prometheus alert action to call the operator\u0026rsquo;s REST scale API.  On-demand, updating the domain resource directly The easiest way to scale a WebLogic cluster in Kubernetes is to simply edit the replicas property within a domain resource. This can be done by using the kubectl command-line interface for running commands against Kubernetes clusters. More specifically, you can modify the domain resource directly by using the kubectl edit command. For example:\n$ kubectl edit domain domain1 -n [namespace]  Here we are editing a domain resource named domain1. The kubectl edit command will open the domain resource definition in an editor and allow you to modify the replicas value directly. Once committed, the operator will be notified of the change and will immediately attempt to scale the corresponding dynamic cluster by reconciling the number of running pods/Managed Server instances with the replicas value specification.\nspec: ... clusters: - clusterName: cluster-1 replicas: 1 ...  Alternatively, you can specify a default replicas value for all the clusters. If you do this, then you don\u0026rsquo;t need to list the cluster in the domain resource (unless you want to customize another property of the cluster).\nspec: ... replicas: 1 ...  Calling the operator\u0026rsquo;s REST scale API Scaling up or scaling down a WebLogic cluster provides increased reliability of customer applications as well as optimization of resource usage. In Kubernetes cloud environments, scaling WebLogic clusters involves scaling the corresponding pods in which WebLogic Managed Server instances are running. Because the operator manages the life cycle of a WebLogic domain, the operator exposes a REST API that allows an authorized actor to request scaling of a WebLogic cluster.\nThe following URL format is used for describing the resources for scaling (up and down) a WebLogic cluster:\nhttp(s)://${OPERATOR_ENDPOINT}/operator/\u0026lt;version\u0026gt;/domains/\u0026lt;domainUID\u0026gt;/clusters/\u0026lt;clusterName\u0026gt;/scale  For example:\nhttp(s)://${OPERATOR_ENDPOINT}/operator/v1/domains/domain1/clusters/cluster-1/scale  In this URL format:\n OPERATOR_ENDPOINT is the host and port of the operator REST endpoint (internal or external). \u0026lt;version\u0026gt; denotes the version of the REST resource. \u0026lt;domainUID\u0026gt; is the unique identifier of the WebLogic domain. \u0026lt;clusterName\u0026gt; is the name of the WebLogic cluster to be scaled.  The /scale REST endpoint accepts an HTTP POST request and the request body supports the JSON \u0026quot;application/json\u0026quot; media type. The request body will be a simple name-value item named managedServerCount; for example:\n{ \u0026quot;managedServerCount\u0026quot;: 3 }  The managedServerCount value designates the number of WebLogic Server instances to scale to. Note that the scale resource is implemented using the JAX-RS framework, and so a successful scaling request will return an HTTP response code of 204 (“No Content”) because the resource method’s return type is void and does not return a message body.\nWhen you POST to the /scale REST endpoint, you must send the following headers:\n X-Requested-By request value. The value is an arbitrary name such as MyClient.\n Authorization: Bearer request value. The value of the Bearer token is the WebLogic domain service account token.  For example, when using curl:\ncurl -v -k -H X-Requested-By:MyClient -H Content-Type:application/json -H Accept:application/json -H \u0026quot;Authorization:Bearer ...\u0026quot; -d '{ \u0026quot;managedServerCount\u0026quot;: 3 }' https://.../scaling  If you omit the header, you\u0026rsquo;ll get a 400 (bad request) response without any details explaining why the request was bad. If you omit the Bearer Authentication header, then you\u0026rsquo;ll get a 401 (Unauthorized) response.\nOperator REST endpoints The WebLogic Kubernetes Operator can expose both an internal and external REST HTTPS endpoint. The internal REST endpoint is only accessible from within the Kubernetes cluster. The external REST endpoint is accessible from outside the Kubernetes cluster. The internal REST endpoint is enabled by default and thus always available, whereas the external REST endpoint is disabled by default and only exposed if explicitly configured. Detailed instructions for configuring the external REST endpoint are available here.\nRegardless of which endpoint is being invoked, the URL format for scaling is the same.\n What does the operator do in response to a scaling request? When the operator receives a scaling request, it will:\n Perform an authentication and authorization check to verify that the specified user is allowed to perform the specified operation on the specified resource. Validate that the specified domain, identified by domainUID, exists. Validate that the WebLogic cluster, identified by clusterName, exists. Verify that the specified WebLogic cluster has a sufficient number of configured servers to satisfy the scaling request. Initiate scaling by setting the replicas property within the corresponding domain resource, which can be done in either:  A cluster entry, if defined within its cluster list. At the domain level, if not defined in a cluster entry.   In response to a change to either replicas property, in the domain resource, the operator will increase or decrease the number of pods (Managed Servers) to match the desired replica count.\nUsing a WLDF policy rule and script action to call the operator\u0026rsquo;s REST scale API The WebLogic Diagnostics Framework (WLDF) is a suite of services and APIs that collect and surface metrics that provide visibility into server and application performance. To support automatic scaling of WebLogic clusters in Kubernetes, WLDF provides the Policies and Actions component, which lets you write policy expressions for automatically executing scaling operations on a cluster. These policies monitor one or more types of WebLogic Server metrics, such as memory, idle threads, and CPU load. When the configured threshold in a policy is met, the policy is triggered, and the corresponding scaling action is executed. The WebLogic Kubernetes Operator project provides a shell script, scalingAction.sh, for use as a Script Action, which illustrates how to issue a request to the operator’s REST endpoint.\nConfigure automatic scaling of WebLogic clusters in Kubernetes with WLDF The following steps are provided as a guideline on how to configure a WLDF Policy and Script Action component for issuing scaling requests to the operator\u0026rsquo;s REST endpoint:\n Copy the scalingAction.sh script to a directory (such as $DOMAIN_HOME/bin/scripts) so that it\u0026rsquo;s accessible within the Administration Server pod.\n Configure a WLDF policy and action as part of a diagnostic module targeted to the Administration Server. For information about configuring the WLDF Policies and Actions component, see Configuring Policies and Actions in Configuring and Using the Diagnostics Framework for Oracle WebLogic Server.\na. Configure a WLDF policy with a rule expression for monitoring WebLogic Server metrics, such as memory, idle threads, and CPU load for example.\nb. Configure a WLDF script action and associate the scalingAction.sh script.\n  Important notes about the configuration properties for the Script Action:\nThe scalingAction.sh script requires access to the SSL certificate of the operator’s endpoint and this is provided through the environment variable INTERNAL_OPERATOR_CERT.\nThe operator’s SSL certificate can be found in the internalOperatorCert entry of the operator’s ConfigMap weblogic-operator-cm:\nFor example:\n#\u0026gt; kubectl describe configmap weblogic-operator-cm -n weblogic-operator ... Data ==== internalOperatorCert: ---- LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR3akNDQXFxZ0F3SUJBZ0lFRzhYT1N6QU... ...  The scalingAction.sh script accepts a number of customizable parameters:\n action - scaleUp or scaleDown (Required)\n domain_uid - WebLogic domain unique identifier (Required)\n cluster_name - WebLogic cluster name (Required)\n kubernetes_master - Kubernetes master URL, default=https://kubernetes\n  Set this to https://kubernetes.default.svc when invoking scalingAction.sh from the Administration Server pod.\n  access_token - Service Account Bearer token for authentication and authorization for access to REST Resources\n wls_domain_namespace - Kubernetes namespace in which the WebLogic domain is defined, default=default\n operator_service_name - WebLogic Operator Service name of the REST endpoint, default=internal-weblogic-operator-service\n operator_service_account - Kubernetes Service Account name for the WebLogic Operator, default=weblogic-operator\n operator_namespace – Namespace in which the WebLogic Operator is deployed, default=weblogic-operator\n scaling_size – Incremental number of WebLogic Server instances by which to scale up or down, default=1\n  You can use any of the following tools to configure policies for diagnostic system modules:\n WebLogic Server Administration Console WLST REST JMX application\n  A more in-depth description and example on using WLDF\u0026rsquo;s Policies and Actions component for initiating scaling requests through the operator\u0026rsquo;s REST endpoint can be found in the blogs:\n Automatic Scaling of WebLogic Clusters on Kubernetes WebLogic Dynamic Clusters on Kubernetes  Create cluster role bindings to allow a namespace user to query WLS Kubernetes cluster information The script scalingAction.sh, specified in the WLDF script action above, needs the appropriate RBAC permissions granted for the service account user (in the namespace in which the WebLogic domain is deployed) in order to query the Kubernetes API server for both configuration and runtime information of the domain resource. The following is an example YAML file for creating the appropriate Kubernetes cluster role bindings:\nIn the example cluster role binding definition below, the WebLogic domain is deployed to a namespace weblogic-domain. Replace the namespace value with the name of the namespace in which the WebLogic domain is deployed in your Kubernetes environment.\n kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: weblogic-domain-cluster-role rules: - apiGroups: [\u0026quot;weblogic.oracle\u0026quot;] resources: [\u0026quot;domains\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;update\u0026quot;] --- # # creating role-bindings for cluster role # kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: domain-cluster-rolebinding subjects: - kind: ServiceAccount name: default namespace: weblogic-domain apiGroup: \u0026quot;\u0026quot; roleRef: kind: ClusterRole name: weblogic-domain-cluster-role apiGroup: \u0026quot;rbac.authorization.k8s.io\u0026quot; --- # # creating role-bindings # kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: weblogic-domain-operator-rolebinding namespace: weblogic-operator subjects: - kind: ServiceAccount name: default namespace: weblogic-domain apiGroup: \u0026quot;\u0026quot; roleRef: kind: ClusterRole name: cluster-admin apiGroup: \u0026quot;rbac.authorization.k8s.io\u0026quot; ---  Using a Prometheus alert action to call the operator\u0026rsquo;s REST scale API In addition to using the WebLogic Diagnostic Framework for automatic scaling of a dynamic cluster, you can use a third-party monitoring application like Prometheus. Please read the following blog for details about Using Prometheus to Automatically Scale WebLogic Clusters on Kubernetes.\nHelpful Tips Debugging scalingAction.sh The scalingAction.sh script was designed to be executed within the Administration Server pod because the associated diagnostic module is targed to the Administration Server.\nThe easiest way to verify and debug the scalingAction.sh script is to open a shell on the running Administration Server pod and execute the script on the command line.\nThe following example illustrates how to open a bash shell on a running Administration Server pod named domain1-admin-server and execute the scriptAction.sh script. It assumes that:\n The domain home is in /u01/oracle/user-projects/domains/domain1 (that is, the domain home is inside a Docker image). The Dockerfile copied scalingAction.sh to /u01/oracle/user-projects/domains/domain1/bin/scripts/scalingAction.sh.\n\u0026gt; kubectl exec -it domain1-admin-server /bin/bash # bash\u0026gt; cd /u01/oracle/user-projects/domains/domain1/bin/scripts # bash\u0026gt; ./scalingAction.sh   A log, scalingAction.log, will be generated in the same directory in which the script was executed and can be examined for errors.\nExample on accessing the external REST endpoint The easiest way to test scaling using the external REST endpoint is to use a command-line tool like curl. Using curl to issue an HTTPS scale request requires these mandatory header properties:\n Bearer Authorization token SSL certificate for the operator\u0026rsquo;s external REST endpoint X-Requested-By header value  The following shell script is an example of how to issue a scaling request, with the necessary HTTP request header values, using curl. This example assumes the operator and domain resource are configured with the following properties in Kubernetes:\n Operator properties:  externalRestEnabled: true externalRestHttpsPort: 31001 operator\u0026rsquo;s namespace: weblogic-operator operator\u0026rsquo;s hostname is the same as the host shell script is executed on.  Domain resource properties:\n WebLogic cluster name: DockerCluster Domain UID: domain1\n#!/bin/sh # Setup properties ophost=`uname -n` opport=31001 #externalRestHttpsPort cluster=cluster-1 size=3 #New cluster size ns=weblogic-operator # Operator NameSpace sa=weblogic-operator # Operator ServiceAccount domainuid=domain1 # Retrieve service account name for given namespace sec=`kubectl get serviceaccount ${sa} -n ${ns} -o jsonpath='{.secrets[0].name}'` #echo \u0026quot;Secret [${sec}]\u0026quot; # Retrieve base64 encoded secret for the given service account enc_token=`kubectl get secret ${sec} -n ${ns} -o jsonpath='{.data.token}'` #echo \u0026quot;enc_token [${enc_token}]\u0026quot; # Decode the base64 encoded token token=`echo ${enc_token} | base64 --decode` #echo \u0026quot;token [${token}]\u0026quot; # clean up any temporary files rm -rf operator.rest.response.body operator.rest.stderr operator.cert.pem # Retrieve SSL certificate from the Operator's external REST endpoint `openssl s_client -showcerts -connect ${ophost}:${opport} \u0026lt;/dev/null 2\u0026gt;/dev/null | openssl x509 -outform PEM \u0026gt; operator.cert.pem` echo \u0026quot;Rest EndPoint url https://${ophost}:${opport}/operator/v1/domains/${domainuid}/clusters/${cluster}/scale\u0026quot; # Issue 'curl' request to external REST endpoint curl --noproxy '*' -v --cacert operator.cert.pem \\ -H \u0026quot;Authorization: Bearer ${token}\u0026quot; \\ -H Accept:application/json \\ -H \u0026quot;Content-Type:application/json\u0026quot; \\ -H \u0026quot;X-Requested-By:WLDF\u0026quot; \\ -d \u0026quot;{\\\u0026quot;managedServerCount\\\u0026quot;: $size}\u0026quot; \\ -X POST https://${ophost}:${opport}/operator/v1/domains/${domainuid}/clusters/${cluster}/scale \\ -o operator.rest.response.body \\ --stderr operator.rest.stderr    "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-domains/persistent-storage/",
	"title": "Persistent storage",
	"tags": [],
	"description": "",
	"content": " This document outlines how to set up a Kubernetes persistent volume and persistent volume claim which can be used as storage for WebLogic domain homes and log files. A persistent volume can be shared by multiple WebLogic domains or dedicated to a particular domain.\nPrerequisites The following prerequisites must be fulfilled before proceeding with the creation of the volume:\n Create a Kubernetes namespace for the persistent volume claim unless the intention is to use the default namespace. Note that a persistent volume claim has to be in the same namespace as the domain resource that uses it. Make sure that all the servers in the WebLogic domain are able to reach the storage location. Make sure that the host directory that will be used, already exists and has the appropriate file permissions set.  Storage locations Persistent volumes can point to different storage locations, for example NFS servers or a local directory path. The list of available options is listed in the Kubernetes documentation.\nNote regarding HostPath: In a single-node Kubernetes cluster, such as may be used for testing or proof of concept activities, HOST_PATH provides the simplest configuration. In a multinode Kubernetes cluster, a HOST_PATH that is located on shared storage mounted by all nodes in the Kubernetes cluster is the simplest configuration. If nodes do not have shared storage, then NFS is probably the most widely available option. There are other options listed in the referenced table.\nThe persistent volume for the domain must be created using the appropriate tools before running the script to create the domain. In the simplest case, namely the HOST_PATH provider, this means creating a directory on the Kubernetes master and ensuring that it has the correct permissions:\n$ mkdir -m 777 -p /path/to/domain1PersistentVolume  Note regarding NFS: In the current GA version, the OCI Container Engine for Kubernetes supports network block storage that can be shared across nodes with access permission RWOnce (meaning that only one can write, others can read only). At this time, the WebLogic on Kubernetes domain created by the WebLogic Server Kubernetes Operator, requires a shared file system to store the WebLogic domain configuration, which MUST be accessible from all the pods across the nodes. As a workaround, you need to install an NFS server on one node and share the file system across all the nodes.\nCurrently, we recommend that you use NFS version 3.0 for running WebLogic Server on OCI Container Engine for Kubernetes. During certification, we found that when using NFS 4.0, the servers in the WebLogic domain went into a failed state intermittently. Because multiple threads use NFS (default store, diagnostics store, Node Manager, logging, and domain_home), there are issues when accessing the file store. These issues are removed by changing the NFS to version 3.0.\nPersistent volume GID annotation The HOST_PATH directory permissions can be made more secure by using a Kubernetes annotation on the persistent volume that provides the group identifier (GID) which will be added to pods using the persistent volume.\nFor example, if the GID of the directory is 6789, then the directory can be updated to remove permissions other than for the user and group along with the persistent volume being annotated with the specified GID:\n$ chmod 770 /path/to/domain1PersistentVolume $ kubectl annotate pv domain1-weblogic-sample-pv pv.beta.kubernetes.io/gid=6789  Typically, after the domain is created and servers are running, the group ownership of the persistent volume files can be updated to the specified GID which will provide read access to the group members. Normally files created from a pod onto the persistent volume will have UID 1000 and GID 1000 which is the oracle user from the WebLogic Docker image.\nAn example of updating the group ownership on the persistent volume would be as follows:\n$ cd /path/to/domain1PersistentVolume $ sudo chgrp 6789 applications domains logs stores $ sudo chgrp -R 6789 domains/ $ sudo chgrp -R 6789 logs/  YAML files Persistent volumes and claims are described in YAML files. For each persistent volume, you should create one persistent volume YAML file and one persistent volume claim YAML file. In the example below, you will find two YAML templates, one for the volume and one for the claim. As stated above, they either can be dedicated to a specific domain, or shared across multiple domains. For the use cases where a volume will be dedicated to a particular domain, it is a best practice to label it with weblogic.domainUID=[domain name]. This makes it easy to search for, and clean up resources associated with that particular domain.\nFor sample YAML templates, refer to the Persistent volumes example.\nKubernetes resources After you have written your YAML files, use them to create the persistent volume by creating Kubernetes resources using the kubectl create -f command:\n$ kubectl create -f pv.yaml $ kubectl create -f pvc.yaml  Verify the results To confirm that the persistent volume was created, use these commands:\n$ kubectl describe pv [persistent volume name] $ kubectl describe pvc -n NAMESPACE [persistent volume claim name]  Common problems This section provides details of common problems that might occur while running the script and how to resolve them.\nPersistent volume provider not configured correctly Possibly the most common problem experienced during testing was the incorrect configuration of the persistent volume provider. The persistent volume must be accessible to all Kubernetes nodes, and must be able to be mounted as Read/Write/Many. If this is not the case, the persistent volume creation will fail.\nThe simplest case is where the HOST_PATH provider is used. This can be either with one Kubernetes node, or with the HOST_PATH residing in shared storage available at the same location on every node (for example, on an NFS mount). In this case, the path used for the persistent volume must have its permission bits set to 777.\nFurther reading  See the blog, How to run WebLogic clusters on the Oracle Cloud Infrastructure Container Engine for Kubernetes.  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-operators/",
	"title": "Manage operators",
	"tags": [],
	"description": "Helm is used to create and deploy necessary operator resources and to run the operator in a Kubernetes cluster. Use the operator&#39;s Helm chart to install and manage the operator.",
	"content": " Overview Helm is a framework that helps you manage Kubernetes applications, and Helm charts help you define and install Helm applications into a Kubernetes cluster. The operator\u0026rsquo;s Helm chart is located in the kubernetes/charts/weblogic-operator directory.\nImportant note for users of operator releases before 2.0   Click here to expand   If you have an older version of the operator installed on your cluster, for example, a 1.x version or one of the 2.0 release candidates, then you must remove it before installing this version. This includes the 2.0-rc1 version; it must be completely removed. You should remove the deployment (for example, kubectl delete deploy weblogic-operator -n your-namespace) and the custom resource definition (for example, kubectl delete crd domain). If you do not remove the custom resource definition, then you might see errors like this:\nError from server (BadRequest): error when creating \u0026quot;/scratch/output/uidomain/weblogic-domains/uidomain/domain.yaml\u0026quot;: the API version in the data (weblogic.oracle/v2) does not match the expected API version (weblogic.oracle/v1    \nInstall Helm and Tiller Helm has two parts: a client (Helm) and a server (Tiller). Tiller runs inside of your Kubernetes cluster, and manages releases (installations) of your charts. See https://github.com/kubernetes/helm/blob/master/docs/install.md for detailed instructions on installing Helm and Tiller.\nIn order to use Helm to install and manage the operator, you need to ensure that the service account that Tiller uses has the cluster-admin role. The default would be default in namespace kube-system. You can give that service account the necessary permissions with this command:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: helm-user-cluster-admin-role roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: default namespace: kube-system EOF  Oracle strongly recommends that you create a new service account to be used exclusively by Tiller and grant cluster-admin to that service account, rather than using the default one.\n Operator\u0026rsquo;s Helm Chart Configuration The operator Helm chart is pre-configured with default values for the configuration of the operator.\nYou can override these values by doing one of the following:\n Creating a custom YAML file with only the values to be overridden, and specifying the --value option on the Helm command line. Overriding individual values directly on the Helm command line, using the --set option.  You can find out the configuration values that the Helm chart supports, as well as the default values, using this command:\n$ helm inspect values kubernetes/charts/weblogic-operator  The available configuration values are explained by category in Operator Helm configuration values.\nHelm commands are explained in more detail in Useful Helm operations.\nOptional: Configure the operator\u0026rsquo;s external REST HTTPS interface The operator can expose an external REST HTTPS interface which can be accessed from outside the Kubernetes cluster. As with the operator\u0026rsquo;s internal REST interface, the external REST interface requires an SSL/TLS certificate and private key that the operator will use as the identity of the external REST interface (see below).\nTo enable the external REST interface, configure these values in a custom configuration file, or on the Helm command line:\n Set externalRestEnabled to true. Set externalRestIdentitySecret to the name of the kubernetes tls secret that contains the certificate(s) and private key. Optionally, set externalRestHttpsPort to the external port number for the operator REST interface (defaults to 31001).  For more detailed information, see the REST interface configuration values.\nSample SSL certificate and private key for the REST interface For testing purposes, the WebLogic Kubernetes Operator project provides a sample script that generates a self-signed certificate and private key for the operator external REST interface. The generated certificate and key is stored in a Kubernetes tls secret and the sample script outputs the corresponding configuration values in YAML format. These values can be added to your custom YAML configuration file, for use when the operator\u0026rsquo;s Helm chart is installed.\nThe sample script should not be used in a production environment because typically a self-signed certificate for external communucation is not considered safe. A certficate signed by a commercial certificate authority is more widely accepted and should contain valid host names, expiration dates and key constraints.\n For more detailed information about the sample script and how to run it, see the REST APIs in the Samples section.\nOptional: Elastic Stack (Elasticsearch, Logstash, and Kibana) integration The operator Helm chart includes the option of installing the necessary Kubernetes resources for Elastic Stack integration.\nYou are responsible for configuring Kibana and Elasticsearch, then configuring the operator Helm chart to send events to Elasticsearch. In turn, the operator Helm chart configures Logstash in the operator deployment to send the operator\u0026rsquo;s log contents to that Elasticsearch location.\nElastic Stack per-operator configuration As part of the Elastic Stack integration, Logstash configuration occurs for each deployed operator instance. You can use the following configuration values to configure the integration:\n Set elkIntegrationEnabled is true to enable the integration. Set logStashImage to override the default version of Logstash to be used (logstash:6.2). Set elasticSearchHost and elasticSearchPort to override the default location where Elasticsearch is running (elasticsearch2.default.svc.cluster.local:9201). This will configure Logstash to send the operator\u0026rsquo;s log contents there.  For more detailed information, see the Operator Helm configuration values.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/overview/database/",
	"title": "Run a database",
	"tags": [],
	"description": "",
	"content": " Run the Oracle database in Kubernetes If you wish to run the Oracle database inside your Kubernetes cluster, in order to place your state store, leasing tables, and such, in that database, then you can use this sample to install the database.\nYou must configure your database to store its DB files on persistent storage. Refer to your cloud vendor\u0026rsquo;s documentation for details of available storage providers and how to create a persistent volume and attach it to a pod.\nFirst create a namespace for the database:\nkubectl create namespace database-namespace  Next, create a file called database.yml with the following content. Make sure you update the password field with your chosen administrator password for the database.\napiVersion: extensions/v1beta1 kind: Deployment metadata: name: database namespace: database-namespace labels: app: database version: 12.1.0.2 spec: replicas: 1 selector: matchLabels: app: database version: 12.1.0.2 template: metadata: name: database labels: app: database version: 12.1.0.2 spec: volumes: - name: dshm emptyDir: medium: Memory # add your volume mount for your persistent storage here containers: - name: database command: - /home/oracle/setup/dockerInit.sh image: container-registry.oracle.com/database/enterprise:12.1.0.2 imagePullPolicy: IfNotPresent resources: requests: memory: 10Gi ports: - containerPort: 1521 hostPort: 1521 volumeMounts: - mountPath: /dev/shm name: dshm # add your persistent storage for DB files here env: - name: DB_SID value: OraDoc - name: DB_PDB value: OraPdb - name: DB_PASSWD value: *password* - name: DB_DOMAIN value: my.domain.com - name: DB_BUNDLE value: basic - name: DB_MEMORY value: 8g imagePullSecrets: - name: regsecret --- apiVersion: v1 kind: Service metadata: name: database namespace: database-namespace spec: selector: app: database version: 12.1.0.2 ports: - protocol: TCP port: 1521 targetPort: 1521  If you have not previously done so, you will need to go to the Oracle Container Registry and accept the license for the Oracle database image.\nCreate a Docker registry secret so that Kubernetes can pull the database image:\nkubectl create secret docker-registry regsecret \\ --docker-server=container-registry.oracle.com \\ --docker-username=your.email@some.com \\ --docker-password=your-password \\ --docker-email=your.email@some.com \\ -n database-namespace  Now use the following command to install the database:\nkubectl apply -f database.yml  This will start up the database and expose it in the cluster at the following address:\ndatabase.database-namespace.svc.cluster.local:1521  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/reference/domain-resource/",
	"title": "Domain resource",
	"tags": [],
	"description": "Use this document to set up and configure your own domain resource.",
	"content": "View a domain resource reference document here.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/quickstart/get-images/",
	"title": "Get images",
	"tags": [],
	"description": "",
	"content": " Get these images and put them into your local registry.  If you don\u0026rsquo;t already have one, obtain a Docker store account, log in to the Docker store and accept the license agreement for the WebLogic Server image.\n Log in to the Docker store from your Docker client:\n$ docker login   Pull the operator image:  $ docker pull oracle/weblogic-kubernetes-operator:2.1  Pull the Traefik load balancer image:\n$ docker pull traefik:1.7.6   Pull the WebLogic 12.2.1.3 install image:  $ docker pull store/oracle/weblogic:12.2.1.3  The existing WebLogic Docker image, store/oracle/weblogic:12.2.1.3, was updated on January 17, 2019, and has all the necessary patches applied; a docker pull is required if you pulled the image prior to that date.\n  Copy the image to all the nodes in your cluster, or put it in a Docker registry that your cluster can access.\n  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/samples/",
	"title": "Samples",
	"tags": [],
	"description": "",
	"content": "The samples provide demonstrations of how to accomplish common tasks. These samples are provided for educational and demonstration purposes only; they are not intended to be used in production deployments or to be depended upon to create production environments.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/cicd/mutate-the-domain-layer/",
	"title": "Mutate the domain layer",
	"tags": [],
	"description": "How to mutate the domain layer.",
	"content": "If you need to mutate the domain layer, and keep the same domain encryption keys, then there are some choices about how to implement that, as alluded to previously. Let\u0026rsquo;s explore those in some more detail now.\nThe first option is to implement each mutation as a delta to the previous state. This is conceptually similar to how immutable objects (like Java Strings) are implemented, a \u0026ldquo;copy on write\u0026rdquo; approach applied to the domain configuration as a unit. This does have the advantage that it is simple to implement, but the disadvantage that your builds would depend on the previous good build and this is somewhat contrary to typical CI/CD practices. You also have to work out what to do with the bad builds, or \u0026ldquo;holes\u0026rdquo; in the sequence.\nAn alternative is to capture a \u0026ldquo;primordial state\u0026rdquo; of the domain before starting the sequence. In practical terms, this might mean creating a very simple domain with no applications or resources in it, and \u0026ldquo;saving\u0026rdquo; it before ever starting any servers. This primordial domain (let’s call it t=0) would then be used to build each mutation. So each state is built from t=0, plus all of the changes up to that point.\nSaid another way, each build would start with t=0 as the base image and extend it. This eliminates the need to keep each intermediate state, and would also likely have benefits when you remove things from the domain, because you would not have \u0026ldquo;lost\u0026rdquo; (\u0026ldquo;whited out\u0026rdquo; is the Docker layer term) space in the intermediate layers. Although, these layers tend to be relatively small, so this is possibly not a big issue.\nThis approach is probably an improvement. It does get interesting though when you update a lower layer, for example when you patch WebLogic or update the JDK. When this happens, you need to create another base image, shown in the diagram as v2 t-0. All of the mutations in this new chain are based on this new base image. So that still leaves us with the problem of how to take the domain from the first series (v1 t=0 to t=3) and \u0026ldquo;copy\u0026rdquo; it across to the second series (v2).\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/introduction/design/",
	"title": "Design philosophy",
	"tags": [],
	"description": "The Oracle WebLogic Server Kubernetes Operator (the “operator”) is designed to fulfill a similar role to that which a human operator would fill in a traditional data center deployment.  It contains a set of useful built-in knowledge about how to perform various life cycle operations on a domain correctly.",
	"content": "The Oracle WebLogic Server Kubernetes Operator (the “operator”) is designed to fulfill a similar role to that which a human operator would fill in a traditional data center deployment. It contains a set of useful built-in knowledge about how to perform various life cycle operations on a domain correctly.\nHuman operators are normally responsible for starting and stopping environments, initiating backups, performing scaling operations, performing manual tasks associated with disaster recovery and high availability needs and coordinating actions with other operators in other data centers. It is envisaged that the operator will have similar responsibilities in a Kubernetes environment.\nIt is important to note the distinction between an operator and an administrator. A WebLogic Server administrator typically has different responsibilities centered around managing the detailed configuration of the WebLogic domains. The operator has only limited interest in the domain configuration, with its main concern being the high-level topology of the domain; for example, how many clusters and servers, and information about network access points, such as channels.\nHuman operators may manage more than one domain, and the operator is also designed to be able to manage more than one domain. Like its human counterpart, the operator will only take actions against domains that it is told to manage, and will ignore any other domains that may be present in the same environment.\nLike a human operator, the operator is designed to be event-based. It waits for a significant event to occur, or for a scheduled time to perform some action, and then takes the appropriate action. Examples of significant events include being made aware of a new domain that needs to be managed, receiving a request to scale up a WebLogic cluster, or receiving a request to perform a backup of a domain.\nThe operator is designed with security in mind from the outset. Some examples of the specific security practices we follow are:\n During the deployment of the operator, Kubernetes roles are defined and assigned to the operator. These roles are designed to give the operator the minimum amount of privileges that it requires to perform its tasks. The code base is regularly scanned with security auditing tools and any issues that are identified are promptly resolved. All HTTP communications – between the operator and an external client, between the operator and WebLogic Administration Servers, and so on – are configured to require SSL and TLS 1.2. Unused code is pruned from the code base regularly. Dependencies are kept as up-to-date as possible and are regularly reviewed for security vulnerabilities.  The operator is designed to avoid imposing any arbitrary restriction on how WebLogic Server may be configured or used in Kubernetes. Where there are restrictions, these are based on the availability of some specific feature in Kubernetes; for example, multicast support.\nThe operator learns of WebLogic domains through instances of a domain Kubernetes resource. When the operator is installed, it creates a Kubernetes Custom Resource Definition. This custom resource definition defines the domain resource type. After this type is defined, you can manage domain resources using kubectl just like any other resource type. For instance, kubectl get domain or kubectl edit domain domain1.\nSchema for domain resources is here.\nThe schema for the domain resource is designed to be as sparse as possible. It includes the connection details for the Administration Server, but all of the other content is operational details about which servers should be started, environment variables, and details about what should be exposed outside the Kubernetes cluster. This way, the WebLogic domain\u0026rsquo;s configuration remains the normative configuration.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/security/service-accounts/",
	"title": "Service accounts",
	"tags": [],
	"description": "Kubernetes service accounts for the WebLogic operator",
	"content": " WebLogic operator service account When the WebLogic operator is installed, the Helm chart property, serviceAccount, can be specified where the value contains the name of the Kubernetes ServiceAccount in the namespace in which the WebLogic operator will be installed. For more information about the Helm chart, see the operator Helm configuration values.\nThe WebLogic operator will use this ServiceAccount when calling the Kubernetes API server and the appropriate access controls will be created for this ServiceAccount by the operator\u0026rsquo;s Helm chart.\nFor more information about access controls, see RBAC under Security.\n In order to display the ServiceAccount used by the WebLogic operator, where the operator was installed using the Helm release name weblogic-operator, look for the serviceAccount value using the Helm command:\n$ helm get values --all weblogic-operator  Additional reading  Helm service account Operator Helm chart service account configuration  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/samples/simple/domains/delete-domain/",
	"title": "Delete domain resources",
	"tags": [],
	"description": "Delete the domain resources created while executing the samples.",
	"content": " After running the samples, you will need to release domain resources that can then be used for other purposes. The script in this sample demonstrates one approach to releasing domain resources.\nUse this script to delete domain resources $ ./delete-weblogic-domain-resources.sh \\ -d domain-uid[,domain-uid...] \\ [-s max-seconds] \\ [-t]  The required option -d takes domain-uid values (separated by commas and no spaces) to identify the domain resources that should be deleted.\nTo limit the amount of time spent on attempting to delete domain resources, use -s. The option must be followed by an integer that represents the total number of seconds that will be spent attempting to delete resources. The default number of seconds is 120.\nThe optional option -t shows what the script will delete without executing the deletion.\nTo see the help associated with the script:\n$ ./delete-weblogic-domain-resources.sh -h  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/samples/simple/rest/",
	"title": "REST APIs",
	"tags": [],
	"description": "Sample for generating a self-signed certificate and private key that can be used for the operator&#39;s external REST API.",
	"content": " Sample to create certificate and key When a user enables the operator\u0026rsquo;s external REST API (by setting externalRestEnabled to true when installing or upgrading the operator Helm chart), the user also needs to provide the certificate(s) and private key used for the SSL/TLS identity on the external REST API endpoint by creating a kubernetes tls secret and using that secret\u0026rsquo;s name with the operator Helm chart values.\nThis sample script generates a self-signed certificate and private key that can be used for the operator\u0026rsquo;s external REST API when experimenting with the operator.\nThe certificate and key generated with this script should not be used in a production environment.\n The syntax of the script is:\n$ kubernetes/samples/scripts/rest/generate-external-rest-identity.sh \\ -a \u0026lt;SANs\u0026gt; -n \u0026lt;operator-namespace\u0026gt; [-s \u0026lt;secret-name\u0026gt;]  Where \u0026lt;SANs\u0026gt; lists the subject alternative names to put into the generated self-signed certificate for the external WebLogic Operator REST HTTPS interface, \u0026lt;operator-namespace\u0026gt; should match the namespace where the operator will be installed, and optionally the secret name, which defaults to weblogic-operator-external-rest-identity.\nYou should include the addresses of all masters and load balancers (i.e. what a client specifies to access the external REST endpoint) in the subject alternative name list. In addition, each name must be prefaced by DNS: for a host name, or IP: for an address, as with this example:\n-a \u0026quot;DNS:myhost,DNS:localhost,IP:127.0.0.1\u0026quot;  The external certificate and key can be changed after installation of the operator. For more information, see updating operator external certificate in the Security section.\nThe script as used below will create the tls secret named weblogic-operator-identity in the namespace weblogic-operator-ns using a self-signed certificate and private key:\n$ echo \u0026quot;externalRestEnabled: true\u0026quot; \u0026gt; my_values.yaml $ generate-external-rest-identity.sh \\ -a \u0026quot;DNS:${HOSTNAME},DNS:localhost,IP:127.0.0.1\u0026quot; \\ -n weblogic-operator-ns -s weblogic-operator-identity \u0026gt;\u0026gt; my_values.yaml # $ kubectl -n weblogic-operator-ns describe secret weblogic-operator-identity # $ helm install kubernetes/charts/weblogic-operator --name my_operator \\ --namespace weblogic-operator-ns --values my_values.yaml --wait  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/developerguide/integration-tests/",
	"title": "Integration tests",
	"tags": [],
	"description": "",
	"content": "The project includes integration tests that can be run against a Kubernetes cluster. If you want to use these tests, you will need to provide your own Kubernetes cluster. The Kubernetes cluster must meet the version number requirements and have Helm installed. Ensure that the operator Docker image is in a Docker registry visible to the Kubernetes cluster.\nYou will need to obtain the kube.config file for an administrative user and make it available on the machine running the build. To run the tests, update the KUBECONFIG environment variable to point to your config file and then execute:\n$ mvn clean verify -P java-integration-tests  When you run the integrations tests, they do a cleanup of any operator or domains on that cluster.\n "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/developerguide/branching/",
	"title": "Branching",
	"tags": [],
	"description": "",
	"content": "The master branch is protected and contains source for the most recently published release, including release candidates.\nThe develop branch is protected and contains source for the latest completed features and bug fixes. While this branch contains active work, we expect to keep it always \u0026ldquo;ready to release.\u0026rdquo; Therefore, longer running feature work will be performed on specific branches, such as feature/dynamic-clusters.\nBecause we want to balance separating destabilizing work into feature branches against the possibility of later difficult merges, we encourage developers working on features to pull out any necessary refactoring or improvements that are general purpose into their own shorter-lived branches and create pull requests to develop when these smaller work items are completed.\nAll commits to develop must pass the integration test suite. Please run these tests locally before submitting a pull request. Additionally, each push to a branch in our GitHub repository triggers a run of a subset of the integration tests with the results visible here.\nPlease submit pull requests to the develop branch unless you are collaborating on a feature and have another target branch. Please see details on the Oracle Contributor Agreement (OCA) and guidelines for pull requests on the README.\nWe will create git tags for each release candidate and generally available (GA) release of the operator.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-domains/domain-in-image/",
	"title": "Domain in image",
	"tags": [],
	"description": "",
	"content": "  Base images  Creating or obtaining WebLogic Docker images.\n Oracle strongly recommends storing a domain image as private in the registry. A Docker image that contains a WebLogic domain home has sensitive information including keys and credentials that are used to access external resources (for example, data source password). For more information, see domain home in image protection in the Security section.\n "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-domains/",
	"title": "Manage domains",
	"tags": [],
	"description": "Important considerations for WebLogic domains in Kubernetes.",
	"content": " Contents  Important considerations for WebLogic domains in Kubernetes Creating and managing WebLogic domains Modifying domain configurations About the domain resource Managing life cycle operations Scaling clusters  Important considerations for WebLogic domains in Kubernetes Please be aware of the following important considerations for WebLogic domains running in Kubernetes:\n Domain Home Location: The WebLogic domain home location is determined by the domain resource domainHome if set; otherwise, a default location is determined by the domainHomeInImage setting. If a domain resource domainHome field is not set and domainHomeInImage is true (the default), then the operator will assume that the domain home is a directory under /u01/oracle/user_projects/domains/ and report an error if no domain is found or more than one domain is found. If a domain resource domainHome field is not set and domainHomeInImage is false, then the operator will assume that the domain home is /shared/domains/DOMAIN_UID. Oracle strongly recommends storing an image containing a WebLogic domain home as private in the registry (for example, Oracle Cloud Infrastructure Registry, Docker Hub, and such). A Docker image that contains a WebLogic domain has sensitive information including keys and credentials that are used access external resources (for example, data source password). For more information, see domain home in image protection in the Security section.\n  Log File Locations: The operator can automatically override WebLogic domain and server log locations using situational configuration overrides. This occurs if the domain resource logHomeEnabled field is explicitly set to true, or if logHomeEnabled isn\u0026rsquo;t set and domainHomeInImage is explicitly set to false. When overriding, the log location will be the location specified by the logHome setting.\n Listen Address Overrides: The operator will automatically override all WebLogic domain default, SSL, admin, or custom channel listen addresses (using situational configuration overrides). These will become domainUID followed by a hyphen and then the server name, all lower case, and underscores converted to hyphens. For example, if domainUID=domain1 and the WebLogic server name is Admin_Server, then its listen address becomes domain1-admin-server.\n Domain, Cluster, Server, and Network-Access-Point Names: WebLogic domain, cluster, server, and network-access-point (channel) names must contain only the characters A-Z, a-z, 0-9, -, or _. This ensures that they can be converted to meet Kubernetes resource and DNS1123 naming requirements. (When generating pod and service names, the operator will convert configured names to lower case and substitute a hyphen (-) for each underscore (_).)\n Node Ports: If you choose to expose any WebLogic channels outside the Kubernetes cluster via a NodePort, for example, the administration port or a T3 channel to allow WLST access, you need to ensure that you allocate each channel a unique port number across the entire Kubernetes cluster. If you expose the administration port in each WebLogic domain in the Kubernetes cluster, then each one must have a different port. This is required because NodePorts are used to expose channels outside the Kubernetes cluster.\nExposing admin, RMI, or T3 capable channels via a Kubernetes NodePort can create an insecure configuration. In general, only HTTP protocols should be made available externally and this exposure is usually accomplished by setting up an external load balancer that can access internal (non-NodePort) services. For more information, see T3 channels in the Security section.\n  Host Path Persistent Volumes: If using a hostPath persistent volume, then it must be available on all worker nodes in the cluster and have read/write/many permissions for all container/pods in the WebLogic Server deployment. Be aware that many cloud provider\u0026rsquo;s volume providers may not support volumes across availability zones. You may want to use NFS or a clustered file system to work around this limitation.\n Security Note: The USER_MEM_ARGS environment variable defaults to -Djava.security.egd=file:/dev/./urandom in all WebLogic Server pods and the WebLogic introspection job. It can be explicitly set to another value in your domain resource YAML file using the env attribute under the serverPod configuration.\n  The following features are not certified or supported in this release:\n Whole server migration Consensus leasing Node Manager (although it is used internally for the liveness probe and to start WebLogic Server instances) Multicast Multitenancy Production redeployment  Please consult My Oracle Support Doc ID 2349228.1 for up-to-date information about the features of WebLogic Server that are supported in Kubernetes environments.\nCreating and managing WebLogic domains You can locate a WebLogic domain either in a persistent volume (PV) or in a Docker image. For examples of each, see the WebLogic operator samples.\nIf you want to create your own Docker images, for example, to choose a specific set of patches or to create a domain with a specific configuration and/or applications deployed, then you can create the domain custom resource manually to deploy your domain. This process is documented in this sample.\nModifying domain configurations You can modify the WebLogic domain configuration for both the \u0026ldquo;domain in persistent volume\u0026rdquo; and the \u0026ldquo;domain in image\u0026rdquo; options before deploying a domain resource:\n When the domain is in a persistent volume, you can use WLST or WDT to change the configuration. For either case, you can use configuration overrides.\n  Configuration overrides allow changing a configuration without modifying its original config.xml or system resource XML files, and also support parameterizing overrides so that you can inject values into them from Kubernetes secrets. For example, you can inject database user names, passwords, and URLs that are stored in a secret.\nAbout the domain resource For information about the domain resource, see Domain resource.\nManaging life cycle operations You can perform life cycle operations on WebLogic servers, clusters, or domains. See Starting, stopping, and restarting servers and Restarting WebLogic servers.\nScaling clusters The operator let\u0026rsquo;s you initiate scaling of clusters in various ways:\n Using kubectl to edit the domain resource Using the operator\u0026rsquo;s REST APIs Using WLDF policies Using a Prometheus action  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/quickstart/install/",
	"title": "Install the operator and load balancer",
	"tags": [],
	"description": "",
	"content": " Grant the Helm service account the cluster-admin role. $ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: helm-user-cluster-admin-role roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: default namespace: kube-system EOF  Create a Traefik (Ingress-based) load balancer. Use helm to install the Traefik load balancer. Use the values.yaml in the sample but set kubernetes.namespaces specifically.\n$ helm install stable/traefik \\ --name traefik-operator \\ --namespace traefik \\ --values kubernetes/samples/charts/traefik/values.yaml \\ --set \u0026quot;kubernetes.namespaces={traefik}\u0026quot; \\ --wait  Install the operator.  Create a namespace for the operator:\n$ kubectl create namespace sample-weblogic-operator-ns   Create a service account for the operator in the operator\u0026rsquo;s namespace:  $ kubectl create serviceaccount -n sample-weblogic-operator-ns sample-weblogic-operator-sa  Use helm to install and start the operator from the directory you just cloned:\n$ helm install kubernetes/charts/weblogic-operator \\ --name sample-weblogic-operator \\ --namespace sample-weblogic-operator-ns \\ --set image=oracle/weblogic-kubernetes-operator:2.1 \\ --set serviceAccount=sample-weblogic-operator-sa \\ --set \u0026quot;domainNamespaces={}\u0026quot; \\ --wait   Verify that the operator\u0026rsquo;s pod is running, by listing the pods in the operator\u0026rsquo;s namespace. You should see one for the operator.  $ kubectl get pods -n sample-weblogic-operator-ns  Verify that the operator is up and running by viewing the operator pod\u0026rsquo;s log:\n$ kubectl logs -n sample-weblogic-operator-ns -c weblogic-operator deployments/weblogic-operator   "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/developerguide/",
	"title": "Developer Guide",
	"tags": [],
	"description": "",
	"content": " Developer Guide The Developer Guide provides information for developers who want to understand or contribute to the code.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/cicd/how-to-copy-domains/",
	"title": "Copy domains",
	"tags": [],
	"description": "How to copy domains.",
	"content": "The recommended approach to save a copy of a domain is to simply ZIP (or tar) the domain directory. However, there is a very important caveat with this recommendation - when you unzip the domain, it must go back into exactly the same location (Domain Home) in the (new) file system. Using this approach will maintain the same domain encryption key.\nThe best practice/recommended approach is to create a \u0026ldquo;primordial domain\u0026rdquo; which does not contain any applications or resources, and to create a ZIP file of this domain before starting any servers.\n The domain ZIP must be created before starting servers.\n When servers are started the first time, they will encrypt various other data. Make sure that you create the ZIP file before starting servers for the first time. The primordial domain ZIP file should be stored in a safe place where the CI/CD can get it when needed, for example in a secured Artifactory repository (or something similar).\nRemember, anyone who gets access to this ZIP file can get access to the domain encryption key, so it needs to be protected appropriately.\n Every time you run your CI/CD pipeline to create a new mutation of the domain, it should retrieve and unzip the primordial domain first, and then apply changes to that domain using tools like WDT or WLST (see here).\n Always use external state.\n You should always keep state outside the Docker image. This means that you should use JDBC stores for leasing tables, JMS and Transaction stores, EJB timers, JMS queues, and so on. This ensures that data will not be lost when a container is destroyed.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/cicd/",
	"title": "CI/CD considerations",
	"tags": [],
	"description": "Learn about managing domain images with continuous integration and continuous delivery (CI/CD).",
	"content": " Overview In this section, we will discuss the recommended techniques for managing the evolution and mutation of Docker images to run WebLogic in Kubernetes. There are several approaches and techniques available, and the choice of which to use depends very much on your particular requirements. We will start with a review of the \u0026ldquo;problem space,\u0026rdquo; and then talk about the considerations that would lead us to choose various approaches. We will provide details about several approaches to implementing CI/CD and links to examples.\nReview of the problem space Kubernetes makes a fundamental assumption that Docker images are immutable, that they contain no state, and that updating them is as simple as throwing away a pod/container and replacing it with a new one that uses a newer version of the Docker image. These assumptions work very well for microservices applications, but for more traditional workloads, we need to do some extra thinking and some extra work to get the behavior we want.\nCI/CD is an area where the standard assumptions aren\u0026rsquo;t always suitable. In the microservices architecture, you typically minimize dependencies and build images from scratch with all of the dependencies in them. You also typically keep all of the configuration outside of the image, for example, in Kubernetes config maps or secrets, and all of the state outside of the image too. This makes it very easy to update running pods with a new image.\nLet\u0026rsquo;s consider how a WebLogic image is different. There will, of course, be a base layer with the operating system; let\u0026rsquo;s assume it is Oracle Linux \u0026ldquo;slim\u0026rdquo;. Then you need a JDK and this is very commonly in another layer. Many people will use the officially supported JDK images from the Docker Store, like the Server JRE image, for example. On top of this, you need the WebLogic Server binaries (the \u0026ldquo;Oracle Home\u0026rdquo;). On top of that, you may wish to have some patches or updates installed. And then you need your domain, that is the configuration.\nThere is also other information associated with a domain that needs to live somewhere, for example leasing tables, message and transaction stores, and so on. We recommend that these be kept in a database to take advantage of built-in database server HA, and the fact that disaster recovery of sites across all but the shortest distances almost always requires using a single database server to consolidate and replicated data (DataGuard).\nThere are two common approaches on how to structure these components. The first, which we call \u0026ldquo;domain on persistent volume,\u0026rdquo; places the JDK and WebLogic binaries in the Docker image, but the domain is kept on a separate persistent storage outside of the image. The second approach puts the JDK, WebLogic binaries and the domain all in the Docker image. Both of these approaches are perfectly valid (and fully supported) and they have various advantages and disadvantages.\nWe have listed the relative advantages of these two approaches here.\nOne of the key differences between these approaches is how many Docker images you have, and therefore, how you build and maintain them - your image CI/CD process. Let\u0026rsquo;s take a short detour and talk about Docker image layering.\n Docker image layering  Learn about Docker image layering and why it is important.\n Why layering matters  Learn why Docker image layering affects CI/CD processes.\n Choose an approach  How to choose an approach.\n Mutate the domain layer  How to mutate the domain layer.\n Copy domains  How to copy domains.\n Tools  Tools that are available to build CI/CD pipelines.\n "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/samples/simple/ingress/",
	"title": "Ingress",
	"tags": [],
	"description": "Load balancer sample scripts.",
	"content": "The Oracle WebLogic Server Kubernetes Operator supports three load balancers: Traefik, Voyager, and Apache. We provide samples that demonstrate how to install and configure each one. The samples are located in following folders:\n traefik voyager apache-samples/custom-sample apache-samples/default-sample ingress-per-domain apache-webtier  The apache-webtier script contains a Helm chart that is used in the Apache samples.\n "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/developerguide/coding-standards/",
	"title": "Coding standards",
	"tags": [],
	"description": "",
	"content": " This project has adopted the following coding standards:\n Code will be formated using Oracle / WebLogic standards, which are identical to the Google Java Style. Javadoc must be provided for all public packages, classes, and methods, and must include all parameters and returns. Javadoc is not required for methods that override or implement methods that are already documented. All non-trivial methods should include LOGGER.entering() and LOGGER.exiting() calls. The LOGGER.exiting() call should include the value that is going to be returned from the method, unless that value includes a credential or other sensitive information. All logged messages must be internationalized using the resource bundle src/main/resources/Operator.properties and using a key itemized in src/main/java/oracle/kubernetes/operator/logging/MessageKeys.java. After operator initialization, all operator work must be implemented using the asynchronous call model (described below). In particular, worker threads must not use sleep() or IO or lock-based blocking methods.  Code formatting plugins The following IDE plugins are available to assist with following the code formatting standards\nIntelliJ An IntelliJ plugin is available from the plugin repository.\nThe plugin will be enabled by default. To disable it in the current project, go to File \u0026gt; Settings... \u0026gt; google-java-format Settings (or IntelliJ IDEA \u0026gt; Preferences... \u0026gt; Other Settings \u0026gt; google-java-format Settings on macOS) and uncheck the \u0026ldquo;Enable google-java-format\u0026rdquo; checkbox.\nTo disable it by default in new projects, use File \u0026gt; Other Settings \u0026gt; Default Settings....\nWhen enabled, it will replace the normal \u0026ldquo;Reformat Code\u0026rdquo; action, which can be triggered from the \u0026ldquo;Code\u0026rdquo; menu or with the Ctrl-Alt-L (by default) keyboard shortcut.\nThe import ordering is not handled by this plugin, unfortunately. To fix the import order, download the IntelliJ Java Google Style file and import it into File→Settings→Editor→Code Style.\nEclipse An Eclipse plugin can be downloaded from the releases page. Drop it into the Eclipse drop-ins folder to activate the plugin.\nThe plugin adds a google-java-format formatter implementation that can be configured in Eclipse \u0026gt; Preferences \u0026gt; Java \u0026gt; Code Style \u0026gt; Formatter \u0026gt; Formatter Implementation.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/security/rbac/",
	"title": "RBAC",
	"tags": [],
	"description": "Role based authorization for the WebLogic operator",
	"content": " Contents  Overview Operator RBAC definitions  Role and role binding naming convention Cluster role and cluster role binding naming convention  Role bindings Cluster role bindings  Overview The operator assumes that certain Kubernetes roles are created in the Kubernetes cluster. The operator Helm chart creates the required cluster roles, cluster role bindings, roles and role bindings for the ServiceAccount that is used by the operator. The operator will also attempt to verify that the RBAC settings are correct when the operator starts running.\nFor more information about the Kubernetes ServiceAccount used by the operator, see Service Accounts under Security.\n The general design goal is to provide the operator with the minimum amount of permissions that the operator requires and to favor built-in roles over custom roles where it make sense to use the Kubernetes built-in roles.\nFor more information about Kubernetes roles, see the Kubernetes RBAC documentation.\n Operator RBAC definitions To display the Kubernetes roles and related bindings used by the WebLogic operator where the operator was installed using the Helm release name weblogic-operator, look for the Kubernetes objects:\n Role RoleBinding ClusterRole ClusterRoleBinding  when using the Helm status command:\n$ helm status weblogic-operator  Assuming the operator was installed into the namespace weblogic-operator-ns with a target namespaces of domain1-ns, the following commands can be used to display a subset of the Kubernetes roles and related role bindings:\n$ kubectl describe clusterrole \\ weblogic-operator-ns-weblogic-operator-clusterrole-general $ kubectl describe clusterrolebinding \\ weblogic-operator-ns-weblogic-operator-clusterrolebinding-general $ kubectl -n weblogic-operator-ns \\ describe role weblogic-operator-role $ kubectl -n domain1-ns \\ describe rolebinding weblogic-operator-rolebinding-namespace \\  Kubernetes role and role binding naming convention The following naming pattern is used for the Role and RoleBinding objects:\n weblogic-operator-\u0026lt;type\u0026gt;-\u0026lt;optional-role-name\u0026gt;  Using:\n \u0026lt;type\u0026gt; as the kind of Kubernetes object:  role rolebinding  \u0026lt;optional-role-name\u0026gt; as an optional name given to the role or role binding  For example: namespace   A complete name for an operator created Kubernetes RoleBinding would be:\n weblogic-operator-rolebinding-namespace\n Kubernetes cluster role and cluster role binding naming convention The following naming pattern is used for the ClusterRole and ClusterRoleBinding objects:\n \u0026lt;operator-ns\u0026gt;-weblogic-operator-\u0026lt;type\u0026gt;-\u0026lt;role-name\u0026gt;  Using:\n \u0026lt;operator-ns\u0026gt; as the namespace the operator is installed in  For example: weblogic-operator-ns  \u0026lt;type\u0026gt; as the kind of Kubernetes object:  clusterrole clusterrolebinding  \u0026lt;role-name\u0026gt; as the name given to the role or role binding  For example: general   A complete name for an operator created Kubernetes ClusterRoleBinding would be:\n weblogic-operator-ns-weblogic-operator-clusterrolebinding-general\n Role bindings Assuming that the WebLogic operator was installed into the Kubernetes namespace weblogic-operator-ns, and a target namespace for the operator is domain1-ns, the following RoleBinding entries are mapped to a Role or ClusterRole granting permission to the operator.\n   Role Binding Mapped to Role Resource Access Notes     weblogic-operator-rolebinding weblogic-operator-role Edit: secrets, configmaps, events The role binding is created in the namespace weblogic-operator-ns 1   weblogic-operator-rolebinding-namespace Operator Cluster Role namespace Read: secrets, pod/log, storageclasses The role binding is created in the namespace domain1-ns 2     Edit: configmaps, events, pods, podtemplates, services, persistentvolumeclaims, newtworkpolicies, podsecuritypolicies, podpresets, jobs.batch, cronjobs.batch      Create: pods/exec     Cluster role bindings Assuming that the WebLogic operator was installed into the Kubernetes namespace weblogic-operator-ns, the following ClusterRoleBinding entries are mapped to a ClusterRole granting permission to the operator.\nNote: The Operator names in table below represent the \u0026lt;role-name\u0026gt; from cluster names section.\n   Cluster Role Binding Mapped to Cluster Role Resource Access Notes     Operator general Operator general Read: namespaces 1     Edit: customresourcedefinitions, ingresses, persistentvolumes      Update: domains (weblogic.oracle), domains/status      Create: tokenreviews, subjectaccessreviews, localsubjectaccessreviews, selfsubjectaccessreviews, selfsubjectrulesreviews    Operator nonresource Operator nonresource Get: /version/* 1   Operator discovery Kubernetes system:discovery See: Kubernetes Discovery Roles 1   Operator auth-delegator Kubernetes system:auth-delegator See: Kubernetes Component Roles 1     The binding is assigned to the operator ServiceAccount. [return] The binding is assigned to the operator ServiceAccount in each namespace listed with the domainNamespaces setting. The domainNamespaces setting contains the list of namespaces that the operator is configured to manage.\n[return]   "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-domains/configoverrides/",
	"title": "Configuration overrides",
	"tags": [],
	"description": "",
	"content": " Contents  Overview Prerequisites Typical overrides Unsupported overrides Override template names and syntax  Override template names Override template schemas Override template macros Override template syntax special requirements Override template samples  Step-by-step guide Debugging Internal design flow  Overview Use configuration overrides (also called situational configuration) to customize a WebLogic domain home configuration without modifying the domain\u0026rsquo;s actual config.xml or system resource files. For example, you may want to override a JDBC data source XML module user name, password, and URL so that it references a local database.\nYou can use overrides to customize domains as they are moved from QA to production, are deployed to different sites, or are even deployed multiple times at the same site.\nHow do you specify overrides?  Make sure your domain home meets the prerequisites. See Prerequisites. Make sure your overrides are supported. See Typical overrides and Unsupported overrides. Create a Kubernetes configuration map that contains:  Override templates (also known as situational configuration templates), with names and syntax as described in Override template names and syntax. A file named version.txt that contains the exact string 2.0.  Set your domain resource configOverrides to the name of this configuration map. If templates leverage secret macros:  Create Kubernetes secrets that contain template macro values. Set your domain configOverrideSecrets to reference the aforementioned secrets.  Stop all running WebLogic Server pods in your domain. (See Starting and stopping servers.) Start or restart your domain. (See Starting and stopping servers and Restarting servers.) Verify your overrides are taking effect. (See Debugging).  For a detailed walk-through of these steps, see the Step-by-step guide.\nHow do overrides work during runtime?  When a domain is first deployed, or is restarted after shutting down all the WebLogic Server pods, the operator will:  Resolve any macros in your override templates. Place expanded override templates in the optconfig directory located in each WebLogic domain home directory.\n  When the WebLogic Servers start, they will:  Automatically load the override files from the optconfig directory. Use the override values in the override files instead of the values specified in their config.xml or system resource XML files.   For a detailed walk-through of the runtime flow, see the Internal design flow.\nPrerequisites  A WebLogic domain home must not contain any situational configuration XML file in its optconfig directory that was not placed there by the operator. Any existing situational configuration XML files in this directory will be deleted and replaced by your operator override templates (if any).\n If you want to override a JDBC, JMS, or WLDF (diagnostics) module, then the original module must be located in your domain home config/jdbc, config/jms, and config/diagnostics directory, respectively. These are the default locations for these types of modules.\n  Typical overrides Typical attributes for overrides include:\n User names, passwords, and URLs for:  JDBC data sources JMS bridges, foreign servers, and SAF  Network channel public addresses:  For remote RMI clients (T3, JMS, EJB, JTA) For remote WLST clients  Debugging Tuning (MaxMessageSize, and such)  Unsupported overrides IMPORTANT: The operator does not support custom overrides in the following areas.\n Domain topology (cluster members) Network channel listen address, port, and enabled configuration Server and domain log locations Node Manager related configuration Changing any existing MBean name  Specifically, do not use custom overrides for:\n Adding or removing:  Servers Clusters Network Access Points (custom channels)  Changing any of the following:  Dynamic cluster size Default, SSL, and Admin channel Enabled, listen address, and port Network Access Point (custom channel), listen address, or port Server and domain log locations \u0026ndash; use the logHome domain setting instead Node Manager access credentials Any existing MBean name (for example, you cannot change the domain name)   Note that it\u0026rsquo;s OK, even expected, to override Network Access Point public or external addresses and ports.\nThe behavior when using an unsupported override is undefined.\nOverride template names and syntax Overrides leverage a built-in WebLogic feature called \u0026ldquo;Configuration Overriding\u0026rdquo; which is often informally called \u0026ldquo;Situational Configuration.\u0026rdquo; Situational configuration consists of XML formatted files that closely resemble the structure of WebLogic config.xml and system resource module XML files. In addition, the attribute fields in these files can embed add, replace, and delete verbs to specify the desired override action for the field.\nOverride template names The operator requires a different file name format for override templates than WebLogic\u0026rsquo;s built-in situational configuration feature. It converts the names to the format required by situational configuration when it moves the templates to the domain home optconfig directory. The following table describes the format:\n   Original Configuration Required Override Name     config.xml config.xml   JMS module jms-MODULENAME.xml   JDBC module jdbc-MODULENAME.xml   Diagnostics module diagnostics-MODULENAME.xml    A MODULENAME must correspond to the MBean name of a system resource defined in your original config.xml file.\nOverride template schemas An override template must define the exact schemas required by the situational configuration feature. The schemas vary based on the file type you wish to override.\nconfig.xml\n\u0026lt;?xml version='1.0' encoding='UTF-8'?\u0026gt; \u0026lt;d:domain xmlns:d=\u0026quot;http://xmlns.oracle.com/weblogic/domain\u0026quot; xmlns:f=\u0026quot;http://xmlns.oracle.com/weblogic/domain-fragment\u0026quot; xmlns:s=\u0026quot;http://xmlns.oracle.com/weblogic/situational-config\u0026quot;\u0026gt; ... \u0026lt;/d:domain\u0026gt;  jdbc-MODULENAME.xml\n\u0026lt;?xml version='1.0' encoding='UTF-8'?\u0026gt; \u0026lt;jdbc:jdbc-data-source xmlns:jdbc=\u0026quot;http://xmlns.oracle.com/weblogic/jdbc-data-source\u0026quot; xmlns:f=\u0026quot;http://xmlns.oracle.com/weblogic/jdbc-data-source-fragment\u0026quot; xmlns:s=\u0026quot;http://xmlns.oracle.com/weblogic/situational-config\u0026quot;\u0026gt; ... \u0026lt;/jdbc:jdbc-data-source\u0026gt;  jms-MODULENAME.xml\n\u0026lt;?xml version='1.0' encoding='UTF-8'?\u0026gt; \u0026lt;jms:weblogic-jms xmlns:jms=\u0026quot;http://xmlns.oracle.com/weblogic/weblogic-jms\u0026quot; xmlns:f=\u0026quot;http://xmlns.oracle.com/weblogic/weblogic-jms-fragment\u0026quot; xmlns:s=\u0026quot;http://xmlns.oracle.com/weblogic/situational-config\u0026quot; \u0026gt; ... \u0026lt;/jms:weblogic-jms\u0026gt;  diagnostics-MODULENAME.xml\n\u0026lt;?xml version='1.0' encoding='UTF-8'?\u0026gt; \u0026lt;wldf:wldf-resource xmlns:wldf=\u0026quot;http://xmlns.oracle.com/weblogic/weblogic-diagnostics\u0026quot; xmlns:f=\u0026quot;http://xmlns.oracle.com/weblogic/weblogic-diagnostics-fragment\u0026quot; xmlns:s=\u0026quot;http://xmlns.oracle.com/weblogic/situational-config\u0026quot; \u0026gt; ... \u0026lt;/wldf:wldf-resource\u0026gt;  Override template macros The operator supports embedding macros within override templates. This helps make your templates flexibly handle multiple use cases, such as specifying a different URL, user name, and password for a different deployment.\nTwo types of macros are supported, environment variable macros and secret macros:\n Environment variable macros have the syntax ${env:ENV-VAR-NAME}, where the supported environment variables include DOMAIN_UID, DOMAIN_NAME, DOMAIN_HOME, and LOG_HOME.\n Secret macros have the syntax ${secret:SECRETNAME.SECRETKEY} and ${secret:SECRETNAME.SECRETKEY:encrypt}.\n  The secret macro SECRETNAME field must reference the name of a Kubernetes secret, and the SECRETKEY field must reference a key within that secret. For example, if you have created a secret named dbuser with a key named username that contains the value scott, then the macro ${secret:dbuser.username} will be replaced with the word scott before the template is copied into its WebLogic Server pod.\nSECURITY NOTE: Use the :encrypt suffix in a secret macro to encrypt its replacement value with the WebLogic WLST encrypt command (instead of leaving it at its plain text value). This is useful for overriding MBean attributes that expect encrypted values, such as the password-encrypted field of a data source, and is also useful for ensuring that a custom override situational configuration file the operator places in the domain home does not expose passwords in plain-text.\nOverride template syntax special requirements Check each item below to ensure custom situational configuration takes effect:\n Reference the name of the current bean and each parent bean in any hierarchy you override.  Note that the combine-mode verbs (add and replace) should be omitted for beans that are already defined in your original domain home configuration.  See Override template samples for examples.   Use situational config replace and add verbs as follows:  If you are adding a new bean that doesn\u0026rsquo;t already exist in your original domain home config.xml, then specify add on the MBean itself and on each attribute within the bean.  See the server-debug stanza in Override template samples for an example.  If you are adding a new attribute to an existing bean in the domain home config.xml, then the attribute needs an add verb.  See the max-message-size stanza in Override template samples for an example.  If you are changing the value of an existing attribute within a domain home config.xml, then the attribute needs a replace verb. See the public-address stanza in Override template samples for an example.  When overriding config.xml:  The XML namespace (xmlns: in the XML) must be exactly as specified in Override template schemas.  For example, use d: to reference config.xml beans and attributes, f: for add and replace domain-fragment verbs, and s: to reference the situational configuration schema.  Avoid specifying the domain name stanza, as this may cause some overrides to be ignored (for example, server-template scoped overrides).  When overriding modules:  It is a best practice to use XML namespace abbreviations jms:, jdbc:, and wldf: respectively for JMS, JDBC, and WLDF (diagnostics) module override files.   Override template samples Here are some sample template override files.\nOverriding config.xml The following config.xml override file demonstrates:\n Setting the max-message-size field on a WebLogic Server named admin-server. It assumes the original config.xml does not define this value, and so uses add instead of replace. Sets the public-address and public-port fields with values obtained from a secret named test-host with keys hostname and port. It assumes the original config.xml already sets these fields, and so uses replace instead of add. Sets two debug settings. It assumes the original config.xml does not have a server-debug stanza, so it uses add throughout the entire stanza.\n\u0026lt;?xml version='1.0' encoding='UTF-8'?\u0026gt; \u0026lt;d:domain xmlns:d=\u0026quot;http://xmlns.oracle.com/weblogic/domain\u0026quot; xmlns:f=\u0026quot;http://xmlns.oracle.com/weblogic/domain-fragment\u0026quot; xmlns:s=\u0026quot;http://xmlns.oracle.com/weblogic/situational-config\u0026quot; \u0026gt; \u0026lt;d:server\u0026gt; \u0026lt;d:name\u0026gt;admin-server\u0026lt;/d:name\u0026gt; \u0026lt;d:max-message-size f:combine-mode=\u0026quot;add\u0026quot;\u0026gt;78787878\u0026lt;/d:max-message-size\u0026gt; \u0026lt;d:server-debug f:combine-mode=\u0026quot;add\u0026quot;\u0026gt; \u0026lt;d:debug-server-life-cycle f:combine-mode=\u0026quot;add\u0026quot;\u0026gt;true\u0026lt;/d:debug-server-life-cycle\u0026gt; \u0026lt;d:debug-jmx-core f:combine-mode=\u0026quot;add\u0026quot;\u0026gt;true\u0026lt;/d:debug-jmx-core\u0026gt; \u0026lt;/d:server-debug\u0026gt; \u0026lt;d:network-access-point\u0026gt; \u0026lt;d:name\u0026gt;T3Channel\u0026lt;/d:name\u0026gt; \u0026lt;d:public-address f:combine-mode=\u0026quot;replace\u0026quot;\u0026gt;${secret:test-host.hostname}\u0026lt;/d:public-address\u0026gt; \u0026lt;d:public-port f:combine-mode=\u0026quot;replace\u0026quot;\u0026gt;${secret:test-host.port}\u0026lt;/d:public-port\u0026gt; \u0026lt;/d:network-access-point\u0026gt; \u0026lt;/d:server\u0026gt; \u0026lt;/d:domain\u0026gt;   Overriding a data source module The following jdbc-testDS.xml override template demonstrates setting the URL, user name, and password-encrypted fields of a JDBC module named testDS via secret macros. The generated situational configuration that replaces the macros with secret values will be located in the DOMAIN_HOME/optconfig/jdbc directory. The password-encrypted field will be populated with an encrypted value because it uses a secret macro with an :encrypt suffix. The secret is named dbsecret and contains three keys: url, username, and password.\n\u0026lt;?xml version='1.0' encoding='UTF-8'?\u0026gt; \u0026lt;jdbc:jdbc-data-source xmlns:jdbc=\u0026quot;http://xmlns.oracle.com/weblogic/jdbc-data-source\u0026quot; xmlns:f=\u0026quot;http://xmlns.oracle.com/weblogic/jdbc-data-source-fragment\u0026quot; xmlns:s=\u0026quot;http://xmlns.oracle.com/weblogic/situational-config\u0026quot;\u0026gt; \u0026lt;jdbc:name\u0026gt;testDS\u0026lt;/jdbc:name\u0026gt; \u0026lt;jdbc:jdbc-driver-params\u0026gt; \u0026lt;jdbc:url f:combine-mode=\u0026quot;replace\u0026quot;\u0026gt;${secret:dbsecret.url}\u0026lt;/jdbc:url\u0026gt; \u0026lt;jdbc:properties\u0026gt; \u0026lt;jdbc:property\u0026gt; \u0026lt;jdbc:name\u0026gt;user\u0026lt;/jdbc:name\u0026gt; \u0026lt;jdbc:value f:combine-mode=\u0026quot;replace\u0026quot;\u0026gt;${secret:dbsecret.username}\u0026lt;/jdbc:value\u0026gt; \u0026lt;/jdbc:property\u0026gt; \u0026lt;/jdbc:properties\u0026gt; \u0026lt;jdbc:password-encrypted f:combine-mode=\u0026quot;replace\u0026quot;\u0026gt;${secret:dbsecret.password:encrypt}\u0026lt;/jdbc:password-encrypted\u0026gt; \u0026lt;/jdbc:jdbc-driver-params\u0026gt; \u0026lt;/jdbc:jdbc-data-source\u0026gt;  Step-by-step guide  Make sure your domain home meets the prerequisites. See Prerequisites. Make sure your overrides are supported. See Typical overrides and Unsupported overrides. Create a directory containing (A) a set of situational configuration templates for overriding the MBean properties you want to replace and (B) a version.txt file.  This directory must not contain any other files. The version.txt file must contain exactly the string 2.0.  Note: This version.txt file must stay 2.0 even when you are updating your templates from a previous deployment.  Templates must not override the settings listed in Unsupported overrides. Templates must be formatted and named as per Override template names and syntax. Templates can embed macros that reference environment variables or Kubernetes secrets. See Override template macros.  Create a Kubernetes configuration map from the directory of templates.\n The configuration map must be in the same Kubernetes namespace as the domain. If the configuration map is going to be used by a single DOMAIN_UID, then we recommend adding the weblogic.domainUID=\u0026lt;mydomainuid\u0026gt; label to help track the resource. For example, assuming ./mydir contains your version.txt and situation configuration template files:\nkubectl -n MYNAMESPACE create cm MYCMNAME --from-file ./mydir kubectl -n MYNAMESPACE label cm MYCMNAME weblogic.domainUID=DOMAIN_UID   Create any Kubernetes secrets referenced by a template \u0026lsquo;secret macro\u0026rsquo;.\n Secrets can have multiple keys (files) that can hold either cleartext or base64 values. We recommend that you use base64 values for passwords via Opaque type secrets in their data field, so that they can\u0026rsquo;t be easily read at a casual glance. For more information, see https://kubernetes.io/docs/concepts/configuration/secret/. Secrets must be in the same Kubernetes namespace as the domain. If a secret is going to be used by a single DOMAIN_UID, then we recommend adding the weblogic.domainUID=\u0026lt;mydomainuid\u0026gt; label to help track the resource. For example:\nkubectl -n MYNAMESPACE create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret kubectl -n MYNAMESPACE label secret my-secret weblogic.domainUID=DOMAIN_UID   Configure the name of the configuration map in the domain CR configOverrides field.\n Configure the names of each secret in domain CR.\n If the secret contains the WebLogic admin username and password keys, then set the domain CR webLogicCredentialsSecret field. For all other secrets, add them to the domain CR configOverrideSecrets field. Note: This must be in an array format even if you only add one secret (see the sample domain resource YAML below).  Any override changes require stopping all WebLogic pods, applying your domain resource (if it changed), and restarting the WebLogic pods before they can take effect.\n Custom override changes on an existing running domain, such as updating an override configuration map, a secret, or a domain resource, will not take effect until all running WebLogic Server pods in your domain are shutdown (so no servers are left running), and the domain is subsequently restarted with your new domain resource (if it changed), or with your existing domain resource (if you haven\u0026rsquo;t changed it). To stop all running WebLogic Server pods in your domain, apply a changed resource, and then start/restart the domain:  Set your domain resource serverStartPolicy to NEVER, wait, and apply your latest domain resource with the serverStartPolicy restored back to ALWAYS or IF_NEEDED (See Starting and stopping servers.) Or delete your domain resource, wait, and apply your (potentially changed) domain resource.   See Debugging for ways to check if the situational configuration is taking effect or if there are errors.\n  Example domain resource YAML:\napiVersion: \u0026quot;weblogic.oracle/v2\u0026quot; kind: Domain metadata: name: domain1 namespace: default labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: [ ... ] webLogicCredentialsSecret: name: domain1-wl-credentials-secret configOverrides: domain1-overrides-config-map configOverrideSecrets: [my-secret, my-other-secret] [ ... ]  Debugging Incorrectly formatted override files may be accepted without warnings or errors and will not prevent WebLogic pods from booting. So, it is important to make sure that the template files are correct in a QA environment, otherwise your WebLogic Servers may start even though critically required overrides are failing to take effect.\n Make sure you\u0026rsquo;ve followed each step in the Step-by-step guide.\n If WebLogic pods do not come up at all, then:\n In the domain\u0026rsquo;s namespace, see if you can find a job named DOMAIN_UID-introspect-domain-job and a corresponding pod named something like DOMAIN_UID-introspect-domain-job-xxxx. If so, examine:  kubectl -n MYDOMAINNAMESPACE describe job INTROSPECTJOBNAME kubectl -n MYDOMAINNAMESPACE logs INTROSPECTPODNAME  Check your operator log for Warning/Error/Severe messages.  kubectl -n MYOPERATORNAMESPACE logs OPERATORPODNAME   If WebLogic pods do start, then:\n Search your Administration Server pod\u0026rsquo;s kubectl log for the keyword situational, for example kubectl logs MYADMINPOD | grep -i situational.\n The only WebLogic Server log lines that match should look something like:  \u0026lt;Dec 14, 2018 12:20:47 PM UTC\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Management\u0026gt; \u0026lt;BEA-141330\u0026gt; \u0026lt;Loading situational configuration file: /shared/domains/domain1/optconfig/custom-situational-config.xml\u0026gt; This line indicates a situational configuration file has been loaded.  If the search yields Warning or Error lines, then the format of the custom situational configuration template is incorrect, and the Warning or Error text should describe the problem. Note: The following exception may show up in your server logs when overriding JDBC modules. It is not expected to affect runtime behavior, and can be ignored (a fix is pending for them):\njava.lang.NullPointerException at weblogic.management.provider.internal.situationalconfig.SituationalConfigManagerImpl.registerListener(SituationalConfigManagerImpl.java:227) at weblogic.management.provider.internal.situationalconfig.SituationalConfigManagerImpl.start(SituationalConfigManagerImpl.java:319) ... at weblogic.management.configuration.DomainMBeanImpl.setJDBCSystemResources(DomainMBeanImpl.java:11444) ...   Look in your DOMAIN_HOME/optconfig directory.\n This directory, or a subdirectory within this directory, should contain each of your custom situational configuration files. If it doesn\u0026rsquo;t, then this likely indicates your domain resource configOverrides was not set to match your custom override configuration map name, or that your custom override configuration map does not contain your override files.   If you\u0026rsquo;d like to verify that the situational configuration is taking effect in the WebLogic MBean tree, then one way to do this is to compare the server config and domain config MBean tree values.\n The domain config value should reflect the original value in your domain home configuration. The server config value should reflect the overridden value. For example, assuming your DOMAIN_UID is domain1, and your domain contains a WebLogic Server named admin-server, then:\nkubectl exec -it domain1-admin-server /bin/bash $ wlst.sh \u0026gt; connect(MYADMINUSERNAME, MYADMINPASSWORD, 't3://domain1-admin-server:7001') \u0026gt; domainConfig() \u0026gt; get('/Servers/admin-server/MaxMessageSize') \u0026gt; serverConfig() \u0026gt; get('/Servers/admin-server/MaxMessageSize') \u0026gt; exit()   To cause the WebLogic situational configuration feature to produce additional debugging information in the WebLogic Server logs, configure the JAVA_OPTIONS environment variable in your domain resource with:\n-Dweblogic.debug.DebugSituationalConfig=true -Dweblogic.debug.DebugSituationalConfigDumpXml=true  NOTE: The WebLogic console will not reflect any override changes. You cannot use the console to verify overrides are taking effect.\n  Internal design flow  When a domain is first deployed, or is restarted, the operator runtime creates an introspector Kubernetes job named DOMAIN_UID-introspect-domain-job. The introspector job\u0026rsquo;s pod:  Mounts the Kubernetes configuration map and secrets specified via the operator domain resource configOverrides, webLogicCredentialsSecret, and configOverrideSecrets fields. Reads the mounted situational configuration templates from the configuration map and expands them to create the actual situational configuration files for the domain:  It expands some fixed replaceable values (for example, ${env:DOMAIN_UID}). It expands referenced secrets by reading the value from the corresponding mounted secret file (for example, ${secret:mysecret.mykey}). It optionally encrypts secrets using offline WLST to encrypt the value - useful for passwords (for example, ${secret:mysecret.mykey:encrypt}). It returns expanded situational configuration files to the operator. It reports any errors when attempting expansion to the operator.   The operator runtime:  Reads the expanded situational configuration files and/or errors from the introspector. And, if the introspector reported no errors, it:  Puts situational configuration files in a configuration map named DOMAIN_UID-weblogic-domain-introspect-cm. Mounts this configuration map into the WebLogic Server pods. Starts the WebLogic Server pods.  Otherwise, if the introspector reported errors, it:  Logs warning, error, or severe messages. Will not start WebLogic Server pods.   The startServer.sh script in the WebLogic Server pods:  Copies the expanded situational configuration files to a special location where the WebLogic runtime can find them:  config.xml overrides are copied to the optconfig directory in its domain home. Module overrides are copied to the optconfig/jdbc, optconfig/jms, or optconfig/diagnostics directory.  Deletes any situational configuration files in the optconfig directory that do not have corresponding template files in the configuration map.  WebLogic Servers read their overrides from their domain home\u0026rsquo;s optconfig directory.  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/reference/",
	"title": "Reference",
	"tags": [],
	"description": "",
	"content": "See the following reference documentation.\n Javadoc  Java API documentation.\n Swagger  Swagger REST API documentation.\n Domain resource  Use this document to set up and configure your own domain resource.\n "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/quickstart/prepare/",
	"title": "Prepare for a domain",
	"tags": [],
	"description": "",
	"content": " Create a namespace that can host one or more domains:\n$ kubectl create namespace sample-domain1-ns   Use helm to configure the operator to manage domains in this namespace:  $ helm upgrade \\ --reuse-values \\ --set \u0026quot;domainNamespaces={sample-domain1-ns}\u0026quot; \\ --wait \\ sample-weblogic-operator \\ kubernetes/charts/weblogic-operator  Configure Traefik to manage Ingresses created in this namespace:\n$ helm upgrade \\ --reuse-values \\ --set \u0026quot;kubernetes.namespaces={traefik,sample-domain1-ns}\u0026quot; \\ --wait \\ traefik-operator \\ stable/traefik   "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/cicd/tools/",
	"title": "Tools",
	"tags": [],
	"description": "Tools that are available to build CI/CD pipelines.",
	"content": " WebLogic Deploy Tooling (WDT) You can use several of the WDT tools in a CI/CD pipeline. For example, the createDomain tool creates a new domain based on a simple model, and updateDomain (and deployApps) uses the same model concept to update an existing domain (preserving the same domain encryption key). The deployApps tool is very similar to the updateDomain tool, but limits what can be updated to application-related configuration attributes such as data sources and application archives. The model used by these tools is a sparse set of attributes needed to create or update the domain. A model can be as sparse as providing only the WebLogic Server administrative password, although not very interesting. A good way to get a jumpstart on a model is to use the discoverDomain tool in WDT which builds a model based on an existing domain.\nOther than the tools themselves, there are three components to the WDT tools:\n The Domain Model - Metadata model describing the desired domain.\nThe metadata domain model can be written in YAML or JSON and is documented here. The Archive ZIP - Binaries to supplement the model.\nAll binaries needed to supplement the model must be specified in an archive file, which is just a ZIP file with a specific directory structure. Optionally, the model can be stored inside the ZIP file, if desired. Any binaries not already on the target system must be in the ZIP file so that the tooling can extract them in the target domain. The Properties File - A standard Java properties file.\nA property file used to provide values to placeholders in the model.  WDT Create Domain Samples  (Docker) A sample for creating a domain in a Docker image with WDT can be found here. (Kubernetes) A similar sample of creating a domain in a Docker image with WDT can be found in the WebLogic Kubernetes Operator project for creating a domain-in-image with WDT.  WebLogic Scripting Tool (WLST) You can use WLST scripts to create and/or update domains in a CI/CD pipeline. We recommend that you use offline WLST for this purpose. There may be some scenarios where it is necessary to use WLST online, but we recommend that you do that only as an exception, and when absolutely necessary.\nIf you do not already have WLST scripts, we recommend that you consider using WebLogic Deploy Tooling (WDT) instead. It provides a more declarative approach to domain creation, whereas WLST is more of an imperative scripting language. WDT provides advantages like being able to use the same model with different versions of WebLogic, whereas you may need to update WLST scripts manually when migrating to a new version of WebLogic for example.\nWebLogic pack and unpack tools WebLogic Server provides tools called \u0026ldquo;pack\u0026rdquo; and \u0026ldquo;unpack\u0026rdquo; that can be used to \u0026ldquo;clone\u0026rdquo; a domain. These tools do not preserve the domain encryption key. You can use these tools to make copies of domains in scenarios when you do not need the same domain encryption key.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-domains/ingress/",
	"title": "Ingress",
	"tags": [],
	"description": "",
	"content": " Ingresses are one approach provided by Kubernetes to configure load balancers. Depending on the version of Kubernetes you are using, and your cloud provider, you may need to use Ingresses. Please refer to the Ingress documentation for more information about Ingresses.\nWebLogic clusters as backends of an Ingress In an Ingress object, a list of backends are provided for each target that will be load balanced. Each backend is typically a Kubernetes service, more specifically, a combination of a serviceName and a servicePort.\nWhen the WebLogic operator creates a WebLogic domain, it also creates a service for each WebLogic cluster in the domain. The operator defines the service such that its selector will match all WebLogic server pods within the WebLogic cluster which are in the \u0026ldquo;ready\u0026rdquo; state.\nThe name of the service created for a WebLogic cluster follows the pattern \u0026lt;domainUID\u0026gt;-cluster-\u0026lt;clusterName\u0026gt;. For example, if the domainUID is domain1 and the cluster name is cluster-1, the corresponding service will be named domain1-cluster-cluster-1.\nThe service name must comply with standard Kubernetes rules for naming of objects and in particular with DNS-1035: \u0026gt; A DNS-1035 label must consist of lower case alphanumeric characters or \u0026lsquo;-\u0026rsquo;, start with an alphabetic character, and end with an alphanumeric character (e.g. my-name, or abc-123, regex used for validation is [a-z]([-a-z0-9]*[a-z0-9])?).\nTo comply with these requirements, if the domainUID or the cluster name contains some upper-case characters or underscores, then in the service name the upper-case characters will be converted to lower-case and underscores will be converted to hyphens. For example, if the domainUID is myDomain_1 and the cluster name is myCluster_1, the corresponding service will be named mydomain-1-cluster-mycluster-1.\nThe service, serviceName and servicePort, of a WebLogic cluster will be used in the routing rules defined in the Ingress object and the load balancer will route traffic to the WebLogic servers within the cluster based on the rules.\nMost common Ingress controllers, for example Traefik, Voyager, and nginx, understand that there are zero or more actual pods behind the service, and they actually build their backend list and route requests to those backends directly, not through the service. This means that requests are properly balanced across the pods, according to the load balancing algorithm in use. Most Ingress controllers also subscribe to updates on the service and adjust their internal backend sets when additional pods become ready, or pods enter a non-ready state.\n Steps to set up an Ingress load balancer  Install the Ingress controller.\nAfter the Ingress controller is running, it monitors Ingress resources in a given namespace(s) and acts accordingly.\n Create Ingress resource(s).\nIngress resources contain routing rules to one or more backends. An Ingress controller is responsible to apply the rules to the underlying load balancer. There are two approaches to create the Ingress resource:\n Use the Helm chart ingress-per-domain.\nEach Ingress provider supports a number of annotations in Ingress resources. This Helm chart allows you to define the routing rules without dealing with the detailed provider-specific annotations. Currently we support two Ingress providers: Traefik and Voyager.\n Create the Ingress resource manually from a YAML file.\nManually create an Ingress YAML file and then apply it to the Kubernetes cluster.\n   Guide and samples for Traefik and Voyager/HAProxy Traefik and Voyager/HAProxy are both popular Ingress controllers. Information about how to install and configure these to load balance WebLogic clusters is provided here:\n Traefik guide Voyager guide  Samples are also provided for these two Ingress controllers, showing how to manage multiple WebLogic clusters as the backends, using different routing rules, host-routing and path-routing; and TLS termination:\n Traefik samples Voyager samples  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/security/secrets/",
	"title": "Secrets",
	"tags": [],
	"description": "Kubernetes secrets for the WebLogic operator",
	"content": " Contents  WebLogic domain credentials secret WebLogic domain image pull secret WebLogic operator image pull secret WebLogic operator configuration override secrets WebLogic operator external REST interface secret WebLogic operator internal REST interface secret  WebLogic domain credentials secret The credentials for the WebLogic domain are kept in a Kubernetes Secret where the name of the secret is specified using webLogicCredentialsSecret in the WebLogic Domain resource. Also, the domain credentials secret must be created in the namespace where the Domain will be running.\nFor an example of a WebLogic domain resource using webLogicCredentialsSecret, see Docker Image Protection.\n The samples supplied with the WebLogic operator use a naming convention that follows the pattern \u0026lt;domainUID\u0026gt;-weblogic-credentials, where \u0026lt;domainUID\u0026gt; is the unique identifier of the domain, for example, domain1-weblogic-credentials.\nIf the WebLogic domain will be started in domain1-ns and the \u0026lt;domainUID\u0026gt; is domain1, an example of creating a Kubernetes generic secret is as follows:\n$ kubectl -n domain1-ns create secret generic domain1-weblogic-credentials \\ --from-file=username --from-file=password $ kubectl -n domain1-ns label secret domain1-weblogic-credentials \\ weblogic.domainUID=domain1 weblogic.domainName=domain1  Oracle recommends that you not include unencrypted passwords on command lines. Passwords and other sensitive data can be prompted for or looked up by shell scripts and/or tooling. For more information about creating Kubernetes secrets, see the Kubernetes Secrets documentation.\n The WebLogic operator\u0026rsquo;s introspector job will expect the secret key names to be:\n username password  For example, here is what results when describing the Kubernetes Secret:\n$ kubectl -n domain1-ns describe secret domain1-weblogic-credentials Name: domain1-weblogic-credentials Namespace: domain1-ns Labels: weblogic.domainName=domain1 weblogic.domainUID=domain1 Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== password: 8 bytes username: 8 bytes  WebLogic domain image pull secret The WebLogic domain that the operator manages can have images that are protected in the registry. The imagePullSecrets setting on the Domain can be used to specify the Kubernetes Secret that holds the registry credentials.\nFor more information, see Docker Image Protection under Domain security.\n WebLogic operator image pull secret The Helm chart for installing the operator has an option to specify the image pull secret used for the operator\u0026rsquo;s image when using a private registry. The Kubernetes Secret of type docker-registry should be created in the namespace where the operator is deployed.\nHere is an example of using the helm install command to set the image name and image pull secret:\n$ helm install kubernetes/charts/weblogic-operator \\ --set \u0026quot;image=my.io/my-operator-image:1.0\u0026quot; \\ --set \u0026quot;imagePullSecrets[0].name=my-operator-image-pull-secret\u0026quot; \\ --name my-weblogic-operator --namespace weblogic-operator-ns \\ --wait  For more information, see Install the operator Helm chart under User Guide.\n WebLogic operator configuration override secrets The WebLogic operator supports embedding macros within configuration override templates that reference Kubernetes secrets. These Kubernetes secrets can be created with any name in the namespace where the Domain will be running. The Kubernetes secret names are specified using configOverrideSecrets in the WebLogic Domain resource.\nFor more information, see Configuration overrides under User Guide.\n WebLogic operator external REST interface secret The operator can expose an external REST HTTPS interface which can be accessed from outside the Kubernetes cluster. A Kubernetes tls secret is used to hold the certificate(s) and private key.\nFor more information, see Certificates under Securty.\n WebLogic operator internal REST interface secret The operator exposes an internal REST HTTPS interface with a self-signed certificate. The certificate is kept in a Kubernetes ConfigMap with the name weblogic-operator-cm using the key internalOperatorCert. The private key is kept in a Kubernetes Secret with the name weblogic-operator-secrets using the key internalOperatorKey. These Kubernetes objects are managed by the operator\u0026rsquo;s Helm chart and are part of the namespace where the WebLogic operator is installed.\nFor example, to see all the operator\u0026rsquo;s config maps and secrets when installed into the Kubernetes namespace weblogic-operator-ns, use:\n$ kubectl -n weblogic-operator-ns get cm,secret  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": "  Certificates  SSL/TLS certificate handling for the WebLogic operator.\n Domain security  WebLogic domain security and the WebLogic operator\n Encryption  WebLogic domain encryption and the WebLogic operator\n Service accounts  Kubernetes service accounts for the WebLogic operator\n RBAC  Role based authorization for the WebLogic operator\n Secrets  Kubernetes secrets for the WebLogic operator\n "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/samples/simple/elastic-stack/",
	"title": "Elastic Stack",
	"tags": [],
	"description": "Sample for configuring the Elasticsearch and Kibana deployments and services for the operator&#39;s logs.",
	"content": "When you install the WebLogic operator Helm chart, you can set elkIntegrationEnabled to true in your values.yaml file to tell the operator to send the contents of the operator\u0026rsquo;s logs to Elasticsearch.\nTypically, you would have already configured Elasticsearch and Kibana in the Kubernetes cluster, and also would have specified elasticSearchHost and elasticSearchPort in your values.yaml file to point to where Elasticsearch is already running.\nThis sample configures the Elasticsearch and Kibana deployments and services. It\u0026rsquo;s useful for trying out the operator in a Kubernetes cluster that doesn\u0026rsquo;t already have them configured.\nIt runs the Elastic Stack on the same host and port that the operator\u0026rsquo;s Helm chart defaults to, therefore, you only need to set elkIntegrationEnabled to true in your values.yaml file.\nTo control Elasticsearch memory parameters (Heap allocation and Enabling/Disabling swapping) please open the file elasticsearch_and_kibana.yaml, search for env variables of the elasticsearch container and change the values of the following.\n ES_JAVA_OPTS: value may contain, for example, -Xms512m -Xmx512m to lower the default memory usage (please be aware that this value is only applicable for demo purpose and it is not the one recommended by Elasticsearch). bootstrap.memory_lock: value may contain true (enables the usage of mlockall to try to lock the process address space into RAM, preventing any Elasticsearch memory from being swapped out) or false (disables the usage of mlockall to try to lock the process address space into RAM, preventing any Elasticsearch memory from being swapped out).  To install Elasticsearch and Kibana, use:\n$ kubectl apply -f kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml  To remove them, use:\n$ kubectl delete -f kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/developerguide/code-structure/",
	"title": "Code structure",
	"tags": [],
	"description": "",
	"content": " This project has the following directory structure:\n docs: Generated Javadoc and Swagger integration-tests: Integration test suite json-schema: Java model to JSON schema generator json-schema-maven-plugin: Maven plugin for schema generator kubernetes/charts: Helm charts kubernetes/samples: All samples, including for WebLogic domain creation model: Domain resource Java model operator: Operator runtime site: This documentation src/scripts: Scripts operator injects into WebLogic server instance Pods swagger: Swagger files for the Kubernetes API server and domain resource  Watch package The Watch API in the Kubernetes Java client provides a watch capability across a specific list of resources for a limited amount of time. As such, it is not ideally suited for our use case, where a continuous stream of watches is desired, with watch events generated in real time. The watch-wrapper in this repository extends the default Watch API to provide a continuous stream of watch events until the stream is specifically closed. It also provides resourceVersion tracking to exclude events that have already been seen. The watch-wrapper provides callbacks so events, as they occur, can trigger actions.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/quickstart/create-domain/",
	"title": "Create a domain",
	"tags": [],
	"description": "",
	"content": " Create a Kubernetes secret containing the username and password for the domain using the create-weblogic-credentials script:\n$ kubernetes/samples/scripts/create-weblogic-domain-credentials/create-weblogic-credentials.sh \\ -u weblogic -p welcome1 -n sample-domain1-ns -d sample-domain1  The sample will create a secret named domainUID-weblogic-credentials where the domainUID is replaced with the value you provided. For example, the command above would create a secret named sample-domain1-weblogic-credentials.\n Create a new image with a domain home by running the create-domain script. Follow the directions in the sample, including:   Copying the sample kubernetes/samples/scripts/create-weblogic-domain/domain-home-in-image/create-domain-inputs.yaml file and updating your copy with the domainUID (sample-domain1), domain namespace (sample-domain1-ns), and the domainHomeImageBase (store/oracle/weblogic:12.2.1.3).\n Setting weblogicCredentialsSecretName to the name of the secret containing the WebLogic credentials, in this case, sample-domain1-weblogic-credentials.\n Leaving the image empty unless you need to tag the new image that the script builds to a different name. If you set the domainHomeImageBuildPath property to ./docker-images/OracleWebLogic/samples/12213-domain-home-in-image-wdt, make sure that your JAVA_HOME is set to a Java JDK version 1.8 or later.\n   For example, assuming you named your copy my-inputs.yaml:\n$ cd kubernetes/samples/scripts/create-weblogic-domain/domain-home-in-image $ ./create-domain.sh -i my-inputs.yaml -o /some/output/directory -u weblogic -p welcome1 -e  You need to provide the WebLogic administration user name and password in the -u and -p options respectively, as shown in the example. When using this sample, the WebLogic Server credentials that you specify, in three separate places, must be consistent:\n The secret that you create for the credentials. The properties files in the sample project you choose to create the Docker image from. The parameters you supply to the create-domain.sh script.   If you specify the -e option, the script will generate the Kubernetes YAML files and apply them to your cluster. If you omit the -e option, the script will just generate the YAML files, but will not take any action on your cluster.\nIf you run the sample from a machine that is remote to the Kubernetes cluster, and you need to push the new image to a registry that is local to the cluster, you need to do the following:\n Set the image property in the inputs file to the target image name (including the registry hostname/port, and the tag if needed). If you want Kubernetes to pull the image from a private registry, create a Kubernetes secret to hold your credentials and set the imagePullSecretName property in the inputs file to the name of the secret. The Kubernetes secret must be in the same namespace where the domain will be running. For more information, see domain home in image protection in the Security section.\n  Run the create-domain.sh script without the -e option. Push the image to the registry. Run the following command to create the domain.  $ kubectl apply -f /some/output/directory/weblogic-domains/sample-domain1/domain.yaml   Confirm that the operator started the servers for the domain:  a. Use kubectl to show that the domain resource was created:\n$ kubectl describe domain sample-domain1 -n sample-domain1-ns  b. After a short time, you will see the Administration Server and Managed Servers running.\n$ kubectl get pods -n sample-domain1-ns  c. You should also see all the Kubernetes services for the domain.\n$ kubectl get services -n sample-domain1-ns  Create an Ingress for the domain, in the domain namespace, by using the sample Helm chart:\n$ helm install kubernetes/samples/charts/ingress-per-domain \\ --name sample-domain1-ingress \\ --namespace sample-domain1-ns \\ --set wlsDomain.domainUID=sample-domain1 \\ --set traefik.hostname=sample-domain1.org   To confirm that the load balancer noticed the new Ingress and is successfully routing to the domain\u0026rsquo;s server pods, you can send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo; which will return a HTTP 200 status code, as shown in the example below. If you used the host-based routing Ingress sample, you will need to provide the hostname in the -H option.  Substitute the Node IP address of the worker node for your.server.com. You can find it by running:\n$ kubectl get po -n sample-domain1-ns -o wide  $ curl -v -H 'host: sample-domain1.org' http://your.server.com:30305/weblogic/ready About to connect() to your.server.com port 30305 (#0) Trying 10.196.1.64... Connected to your.server.com (10.196.1.64) port 30305 (#0) \u0026gt; GET /weblogic/ HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Accept: */* \u0026gt; host: domain1.org \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Content-Length: 0 \u0026lt; Date: Thu, 20 Dec 2018 14:52:22 GMT \u0026lt; Vary: Accept-Encoding \u0026lt; Connection #0 to host your.server.com left intact   Depending on where your Kubernetes cluster is running, you may need to open firewall ports or update security lists to allow ingress to this port.\n  To access the WLS Administration Console:\na. Edit the my-inputs.yaml file (assuming that you named your copy my-inputs.yaml) to set exposedAdminNodePort: true.\nb. Open a browser to http://your.server.com:30701.\nc. As in step 5, substitute the Node IP address of the worker node for your.server.com.\n  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-domains/accessing-the-domain/",
	"title": "Accessing the domain",
	"tags": [],
	"description": "",
	"content": "  Using WLST  You can use the WebLogic Scripting Tool (WLST) to manage a domain running in Kubernetes.\n "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/developerguide/asynchronous-call-model/",
	"title": "Asynchronous call model",
	"tags": [],
	"description": "",
	"content": " Our expectation is that customers will task the operator with managing hundreds of WebLogic domains across dozens of Kubernetes namespaces. Therefore, we have designed the operator with an efficient user-level threads pattern. We\u0026rsquo;ve used that pattern to implement an asynchronous call model for Kubernetes API requests. This call model has built-in support for timeouts, retries with exponential back-off, and lists that exceed the requested maximum size using the continuance functionality.\nUser-level thread pattern The user-level thread pattern is implemented by the classes in the oracle.kubernetes.operator.work package.\n Engine: The executor service and factory for Fibers. Fiber: The user-level thread. Fibers represent the execution of a single processing flow through a series of Steps. Fibers may be suspended and later resumed, and do not consume a Thread while suspended. Step: Individual CPU-bound activity in a processing flow. Packet: Context of the processing flow. NextAction: Used by a Step when it returns control to the Fiber to indicate what should happen next. Common \u0026lsquo;next actions\u0026rsquo; are to execute another Step or to suspend the Fiber. Component: Provider of SPI\u0026rsquo;s that may be useful to the processing flow. Container: Represents the containing environment and is a Component.  Each Step has a reference to the next Step in the processing flow; however, Steps are not required to indicate that the next Step be invoked by the Fiber when the Step returns a NextAction to the Fiber. This leads to common use cases where Fibers invoke a series of Steps that are linked by the \u0026lsquo;is-next\u0026rsquo; relationship, but just as commonly, use cases where the Fiber will invoke sets of Steps along a detour before returning to the normal flow.\nIn this sample, the caller creates an Engine, Fiber, linked set of Step instances, and Packet. The Fiber is then started. The Engine would typically be a singleton, since it\u0026rsquo;s backed by a ScheduledExecutorService. The Packet would also typically be pre-loaded with values that the Steps would use in their apply() methods.\nstatic class SomeClass { public static void main(String[] args) { Engine engine = new Engine(\u0026quot;worker-pool\u0026quot;); Fiber fiber = engine.createFiber(); Step step = new StepOne(new StepTwo(new StepThree(null))); Packet packet = new Packet(); fiber.start( step, packet, new CompletionCallback() { @Override public void onCompletion(Packet packet) { // Fiber has completed successfully } @Override public void onThrowable(Packet packet, Throwable throwable) { // Fiber processing was terminated with an exception } }); } }  Steps must not invoke sleep or blocking calls from within apply(). This prevents the worker threads from serving other Fibers. Instead, use asynchronous calls and the Fiber suspend/resume pattern. Step provides a method, doDelay(), which creates a NextAction to drive Fiber suspend/resume that is a better option than sleep precisely because the worker thread can serve other Fibers during the delay. For asynchronous IO or similar patterns, suspend the Fiber. In the callback as the Fiber suspends, initiate the asynchronous call. Finally, when the call completes, resume the Fiber. The suspend/resume functionality handles the case where resumed before the suspending callback completes.\nIn this sample, the step uses asynchronous file IO and the suspend/resume Fiber pattern.\nstatic class StepTwo extends Step { public StepTwo(Step next) { super(next); } @Override public NextAction apply(Packet packet) { return doSuspend((fiber) -\u0026gt; { // The Fiber is now suspended // Start the asynchronous call try { Path path = Paths.get(URI.create(this.getClass().getResource(\u0026quot;/somefile.dat\u0026quot;).toString())); AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.READ); ByteBuffer buffer = ByteBuffer.allocate(1024); fileChannel.read(buffer, 0, buffer, new CompletionHandler\u0026lt;Integer, ByteBuffer\u0026gt;() { @Override void completed(Integer result, ByteBuffer attachment) { // Store data in Packet and resume Fiber packet.put(\u0026quot;DATA_SIZE_READ\u0026quot;, result); packet.put(\u0026quot;DATA_FROM_SOMEFILE\u0026quot;, attachment); fiber.resume(packet); } @Override public void failed(Throwable exc, ByteBuffer attachment) { // log exc completed(0, null); } }); } catch (IOException e) { // log exception // If not resumed here, Fiber will never be resumed } }); } }  Call builder pattern The asynchronous call model is implemented by classes in the oracle.kubernetes.operator.helpers package, including CallBuilder and ResponseStep. The model is based on the Fiber suspend/resume pattern described above. CallBuilder provides many methods having names ending with \u0026ldquo;Async\u0026rdquo;, such as listPodAsync() or deleteServiceAsync(). These methods return a Step that can be returned as part of a NextAction. When creating these Steps, the developer must provide a ResponseStep. Only ResponseStep.onSuccess() must be implemented; however, it is often useful to override onFailure() as Kubernetes treats 404 (Not Found) as a failure.\nIn this sample, the developer is using the pattern to list pods from the default namespace that are labeled as part of cluster-1.\nstatic class StepOne extends Step { public StepOne(Step next) { super(next); } @Override public NextAction apply(Packet packet) { String namespace = \u0026quot;default\u0026quot;; Step step = CallBuilder.create().with($ -\u0026gt; { $.labelSelector = \u0026quot;weblogic.clusterName=cluster-1\u0026quot;; $.limit = 50; $.timeoutSeconds = 30; }).listPodAsync(namespace, new ResponseStep\u0026lt;V1PodList\u0026gt;(next) { @Override public NextAction onFailure(Packet packet, ApiException e, int statusCode, Map\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; responseHeaders) { if (statusCode == CallBuilder.NOT_FOUND) { return onSuccess(packet, null, statusCode, responseHeaders); } return super.onFailure(packet, e, statusCode, responseHeaders); } @Override NextAction onSuccess(Packet packet, V1PodList result, int statusCode, Map\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; responseHeaders) { // do something with the result Pod, if not null return doNext(packet); } }); return doNext(step, packet); } }  Notice that the required parameters, such as namespace, are method arguments, but optional parameters are designated using a simplified builder pattern using with() and a lambda.\nThe default behavior of onFailure() will retry with an exponential backoff the request on status codes 429 (TooManyRequests), 500 (InternalServerError), 503 (ServiceUnavailable), 504 (ServerTimeout) or a simple timeout with no response from the server.\nIf the server responds with status code 409 (Conflict), then this indicates an optimistic locking failure. Common use cases are that the code read a Kubernetes object in one asynchronous step, modified the object, and attempted to replace the object in another asynchronous step; however, another activity replaced that same object in the interim. In this case, retrying the request would give the same result. Therefore, developers may provide an \u0026ldquo;on conflict\u0026rdquo; step when calling super.onFailure(). The conflict step will be invoked after an exponential backoff delay. In this example, that conflict step should be the step that reads the existing Kubernetes object.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/faq/",
	"title": "Frequently asked questions",
	"tags": [],
	"description": "",
	"content": " Chapter 7 Frequently Asked Questions This section provides answers to frequently asked questions.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/quickstart/cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": " Remove the domain.  Remove the domain\u0026rsquo;s Ingress by using helm:\n$ helm delete --purge sample-domain1-ingress   Remove the domain resources by using the sample delete-weblogic-domain-resources script:  $ kubernetes/samples/scripts/delete-domain/delete-weblogic-domain-resources.sh -d sample-domain1  Use kubectl to confirm that the server pods and domain resource are gone:\n$ kubectl get pods -n sample-domain1-ns $ kubectl get domains -n sample-domain1-ns  Remove the domain namespace.  Configure the Traefik load balancer to stop managing the Ingresses in the domain namespace:  $ helm upgrade \\ --reuse-values \\ --set \u0026quot;kubernetes.namespaces={traefik}\u0026quot; \\ --wait \\ traefik-operator \\ stable/traefik  Configure the operator to stop managing the domain:\n$ helm upgrade \\ --reuse-values \\ --set \u0026quot;domainNamespaces={}\u0026quot; \\ --wait \\ sample-weblogic-operator \\ kubernetes/charts/weblogic-operator   Delete the domain namespace:  $ kubectl delete namespace sample-domain1-ns   Remove the operator.  Remove the operator:\n$ helm delete --purge sample-weblogic-operator   Remove the operator\u0026rsquo;s namespace:  $ kubectl delete namespace sample-weblogic-operator-ns   Remove the load balancer.  Remove the Traefik load balancer:\n$ helm delete --purge traefik-operator   Remove the Traefik namespace:  $ kubectl delete namespace traefik   "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/developerguide/domain-processing/",
	"title": "Domain processing",
	"tags": [],
	"description": "",
	"content": "When the operator starts, it lists all existing Domain resources and processes these domains to create the necessary Kubernetes resources, such as Pods and Services, if they don\u0026rsquo;t already exist. This initialization also includes looking for any stranded resources that, while created by the operator, no longer correlate with a Domain resource.\nAfter this, the operator starts watches for changes to Domain resources and any changes to other resources created by the operator. When a watch event is received, the operator processes the modified Domain resource to again bring the runtime presence in to alignment with the desired state.\nThe operator ensures that at most one Fiber is running for any given Domain. For instance, if the customer modifies a Domain resource to trigger a rolling restart, then the operator will create a Fiber to process this activity. However, if while the rolling restart is in process, the customer makes another change to the Domain resource, such as to increase the replicas field for a cluster, then the operator will cancel the in-flight Fiber and replace it with a new Fiber. This replacement processing must be able to handle taking over for the cancelled work regardless of where the earlier processing may have been in its flow. Therefore, domain processing always starts at the beginning of the \u0026ldquo;make right\u0026rdquo; flow without any state other than the current Domain resource.\nFinally, the operator periodically lists all Domains and rechecks them. This is a backstop against the possibility that a watch event is missed, such as because of a temporary network outage. Recheck activities will not interrupt already running processes for a given Domain.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/userguide/managing-domains/domain-lifecycle/",
	"title": "Domain life cycle",
	"tags": [],
	"description": "",
	"content": "Learn how to start, stop, restart, and scale the domain\u0026rsquo;s servers.\n Startup and shutdown  There are properties on the domain resource that specify which servers should be running and which servers should be restarted. To start, stop, or restart servers, modify these properties on the domain resource.\n Restarting  This document describes when to restart servers in the Oracle WebLogic Server in Kubernetes environment.\n Scaling  The operator provides several ways to initiate scaling of WebLogic clusters.\n "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/developerguide/documentation/",
	"title": "Documentation",
	"tags": [],
	"description": "",
	"content": "This documentation is produced using Hugo. To make an update to the documentation, follow this process:\n If you have not already done so, clone the repository.\ngit clone https://github.com/oracle/weblogic-kubernetes-operator  Create a new branch from master.\ngit checkout master git pull origin master git checkout -b your-branch  Make your documentation updates by editing the source files in docs-source/content. Make sure you only check in the changes from the docs-source/content area; do not build the site and check in the static files.\n  If you wish to view your changes, you can run the site locally using these commands; the site will be available on the URL shown here:\ncd docs-source hugo server -b http://localhost:1313/weblogic-kubernetes-operator  When you are ready to submit your changes, push your branch to origin and submit a pull request. Remember to follow the guidelines in the CONTRIBUTING document.\n  "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/developerguide/backwards-compatibility/",
	"title": "Backward compatibility",
	"tags": [],
	"description": "",
	"content": "Starting with the 2.0.1 release, operator releases must be backward compatible with respect to the domain resource schema, operator Helm chart input values, configuration overrides template, Kubernetes resources created by the operator Helm chart, Kubernetes resources created by the operator, and the operator REST interface. We will maintain compatibility for three releases, except in the case of a clearly communicated deprecated feature, which will be maintained for one release after a replacement is available.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/faq/cannot-pull-image/",
	"title": "Cannot Pull Image",
	"tags": [],
	"description": "",
	"content": "  My domain will not start and I see errors like ImagePullBackoff or Cannot pull image\n When you see these kinds of errors, it means that Kubernetes cannot find your Docker image. The most common causes are:\n The image value in your domain resource is set incorrectly, meaning Kubernetes will be trying to pull the wrong image. The image requires authentication or permission in order to pull it and you have not configured Kubernetes with the necessary credentials, for example in an imagePullSecret. You built the image on a machine that is not where your kubelet is running and Kubernetes cannot see the image, meaning you need to copy the image to the worker nodes or put it in a Docker registry that is accessible the to all of the worker nodes.  Let\u0026rsquo;s review what happens when Kubernetes starts a pod.\nThe definition of the pod contains a list of container specifications. Each container specification contains the name (and optionally, tag) of the image that should be used to run that container. In the example above, there is a container called c1 which is configured to use the Docker image some.registry.com/owner/domain1:1.0. This image name is in the format registry address / owner / name : tag, so in this case the registry is some.registry.com, the owner is owner, the image name is domain and the tag is 1.0. Tags are a lot like version numbers, but they are not required to be numbers or to be in any particular sequence or format. If you omit the tag, it is assumed to be latest.\nThe Docker tag latest is confusing - it does not actually mean the latest version of the image that was created or published in the registry; it just literally means whichever version the owner decided to call \u0026ldquo;latest\u0026rdquo;. Docker and Kubernetes make some assumptions about latest, and it is generally recommended to avoid using it and instead specify the actual version/tag that you really want.\n First, Kubernetes will check to see if the requested image is available in the local Docker image store on whichever worker node the pod was scheduled on. If it is there, then it will use that image to start the container. If it is not there, then Kubernetes will attempt to pull the image from a remote Docker registry.\nThere is another setting called imagePullPolicy that can be used to force Kubernetes to always pull the image, even if it is already present in the local Docker image store.\n If the image is available in the remote registry and it is public, that is it does not require authentication, then Kubernetes will pull the image to the local Docker image store and start the container.\nImages that require authentication If the remote Docker registry requires authentication, then you will need to provide the authentication details in a Kubernetes docker-registry secret and tell Kubernetes to use that secret when pulling the image.\nTo create a secret, you can use the following command:\nkubectl create secret docker-registry secret1 \\ --docker-server=some.registry.com \\ --docker-username=bob \\ --docker-password=bigSecret \\ --docker-email=bob@some.com \\ --namespace=default  In this command, you would replace secret1 with the name of the secret; the docker-server is set to the registry name, without the https:// prefix; the docker-username, docker-password and docker-email are set to match the credentials you use to authenticate to the remote Docker registry; and the namespace must be set to the same namespace where you intend to use the image.\nSome registries may need a suffix making the docker-server something like some.registry.com/v2 for example. You will need to check with your registry provider\u0026rsquo;s documentation to see if this is needed.\n After the secret is created, you need to tell Kubernetes to use it. This is done by adding an imagePullSecret to your Kubernetes YAML file. In the case of a WebLogic domain, you add the secret name to the imagePullSecret in the domain custom resource YAML file.\nHere is an example of part of a domain custom resource file with the imagePullSecret above specified:\napiVersion: \u0026quot;weblogic.oracle/v2\u0026quot; kind: Domain metadata: name: domain1 namespace: default labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: domainHome: /u01/oracle/user_projects/domains/domain1 domainHomeInImage: true image: \u0026quot;some.registry.com/owner/domain1:1.0\u0026quot; imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; imagePullSecrets: - name: secret1  Alternatively, you can associate the secret with the service account that will be used to run the pod. If you do this, then you will not need to add the imagePullSecret to the domain resource. This is useful if you are running multiple domains in the same namespace.\nTo add the secret shown above to the default service account in the weblogic namespace, you would use a command like this:\nkubectl patch serviceaccount default \\ -n weblogic \\ -p '{\u0026quot;imagePullSecrets\u0026quot;: [{\u0026quot;name\u0026quot;: \u0026quot;secret1\u0026quot;}]}'  You can provide mutliple imagePullSecrets if you need to pull Docker images from multiple remote Docker registries or if your images require different authentication credentials. For more information, see Docker Image Protection under Security.\n Manually copying the image to your worker nodes If you are not able to use a remote Docker registry, for example if your Kubernetes cluster is in a secure network with no external access, you can manually copy the Docker images to the cluster instead.\nOn the machine where you created the image, export it into a tar file using this command:\ndocker save domain1:1.0 \u0026gt; domain1.tar  Then copy that tar file to each worker node in your Kubernetes cluster and run this command on each node:\ndocker load \u0026lt; domain1.tar  Restart pods to clear the error After you have ensured that the images are accessible on all worker nodes, you may need to restart the pods so that Kubernetes will attempt to pull the images again. You can do this by deleting the pods themselves, or deleting the domain resource and then recreating it.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": " Recent changes    Date Version Introduces backward incompatibilities Change     April 4, 2019 v2.1 no Customers can add init and sidecar containers to generated pods.   March 4, 2019 v2.0.1 no OpenShift support is now certified. Many bug fixes, including fixes for configuration overrides, cluster services, and domain status processing.   January 24, 2019 v2.0 yes; not compatible with 1.x releases, but is compatible with 2.0-rc2. Final version numbers and documentation updates.   January 16, 2019 v2.0-rc2 yes Schema updates are completed, and various bugs fixed.   December 20, 2018 v2.0-rc1 yes Operator is now installed via Helm charts, replacing the earlier scripts. The operator now supports the domain home on persistent volume or in Docker image use cases, which required a redesign of the domain schema. You can override the domain configuration using configuration override templates. Now load balancers and Ingresses can be independently configured. You can direct WebLogic logs to a persistent volume or to the pod\u0026rsquo;s log. Added life cycle support for servers and significantly enhanced configurability for generated pods. The final v2.0 release will be the initial release where the operator team intends to provide backward compatibility as part of future releases.   September 11, 2018 v1.1 no Enhanced the documentation and fixed various bugs.   May 7, 2018 v1.0 no Added support for dynamic clusters, the Apache HTTP Server, the Voyager Ingress Controller, and for PV in NFS storage for multi-node environments.   April 4, 2018 0.2 yes Many Kubernetes artifact names and labels have changed. Also, the names of generated YAML files for creating a domain\u0026rsquo;s PV and PVC have changed. Because of these changes, customers must recreate their operators and domains.   March 20, 2018  yes Several files and input parameters have been renamed. This affects how operators and domains are created. It also changes generated Kubernetes artifacts, therefore customers must recreate their operators and domains.    Known issues    Issue Description     None currently     "
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Oracle WebLogic Server Kubernetes Operator Oracle is finding ways for organizations using WebLogic Server to run important workloads, to move those workloads into the cloud. By certifying on industry standards, such as Docker and Kubernetes, WebLogic now runs in a cloud neutral infrastructure. In addition, we\u0026rsquo;ve provided an open-source Oracle WebLogic Server Kubernetes Operator (the “operator”) which has several key features to assist you with deploying and managing WebLogic domains in a Kubernetes environment. You can:\n Create WebLogic domains in a Kubernetes persistent volume. This persistent volume can reside in an NFS file system or other Kubernetes volume types. Create a WebLogic domain in a Docker image. Override certain aspects of the WebLogic domain configuration. Define WebLogic domains as a Kubernetes resource (using a Kubernetes custom resource definition). Start servers based on declarative startup parameters and desired states. Manage WebLogic configured or dynamic clusters. Expose the WebLogic Server Administration Console outside the Kubernetes cluster, if desired. Expose T3 channels outside the Kubernetes domain, if desired. Expose HTTP paths on a WebLogic domain outside the Kubernetes domain with load balancing and update the load balancer when Managed Servers in the WebLogic domain are started or stopped. Scale WebLogic domains by starting and stopping Managed Servers on demand, or by integrating with a REST API to initiate scaling based on WLDF, Prometheus, Grafana, or other rules. Publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana.  The fastest way to experience the operator is to follow the Quick Start guide, or you can peruse our documentation, read our blogs, or try out the samples.\nThe current release of the operator is 2.1. This release was published on April 4, 2019.\nOperator earlier versions Documentation for prior releases of the operator is available here.\nBackward compatibility guidelines Starting from the 2.0.1 release, operator releases are backward compatible with respect to the domain resource schema, operator Helm chart input values, configuration overrides template, Kubernetes resources created by the operator Helm chart, Kubernetes resources created by the operator, and the operator REST interface. We intend to maintain compatibility for three releases, except in the case of a clearly communicated deprecated feature, which will be maintained for one release after a replacement is available.\nAbout this documentation This documentation includes sections targeted to different audiences. To help you find what you are looking for more easily, please consult this table of contents:\n The Quick Start guide explains how to quickly get the operator running, using the defaults, nothing special. The User guide contains detailed usage information, including how to install and configure the operator, and how to use it to create and manage WebLogic domains.\n The Samples provide detailed example code and instructions that show you how to perform various tasks related to the operator. The Developer guide provides details for people who want to understand how the operator is built, tested, and so on. Those who wish to contribute to the operator code will find useful information here. This section also includes API documentation (Javadoc) and Swagger/OpenAPI documentation for the REST APIs. The Contributing section provides information about contribution requirements.  User guide The User guide provides detailed information about all aspects of using the operator including:\n Installing and configuring the operator. Using the operator to create and manage WebLogic domains. Manually creating WebLogic domains to be managed by the operator. Scaling WebLogic clusters. Configuring Kubernetes load balancers. Configuring Elasticsearch and Kibana to access the operator\u0026rsquo;s log files. Shutting down domains. Removing/deleting domains. And much more!  Samples Please refer to our samples for information about the available sample code.\nNeed more help? Have a suggestion? Come and say, \u0026ldquo;Hello!\u0026rdquo; We have a public Slack channel where you can get in touch with us to ask questions about using the operator or give us feedback or suggestions about what features and improvements you would like to see. We would love to hear from you. To join our channel, please visit this site to get an invitation. The invitation email will include details of how to access our Slack workspace. After you are logged in, please come to #operator and say, \u0026ldquo;hello!\u0026rdquo;\nRecent changes and known issues See the Release Notes for recent changes to the operator and known issues.\nDeveloper guide Developers interested in this project are encouraged to read the Developer guide to learn how to build the project, run tests, and so on. The Developer guide also provides details about the structure of the code, coding standards, and the Asynchronous Call facility used in the code to manage calls to the Kubernetes API.\nPlease take a look at our wish list to get an idea of the kind of features we would like to add to the operator. Maybe you will see something to which you would like to contribute!\nAPI documentation Documentation for APIs:\n The operator provides a REST API that you can use to obtain configuration information and to initiate scaling actions. For details about how to use the REST APIs, see Use the operator\u0026rsquo;s REST services.\n See the Swagger documentation for the operator\u0026rsquo;s REST interface.\n See the Javadoc for the operator.\n  Contributing to the operator Oracle welcomes contributions to this project from anyone. Contributions may be reporting an issue with the operator or submitting a pull request. Before embarking on significant development that may result in a large pull request, it is recommended that you create an issue and discuss the proposed changes with the existing developers first.\nIf you want to submit a pull request to fix a bug or enhance an existing feature, please first open an issue and link to that issue when you submit your pull request.\nIf you have any questions about a possible submission, feel free to open an issue too.\nContributing to the Oracle WebLogic Server Kubernetes Operator repository Pull requests can be made under The Oracle Contributor Agreement (OCA), which is available at https://www.oracle.com/technetwork/community/oca-486395.html.\nFor pull requests to be accepted, the bottom of the commit message must have the following line, using the contributor’s name and e-mail address as it appears in the OCA Signatories list.\nSigned-off-by: Your Name \u0026lt;you@example.org\u0026gt;  This can be automatically added to pull requests by committing with:\ngit commit --signoff  Only pull requests from committers that can be verified as having signed the OCA can be accepted.\nPull request process  Fork the repository. Create a branch in your fork to implement the changes. We recommend using the issue number as part of your branch name, for example, 1234-fixes. Ensure that any documentation is updated with the changes that are required by your fix. Ensure that any samples are updated if the base image has been changed. Submit the pull request. Do not leave the pull request blank. Explain exactly what your changes are meant to do and provide simple steps on how to validate your changes. Ensure that you reference the issue you created as well. We will assign the pull request to 2-3 people for review before it is merged.  Introducing a new dependency Please be aware that pull requests that seek to introduce a new dependency will be subject to additional review. In general, contributors should avoid dependencies with incompatible licenses, and should try to use recent versions of dependencies. Standard security vulnerability checklists will be consulted before accepting a new dependency. Dependencies on closed-source code, including WebLogic Server, will most likely be rejected.\n"
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://oracle.github.io/weblogic-kubernetes-operator/2.1/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]