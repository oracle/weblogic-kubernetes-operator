[
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/elastic-stack/operator/",
	"title": "Operator",
	"tags": [],
	"description": "Sample for configuring the Elasticsearch and Kibana deployments and services for the operator&#39;s logs.",
	"content": "When you install the operator Helm chart, you can set elkIntegrationEnabled to true in your values.yaml file to direct the operator to send the contents of the operator\u0026rsquo;s logs to Elasticsearch.\nTypically, you would have already configured Elasticsearch and Kibana in the Kubernetes cluster, and also would have specified elasticSearchHost and elasticSearchPort in your values.yaml file to point to where Elasticsearch is already running.\nThis sample configures the Elasticsearch and Kibana deployments and services. It\u0026rsquo;s useful for trying out the operator in a Kubernetes cluster that doesn\u0026rsquo;t already have them configured.\nIt runs the Elastic Stack on the same host and port that the operator\u0026rsquo;s Helm chart defaults to, therefore, you only need to set elkIntegrationEnabled to true in your values.yaml file.\nTo control Elasticsearch memory parameters (Heap allocation and Enabling/Disabling swapping), open the file elasticsearch_and_kibana.yaml, search for env variables of the Elasticsearch container and change the values of the following:\n ES_JAVA_OPTS: value may contain, for example, -Xms512m -Xmx512m to lower the default memory usage (please be aware that this value is applicable for demonstration purposes only and it is not the one recommended by Elasticsearch). bootstrap.memory_lock: value may contain true (enables the usage of mlockall, to try to lock the process address space into RAM, preventing any Elasticsearch memory from being swapped out) or false (disables the usage of mlockall).  To install Elasticsearch and Kibana, use:\n$ kubectl apply -f kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml To remove them, use:\n$ kubectl delete -f kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml "
},
{
	"uri": "/weblogic-kubernetes-operator/faq/namespace-management/",
	"title": "Managing domain namespaces",
	"tags": [],
	"description": "",
	"content": "Each operator deployment manages a number of Kubernetes Namespaces. For more information, see Operator Helm configuration values. A number of Kubernetes resources must be present in a namespace before any WebLogic domain custom resources can be successfully deployed into it. Those Kubernetes resources are created either as part of the installation of the operator\u0026rsquo;s Helm chart, or created by the operator at runtime.\nThis FAQ describes some considerations to be aware of when you manage the namespaces while the operator is running. For example:\n Check the namespaces that the operator manages Add a namespace for the operator to manage Delete a namespace from the operator\u0026rsquo;s domain namespace list Delete and recreate a Kubernetes Namespace that the operator manages  For others, see Common Mistakes and Solutions.\nThere can be multiple operators in a Kubernetes cluster, and in that case, you must ensure that their respective lists of domainNamespaces do not overlap.\n Check the namespaces that the operator manages You can find the list of the namespaces that the operator manages using the helm get values command. For example, the following command shows all the values of the operator release weblogic-operator; the domainNamespaces list contains default and ns1.\n$ helm get values weblogic-operator domainNamespaces: - default - ns1 elasticSearchHost: elasticsearch.default.svc.cluster.local elasticSearchPort: 9200 elkIntegrationEnabled: false externalDebugHttpPort: 30999 externalRestEnabled: false externalRestHttpsPort: 31001 image: oracle/weblogic-kubernetes-operator:2.5.0 imagePullPolicy: IfNotPresent internalDebugHttpPort: 30999 istioEnabled: false javaLoggingLevel: INFO logStashImage: logstash:6.6.0 remoteDebugNodePortEnabled: false serviceAccount: default suspendOnDebugStartup: false If you don\u0026rsquo;t know the release name of the operator, you can use helm list to list all the releases for a specified namespace or all namespaces:\n$ helm list --namespace \u0026lt;namespace\u0026gt; $ helm list --all-namespaces Add a Kubernetes Namespace to the operator If you want an operator deployment to manage a namespace, you need to add the namespace to the operator\u0026rsquo;s domainNamespaces list. Note that the namespace has to already exist, for example, using the kubectl create command.\nAdding a namespace to the domainNamespaces list tells the operator deployment or runtime to initialize the necessary Kubernetes resources for the namespace so that the operator is ready to host WebLogic domain resources in that namespace.\nWhen the operator is running and managing the default namespace, the following example Helm command adds the namespace ns1 to the domainNamespaces list, where weblogic-operator is the release name of the operator, and kubernetes/charts/weblogic-operator is the location of the operator\u0026rsquo;s Helm charts.\n$ helm upgrade \\ --reuse-values \\ --set \u0026quot;domainNamespaces={default,ns1}\u0026quot; \\ --wait \\ --force \\ weblogic-operator \\ kubernetes/charts/weblogic-operator  Changes to the domainNamespaces list might not be picked up by the operator right away because the operator monitors the changes to the setting periodically. The operator becomes ready to host domain resources in a namespace only after the required configmap (namely weblogic-domain-cm) is initialized in the namespace.\n You can verify that the operator is ready to host domain resources in a namespace by confirming the existence of the required configmap resource.\n$ kubetctl get cm -n \u0026lt;namespace\u0026gt; For example, the following example shows that the domain configmap resource exists in the namespace ns1.\nbash-4.2$ kubectl get cm -n ns1 NAME DATA AGE weblogic-domain-cm 14 12m Delete a Kubernetes Namespace from the operator When you no longer want a namespace to be managed by the operator, you need to remove it from the operator\u0026rsquo;s domainNamespaces list, so that the corresponding Kubernetes resources that are associated with the namespace can be cleaned up.\nWhile the operator is running and managing the default and ns1 namespaces, the following example Helm command removes the namespace ns1 from the domainNamespaces list, where weblogic-operator is the release name of the operator, and kubernetes/charts/weblogic-operator is the location of the operator Helm charts.\n$ helm upgrade \\ --reuse-values \\ --set \u0026quot;domainNamespaces={default}\u0026quot; \\ --wait \\ --force \\ weblogic-operator \\ kubernetes/charts/weblogic-operator Recreate a previously deleted Kubernetes Namespace If you need to delete a namespace (and the resources in it) and then recreate it, remember to remove the namespace from the operator\u0026rsquo;s domainNamespaces list after you delete the namespace, and add it back to the domainNamespaces list after you recreate the namespace using the helm upgrade commands that were illustrated previously.\nMake sure that you wait a sufficient period of time between deleting and recreating the namespace because it takes time for the resources in a namespace to go away after the namespace is deleted. In addition, as mentioned above, changes to the domainNamespaces setting is monitored by the operator periodically, and the operator becomes ready to host domain resources only after the required domain configmap (namely weblogic-domain-cm) is initialized in the namespace.\n If a domain custom resource is created before the namespace is ready, you might see that the introspector job pod fails to start, with a warning like the following, when you review the description of the introspector pod. Note that domain1 is the name of the domain in the following example output.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned domain1-introspect-domain-job-bz6rw to slc16ffk Normal SuccessfulMountVolume 1m kubelet, slc16ffk MountVolume.SetUp succeeded for volume \u0026quot;weblogic-credentials-volume\u0026quot; Normal SuccessfulMountVolume 1m kubelet, slc16ffk MountVolume.SetUp succeeded for volume \u0026quot;default-token-jzblm\u0026quot; Warning FailedMount 27s (x8 over 1m) kubelet, slc16ffk MountVolume.SetUp failed for volume \u0026quot;weblogic-domain-cm-volume\u0026quot; : configmaps \u0026quot;weblogic-domain-cm\u0026quot; not found If you still run into problems after you perform the helm upgrade to re-initialize a namespace that is deleted and recreated, you can restart the operator pod as shown in the following examples, where the operator itself is running in the namespace weblogic-operator-namespace with the release name, weblogic-operator.\n Kill the operator pod, and let Kubernetes restart it.  $ kubectl delete pod/weblogic-operator-65b95bc5b5-jw4hh -n weblogic-operator-namespace  Scale the operator deployment to 0 and then back to 1 by changing the value of the replicas.  $ kubectl scale deployment.apps/weblogic-operator -n weblogic-operator-namespace --replicas=0 $ kubectl scale deployment.apps/weblogic-operator -n weblogic-operator-namespace --replicas=1 Note that restarting the operator pod makes the operator temporarily unavailable for managing its namespaces. For example, a domain that is created while the operator is restarting will not be started until the operator pod is fully up again.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/experimental/istio/",
	"title": "Istio support",
	"tags": [],
	"description": "",
	"content": "Overview WebLogic Server Kubernetes Operator version 2.3 and later includes experimental support for Istio 1.2.2 and later. This support allows you to run the operator itself, and WebLogic domains managed by the operator, with Istio sidecar injection enabled. It will allow you to use Istio gateways and virtual services to access applications deployed in these domains. If your applications have suitable tracing code in them, then you will also be able to use distributed tracing, such as Jaeger, to trace requests across domains and to other components and services that have tracing enabled.\nLimitations The current experimental support for Istio has these limitations:\n It is tested with Istio 1.2.2 and later (up to 1.5), however it is tested with both single and multicluster installations of Istio. Support is provided only for domains that are stored in persistent volumes and created with the provided sample using the WLST option. We intend to support Domain in Image and WDT options as well, but that is not currently available. Support is provided only for domains with a single dynamic cluster. Multiple clusters and configured clusters are not currently supported.  Using the operator with experimental Istio support These instructions assume that you are using a Kubernetes cluster with Istio installed and configured already. The operator will not install Istio for you.\n You can deploy the operator into a namespace which has Istio automatic sidecar injection enabled. Before installing the operator, create the namespace you wish to run the operator in, and label it for automatic injection.\n$ kubectl create namespace weblogic-operator $ kubectl label namespace weblogic-operator istio-injection=enabled After the namespace is labeled, you can install the operator using the normal method. When the operator pod starts, you will notice that Istio automatically injects an initContainer called istio-init and the envoy container istio-proxy.\nYou can check this using the following commands:\n$ kubectl --namespace weblogic-operator get pods $ kubectl --namespace weblogic-operator get pod weblogic-operator-xxx-xxx -o yaml In the second command, change weblogic-operator-xxx-xxx to the name of your pod.\nCreating a domain with experimental Istio support You can configure your domains to run with Istio automatic sidecar injection enabled. Before creating your domain, create the namespace you wish to run the domain in, and label it for automatic injection.\n$ kubectl create namespace domain1 $ kubectl label namespace domain1 istio-injection=enabled Currently, the experimental Istio support is provided only for domains stored on persistent volumes. To enable the support for a domain, you need to add the experimental section to your domain custom resource YAML file as shown in the example below.\nThis must be done in the inputs file before running the create-domain.sh script in the sample directory because the create-domain.sh script makes the necessary adjustments to the domain to make it work in an Istio environment.\n# If you are using 3.0.0-rc1, then the version on the following line # should be `v7` not `v6`. apiVersion: \u0026quot;weblogic.oracle/v6\u0026quot; kind: Domain metadata: name: domain2 namespace: domain1 labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain2 spec: ... other content ... experimental: istio: enabled: true readinessPort: 8888 To enable the experimental Istio support, you must include the istio section and you must set enabled: true as shown. The readniessPort is optional and defaults to 8888 if not provided.\nHow Istio-enabled domains differ from regular domains Istio enforces a number of requirements on pods. When you enable Istio support, the domain on a persistent volume sample scripts will make the following adjustments to your domain in order to satisy Istio\u0026rsquo;s requirements:\n On the Administration Server:  Create a channel called istio-probe with listen address 127.0.0.1:8888 (or the port you specified in the readinessPort setting). Create a channel called istio-t3 with listen address 127.0.0.1 and the port you specified as the admin port. Create a channel called istio-ldap with listen address 127.0.0.1 and the port you specified as the admin port, with only the LDAP protocol enabled. Create a channel called istio-T3Channel with listen address 127.0.0.1 and the port you specified as the T3 port.   In the server template that is used to create Managed Servers in clusters:  Create a channel called istio-probe with listen address 127.0.0.1:8888 (or the port you specified in the readinessPort setting) and the public address set to the Kubernetes Service for the Managed Server. Create a channel called istio-t3 with listen address 127.0.0.1 and the port you specified as the admin port and the public address set to the Kubernetes Service for the Managed Server. Create a channel called istio-cluster with listen address 127.0.0.1 and the port you specified as the admin port, with only the CLUSTER_BROADCAST protocol enabled, and the public address set to the Kubernetes Service for the Managed Server. Create a channel called istio-http with listen address 127.0.0.1:31111 and the public address set to the Kubernetes Service for the Managed Server. Note that 31111 is the Istio proxy (envoy) port.   The create domain job will be configured to disable injection of the Istio sidecar.  Additionally, when the experimental Istio support is enabled for a domain, the operator will ensure that the Istio sidecar is not injected into the introspector job\u0026rsquo;s pods.\nExposing applications in Istio-enabled domains When a domain is running with the experimental Istio support, you should use the Istio gateway to provide external access to applications, instead of using an ingress controller like Traefik. Using the Istio gateway will enable you to view the traffic in Kiali and to use distributed tracing all the way from the entry point to the cluster, for example, the Istio gateway.\nTo configure external access to your domain, you need to create an Istio gateway and virtualservice as shown in the example below:\n--- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: domain1-gateway namespace: domain1 spec: selector: istio: ingressgateway servers: - hosts: - '*' port: name: http number: 80 protocol: HTTP --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: domain1-virtualservice namespace: domain1 spec: gateways: - domain1-gateway hosts: - '*' http: - match: - uri: prefix: / - port: 8001 route: - destination: host: domain1-cluster-cluster-1.domain1.svc.cluster.local port: number: 8001 This example creates a gateway that will accept requests with any host name using HTTP on port 80, and a virtual service that will route all of those requests to the cluster service for cluster-1 in domain1 in the namespace domain1.\nFor more information about providing ingress using Istio, refer to the Istio documentation.\nTraffic management Istio provides traffic management capabilities, including the ability to visualize traffic in Kiali. You do not need to change your applications to use this feature. The Istio proxy (envoy) sidecar that is injected into your pods provides this visibility. The experimental Istio support does enable traffic management. The image below shows an example with traffic flowing:\n In from the Istio gateway on the left. To a domain called bobbys-front-end. To a non-WebLogic application, in this case a Helidon microservice called bobbys-helidon-stock-application. To a second domain called bobs-bookstore.  In this example you can see how the traffic flows to the cluster services and then to the individual Managed Servers.\nYou can learn more about Istio traffic management in their documentation.\nDistributed tracing Istio provides distributed tracing capabilities, including the ability to view traces in Jaeger. In order to use distributed tracing though, you will need to instrument your WebLogic application first, for example, using the Jaeger Java client. The image below shows an example of a distributed trace that shows a transaction following the same path through the system as shown in the image above.\nYou can learn more about distrubting tracing in Istio in their documentation.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-fmw-domains/fmw-infra/",
	"title": "Manage FMW Infrastructure domains",
	"tags": [],
	"description": "FMW Infrastructure domains contain the Java Required Files (JRF) feature and are the prerequisite for upper stack products like Oracle SOA Suite.",
	"content": "Contents  Limitations Obtaining the FMW Infrastructure Docker image Creating an FMW Infrastructure Docker image Configuring access to your database Running the Repository Creation Utility to set up your database schema Create a Kubernetes Secret with the RCU credentials Creating an FMW Infrastructure domain Patching the FMW Infrastructure image Additional considerations for Coherence  Starting with the 2.2.0 release, the operator supports FMW Infrastructure domains, that is, domains that are created with the FMW Infrastructure installer rather than the WebLogic Server installer. These domains contain the Java Required Files (JRF) feature and are the prerequisite for \u0026ldquo;upper stack\u0026rdquo; products like Oracle SOA Suite, for example. These domains also require a database and the use of the Repository Creation Utility (RCU).\nThis document provides details about the special considerations for running FMW Infrastructure domains with the operator. Other than those considerations listed here, FMW Infrastructure domains work in the same way as WebLogic Server domains. The remainder of the documentation in this site applies equally to FMW Infrastructure domains and WebLogic Server domains.\nFMW Infrastructure domains are supported using both Domain in PV and Domain in Image domain home source types. If you plan to experiment with upper stack products (which are not officially supported by the operator yet), we strongly recommend using the domain on a persistent volume approach.\nLimitations Compared to running a WebLogic Server domain in Kubernetes using the operator, the following limitations currently exist for FMW Infrastructure domains:\n The WebLogic Logging Exporter currently supports WebLogic Server logs only. Other logs will not be sent to Elasticsearch. Note, however, that you can use a sidecar with a log handling tool like Logstash or Fluentd to get logs. The WebLogic Monitoring Exporter currently supports the WebLogic MBean trees only. Support for JRF MBeans has not been added yet. Only configured clusters are supported. Dynamic clusters are not supported for FMW Infrastructure domains. Note that you can still use all of the scaling features, you just need to define the maximum size of your cluster at domain creation time. FMW Infrastructure domains are not supported with any version of the operator before version 2.2.0.  Obtaining the FMW Infrastructure Docker Image The Oracle WebLogic Server Kubernetes Operator requires patch 29135930. The standard pre-built FMW Infrastructure image, container-registry.oracle.com/middleware/fmw-infrastrucutre:12.2.1.3, already has this patch applied. For detailed instructions on how to log in to the Oracle Container Registry and accept license agreement, see this document. The FMW Infrastructure 12.2.1.4.0 images do not require patches.\nTo pull an image from the Oracle Container Registry, in a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms is stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nFirst, you will need to log in to the Oracle Container Registry:\n$ docker login container-registry.oracle.com Then, you can pull the image with this command:\n$ docker pull container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 If desired, you can:\n  Check the WLS version with docker run container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 sh -c 'source $ORACLE_HOME/wlserver/server/bin/setWLSEnv.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; java weblogic.version'\n  Check the WLS patches with docker run container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 sh -c '$ORACLE_HOME/OPatch/opatch lspatches'\n  Additional information about using this image is available in the Oracle Container Registry.\nCreating an FMW Infrastructure Docker image You can also create a Docker image containing the FMW Infrastructure binaries. We provide a sample in the Oracle GitHub account that demonstrates how to create a Docker image to run the FMW Infrastructure. Please consult the README file associated with this sample for important prerequisite steps, such as building or pulling the Server JRE Docker image and downloading the Fusion Middleware Infrastructure installer binary.\nAfter cloning the repository and downloading the installer from Oracle Technology Network or e-delivery, you create your image by running the provided script:\ncd docker-images/OracleFMWInfrastructure/dockerfiles ./buildDockerImage.sh -v 12.2.1.4 -s The image produced will be named oracle/fmw-infrastructure:12.2.1.4.\nYou must also install the required patch to use this image with the operator. We provide a sample that demonstrates how to create a Docker image with the necessary patch installed.\nAfter downloading the patch from My Oracle Support, you create the patched image by running the provided script:\ncd docker-images/OracleFMWInfrastructure/samples/12213-patch-fmw-for-k8s ./build.sh This will produce an image named oracle/fmw-infrastructure:12213-update-k8s.\nAll samples and instructions reference the pre-built image, container-registry.oracle.com/middleware/fmw_infrastructure:12.2.1.4. Because these samples build an image based on WebLogic Server 12.2.1.3 and use the tag, oracle/fmw-infrastructure:12213-update-k8s, be sure to update your sample inputs to use this image value.\nThese samples allow you to create a Docker image containing the FMW Infrastructure binaries and the necessary patch. You can use this image to run the Repository Creation Utility and to run your domain using the \u0026ldquo;domain on a persistent volume\u0026rdquo; model. If you want to use the \u0026ldquo;domain in a Docker image\u0026rdquo; model, you will need to go one step further and add another layer with your domain in it. You can use WLST or WDT to create your domain.\nBefore creating a domain, you will need to set up the necessary schemas in your database.\nConfiguring access to your database FMW Infrastructure domains require a database with the necessary schemas installed in them. We provide a utility, called the Repository Creation Utility (RCU), which allows you to create those schemas. You must set up the database before you create your domain. There are no additional requirements added by running FMW Infrastructure in Kubernetes; the same existing requirements apply.\nFor testing and development, you may choose to run your database inside Kubernetes or outside of Kubernetes.\nThe Oracle Database Docker images are only supported for non-production use. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1)\n Running the database inside Kubernetes If you wish to run the database inside Kubernetes, you can use the official Docker image from Docker Hub or the Oracle Container Registry. Please note that there is a Slim Variant (12.2.0.1-slim tag) of EE that has reduced disk space (4GB) requirements and a quicker container startup.\nRunning the database inside the Kubernetes cluster is possibly more relevant or desirable in test/development or CI/CD scenarios.\nHere is an example of a Kubernetes YAML file to define a deployment of the Oracle database:\napiVersion: extensions/v1beta1 kind: Deployment metadata: name: oracle-db namespace: default spec: replicas: 1 selector: matchLabels: app.kubernetes.io/instance: dev app.kubernetes.io/name: oracle-db strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: creationTimestamp: null labels: app.kubernetes.io/instance: dev app.kubernetes.io/name: oracle-db spec: containers: - env: - name: DB_SID value: devcdb - name: DB_PDB value: devpdb - name: DB_DOMAIN value: k8s image: container-registry.oracle.com/database/enterprise:12.2.0.1-slim imagePullPolicy: IfNotPresent name: oracle-db ports: - containerPort: 1521 name: tns protocol: TCP resources: limits: cpu: \u0026#34;1\u0026#34; memory: 2Gi requests: cpu: 200m terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 Notice that you can pass in environment variables to set the SID, the name of the PDB, and so on. The documentation describes the other variables that are available. The sys password defaults to Oradoc_db1. Follow the instructions in the documentation to reset this password.\nYou should also create a service to make the database available within the Kubernetes cluster with a well known name. Here is an example:\napiVersion: v1 kind: Service metadata: name: oracle-db namespace: default spec: ports: - name: tns port: 1521 protocol: TCP targetPort: 1521 selector: app.kubernetes.io/instance: dev app.kubernetes.io/name: oracle-db sessionAffinity: None type: ClusterIP In the example above, the database would be visible in the cluster using the address oracle-db.default.svc.cluster.local:1521/devpdb.k8s.\nWhen you run the database in the Kubernetes cluster, you will probably want to also run RCU from a pod inside your network, though this is not strictly necessary. You could create a NodePort to expose your database outside the Kubernetes cluster and run RCU on another machine with access to the cluster.\nRunning the database outside Kubernetes If you wish to run the database outside Kubernetes, you need to create a way for containers running in pods in Kubernetes to see the database. This can be done by defining a Kubernetes Service with no selector and associating it with an endpoint definition, as shown in the example below:\nkind: Service apiVersion: v1 metadata: name: database spec: type: ClusterIP ports: - port: 1521 targetPort: 1521 --- kind: Endpoints apiVersion: v1 metadata: name: database subsets: - addresses: - ip: 129.123.1.4 ports: - port: 1521 This creates a DNS name database in the current namespace, ordefault if no namespace is specified, as in the example above. In this example, the fully qualified name would be database.default.svc.cluster.local. The second part is the namespace. If you looked up the ClusterIP for such a service, it would have an IP address on the overlay network, that is the network inside the Kubernetes cluster. If you are using flannel, for example, the address might be something like 10.0.1.25. Note that this is usually a non-routed address.\nFrom a container in a pod running in Kubernetes, you can make a connection to that address and port 1521. Kubernetes will route the connection to the address provided in the endpoint definition, in this example, 129.123.1.4:1521. This IP address (or name) is resolved from the point of view of the Kubernetes Node\u0026rsquo;s IP stack, not the overlay network inside the Kubernetes cluster. Note that this is a \u0026ldquo;real\u0026rdquo; routed IP address.\nWhen you create your data sources, you would use the internal address, for example, database:1521/some.service.\nBecause your database is externally accessible, you can run RCU in the normal way, from any machine on your network.\nRunning the Repository Creation Utility to set up your database schema If you want to run RCU from a pod inside the Kubernetes cluster, you can use the Docker image that you built earlier as a \u0026ldquo;service\u0026rdquo; pod to run RCU. To do this, start up a pod using that image as follows:\nkubectl run rcu --generator=run-pod/v1 --image container-registry.oracle.com/middleware/fmw_infrastructure:12.2.1.4 -- sleep infinity This will create a Kubernetes Deployment called rcu containing a pod running a container created from the container-registry.oracle.com/middleware/fmw_infrastructure:12.2.1.4 image which will just run sleep infinity, which essentially creates a pod that we can \u0026ldquo;exec\u0026rdquo; into and use to run whatever commands we need to run.\nTo get inside this container and run commands, use this command:\nkubectl exec -ti rcu /bin/bash When you are finished with this pod, you can remove it with this command:\nkubectl delete pod rcu  You can use the same approach to get a temporary pod to run other utilities like WLST.\n Creating schemas Inside this pod, you can use the following command to run RCU in command-line (no GUI) mode to create your FMW schemas. You will need to provide the right prefix and connect string. You will be prompted to enter the password for the sys user, and then the password to use for the regular schema users:\n/u01/oracle/oracle_common/bin/rcu \\  -silent \\  -createRepository \\  -databaseType ORACLE \\  -connectString oracle-db.default:1521/devpdb.k8s \\  -dbUser sys \\  -dbRole sysdba \\  -useSamePasswordForAllSchemaUsers true \\  -selectDependentsForComponents true \\  -schemaPrefix FMW1 \\  -component MDS \\  -component IAU \\  -component IAU_APPEND \\  -component IAU_VIEWER \\  -component OPSS \\  -component WLS \\  -component STB You need to make sure that you maintain the association between the database schemas and the matching domain just like you did in a non-Kubernetes environment. There is no specific functionality provided to help with this. We recommend that you consider making the RCU prefix (value of the schemaPrefix argument) the same as your domainUID to help maintain this association.\nDropping schemas If you want to drop the schema, you can use a command like this:\n/u01/oracle/oracle_common/bin/rcu \\  -silent \\  -dropRepository \\  -databaseType ORACLE \\  -connectString oracle-db.default:1521/devpdb.k8s \\  -dbUser sys \\  -dbRole sysdba \\  -selectDependentsForComponents true \\  -schemaPrefix FMW1 \\  -component MDS \\  -component IAU \\  -component IAU_APPEND \\  -component IAU_VIEWER \\  -component OPSS \\  -component WLS \\  -component STB Again, you will need to set the right prefix and connection string, and you will be prompted to enter the sys user password.\nCreate a Kubernetes Secret with the RCU credentials You also need to create a Kubernetes Secret containing the credentials for the database schemas. When you create your domain using the sample provided below, it will obtain the RCU credentials from this secret.\nWe provide a sample that demonstrates how to create the secret. The schema owner user name required will be the schemaPrefix value followed by an underscore and a component name, such as FMW1_STB. The schema owner password will be the password you provided for regular schema users during RCU creation.\nCreating an FMW Infrastructure domain Now that you have your Docker images and you have created your RCU schemas, you are ready to create your domain. We provide a sample that demonstrates how to create an FMW Infrastructure domain.\nPatching the FMW Infrastructure image There are two kinds of patches that can be applied to the FMW Infrastructure binaries:\n Patches which are eligible for Zero Downtime Patching (ZDP), meaning that they can be applied with a rolling restart. Non-ZDP eligible compliant patches, meaning that the domain must be shut down and restarted.  You can find out whether or not a patch is eligible for Zero Downtime Patching by consulting the README file that accompanies the patch.\nIf you wish to apply a ZDP compliant patch which can be applied with a rolling restart, after you have patched the FMW Infrastructure image as shown in this sample, you can edit the domain custom resource with the name of the new image and the operator will initiate a rolling restart of the domain.\nIf you wish to apply a non-ZDP compliant patch to the FMW Infrastructure binary image, you must shut down the entire domain before applying the patch. Please see the documentation on domain lifecycle operations for more information.\nAn example of a non-ZDP compliant patch is one that includes a schema change that can not be applied dynamically.\nAdditional considerations for Coherence If you are running a domain which contains Coherence, please refer to Coherence requirements for more information.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/cicd/layering/",
	"title": "Docker image layering",
	"tags": [],
	"description": "Learn about Docker image layering and why it is important.",
	"content": "Docker images are composed of layers, as shown in the diagram below. If you download the standard weblogic:12.2.1.4 image from the Oracle Container Registry, then you can see these layers using the command docker inspect container-registry.oracle.com/middleware/weblogic:12.2.1.4 (the domain layer will not be there). You are not required to use layers, but efficient use of layers is considered a best practice.\nWhy is it important to maintain the layering of images? Layering is an important technique in Docker images. Layers are important because they are shared between images. Let\u0026rsquo;s consider an example. In the diagram below, we have two domains that we have built using layers. The second domain has some additional patches that we needed on top of those provided in the standard WebLogic image. Those are installed in their own layer, and then the second domain is created in another layer on top of that.\nLet\u0026rsquo;s assume we have a three-node Kubernetes cluster and we are running both domains in this cluster. Sooner or later, we will end up with servers in each domain running on each node, so eventually all of the image layers are going to be needed on all of the nodes. Using the approach shown below (that is, standard Docker layering techniques) we are going to need to store all six of these layers on each node. If you add up the sizes, then you will see that it comes out to about 1.5GB per node.\nNow, let\u0026rsquo;s consider the alternative, where we do not use layers, but instead, build images for each domain and put everything in one big layer (this is often called \u0026ldquo;squashing\u0026rdquo; the layers). In this case, we have the same content, but if you add up the size of the images, you get 2.9GB per node. That’s almost twice the size!\nWith only two domains, you start to see the problem. In the layered approach, each new domain is adding only a relatively very small increment. In the non-layered approach, each new domain is essentially adding the entire stack over again. Imagine if we had ten domains, now the calculation looks like this:\n    With Layers Without Layers     Shared Layers 1.4GB 0GB   Dedicated/different layers 10 x 10MB = 100MB 10 x 1.5GB = 15GB   Total per node 1.5GB 15GB    You can see how the amount of storage for images really starts to add up, and it is not just a question of storage. When Kubernetes creates a container from an image, the size of the image has an impact on how long it takes to create and start the container.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/security/domain-security/image-protection/",
	"title": "Docker image protection",
	"tags": [],
	"description": "WebLogic domain in Docker image protection",
	"content": "WebLogic domain in Docker image protection Oracle strongly recommends storing the Docker images that contain a WebLogic domain home as private in the Docker registry. In addition to any local registry, public Docker registries include Docker Hub and the Oracle Cloud Infrastructure Registry (OCIR).\n The WebLogic domain home that is part of a Docker image contains sensitive information about the domain including keys and credentials that are used to access external resources (for example, the data source password). In addition, the Docker image may be used to create a running server that further exposes the WebLogic domain outside of the Kubernetes cluster.\nThere are two main options to pull images from a private registry:\n Specify the image pull secret on the WebLogic Domain resource. Set up the ServiceAccount in the domain namespace with an image pull secret.  1. Use imagePullSecrets with the Domain resource. In order to access a Docker image that is protected by a private registry, the imagePullSecrets should be specified in the Kubernetes Domain resource definition:\napiVersion: \u0026#34;weblogic.oracle/v2\u0026#34; kind: Domain metadata: name: domain1 namespace: domain1-ns labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: domainHomeSourceType: Image image: \u0026#34;my-domain-home-in-image\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; imagePullSecrets: - name: \u0026#34;my-registry-pull-secret\u0026#34; webLogicCredentialsSecret: name: \u0026#34;domain1-weblogic-credentials\u0026#34; To create the Kubernetes Secret, my-registry-pull-secret, in the namespace where the domain will be running, domain1-ns, the following command can be used:\n$ kubectl create secret docker-registry my-registry-pull-secret \\  -n domain1-ns \\  --docker-server=\u0026lt;registry-server\u0026gt; \\  --docker-username=\u0026lt;name\u0026gt; \\  --docker-password=\u0026lt;password\u0026gt; \\  --docker-email=\u0026lt;email\u0026gt; For more information about creating Kubernetes Secrets for accessing the registry, see the Kubernetes documentation about pulling an image from a private registry.\n2. Set up the Kubernetes ServiceAccount with imagePullSecrets. An additional option for accessing a Docker image protected by a private registry is to set up the Kubernetes ServiceAccount in the namespace running the WebLogic domain with a set of image pull secrets thus avoiding the need to set imagePullSecrets for each Domain resource being created (because each resource instance represents a WebLogic domain that the operator is managing).\nThe Kubernetes Secret would be created in the same manner as shown above and then the ServiceAccount would be updated to include this image pull secret:\n$ kubectl patch serviceaccount default -n domain1-ns \\  -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;my-registry-pull-secret\u0026#34;}]}\u0026#39; For more information about updating a Kubernetes ServiceAccount for accessing the registry, see the Kubernetes documentation about configuring service accounts.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/security/certificates/",
	"title": "Certificates",
	"tags": [],
	"description": "Operator SSL/TLS certificate handling",
	"content": "Updating operator external certificates If the operator needs to update the external certificate and key currently being used or was installed without an external REST API SSL/TLS identity, the helm upgrade command is used to restart the operator with the new or updated Kubernetes tls secret that contains the desired certificates.\nThe operator requires a restart in order to begin using the new or updated external certificate. The Helm --recreate-pods flag is used to cause the existing Kubernetes Pod to be terminated and a new pod to be started with the updated configuration.\nFor example, if the operator was installed with the Helm release name weblogic-operator in the namespace weblogic-operator-ns and the Kubernetes tls secret is named weblogic-operator-cert, the following commands can be used to update the operator certificates and key:\n$ kubectl create secret tls weblogic-operator-cert -n weblogic-operator-ns \\  --cert=\u0026lt;path-to-certificate\u0026gt; --key=\u0026lt;path-to-private-key\u0026gt; $ helm get values weblogic-operator -n weblogic-operator-ns $ helm -n weblogic-operator-ns upgrade weblogic-operator kubernetes/charts/weblogic-operator \\  --wait --recreate-pods --reuse-values \\  --set externalRestEnabled=true \\  --set externalRestIdentitySecret=weblogic-operator-cert Additional reading  Configure the external REST interface SSL/TLS identity REST interface configuration settings Sample to create external certificate and key  "
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/credentials/",
	"title": "Credentials",
	"tags": [],
	"description": "Sample for creating a Kubernetes Secret that contains the Administration Server credentials. This secret can be used in creating a WebLogic domain resource.",
	"content": "Creating credentials for a WebLogic domain This sample demonstrates how to create a Kubernetes Secret containing the credentials for a WebLogic domain. The operator expects this secret to be named following the pattern domainUID-weblogic-credentials, where domainUID is the unique identifier of the domain. It must be in the same namespace that the domain will run in.\nTo use the sample, run the command:\n$ ./create-weblogic-credentials.sh -u \u0026lt;username\u0026gt; -p \u0026lt;password\u0026gt; -d domainUID -n namespace -s secretName The parameters are as follows:\n -u user name, must be specified. -p password, must be specified. -d domainUID, optional. The default value is domain1. If specified, the secret will be labeled with the domainUID unless the given value is an empty string. -n namespace, optional. Use the default namespace if not specified. -s secretName, optional. If not specified, the secret name will be determined based on the domainUID value. This creates a generic secret containing the user name and password as literal values.\nYou can check the secret with the kubectl get secret command. An example is shown below, including the output:\n$ kubectl -n domain-namespace-1 get secret domain1-weblogic-credentials -o yaml apiVersion: v1 data: password: d2VsY29tZTE= username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: 2018-12-12T20:25:20Z labels: weblogic.domainName: domain1 weblogic.domainUID: domain1 name: domain1-weblogic-credentials namespace: domain-namespace-1 resourceVersion: \u0026quot;5680\u0026quot; selfLink: /api/v1/namespaces/domain-namespace-1/secrets/domain1-weblogic-credentials uid: 0c2b3510-fe4c-11e8-994d-00001700101d type: Opaque "
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/domains/manually-create-domain/",
	"title": "Manually",
	"tags": [],
	"description": "Sample for creating the domain custom resource manually.",
	"content": "In some circumstances you may wish to manually create your domain custom resource. If you have created your own Docker image containing your domain and the specific patches that it requires, then this approach will probably be the most suitable for your needs.\nTo create the domain custom resource, just make a copy of the sample domain.yaml, and then edit it using the instructions provided in the comments in that file. When it is ready, you can create the domain in your Kubernetes cluster using the command:\n$ kubectl apply -f domain.yaml You can verify the domain custom resource was created using this command:\n$ kubectl -n YOUR_NAMESPACE get domains You can view the details of the domain using this command:\n$ kubectl -n YOUR_NAMESPACE describe domain YOUR_DOMAIN In both of these commands, replace YOUR_NAMESPACE with the namespace in which you created the domain, and replace YOUR_DOMAIN with the domainUID you chose.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/storage/",
	"title": "Storage",
	"tags": [],
	"description": "Sample for creating a PV or PVC that can be used by a domain resource as the persistent storage for the WebLogic domain home or log files.",
	"content": "Sample PersistentVolume and PersistentVolumeClaim The sample scripts demonstrate the creation of a Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC), which can then be used in a domain resource as a persistent storage for the WebLogic domain home or log files.\nA PV and PVC can be shared by multiple WebLogic domains or dedicated to a particular domain.\nPrerequisites Before you begin, read this document, Persistent storage.\nUsing the scripts to create a PV and PVC Prior to running the create-pv-pvc.sh script, make a copy of the create-pv-pvc-inputs.yaml file, and uncomment and explicitly configure the weblogicDomainStoragePath property in the inputs file.\nRun the create script, pointing it at your inputs file and an output directory:\n$ ./create-pv-pvc.sh \\ -i create-pv-pvc-inputs.yaml \\ -o /path/to/output-directory The create-pv-pvc.sh script will create a subdirectory pv-pvcs under the given /path/to/output-directory directory. By default, the script generates two YAML files, namely weblogic-sample-pv.yaml and weblogic-sample-pvc.yaml, in the /path/to/output-directory/pv-pvcs. These two YAML files can be used to create the Kubernetes resources using the kubectl create -f command.\n$ kubectl create -f weblogic-sample-pv.yaml $ kubectl create -f weblogic-sample-pvc.yaml As a convenience, the script can optionally create the PV and PVC resources using the -e option.\nThe usage of the create script is as follows:\n$ sh create-pv-pvc.sh -h usage: create-pv-pvc.sh -i file -o dir [-e] [-h] -i Parameter inputs file, must be specified. -o Output directory for the generated yaml files, must be specified. -e Also create the Kubernetes objects using the generated yaml files -h Help If you copy the sample scripts to a different location, make sure that you copy everything in the \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts directory together into the target directory, maintaining the original directory hierarchy.\nConfiguration parameters The PV and PVC creation inputs can be customized by editing the create-pv-pvc-inputs.yaml file.\n   Parameter Definition Default     domainUID ID of the domain resource to which the generated PV and PVC will be dedicated. Leave it empty if the PV and PVC are going to be shared by multiple domains. no default   namespace Kubernetes Namespace to create the PVC. default   baseName Base name of the PV and PVC. The generated PV and PVC will be \u0026lt;baseName\u0026gt;-pv and \u0026lt;baseName\u0026gt;-pvc respectively. weblogic-sample   weblogicDomainStoragePath Physical path of the storage for the PV. When weblogicDomainStorageType is set to HOST_PATH, this value should be set the to path to the domain storage on the Kubernetes host. When weblogicDomainStorageType is set to NFS, then weblogicDomainStorageNFSServer should be set to the IP address or name of the DNS server, and this value should be set to the exported path on that server. Note that the path where the domain is mounted in the WebLogic containers is not affected by this setting; that is determined when you create your domain. no default   weblogicDomainStorageReclaimPolicy Kubernetes PVC policy for the persistent storage. The valid values are: Retain, Delete, and Recycle. Retain   weblogicDomainStorageSize Total storage allocated for the PVC. 10Gi   weblogicDomainStorageType Type of storage. Legal values are NFS and HOST_PATH. If using NFS, weblogicDomainStorageNFSServer must be specified. HOST_PATH   weblogicDomainStorageNFSServer Name or IP address of the NFS server. This setting only applies if weblogicDomainStorateType is NFS. no default    Shared versus dedicated PVC By default, the domainUID is left empty in the inputs file, which means the generated PV and PVC will not be associated with a particular domain, but can be shared by multiple domain resources in the same Kubernetes Namespaces as the PV and PVC. If the PV/PVC is being shared across domains, then, as a best practice, you should specify a unique baseName.\nFor the use cases where dedicated PV and PVC are desired for a particular domain, the domainUID needs to be set in the create-pv-pvc-inputs.yaml file. The presence of a non-empty domainUID in the inputs file will cause the generated PV and PVC to be associated with the specified domainUID. The association includes that the names of the generated YAML files and the Kubernetes PV and PVC objects are decorated with the domainUID, and the PV and PVC objects are also labeled with the domainUID.\nVerify the results The create script will verify that the PV and PVC were created, and will report a failure if there was any error. However, it may be desirable to manually verify the PV and PVC, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nGenerated YAML files with the default inputs The content of the generated weblogic-sample-pvc.yaml:\n# Copyright 2018, 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: weblogic-sample-pvc namespace: default labels: weblogic.resourceVersion: domain-v2 storageClassName: weblogic-sample-storage-class accessModes: - ReadWriteMany resources: requests: storage: 10Gi The content of the generated weblogic-sample-pv.yaml:\n# Copyright 2018, 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. apiVersion: v1 kind: PersistentVolume metadata: name: weblogic-sample-pv labels: weblogic.resourceVersion: domain-v2 # weblogic.domainUID: spec: storageClassName: weblogic-sample-storage-class capacity: storage: 10Gi accessModes: - ReadWriteMany # Valid values are Retain, Delete or Recycle persistentVolumeReclaimPolicy: Retain hostPath: # nfs: # server: %SAMPLE_STORAGE_NFS_SERVER% path: \u0026quot;/scratch/k8s_dir\u0026quot; Generated YAML files for dedicated PV and PVC The content of the generated domain1-weblogic-sample-pvc.yaml when domainUID is set to domain1:\n# Copyright 2018, 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: domain1-weblogic-sample-pvc namespace: default labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: storageClassName: domain1-weblogic-sample-storage-class accessModes: - ReadWriteMany resources: requests: storage: 10Gi The content of the generated domain1-weblogic-sample-pv.yaml when domainUID is set to domain1:\n# Copyright 2018, 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. apiVersion: v1 kind: PersistentVolume metadata: name: domain1-weblogic-sample-pv labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: storageClassName: domain1-weblogic-sample-storage-class capacity: storage: 10Gi accessModes: - ReadWriteMany # Valid values are Retain, Delete or Recycle persistentVolumeReclaimPolicy: Retain hostPath: # nfs: # server: %SAMPLE_STORAGE_NFS_SERVER% path: \u0026quot;/scratch/k8s_dir\u0026quot; Verify the PV and PVC objects You can use this command to verify the PersistentVolume was created. Note that the Status field should have the value Bound, indicating the that PersistentVolume has been claimed:\n$ kubectl describe pv weblogic-sample-pv Name: weblogic-sample-pv Labels: weblogic.resourceVersion=domain-v2 Annotations: pv.kubernetes.io/bound-by-controller=yes StorageClass: weblogic-sample-storage-class Status: Bound Claim: default/weblogic-sample-pvc Reclaim Policy: Retain Access Modes: RWX Capacity: 10Gi Message: Source: Type: HostPath (bare host directory volume) Path: /scratch/k8s_dir HostPathType: Events: \u0026lt;none\u0026gt; You can use this command to verify the PersistentVolumeClaim was created:\n$ kubectl describe pvc weblogic-sample-pvc Name: weblogic-sample-pvc Namespace: default StorageClass: weblogic-sample-storage-class Status: Bound Volume: weblogic-sample-pv Labels: weblogic.resourceVersion=domain-v2 Annotations: pv.kubernetes.io/bind-completed=yes pv.kubernetes.io/bound-by-controller=yes Finalizers: [] Capacity: 10Gi Access Modes: RWX Events: \u0026lt;none\u0026gt; "
},
{
	"uri": "/weblogic-kubernetes-operator/developerguide/requirements/",
	"title": "Requirements",
	"tags": [],
	"description": "",
	"content": "In addition to the requirements listed in the User guide, the following software is also required to obtain and build the operator:\n Git (1.8 or later recommended) Java Developer Kit (11 required, 11.0.2 recommended) Apache Maven (3.5.3 min, 3.6 recommended)  The operator is written primarily in Java, BASH shell scripts, and WLST scripts.\nBecause the target runtime environment for the operator is Oracle Linux, no particular effort has been made to ensure the build or tests run on any other operating system. Please be aware that Oracle will not provide support, or accept pull requests to add support for other operating systems.\nObtaining the operator source code The operator source code is published on GitHub at https://github.com/oracle/weblogic-kubernetes-operator. Developers may clone this repository to a local machine or, if desired, create a fork in their personal namespace and clone the fork. Developers who are planning to submit a pull request are advised to create a fork.\nTo clone the repository from GitHub, issue this command:\n$ git clone https://github.com/oracle/weblogic-kubernetes-operator.git "
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/",
	"title": "Simple samples",
	"tags": [],
	"description": "",
	"content": "This section provides samples for individual tasks. The samples in this section are intended to be modified before production use.\n Credentials  Sample for creating a Kubernetes Secret that contains the Administration Server credentials. This secret can be used in creating a WebLogic domain resource.\n Storage  Sample for creating a PV or PVC that can be used by a domain resource as the persistent storage for the WebLogic domain home or log files.\n Domains  These samples show various choices for working with domains.\n REST APIs  Sample for generating a self-signed certificate and private key that can be used for the operator\u0026#39;s external REST API.\n Ingress  Load balancer sample scripts.\n Elastic Stack   Operator Sample for configuring the Elasticsearch and Kibana deployments and services for the operator\u0026#39;s logs. WebLogic domain Sample for using Fluentd for WebLogic domain and operator\u0026#39;s logs. SOA domain Samples for publishing logs to Elasticsearch and monitoring a SOA instance.  "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-operators/using-the-operator/using-helm/",
	"title": "Use Helm",
	"tags": [],
	"description": "Useful Helm operations.",
	"content": "Contents  Useful Helm operations Operator Helm configuration values  Overall operator information Creating the operator pod WebLogic domain management Elastic Stack integration REST interface configuration Debugging options   Common mistakes and solutions  Note that the operator Helm chart is available from the GitHub chart repository, see Alternatively, install the operator Helm chart from the GitHub chart repository.\nUseful Helm operations Show the available operator configuration parameters and their default values:\n$ helm inspect values kubernetes/charts/weblogic-operator Show the custom values you configured for the operator Helm release:\n$ helm get values weblogic-operator Show all of the values your operator Helm release is using:\n$ helm get values --all weblogic-operator List the Helm releases for a specified namespace or all namespaces:\n$ helm list --namespace \u0026lt;namespace\u0026gt; $ helm list --all-namespaces Get the status of the operator Helm release:\n$ helm status weblogic-operator Show the history of the operator Helm release:\n$ helm history weblogic-operator Roll back to a previous version of this operator Helm release, in this case, the first version:\n$ helm rollback weblogic-operator 1 Change one or more values in the operator Helm release. In this example, the --reuse-values flag indicates that previous overrides of other values should be retained:\n$ helm upgrade \\ --reuse-values \\ --set \u0026quot;domainNamespaces={sample-domains-ns1}\u0026quot; \\ --set \u0026quot;javaLoggingLevel=FINE\u0026quot; \\ --wait \\ weblogic-operator \\ kubernetes/charts/weblogic-operator Enable operator debugging on port 30999. Again, we use --reuse-values to change one value without affecting the others:\n$ helm upgrade \\ --reuse-values \\ --set \u0026quot;remoteDebugNodePortEnabled=true\u0026quot; \\ --wait \\ weblogic-operator \\ kubernetes/charts/weblogic-operator Operator Helm configuration values This section describes the details of the operator Helm chart\u0026rsquo;s available configuration values.\nOverall operator information serviceAccount Specifies the name of the service account in the operator\u0026rsquo;s namespace that the operator will use to make requests to the Kubernetes API server. You are responsible for creating the service account.\nDefaults to default.\nExample:\nserviceAccount: \u0026quot;weblogic-operator\u0026quot; dedicated Specifies if this operator will manage WebLogic domains only in the same namespace in which the operator itself is deployed. If set to true, then the domainNamespaces value is ignored.\nDefaults to false.\nExample:\ndedicated: false In the dedicated mode, the operator does not require permissions to access the cluster-scoped Kubernetes resources, such as CustomResourceDefinitions, PersistentVolumes, and Namespaces. In those situations, the operator may skip some of its operations, such as verifying the WebLogic domain CustomResoruceDefinition domains.weblogic.oracle (and creating it when it is absent), watching namespace events, and cleaning up PersistentVolumes as part of deleting a domain.\nIt is the responsibility of the administrator to make sure that the required CustomResourceDefinition (CRD) domains.weblogic.oracle is deployed in the Kubernetes cluster before the operator is installed. The creation of the CRD requires the Kubernetes cluster-admin privileges. A YAML file for creating the CRD can be found at domain-crd.yaml.\n javaLoggingLevel Specifies the level of Java logging that should be enabled in the operator. Valid values are: SEVERE, WARNING, INFO, CONFIG, FINE, FINER, and FINEST.\nDefaults to INFO.\nExample:\njavaLoggingLevel: \u0026quot;FINE\u0026quot; Creating the operator pod image Specifies the Docker image containing the operator code.\nDefaults to weblogic-kubernetes-operator:2.5.0.\nExample:\nimage: \u0026quot;weblogic-kubernetes-operator:LATEST\u0026quot; imagePullPolicy Specifies the image pull policy for the operator Docker image.\nDefaults to IfNotPresent.\nExample:\nimage: \u0026quot;Always\u0026quot; imagePullSecrets Contains an optional list of Kubernetes Secrets, in the operator\u0026rsquo;s namespace, that are needed to access the registry containing the operator Docker image. You are responsible for creating the secret. If no secrets are required, then omit this property.\nExample:\nimagePullSecrets: - name: \u0026quot;my-image-pull-secret\u0026quot; WebLogic domain management domainNamespaces Specifies a list of WebLogic domain namespaces which the operator manages. The names must be lower case. You are responsible for creating these namespaces.\nThis property is required.\nExample 1: In the configuration below, the operator will monitor the default Kubernetes Namespace:\ndomainNamespaces: - \u0026quot;default\u0026quot; Example 2: In the configuration below, the Helm installation will manage namespace1 and namespace2:\ndomainNamespaces: [ \u0026quot;namespace1\u0026quot;, \u0026quot;namespace2\u0026quot; ]  These examples show two valid YAML syntax options for arrays.\n You must include the default namespace in the list if you want the operator to monitor both the default namespace and some other namespaces.\n This value is ignored if dedicated is set to true. Then, the operator will manage only domains in its own namespace.\n For more information about managing domainNamespaces, see Managing domain namespaces.\nElastic Stack integration elkIntegrationEnabled Specifies whether or not Elastic Stack integration is enabled.\nDefaults to false.\nExample:\nelkIntegrationEnabled: true logStashImage Specifies the Docker image containing Logstash. This parameter is ignored if elkIntegrationEnabled is false.\nDefaults to logstash:6.6.0.\nExample:\nlogStashImage: \u0026quot;logstash:6.2\u0026quot; elasticSearchHost Specifies the hostname where Elasticsearch is running. This parameter is ignored if elkIntegrationEnabled is false.\nDefaults to elasticsearch.default.svc.cluster.local.\nExample:\nelasticSearchHost: \u0026quot;elasticsearch2.default.svc.cluster.local\u0026quot; elasticSearchPort Specifies the port number where Elasticsearch is running. This parameter is ignored if elkIntegrationEnabled is false.\nDefaults to 9200.\nExample:\nelasticSearchPort: 9201 REST interface configuration externalRestEnabled Determines whether the operator\u0026rsquo;s REST interface will be exposed outside the Kubernetes cluster.\nDefaults to false.\nIf set to true, you must provide the externalRestIdentitySecret property that contains the name of the Kubernetes Secret which contains the SSL certificate and private key for the operator\u0026rsquo;s external REST interface.\nExample:\nexternalRestEnabled: true externalRestHttpsPort Specifies the node port that should be allocated for the external operator REST HTTPS interface.\nOnly used when externalRestEnabled is true, otherwise ignored.\nDefaults to 31001.\nExample:\nexternalRestHttpsPort: 32009 externalRestIdentitySecret Specifies the user supplied secret that contains the SSL/TLS certificate and private key for the external operator REST HTTPS interface. The value must be the name of the Kubernetes tls secret previously created in the namespace where the operator is deployed. This parameter is required if externalRestEnabled is true, otherwise, it is ignored. In order to create the Kubernetes tls secret you can use the following command:\n$ kubectl create secret tls \u0026lt;secret-name\u0026gt; \\ -n \u0026lt;operator-namespace\u0026gt; \\ --cert=\u0026lt;path-to-certificate\u0026gt; \\ --key=\u0026lt;path-to-private-key\u0026gt; There is no default value.\nThe Helm installation will produce an error, similar to the following, if externalRestIdentitySecret is not specified (left blank) and externalRestEnabled is true:\nError: render error in \u0026quot;weblogic-operator/templates/main.yaml\u0026quot;: template: weblogic-operator/templates/main.yaml:9:3: executing \u0026quot;weblogic-operator/templates/main.yaml\u0026quot; at \u0026lt;include \u0026quot;operator.va...\u0026gt;: error calling include: template: weblogic-operator/templates/_validate-inputs.tpl:42:14: executing \u0026quot;operator.validateInputs\u0026quot; at \u0026lt;include \u0026quot;utils.endVa...\u0026gt;: error calling include: template: weblogic-operator/templates/_utils.tpl:22:6: executing \u0026quot;utils.endValidation\u0026quot; at \u0026lt;fail $scope.validati...\u0026gt;: error calling fail: string externalRestIdentitySecret must be specified Example:\nexternalRestIdentitySecret: weblogic-operator-external-rest-identity externalOperatorCert (Deprecated) Use externalRestIdentitySecret instead\n Specifies the user supplied certificate to use for the external operator REST HTTPS interface. The value must be a string containing a Base64 encoded PEM certificate. This parameter is required if externalRestEnabled is true, otherwise, it is ignored.\nThere is no default value.\nThe Helm installation will produce an error, similar to the following, if externalOperatorCert is not specified (left blank) and externalRestEnabled is true:\nError: render error in \u0026quot;weblogic-operator/templates/main.yaml\u0026quot;: template: weblogic-operator/templates/main.yaml:4:3: executing \u0026quot;weblogic-operator/templates/main.yaml\u0026quot; at \u0026lt;include \u0026quot;operator.va...\u0026gt;: error calling include: template: weblogic-operator/templates/_validate-inputs.tpl:53:4: executing \u0026quot;operator.validateInputs\u0026quot; at \u0026lt;include \u0026quot;operator.re...\u0026gt;: error calling include: template: weblogic-operator/templates/_utils.tpl:137:6: executing \u0026quot;operator.reportValidationErrors\u0026quot; at \u0026lt;fail .validationErro...\u0026gt;: error calling fail: The string property externalOperatorCert must be specified. Example:\nexternalOperatorCert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQwakNDQXJxZ0F3S ... externalOperatorKey (Deprecated) Use externalRestIdentitySecret instead\n Specifies user supplied private key to use for the external operator REST HTTPS interface. The value must be a string containing a Base64 encoded PEM key. This parameter is required if externalRestEnabled is true, otherwise, it is ignored.\nThere is no default value.\nThe Helm installation will produce an error, similar to the following, if externalOperatorKey is not specified (left blank) and externalRestEnabled is true:\nError: render error in \u0026quot;weblogic-operator/templates/main.yaml\u0026quot;: template: weblogic-operator/templates/main.yaml:4:3: executing \u0026quot;weblogic-operator/templates/main.yaml\u0026quot; at \u0026lt;include \u0026quot;operator.va...\u0026gt;: error calling include: template: weblogic-operator/templates/_validate-inputs.tpl:53:4: executing \u0026quot;operator.validateInputs\u0026quot; at \u0026lt;include \u0026quot;operator.re...\u0026gt;: error calling include: template: weblogic-operator/templates/_utils.tpl:137:6: executing \u0026quot;operator.reportValidationErrors\u0026quot; at \u0026lt;fail .validationErro...\u0026gt;: error calling fail: The string property externalOperatorKey must be specified. Example:\nexternalOperatorKey: QmFnIEF0dHJpYnV0ZXMKICAgIGZyaWVuZGx5TmFtZTogd2VibG9naWMtb3B ... Debugging options remoteDebugNodePortEnabled Specifies whether or not the operator will start a Java remote debug server on the provided port and suspend execution until a remote debugger has attached.\nDefaults to false.\nExample:\nremoteDebugNodePortEnabled: true internalDebugHttpPort Specifies the port number inside the Kubernetes cluster for the operator\u0026rsquo;s Java remote debug server.\nThis parameter is required if remoteDebugNodePortEnabled is true. Otherwise, it is ignored.\nDefaults to 30999.\nExample:\ninternalDebugHttpPort: 30888 externalDebugHttpPort Specifies the node port that should be allocated for the Kubernetes cluster for the operator\u0026rsquo;s Java remote debug server.\nThis parameter is required if remoteDebugNodePortEnabled is true. Otherwise, it is ignored.\nDefaults to 30999.\nExample:\nexternalDebugHttpPort: 30777 Common mistakes and solutions Installing the operator a second time into the same namespace A new FAILED Helm release is created.\n$ helm install --no-hooks --name op2 --namespace myuser-op-ns --values custom-values.yaml kubernetes/charts/weblogic-operator Error: release op2 failed: secrets \u0026quot;weblogic-operator-secrets\u0026quot; already exists Both the previous and new release own the resources created by the previous operator.\n You can\u0026rsquo;t modify it to change the namespace (because helm upgrade does not let you change the namespace). You can\u0026rsquo;t fix it by deleting this release because it removes your previous operator\u0026rsquo;s resources. You can\u0026rsquo;t fix it by rolling back this release because it is not in the DEPLOYED state. You can\u0026rsquo;t fix it by deleting the previous release because it removes the operator\u0026rsquo;s resources too. All you can do is delete both operator releases and reinstall the original operator. See https://github.com/helm/helm/issues/2349  Installing an operator and having it manage a domain namespace that another operator is already managing A new FAILED Helm release is created.\n$ helm install --no-hooks --name op2 --namespace myuser-op2-ns --values custom-values.yaml kubernetes/charts/weblogic-operator Error: release op2 failed: rolebindings.rbac.authorization.k8s.io \u0026quot;weblogic-operator-rolebinding-namespace\u0026quot; already exists To recover:\n helm delete --purge the failed release.  NOTE: This deletes the role binding in the domain namespace that was created by the first operator release, to give the operator access to the domain namespace.   helm upgrade \u0026lt;old op release\u0026gt; kubernetes/charts/weblogic-operator --values \u0026lt;old op custom-values.yaml\u0026gt;  This recreates the role binding. There might be intermittent failures in the operator for the period of time when the role binding was deleted.    Upgrading an operator and having it manage a domain namespace that another operator is already managing The helm upgrade succeeds, and silently adopts the resources the first operator\u0026rsquo;s Helm chart created in the domain namespace (for example, rolebinding), and, if you also instructed it to stop managing another domain namespace, it abandons the role binding it created in that namespace.\nFor example, if you delete this release, then the first operator will end up without the role binding it needs. The problem is that you don\u0026rsquo;t get a warning, so you don\u0026rsquo;t know that there\u0026rsquo;s a problem to fix.\n This can be fixed by just upgrading the Helm release. This may also be fixed by rolling back the Helm release.  Installing an operator and assigning it the same external REST port number as another operator A new FAILED Helm release is created.\n$ helm install --no-hooks --name op2 --namespace myuser-op2-ns --values o.yaml kubernetes/charts/weblogic-operator Error: release op2 failed: Service \u0026quot;external-weblogic-operator-svc\u0026quot; is invalid: spec.ports[0].nodePort: Invalid value: 31023: provided port is already allocated To recover:\n $ helm delete --purge the failed release. Change the port number and helm install the second operator again.  Upgrading an operator and assigning it the same external REST port number as another operator The helm upgrade fails and moves the release to the FAILED state.\n$ helm upgrade --no-hooks --values o23.yaml op2 kubernetes/charts/weblogic-operator --wait Error: UPGRADE FAILED: Service \u0026quot;external-weblogic-operator-svc\u0026quot; is invalid: spec.ports[0].nodePort: Invalid value: 31023: provided port is already allocated  You can fix this by upgrading the Helm release (to fix the port number). You can also fix this by rolling back the Helm release.  Installing an operator and assigning it a service account that doesn\u0026rsquo;t exist The helm install eventually times out and creates a failed release.\n$ helm install kubernetes/charts/weblogic-operator --name op2 --namespace myuser-op2-ns --values o24.yaml --wait --no-hooks Error: release op2 failed: timed out waiting for the condition kubectl logs -n kube-system tiller-deploy-f9b8476d-mht6v ... [kube] 2018/12/06 21:16:54 Deployment is not ready: myuser-op2-ns/weblogic-operator ... To recover:\n helm delete --purge the failed release. Create the service account. helm install again.  Upgrading an operator and assigning it a service account that doesn\u0026rsquo;t exist The helm upgrade succeeds and changes the service account on the existing operator deployment, but the existing deployment\u0026rsquo;s pod doesn\u0026rsquo;t get modified, so it keeps running. If the pod is deleted, the deployment creates another one using the OLD service account. However, there\u0026rsquo;s an error in the deployment\u0026rsquo;s status section saying that the service account doesn\u0026rsquo;t exist.\nlastTransitionTime: 2018-12-06T23:19:26Z lastUpdateTime: 2018-12-06T23:19:26Z message: 'pods \u0026quot;weblogic-operator-88bbb5896-\u0026quot; is forbidden: error looking up service account myuser-op2-ns/no-such-sa2: serviceaccount \u0026quot;no-such-sa2\u0026quot; not found' reason: FailedCreate status: \u0026quot;True\u0026quot; type: ReplicaFailure To recover:\n Create the service account. helm rollback helm upgrade again.  Installing an operator and having it manage a domain namespace that doesn\u0026rsquo;t exist A new FAILED Helm release is created.\n$ helm install --no-hooks --name op2 --namespace myuser-op2-ns --values o.yaml kubernetes/charts/weblogic-operator Error: release op2 failed: namespaces \u0026quot;myuser-d2-ns\u0026quot; not found To recover:\n helm delete --purge the failed release. Create the domain namespace. helm install again.  Upgrading an operator and having it manage a domain namespace that doesn\u0026rsquo;t exist The helm upgrade fails and moves the release to the FAILED state.\n$ helm upgrade myuser-op kubernetes/charts/weblogic-operator --values o.yaml --no-hooks Error: UPGRADE FAILED: failed to create resource: namespaces \u0026quot;myuser-d2-ns\u0026quot; not found To recover:\n helm rollback Create the domain namespace. helm upgrade again.  Deleting and recreating a namespace that an operator manages without informing the operator If you create a new domain in a namespace that is deleted and recreated, the domain does not start up until you notify the operator. For more information about the problem and solutions, see Managing domain namespaces.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/domain-lifecycle/startup/",
	"title": "Startup and shutdown",
	"tags": [],
	"description": "There are properties on the domain resource that specify which servers should be running and which servers should be restarted. To start, stop, or restart servers, modify these properties on the domain resource.",
	"content": "Contents  Starting and stopping servers  Common starting and stopping scenarios   Shutdown options Restarting servers  Rolling restarts Common restarting scenarios    There are properties on the domain resource that specify which servers should be running, which servers should be restarted and the desired initial state. To start, stop, or restart servers, modify these properties on the domain resource (for example, by using kubectl or the Kubernetes REST API). The operator will notice the changes and apply them. Beginning, with operator version 2.2, there are now properties to control server shutdown handling, such as whether the shutdown will be graceful, the timeout, and if in-flight sessions are given the opportunity to complete.\nStarting and stopping servers The serverStartPolicy property on the domain resource controls which servers should be running. The operator runtime monitors this property and creates or deletes the corresponding server pods.\nDo not use the WebLogic Server Administration Console to start or stop servers.\n serverStartPolicy rules You can specify the serverStartPolicy property at the domain, cluster, and server levels. Each level supports a different set of values.\nAvailable serverStartPolicy values    Level Default Value Supported Values     Domain IF_NEEDED IF_NEEDED, ADMIN_ONLY, NEVER   Cluster IF_NEEDED IF_NEEDED, NEVER   Server IF_NEEDED IF_NEEDED, ALWAYS, NEVER    Administration Server start and stop rules    Domain Admin Server Started / Stopped     NEVER any value Stopped   ADMIN_ONLY, IF_NEEDED NEVER Stopped   ADMIN_ONLY, IF_NEEDED IF_NEEDED, ALWAYS Started    Standalone Managed Server start and stop rules    Domain Standalone Server Started / Stopped     ADMIN_ONLY, NEVER any value Stopped   IF_NEEDED NEVER Stopped   IF_NEEDED IF_NEEDED, ALWAYS Started    Clustered Managed Server start and stop rules    Domain Cluster Clustered Server Started / Stopped     ADMIN_ONLY, NEVER any value any value Stopped   IF_NEEDED NEVER any value Stopped   IF_NEEDED IF_NEEDED NEVER Stopped   IF_NEEDED IF_NEEDED ALWAYS Started   IF_NEEDED IF_NEEDED IF_NEEDED Started if needed to get to the cluster\u0026rsquo;s replicas count    Servers configured as ALWAYS count toward the cluster\u0026rsquo;s replicas count.\n If more servers are configured as ALWAYS than the cluster\u0026rsquo;s replicas count, they will all be started and the replicas count will be ignored.\n Server start state For some use cases, such as an externally managed zero downtime patching (ZDP), it may be necessary to start WebLogic Server so that at the end of its startup process, the server is in an administrative state. This can be achieved using the serverStartState property, which is available at domain, cluster, and server levels. When serverStartState is set to ADMIN, then servers will progress only to the administrative state. Then you could use the WebLogic Server Administration Console, REST API, or a WLST script to make any necessary updates before advancing the server to the running state.\nChanges to the serverStartState property do not affect already started servers.\nCommon starting and stopping scenarios Normal running state Normally, the Administration Server, all of the standalone Managed Servers, and enough Managed Servers in each cluster to satisfy its replicas count, should be started. In this case, the domain resource does not need to specify serverStartPolicy, or list any clusters or servers, but it does need to specify a replicas count.\nFor example:\n kind: Domain metadata: name: domain1 spec: image: ... replicas: 10 Shut down all the servers Sometimes you need to completely shut down the domain (for example, take it out of service).\n kind: Domain metadata: name: domain1 spec: serverStartPolicy: \u0026quot;NEVER\u0026quot; ... Only start the Administration Server Sometimes you want to start the Administration Server only, that is, take the domain out of service but leave the Administration Server running so that you can administer the domain.\n kind: Domain metadata: name: domain1 spec: serverStartPolicy: \u0026quot;ADMIN_ONLY\u0026quot; ... Shut down a cluster To shut down a cluster (for example, take it out of service), add it to the domain resource and set its serverStartPolicy to NEVER.\n kind: Domain metadata: name: domain1 spec: clusters: - clusterName: \u0026quot;cluster1\u0026quot; serverStartPolicy: \u0026quot;NEVER\u0026quot; ... Shut down a specific standalone server To shut down a specific standalone server, add it to the domain resource and set its serverStartPolicy to NEVER.\n kind: Domain metadata: name: domain1 spec: managedServers: - serverName: \u0026quot;server1\u0026quot; serverStartPolicy: \u0026quot;NEVER\u0026quot; ... Force a specific clustered Managed Server to start Normally, all of the Managed Servers in a cluster are identical and it doesn\u0026rsquo;t matter which ones are running as long as the operator starts enough of them to get to the cluster\u0026rsquo;s replicas count. However, sometimes some of the Managed Servers are different (for example, support some extra services that the other servers in the cluster use) and need to always be started.\nThis is done by adding the server to the domain resource and setting its serverStartPolicy to ALWAYS.\n kind: Domain metadata: name: domain1 spec: managedServers: - serverName: \u0026quot;cluster1_server1\u0026quot; serverStartPolicy: \u0026quot;ALWAYS\u0026quot; ...  The server will count toward the cluster\u0026rsquo;s replicas count. Also, if you configure more than the replicas servers count to ALWAYS, they will all be started, even though the replicas count will be exceeded.\n Shutdown options The domain resource includes the element serverPod that is available under spec, adminServer and each entry of clusters and managedServers. The serverPod element controls many details of how pods are created for server instances.\nThe shutdown element of serverPod controls how servers will be shutdown. This element has three properties: shutdownType, timeoutSeconds, and ignoreSessions. The shutdownType property can be set to either Graceful, the default, or Forced specifying the type of shutdown. The timeoutSeconds property configures how long the server is given to complete shutdown before the server is killed. The ignoreSessions property, which is only applicable for graceful shutdown, when false, the default, allows the shutdown process to take longer to give time for any active sessions to complete up to the configured timeout. The operator runtime monitors this property but will not restart any server pods solely to adjust the shutdown options. Instead, server pods created or restarted because of another property change will be configured to shutdown, at the appropriate time, using the shutdown options set when the server pod is created.\nShutdown environment variables The operator runtime configures shutdown behavior with the use of the following environment variables. Users may instead simply configure these environment variables directly. When a user-configured environment variable is present, the operator will not override the environment variable based on the shutdown configuration.\n   Environment Variables Default Value Supported Values     SHUTDOWN_TYPE Graceful Graceful or Forced   SHUTDOWN_TIMEOUT 30 Whole number in seconds where 0 means no timeout   SHUTDOWN_IGNORE_SESSIONS false Boolean indicating if active sessions should be ignored; only applicable if shutdown is graceful    shutdown rules You can specify the serverPod element, including the shutdown element, at the domain, cluster, and server levels. If shutdown is specified at multiple levels, such as for a cluster and for a member server that is part of that cluster, then the shutdown configuration for a specific server is the combination of all of the relevant values with each field having the value from the shutdown element at the most specific scope.\nFor instance, given the following domain resource:\n kind: Domain metadata: name: domain1 spec: serverPod: shutdown: shutdownType: Graceful timeoutSeconds: 45 clusters: - clusterName: \u0026quot;cluster1\u0026quot; serverPod: shutdown: ignoreSessions: true managedServers: - serverName: \u0026quot;cluster1_server1\u0026quot; serverPod: shutdown: timeoutSeconds: 60 ignoreSessions: false ... Graceful shutdown is used for all servers in the domain because this is specified at the domain level and is not overridden at any cluster or server level. The \u0026ldquo;cluster1\u0026rdquo; cluster defaults to ignoring sessions; however, the \u0026ldquo;cluster1_server1\u0026rdquo; server instance will not ignore sessions and will have a longer timeout.\nRestarting servers The operator runtime automatically recreates (restarts) server pods when properties on the domain resource that affect server pods change (such as image, volumes, and env). The restartVersion property on the domain resource lets you force the operator to restart a set of server pods.\nThe operator runtime does rolling restarts of clustered servers so that service is maintained.\nProperties that cause servers to be restarted The operator will restart servers when any of the follow properties on the domain resource that affect the server are changed:\n containerSecurityContext domainHome domainHomeInImage (in releases before 3.0.0-rc1) domainHomeSourceType (introduced in 3.0.0-rc1) env image imagePullPolicy imagePullSecrets includeServerOutInPodLog logHomeEnabled logHome livenessProbe nodeSelector podSecurityContext readinessProbe resources restartVersion volumes volumeMounts  If the only change detected is the addition or modification of a domain-specified label or annotation, the operator will patch the server\u0026rsquo;s pod rather than restarting it. Removing a label or annotation from the domain resource will cause neither a restart nor a patch. It is possible to force a restart to remove such a label or annotation by modifying the restartVersion.\n Prior to version 2.2, the operator incorrectly restarted servers when the serverStartState property was changed. Now, this property has no affect on already running servers.\n Rolling restarts Clustered servers that need to be restarted are gradually restarted (for example, rolling restarted) so that the cluster is not taken out of service and in-flight work can be migrated to other servers in the cluster.\nThe maxUnavailable property on the domain resource determines how many of the cluster\u0026rsquo;s servers may be taken out of service at a time when doing a rolling restart. It can be specified at the domain and cluster levels and defaults to 1 (that is, by default, clustered servers are restarted one at a time).\nWhen using in-memory session replication, Oracle WebLogic Server employs a primary-secondary session replication model to provide high availability of application session state (that is, HTTP and EJB sessions). The primary server creates a primary session state on the server to which the client first connects, and a secondary replica on another WebLogic Server instance in the cluster. Specifying a maxUnavailable property value of 1 protects against inadvertent session state loss which could occur if both the primary and secondary servers are shut down at the same time during the rolling restart process.\nIf you are supplying updated models or secrets for a running Model in Image domain, and you want the configuration updates to take effect using a rolling restart, consult Modifying WebLogic Configuration and Runtime updates before consulting this chapter.\n Common restarting scenarios Using restartVersion to force the operator to restart servers The restartVersion property lets you force the operator to restart servers.\nIt\u0026rsquo;s basically a user-specified string that gets added to new server pods (as a label) so that the operator can tell which servers need to be restarted. If the value is different, then the server pod is old and needs to be restarted. If the value matches, then the server pod has already been restarted.\nEach time you want to restart some servers, you need to set restartVersion to a different string (the particular value doesn\u0026rsquo;t matter).\nThe operator will notice the new value and restart the affected servers (using the same mechanisms as when other properties that affect the server pods are changed, including doing rolling restarts of clustered servers).\nThe restartVersion property can be specified at the domain, cluster, and server levels. A server will be restarted if any of these three values change.\nThe servers will also be restarted if restartVersion is removed from the domain resource (for example, if you had previously specified a value to cause a restart, then you remove that value after the previous restart has completed).\n Restart all the servers in the domain Set restartVersion at the domain level to a new value.\n kind: Domain metadata: name: domain1 spec: restartVersion: \u0026quot;domainV1\u0026quot; ... Restart all the servers in the cluster Set restartVersion at the cluster level to a new value.\n kind: Domain metadata: name: domain1 spec: clusters: - clusterName : \u0026quot;cluster1\u0026quot; restartVersion: \u0026quot;cluster1V1\u0026quot; maxUnavailable: 2 ... Restart the Administration Server Set restartVersion at the adminServer level to a new value.\n kind: Domain metadata: name: domain1 spec: adminServer: restartVersion: \u0026quot;adminV1\u0026quot; ... Restart a standalone or clustered Managed Server Set restartVersion at the managedServer level to a new value.\n kind: Domain metadata: name: domain1 spec: managedServers: - serverName: \u0026quot;standalone_server1\u0026quot; restartVersion: \u0026quot;v1\u0026quot; - serverName: \u0026quot;cluster1_server1\u0026quot; restartVersion: \u0026quot;v1\u0026quot; ... Full domain restarts To do a full domain restart, first shut down all of the domain\u0026rsquo;s servers (Administration Server and Managed Servers), taking the domain out of service, then restart them. Unlike rolling restarts, the operator cannot detect and initiate a full domain restart; you must always manually initiate it.\nTo manually initiate a full domain restart:\n Change the domain level serverStartPolicy on the domain resource to NEVER.   kind: Domain metadata: name: domain1 spec: serverStartPolicy: \u0026quot;NEVER\u0026quot; ...  Wait for the operator to stop ALL the servers for that domain.\n  To restart the domain, set the domain level serverStartPolicy back to IF_NEEDED. Alternatively, you do not have to specify the serverStartPolicy as the default value is IF_NEEDED.\n   kind: Domain metadata: name: domain1 spec: serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; ... The operator will restart all the servers in the domain.  "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-operators/installation/",
	"title": "Install the operator",
	"tags": [],
	"description": "",
	"content": "The operator uses Helm to create and deploy the necessary resources and then run the operator in a Kubernetes cluster. This document describes how to install, upgrade, and remove the operator.\nContent  Install the operator Helm chart Alternatively, install the operator Helm chart from the GitHub chart repository Upgrade the operator Remove the operator  Install the operator Helm chart Use the helm install command to install the operator Helm chart. As part of this, you must specify a \u0026ldquo;release\u0026rdquo; name for the operator.\nYou can override default configuration values in the operator Helm chart by doing one of the following:\n Creating a custom YAML file containing the values to be overridden, and specifying the --value option on the Helm command line. Overriding individual values directly on the Helm command line, using the --set option.  You supply the –namespace argument from the helm install command line to specify the namespace in which the operator should be installed. If not specified, then it defaults to default. If the namespace does not already exist, then Helm will automatically create it (and create a default service account in the new namespace), but will not remove it when the release is deleted. If the namespace already exists, then Helm will re-use it. These are standard Helm behaviors.\nSimilarly, you may override the default serviceAccount configuration value to specify which service account in the operator\u0026rsquo;s namespace, the operator should use. If not specified, then it defaults to default (for example, the namespace\u0026rsquo;s default service account). If you want to use a different service account, then you must create the operator\u0026rsquo;s namespace and the service account before installing the operator Helm chart.\nFor example, using Helm 3.x:\n$ kubectl create namespace weblogic-operator-namespace $ helm install weblogic-operator kubernetes/charts/weblogic-operator \\ --namespace weblogic-operator-namespace \\ --values custom-values.yaml --wait Or:\n$ helm install weblogic-operator kubernetes/charts/weblogic-operator \\ --namespace weblogic-operator-namespace \\ --set \u0026quot;javaLoggingLevel=FINE\u0026quot; --wait This creates a Helm release, named weblogic-operator in the weblogic-operator-namespace namespace, and configures a deployment and supporting resources for the operator.\nYou can verify the operator installation by examining the output from the helm install command.\nFor more information on specifying the registry credentials when the operator image is stored in a private registry, see Operator image pull secret.\n Alternatively, install the operator Helm chart from the GitHub chart repository Add this repository to the Helm installation:\n$ helm repo add weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts Verify that the repository was added correctly:\n$ helm repo list NAME URL weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts Update with the latest information about charts from the chart repositories:\n$ helm repo update Install the operator from the repository:\n$ helm install weblogic-operator weblogic-operator/weblogic-operator Upgrade the operator To upgrade the operator, use the helm upgrade command. When upgrading the operator, the helm upgrade command requires that you supply a new Helm chart and image. For example:\n$ helm upgrade \\ --reuse-values \\ --set image=oracle/weblogic-kubernetes-operator:2.5.0 \\ --namespace weblogic-operator-namespace \\ --wait \\ weblogic-operator \\ kubernetes/charts/weblogic-operator Remove the operator The helm delete command is used to remove an operator release and its associated resources from the Kubernetes cluster. The release name used with the helm delete command is the same release name used with the helm install command (see Install the Helm chart). For example:\n$ helm uninstall weblogic-operator  If the operator\u0026rsquo;s namespace did not exist before the Helm chart was installed, then Helm will create it, however, helm delete will not remove it.\n After removing the operator deployment, you should also remove the domain custom resource definition:\n$ kubectl delete customresourcedefinition domains.weblogic.oracle Note that the domain custom resource definition is shared if there are multiple operators in the same cluster.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/domain-in-image/base-images/",
	"title": "Base images",
	"tags": [],
	"description": "Creating or obtaining WebLogic Docker images.",
	"content": "Contents  Creating or obtaining WebLogic Docker images Setting up secrets to access the Oracle Container Registry Obtaining standard images from the Oracle Container Registry Creating a custom image with patches applied Creating a custom image with your domain inside the image  Creating or obtaining WebLogic Docker images You will need Docker images to run your WebLogic domains in Kubernetes. There are two main options available:\n Use a Docker image which contains the WebLogic Server binaries but not the domain, or Use a Docker image which contains both the WebLogic Server binaries and the domain directory.  If you want to use the first option, then you will need to obtain the standard WebLogic Server image from the Oracle Container Registry; see Obtaining standard images from the Oracle Container Registry. This image already contains the mandatory patches applied, as described in Creating a custom image with patches applied. If you want to use additional patches, you can customize that process to include additional patches.\nIf you want to use the second option, which includes the domain directory inside the Docker image, then you will need to build your own Docker images, as described in Creating a custom image with your domain inside the image.\nSetting up secrets to access the Oracle Container Registry This version of the operator requires WebLogic Server 12.2.1.3.0 plus patch 29135930; the standard image, container-registry.oracle.com/middleware/weblogic:12.2.1.3, already includes this patch pre-applied. Images for WebLogic Server 12.2.1.4.0 do not require any patches.\n In order for Kubernetes to obtain the WebLogic Server Docker image from the Oracle Container Registry (OCR), which requires authentication, a Kubernetes Secret containing the registry credentials must be created. To create a secret with the OCR credentials, issue the following command:\n$ kubectl create secret docker-registry SECRET_NAME \\ -n NAMESPACE \\ --docker-server=container-registry.oracle.com \\ --docker-username=YOUR_USERNAME \\ --docker-password=YOUR_PASSWORD \\ --docker-email=YOUR_EMAIL In this command, replace the uppercase items with the appropriate values. The SECRET_NAME will be needed in later parameter files. The NAMESPACE must match the namespace where the first domain will be deployed, otherwise, Kubernetes will not be able to find it.\nIt may be preferable to manually pull the image in advance, on each Kubernetes worker node, as described in the next section. If you choose this approach, you do not require the Kubernetes Secret.\nObtaining standard images from the Oracle Container Registry The Oracle Container Registry contains images for licensed commercial Oracle software products that you may use in your enterprise. To access the Oracle Registry Server, you must have an Oracle Single Sign-On (SSO) account. The Oracle Container Registry provides a web interface that allows an administrator to authenticate and then to select the images for the software that your organization wishes to use. Oracle Standard Terms and Restrictions terms must be agreed to using the web interface. After the Oracle Standard Terms and Restrictions have been accepted, you can pull images of the software from the Oracle Container Registry using the standard Docker pull command.\nTo pull an image from the Oracle Container Registry, in a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms is stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nThe Oracle Container Registry provides a WebLogic Server 12.2.1.3.0 Docker image, which already has the necessary patches applied, and the Oracle WebLogic Server 12.2.1.4.0 and 14.1.1.0.0 images, which do not require any patches.\nFirst, you will need to log in to the Oracle Container Registry:\n$ docker login container-registry.oracle.com Then, you can pull the image with this command:\n$ docker pull container-registry.oracle.com/middleware/weblogic:12.2.1.4 If desired, you can:\n  Check the WLS version with docker run container-registry.oracle.com/middleware/weblogic:12.2.1.4 sh -c 'source $ORACLE_HOME/wlserver/server/bin/setWLSEnv.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; java weblogic.version'\n  Check the WLS patches with docker run container-registry.oracle.com/middleware/weblogic:12.2.1.4 sh -c '$ORACLE_HOME/OPatch/opatch lspatches'\n  Additional information about using this image is available on the Oracle Container Registry.\nCreating a custom image with patches applied The Oracle WebLogic Server Kubernetes Operator and WebLogic Server 12.2.1.3.0 image requires patch 29135930. This patch has some prerequisite patches that will also need to be applied. The WebLogic Server 12.2.1.4.0 image does not require any patches. To create and customize the WebLogic Server image, and apply the required patches, use the WebLogic Image Tool.\nTo use the Image Tool, follow the Setup instructions and Quick Start Guide.\nTo build the WebLogic Server image and apply the patches:\n  Add the Server JRE and the WebLogic Server installer to the cache command.\n$ imagetool cache addInstaller \\ --type=jdk \\ --version=8u241 \\ --path=/home/acmeuser/wls-installers/jre-8u241-linux-x64.tar.gz $ imagetool cache addInstaller \\ --type=wls \\ --version=12.2.1.4.0 \\ --path=/home/acmeuser/wls-installers/fmw_12.2.1.4.0_wls_Disk1_1of1.zip   Use the Create Tool to build the image and apply the patches.\nFor the Create Tool to download the patches, you must supply your My Oracle Support credentials.\n $ imagetool create \\ --tag=weblogic:12.2.1.3 \\ --type=wls \\ --version=12.2.1.3.0 \\ --jdkVersion=8u241 \\ -–patches=29135930_12.2.1.3.0,27117282_12.2.1.3.0 \\ --user=username.mycompany.com \\ --passwordEnv=MYPWD   After the tool creates the image, verify that the image is in your local repository:\n $ docker images   Creating a custom image with your domain inside the image You can also create a Docker image with the WebLogic domain inside the image. Samples are provided that demonstrate how to create the image using either:\n WLST to define the domain, or WebLogic Deploy Tooling to define the domain.  In these samples, you will see a reference to a \u0026ldquo;base\u0026rdquo; or FROM image. You should use an image with the mandatory patches installed as this base image. This image could be either the standard container-registry.oracle.com/middleware/weblogic:12.2.1.3 pre-patched image or an image you created using the instructions above. WebLogic Server 12.2.1.4.0 images do not require patches.\nOracle recommends that Docker images containing WebLogic domains be kept in a private repository.\n "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/choosing-a-model/",
	"title": "Choose a domain home source type",
	"tags": [],
	"description": "",
	"content": "When using the operator to deploy a WebLogic domain, you have the choice of the following WebLogic domain home source types:\n Domain in PV: Supply your domain home configuration in a persistent volume. Domain in Image: Supply your domain home in a Docker image. Model in Image: Supply a WebLogic Deployment Tool model file in a Docker image.  There are advantages for each domain home source type, but sometimes there are technical limitations of various cloud providers that may make one type better suited to your needs.\nNote that you can use different domain home types for different domains; there\u0026rsquo;s no restriction on having domains with different domain home types deployed to the same Kubernetes cluster or namespace.\n   Domain in PV Domain in Image Model in Image     Lets you use the same standard read-only Docker image for every server in every domain. Requires a different image for each domain, but all servers in that domain use the same image. Different domains can use the same image, but require different domainUID and may have different configuration.   No state is kept in Docker images making them completely throw away (cattle not pets). Runtime state should not be kept in the images, but applications and configuration are. Runtime state should not be kept in the images. Application and configuration may be.   The domain is long-lived, so you can mutate the configuration or deploy new applications using standard methods (Administration Console, WLST, and such). You can also mutate the configuration using configuration overrides. If you want to mutate the domain home configuration, then you can apply configuration overrides or create a new image. If you want to deploy application updates, then you must create a new image. If you want to mutate the domain home configuration, then you can override it with additional model files supplied in a ConfigMap or you can supply a new image. If you want to deploy application updates, then you must create a new image.   You can use configuration overrides to mutate the domain at runtime, but this requires first shutting down the entire domain, and then restarting it for the change to take effect. You can use configuration overrides to mutate the domain home at runtime, but this requires first shutting down the entire domain, and then restarting it for the change to take effect. You should not use the Administration Console or WLST for these domains as changes are ephemeral and will be lost when servers restart. You can deploy model files to a ConfigMap to mutate the domain at runtime, and may not need to restart the entire domain for the change to take effect. Instead, you can initiate a rolling upgrade, which restarts your WebLogic Server pods one at a time. Also, the model file syntax is far simpler and less error prone than the configuration override syntax, and, unlike configuration overrides, allows you to directly add data sources and JMS modules. You should not use the Administration Console or WLST for these domains as changes are ephemeral and will be lost when servers restart.   Logs are automatically placed on persistent storage and sent to the pod\u0026rsquo;s stdout. Logs are kept in the images and sent to the pod\u0026rsquo;s log (stdout) by default. To change their location, you can set the domain resource logHomeEnabled to true and configure the desired directory using logHome. Same as Domain in Image.   Patches can be applied by simply changing the image and rolling the domain. To apply patches, you must update the domain-specific image and then restart or roll the domain depending on the nature of the patch. Same as Domain in PV.   Many cloud providers do not provide persistent volumes that are shared across availability zones, so you may not be able to use a single persistent volume. You may need to use some kind of volume replication technology or a clustered file system. Provided you do not store and state in containers, you do not have to worry about volume replication across availability zones because each pod has its own copy of the domain. WebLogic replication will handle propagation of any online configuration changes. Same as Domain in Image.   CI/CD pipelines may be more complicated because you would probably need to run WLST against the live domain directory to effect changes. CI/CD pipelines are simpler because you can create the whole domain in the image and don\u0026rsquo;t have to worry about a persistent copy of the domain. CI/CD pipelines are even simpler because you don\u0026rsquo;t need to generate a domain home. The operator will create a domain home for you based on the model that you supply.   There are less images to manage and store, which could provide significant storage and network savings. There are more images to manage and store in this approach. Same as Domain in Image.   You may be able to use standard Oracle-provided images or, at least, a very small number of self-built images, for example, with patches installed. You may need to do more work to set up processes to build and maintain your images. Same as Domain in Image.    "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/prepare/",
	"title": "Prepare to run a domain",
	"tags": [],
	"description": "",
	"content": "Perform these steps to prepare your Kubernetes cluster to run a WebLogic domain:\n  Create the domain namespace or namespaces. One or more domains can share a namespace. A single instance of the operator can manage multiple namespaces.\n$ kubectl create namespace domain-namespace-1 Replace domain-namespace-1 with name you want to use. The name must follow standard Kubernetes naming conventions, that is, lower case, numbers, and hyphens.\n  Create a Kubernetes Secret containing the Administration Server boot credentials. You can do this manually or by using the provided sample. To create the secret manually, use this command:\n$ kubectl -n domain-namespace-1 \\ create secret generic domain1-weblogic-credentials \\ --from-literal=username=username \\ --from-literal=password=password  Replace domain-namespace-1 with the namespace that the domain will be in. Replace domain1-weblogic-credentials with the name of the secret. The operator expects the secret name to be the domainUID followed by the literal string -weblogic-credentials, and many of the samples assume this name. Replace the string username in the third line with the user name for the administrative user. Replace the string password in the fourth line with the password.    Optionally, create a PV \u0026amp; PersistentVolumeClaim (PVC) which can hold the domain home, logs, and application binaries. Even if you put your domain in a Docker image, you may want to put the logs on a persistent volume so that they are available after the pods terminate. This may be instead of, or as well as, other approaches like streaming logs into Elasticsearch.\n  Optionally, configure load balancer to manage access to any WebLogic clusters.\n  "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/overview/prepare/",
	"title": "Prepare your environment",
	"tags": [],
	"description": "",
	"content": "Set up your Kubernetes cluster If you need help setting up a Kubernetes environment, check our cheat sheet.\nAfter creating Kubernetes clusters, you can optionally:\n Create load balancers to direct traffic to backend domains. Configure Kibana and Elasticsearch for your operator logs.  Load balance with an ingress controller or a web server You can choose a load balancer provider for your WebLogic domains running in a Kubernetes cluster. For information about the current capabilities and setup instructions for each of the supported load balancers, see the WebLogic Operator Load Balancer Samples.\nConfigure Kibana and Elasticsearch You can send the operator logs to Elasticsearch, to be displayed in Kibana. Use this sample script to configure Elasticsearch and Kibana deployments and services.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/introduction/introduction/",
	"title": "Get started",
	"tags": [],
	"description": "Review the operator prerequisites and supported environments.",
	"content": "An operator is an application-specific controller that extends Kubernetes to create, configure, and manage instances of complex applications. The Oracle WebLogic Server Kubernetes Operator follows the standard Kubernetes operator pattern, and simplifies the management and operation of WebLogic domains and deployments.\nYou can have one or more operators in your Kubernetes cluster that manage one or more WebLogic domains each. We provide a Helm chart to manage the installation and configuration of the operator. Detailed instructions are available here.\nOperator prerequisites For the current production release 2.5.0:\n Kubernetes 1.13.5+, 1.14.8+, and 1.15.7+ (check with kubectl version). Not supported on Kubernetes 1.16 or later; see note below. See note below for OpenShift. Flannel networking v0.9.1-amd64 or later (check with docker images | grep flannel) or OpenShift SDN on OpenShift 4.3 systems. Docker 18.9.1 or 19.03.1 (check with docker version) or CRI-O 1.14.7 (check with crictl version | grep RuntimeVersion). Helm 3.0.3+ (check with helm version --client --short). Either Oracle WebLogic Server 12.2.1.3.0 with patch 29135930, Oracle WebLogic Server 12.2.1.4.0, or Oracle WebLogic Server 14.1.1.0.0.  The existing WebLogic Docker image, container-registry.oracle.com/middleware/weblogic:12.2.1.3 , has all the necessary patches applied. Check the WLS version with docker run container-registry.oracle.com/middleware/weblogic:12.2.1.3 sh -c 'source $ORACLE_HOME/wlserver/server/bin/setWLSEnv.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; java weblogic.version'. Check the WLS patches with docker run container-registry.oracle.com/middleware/weblogic:12.2.1.3 sh -c '$ORACLE_HOME/OPatch/opatch lspatches'.   You must have the cluster-admin role to install the operator. The operator does not need the cluster-admin role at runtime. We do not currently support running WebLogic in non-Linux containers.  For the current preview release 3.0.0-rc1, the above is modified as follows:\n Kubernetes 1.13.5+ support is deprecated and removed. Kubernetes 1.16.0+ support is added.  Important note about Kubernetes 1.16.0+ Kubernetes 1.16 introduced changes to some Kubernetes APIs that are used by the operator. At this time, the current production release of the operator will not work on Kubernetes 1.16.0+. (Note though that the current preview release 3.0.0-rc1 does support Kubernetes 1.16.0+). When we have made the necessary code changes, we will update this page to confirm support of 1.16.\nCloud providers The Oracle Global Pricing and Licensing site provides details about licensing practices and policies. WebLogic Server and the operator are supported on \u0026ldquo;Authorized Cloud Environments\u0026rdquo; as defined in this Oracle licensing policy and this list of eligible products.\nThe official document that defines the supported configurations is here.\nIn accordance with these policies, the operator and WebLogic Server are supported on Oracle Cloud Infrastructure using Oracle Container Engine for Kubernetes, or in a cluster running Oracle Linux Container Services for use with Kubernetes on OCI Compute, and on \u0026ldquo;Authorized Cloud Environments\u0026rdquo;.\nMicrosoft Azure Kubernetes Service Azure Kubernetes Service (AKS) is a hosted Kubernetes environment. The WebLogic Kubernetes Operator, Oracle WebLogic Sever 12c, and Oracle Fusion Middleware Infrastructure 12c are fully supported and certified on Azure Kubernetes Service (as per the documents referenced above).\nAKS support and limitations:\n Both Domain in Image and Domain in PV domain home source types are supported. For Domain in PV, we support Azure Files volumes accessed through a persistent volume claim; see here. Azure Load Balancers are supported when provisioned using a Kubernetes Service of type=LoadBalancer. Oracle databases running in Oracle Cloud Infrastructure are supported for Fusion Middleware Infrastructure MDS data stores only when accessed through an OCI FastConnect. Windows Server containers are not currently supported, only Linux containers.  Oracle Linux Cloud Native Environment Oracle Linux Cloud Native Environment is a fully integrated suite for the development and management of cloud-native applications. Based on Open Container Initiative (OCI) and Cloud Native Computing Foundation (CNCF) standards, Oracle Linux Cloud Native Environment delivers a simplified framework for installations, updates, upgrades, and configuration of key features for orchestrating microservices.\nWebLogic Server and the WebLogic Server Kubernetes Operator are certified and supported on Oracle Linux Cloud Native Environment.\nOpenShift Operator 2.0.1+ is certified for use on OpenShift Container Platform 3.11.43+, with Kubernetes 1.11.5+.\nOperator 2.5.0+ is certified for use on OpenShift Container Platform 4.3.0+ with Kubernetes 1.16.2+.\nWhen using the operator in OpenShift, a security context constraint is required to ensure that WebLogic containers run with a UNIX UID that has the correct permissions on the domain file system. This could be either the anyuid SCC or a custom one that you define for user/group 1000. For more information, see OpenShift in the Security section.\nImportant note about development-focused Kubernetes distributions There are a number of development-focused distributions of Kubernetes, like kind, Minikube, Minishift, and so on. Often these run Kubernetes in a virtual machine on your development machine. We have found that these distributions present some extra challenges in areas like:\n Separate Docker image stores, making it necessary to save/load images to move them between Docker file systems Default virtual machine file sizes and resource limits that are too small to run WebLogic or hold the necessary images Storage providers that do not always support the features that the operator or WebLogic rely on Load balancing implementations that do not always support the features that the operator or WebLogic rely on  As such, we do not recommend using these distributions to run the operator or WebLogic, and we do not provide support for WebLogic or the operator running in these distributions.\nWe have found that Docker for Desktop does not seem to suffer the same limitations, and we do support that as a development/test option.\nOperator Docker image You can find the operator image in Docker Hub.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/reference/javadoc/",
	"title": "Javadoc",
	"tags": [],
	"description": "Java API documentation.",
	"content": "View the Java API documentation here.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/quickstart/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Use this Quick Start guide to create a WebLogic deployment in a Kubernetes cluster with the Oracle WebLogic Server Kubernetes Operator. Please note that this walk-through is for demonstration purposes only, not for use in production. These instructions assume that you are already familiar with Kubernetes. If you need more detailed instructions, please refer to the User guide.\nAll Kubernetes distributions and managed services have small differences. In particular, the way that persistent storage and load balancers are managed varies significantly.\nYou may need to adjust the instructions in this guide to suit your particular flavor of Kubernetes.\n Important note for users of operator releases before 2.0\nIf you have an older version of the operator installed on your cluster, for example, a 1.x version or one of the 2.0 release candidates, then you must remove it before installing this version. This includes the 2.0-rc1 version; it must be completely removed. You should remove the deployment (for example, kubectl delete deploy weblogic-operator -n your-namespace) and the custom resource definition (for example, kubectl delete crd domain). If you do not remove the custom resource definition you may see errors like this:\nError from server (BadRequest): error when creating \u0026quot;/scratch/output/uidomain/weblogic-domains/uidomain/domain.yaml\u0026quot;: the API version in the data (weblogic.oracle/v2) does not match the expected API version (weblogic.oracle/v1  "
},
{
	"uri": "/weblogic-kubernetes-operator/quickstart/",
	"title": "Quick Start",
	"tags": [],
	"description": "",
	"content": "Quick Start The Quick Start guide provides a simple tutorial to help you get the operator up and running quickly.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/elastic-stack/weblogic-domain/",
	"title": "WebLogic domain",
	"tags": [],
	"description": "Sample for using Fluentd for WebLogic domain and operator&#39;s logs.",
	"content": "Overview This document describes to how to configure a WebLogic domain to use Fluentd to send log information to Elasticsearch.\nHere\u0026rsquo;s the general mechanism for how this works:\n fluentd runs as a separate container in the Administration Server and Managed Server pods The log files reside on a volume that is shared between the weblogic-server and fluentd containers fluentd tails the domain logs files and exports them to Elasticsearch A ConfigMap contains the filter and format rules for exporting log records  Sample code The samples in this document assume an existing domain is being edited. However, all changes to the domain YAML file can be performed before the domain is created.\nFor sample purposes, this document will assume a domain with the following attributes is being configured:\n Domain name is bobs-bookstore Kubernetes Namespace is bob Kubernetes Secret is bobs-bookstore-weblogic-credentials  The sample Elasticsearch configuration is:\nelasticsearchhost: elasticsearch.bobs-books.sample.com elasticsearchport: 443 elasticsearchuser: bob elasticsearchpassword: changeme Configure log files to use a volume The domain log files must be written to a volume that can be shared between the weblogic-server and fluentd containers. The following elements are required to accomplish this:\n logHome must be a path that can be shared between containers logHomeEnabled must be set to true so that the logs will be written outside the pod and persist across pod restarts A volume must be defined on which the log files will reside. In the example, emptyDir is a volume that gets created empty when a pod is created. It will persist across pod restarts but deleting the pod would delete the emptyDir content. The volumeMounts mounts the named volume created with emptyDir and establishes the base path for accessing the volume.  NOTE: For brevity, only the paths to the relevant configuration being added is shown. A complete example of a domain definition is at the end of this document.\nExample: kubectl edit domain bobs-bookstore -n bob and make the following edits:\nspec: logHome: /scratch/logs/bobs-bookstore logHomeEnabled: true serverPod: volumes: - emptyDir: {} name: weblogic-domain-storage-volume volumeMounts: - mountPath: /scratch name: weblogic-domain-storage-volume Add Elasticsearch secrets to WebLogic domain credentials The fluentd container will be configured to look for Elasticsearch parameters in the domain credentials. Edit the domain credentials and add the parameters shown in the example below.\nExample: kubectl edit secret bobs-bookstore-weblogic-credentials -n bob and add the base64 encoded values of each Elasticsearch parameter:\nelasticsearchhost: ZWxhc3RpY3NlYXJjaC5ib2JzLWJvb2tzLnNhbXBsZS5jb20= elasticsearchport: NDQz elasticsearchuser: Ym9i elasticsearchpassword: d2VsY29tZTE= Create Fluentd configuration Create a ConfigMap named fluentd-config in the namespace of the domain. The ConfigMap contains the parsing rules and Elasticsearch configuration.\nHere\u0026rsquo;s an explanation of some elements defined in the ConfigMap:\n The @type tail indicates that tail will be used to obtain updates to the log file The path of the log file is obtained from the LOG_PATH environment variable that is defined in the fluentd container The tag value of log records is obtained from the DOMAIN_UID environment variable that is defined in the fluentd container The \u0026lt;parse\u0026gt; section defines how to interpret and tag each element of a log record The \u0026lt;match **\u0026gt; section contains the configuration information for connecting to Elasticsearch and defines the index name of each record to be the domainUID  The following is an example of how to create the ConfigMap:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: labels: weblogic.domainUID: bobs-bookstore weblogic.resourceVersion: domain-v2 name: fluentd-config namespace: bob data: fluentd.conf: | \u0026lt;match fluent.**\u0026gt; @type null \u0026lt;/match\u0026gt; \u0026lt;source\u0026gt; @type tail path \u0026#34;#{ENV[\u0026#39;LOG_PATH\u0026#39;]}\u0026#34; pos_file /tmp/server.log.pos read_from_head true tag \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}\u0026#34; # multiline_flush_interval 20s \u0026lt;parse\u0026gt; @type multiline format_firstline /^####/ format1 /^####\u0026lt;(?\u0026lt;timestamp\u0026gt;(.*?))\u0026gt;/ format2 / \u0026lt;(?\u0026lt;level\u0026gt;(.*?))\u0026gt;/ format3 / \u0026lt;(?\u0026lt;subSystem\u0026gt;(.*?))\u0026gt;/ format4 / \u0026lt;(?\u0026lt;serverName\u0026gt;(.*?))\u0026gt;/ format5 / \u0026lt;(?\u0026lt;serverName2\u0026gt;(.*?))\u0026gt;/ format6 / \u0026lt;(?\u0026lt;threadName\u0026gt;(.*?))\u0026gt;/ format7 / \u0026lt;(?\u0026lt;info1\u0026gt;(.*?))\u0026gt;/ format8 / \u0026lt;(?\u0026lt;info2\u0026gt;(.*?))\u0026gt;/ format9 / \u0026lt;(?\u0026lt;info3\u0026gt;(.*?))\u0026gt;/ format10 / \u0026lt;(?\u0026lt;sequenceNumber\u0026gt;(.*?))\u0026gt;/ format11 / \u0026lt;(?\u0026lt;severity\u0026gt;(.*?))\u0026gt;/ format12 / \u0026lt;(?\u0026lt;messageID\u0026gt;(.*?))\u0026gt;/ format13 / \u0026lt;(?\u0026lt;message\u0026gt;(.*?))\u0026gt;/ # use the timestamp field in the message as the timestamp # instead of the time the message was actually read time_key timestamp keep_time_key true \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;match **\u0026gt; @type elasticsearch host \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_HOST\u0026#39;]}\u0026#34; port \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PORT\u0026#39;]}\u0026#34; user \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_USER\u0026#39;]}\u0026#34; password \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PASSWORD\u0026#39;]}\u0026#34; index_name \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}\u0026#34; scheme https ssl_version TLSv1_2 key_name timestamp types timestamp:time # inject the @timestamp special field (as type time) into the record # so you will be able to do time based queries. # not to be confused with timestamp which is of type string!!! include_timestamp true \u0026lt;/match\u0026gt; EOF Mount the ConfigMap as a volume in the weblogic-server container Edit the domain definition and configure a volume for the ConfigMap containing the fluentd configuration.\nNOTE: For brevity, only the paths to the relevant configuration being added is shown. A complete example of a domain definition is at the end of this document.\nExample: kubectl edit domain bobs-bookstore -n bob and add the following portions to the domain definition.\nspec: serverPod: volumes: - configMap: defaultMode: 420 name: fluentd-config name: fluentd-config-volume Add fluentd container Add a container to the domain that will run fluentd in the Administration Server and Managed Server pods.\nNotice the container definition:\n Defines a LOG_PATH environment variable that points to the log location of bobbys-front-end Defines ELASTICSEARCH_HOST, ELASTICSEARCH_PORT, ELASTICSEARCH_USER, and ELASTICSEARCH_PASSWORD environment variables that are all retrieving their values from the secret bobs-bookstore-weblogic-credentials Has volume mounts for the fluentd-config ConfigMap and the volume containing the domain logs  NOTE: For brevity, only the paths to the relevant configuration being added is shown. A complete example of a domain definition is at the end of this document.\nExample: kubectl edit domain bobs-bookstore -n bob and add the following container definition.\nspec: serverPod: containers: - args: - -c - /etc/fluent.conf env: - name: DOMAIN_UID valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;weblogic.domainUID\u0026#39;] - name: SERVER_NAME valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;weblogic.serverName\u0026#39;] - name: LOG_PATH value: /scratch/logs/bobs-bookstore/$(SERVER_NAME).log - name: FLUENTD_CONF value: fluentd.conf - name: FLUENT_ELASTICSEARCH_SED_DISABLE value: \u0026#34;true\u0026#34; - name: ELASTICSEARCH_HOST valueFrom: secretKeyRef: key: elasticsearchhost name: bobs-bookstore-weblogic-credentials - name: ELASTICSEARCH_PORT valueFrom: secretKeyRef: key: elasticsearchport name: bobs-bookstore-weblogic-credentials - name: ELASTICSEARCH_USER valueFrom: secretKeyRef: key: elasticsearchuser name: bobs-bookstore-weblogic-credentials optional: true - name: ELASTICSEARCH_PASSWORD valueFrom: secretKeyRef: key: elasticsearchpassword name: bobs-bookstore-weblogic-credentials optional: true image: fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3 imagePullPolicy: IfNotPresent name: fluentd resources: {} volumeMounts: - mountPath: /fluentd/etc/fluentd.conf name: fluentd-config-volume subPath: fluentd.conf - mountPath: /scratch name: weblogic-domain-storage-volume Verify logs are exported to Elasticsearch After the Administration Server and Managed Server pods have started with all the changes described above, the logs should now be sent to Elasticsearch.\nYou can check if the fluentd container is successfully tailing the log by executing a command like kubectl logs -f bobs-bookstore-admin-server -n bob fluentd. The log output should look similar to this:\n2019-10-01 16:23:44 +0000 [info]: #0 starting fluentd worker pid=13 ppid=9 worker=0 2019-10-01 16:23:44 +0000 [warn]: #0 /scratch/logs/bobs-bookstore/managed-server1.log not found. Continuing without tailing it. 2019-10-01 16:23:44 +0000 [info]: #0 fluentd worker is now running worker=0 2019-10-01 16:24:01 +0000 [info]: #0 following tail of /scratch/logs/bobs-bookstore/managed-server1.log When you connect to Kibana, you will see an index created for the domainUID.\nExample Kibana log output:\ntimestamp:Oct 1, 2019 4:18:07,111 PM GMT level:Info subSystem:Management serverName:bobs-bookstore-admin-server serverName2: threadName:Thread-8 info1: info2: info3: sequenceNumber:1569946687111 severity:[severity-value: 64] [partition-id: 0] [partition-name: DOMAIN] messageID:BEA-141107 message:Version: WebLogic Server 12.2.1.3.0 Thu Aug 17 13:39:49 PDT 2017 1882952 _id:OQIeiG0BGd1zHsxmUrEJ _type:fluentd _index:bobs-bookstore _score:1 Domain example The following is a complete example of a domain custom resource with a fluentd container configured.\n# If you are using 3.0.0-rc1, then the version on the following line # should be `v7` not `v6`. apiVersion: weblogic.oracle/v6 kind: Domain metadata: labels: weblogic.domainUID: bobs-bookstore weblogic.resourceVersion: domain-v2 name: bobs-bookstore namespace: bob spec: adminServer: adminService: channels: - channelName: default nodePort: 32401 - channelName: T3Channel nodePort: 32402 clusters: - clusterName: cluster-1 serverPod: domainHome: /u01/oracle/user_projects/domains/bobs-bookstore domainHomeSourceType: Image domainUID: bobs-bookstore experimental: istio: enabled: true readinessPort: 8888 image: phx.ocir.io/bobs-bookstore imagePullPolicy: IfNotPresent imagePullSecrets: - name: ocir includeServerOutInPodLog: true logHome: /scratch/logs/bobs-bookstore logHomeEnabled: true replicas: 2 serverPod: containers: - args: - -c - /etc/fluent.conf env: - name: DOMAIN_UID valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;weblogic.domainUID\u0026#39;] - name: SERVER_NAME valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;weblogic.serverName\u0026#39;] - name: LOG_PATH value: /scratch/logs/bobs-bookstore/$(SERVER_NAME).log - name: FLUENTD_CONF value: fluentd.conf - name: FLUENT_ELASTICSEARCH_SED_DISABLE value: \u0026#34;true\u0026#34; - name: ELASTICSEARCH_HOST valueFrom: secretKeyRef: key: elasticsearchhost name: bobs-bookstore-weblogic-credentials - name: ELASTICSEARCH_PORT valueFrom: secretKeyRef: key: elasticsearchport name: bobs-bookstore-weblogic-credentials - name: ELASTICSEARCH_USER valueFrom: secretKeyRef: key: elasticsearchuser name: bobs-bookstore-weblogic-credentials optional: true - name: ELASTICSEARCH_PASSWORD valueFrom: secretKeyRef: key: elasticsearchpassword name: bobs-bookstore-weblogic-credentials optional: true image: fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3 imagePullPolicy: IfNotPresent name: fluentd resources: {} volumeMounts: - mountPath: /fluentd/etc/fluentd.conf name: fluentd-config-volume subPath: fluentd.conf - mountPath: /scratch name: weblogic-domain-storage-volume env: - name: JAVA_OPTIONS value: -Dweblogic.StdoutDebugEnabled=false - name: USER_MEM_ARGS value: \u0026#39;-Djava.security.egd=file:/dev/./urandom -Xms64m -Xmx256m \u0026#39; - name: WL_HOME value: /u01/oracle/wlserver - name: MW_HOME value: /u01/oracle volumeMounts: - mountPath: /scratch name: weblogic-domain-storage-volume volumes: - emptyDir: {} name: weblogic-domain-storage-volume - configMap: defaultMode: 420 name: fluentd-config name: fluentd-config-volume serverStartPolicy: IF_NEEDED webLogicCredentialsSecret: name: bobs-bookstore-weblogic-credentials "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-fmw-domains/soa-suite/",
	"title": "Manage SOA domains",
	"tags": [],
	"description": "SOA domains include the deployment of various Oracle Service-Oriented Architecture (SOA) Suite components, such as SOA, Oracle Service Bus (OSB), and Oracle Enterprise Scheduler (ESS).",
	"content": " Oracle SOA Suite is currently supported only for non-production use in Docker and Kubernetes. The information provided in this document is a preview for early adopters who wish to experiment with Oracle SOA Suite in Kubernetes before it is supported for production use.\n Contents  Introduction Prerequisites for SOA Suite domains Limitations Obtaining the SOA Suite Docker image Creating a SOA Suite Docker image Configuring access to your database Running the Repository Creation Utility to set up your database schemas Create a Kubernetes Secret with the RCU credentials Creating a SOA domain Configuring a load balancer for SOA Suite domains Monitoring a SOA domain  Introduction The operator supports deployment of SOA Suite components such as Oracle Service-Oriented Architecture (SOA), Oracle Service Bus (OSB), and Oracle Enterprise Scheduler (ESS). Currently the operator supports these different domain types:\n soa: Deploys a SOA domain osb: Deploys an OSB (Oracle Service Bus) domain soaess: Deploys a SOA domain with Enterprise Scheduler (ESS) soaosb: Deploys a domain with SOA and OSB soaessosb: Deploys a domain with SOA, OSB, and ESS  This document provides details about the special considerations for deploying and running SOA Suite domains with the operator. Other than those considerations listed here, SOA Suite domains work in the same way as FMW Infrastructure domains and WebLogic Server domains.\nIn this release, SOA Suite domains are supported using the “domain on a persistent volume” domain home source type only, where the domain home is located in a PersistentVolume (PV).\nPrerequisites for SOA Suite domains  Kubernetes 1.13.5+, 1.14.3+ and 1.15.2+ (check with kubectl version). Flannel networking v0.11.0-amd64 (check with docker images | grep flannel). Docker 18.9.1 (check with docker version) Helm 2.14.0+ (check with helm version). Oracle Fusion Middleware Infrastructure 12.2.1.3.0 image with patch 29135930.  The existing Fusion Middleware Infrastructure Docker image, container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.3, has all the necessary patches applied. Check the Fusion Middleware Infrastructure patches with docker run container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.3 sh -c '$ORACLE_HOME/OPatch/opatch lspatches'.   You must have the cluster-admin role to install the operator. We do not currently support running SOA in non-Linux containers.  Limitations Compared to running a WebLogic Server domain in Kubernetes using the operator, the following limitations currently exist for SOA Suite domains:\n The Domain in Image domain home source type is not supported. Only configured clusters are supported. Dynamic clusters are not supported for SOA Suite domains. Note that you can still use all of the scaling features, you just need to define the maximum size of your cluster at domain creation time. Deploying and running SOA Suite domains is supported only in operator versions 2.4.0 and later. The WebLogic Logging Exporter currently supports WebLogic Server logs only. Other logs will not be sent to Elasticsearch. Note, however, that you can use a sidecar with a log handling tool like Logstash or Fluentd to get logs. The WebLogic Monitoring Exporter currently supports the WebLogic MBean trees only. Support for JRF MBeans has not been added yet.  For early access customers, with bundle patch access, we recommend that you build and use the Oracle SOA Suite Docker image with the latest bundle patch for Oracle SOA. The Oracle SOA Suite Docker image in container-registry.oracle.com does not have the bundle patch installed. However, if you do not have access to the bundle patch, you can obtain the Oracle SOA Suite Docker image without the bundle patch from container-registry.oracle.com, as described below.\n Obtaining the SOA Suite Docker Image The pre-built Oracle SOA Suite image is available at, container-registry.oracle.com/middleware/soasuite:12.2.1.3.\nTo pull an image from the Oracle Container Registry, in a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms are stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nTo obtain the image, log in to the Oracle Container Registry:\n$ docker login container-registry.oracle.com Then, you can pull the image with this command:\n$ docker pull container-registry.oracle.com/middleware/soasuite:12.2.1.3 Creating a SOA Suite Docker image You can also create a Docker image containing the Oracle SOA Suite binaries. This is the recommended approach if you have access to the Oracle SOA bundle patch.\nPlease consult the README file for important prerequisite steps, such as building or pulling the Server JRE Docker image, Oracle FMW Infrastructure Docker image, and downloading the Oracle SOA Suite installer and bundle patch binaries.\nFor the Fusion Middleware Infrastructure image, you must install the required patch to use this image with the Oracle WebLogic Kubernetes operator. A pre-built (and already patched) Fusion Middleware Infrastructure image, container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.3-200109, is available at container-registry.oracle.com. We recommend that you pull and rename this image to build the Oracle SOA Suite image.\n$ docker pull container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.3-200316 $ docker tag container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.3-200316 oracle/fmw-infrastructure:12.2.1.3  If you are pulling the Fusion Middleware Infrastructure image from container-registry.oracle.com, you must use the image with the tag 12.2.1.3-200109; no other tagged image will work for the Oracle SOA Suite image build.\n You can also build the Fusion Middleware Infrastructure image with the required patch (29135930) applied. A sample is provided that demonstrates how to create a Docker image with the necessary patch installed. Use this patched Fusion Middleware Infrastructure image for building the Oracle SOA Suite image.\nFollow these steps to build the necessary images - a patched Fusion Middleware Infrastructure image, and then the SOA Suite image as a layer on top of that:\n  Make a local clone of the sample repository.\n$ git clone https://github.com/oracle/docker-images   Build the oracle/fmw-infrastructure:12.2.1.3 image as shown below:\n$ cd docker-images/OracleFMWInfrastructure/dockerfiles $ sh buildDockerImage.sh -v 12.2.1.3 -s   Download the required patch (29135930) from My Oracle Support.\n  Create a Docker image containing the Fusion Middleware Infrastructure binaries with the patch applied by running the provided script:\n$ cd docker-images/OracleFMWInfrastructure/samples/12213-patch-fmw-for-k8s $ ./build.sh This will produce an image named oracle/fmw-infrastructure:12213-update-k8s. You will need to rename this image, for example from oracle/fmw-infrastructure:12213-update-k8s to oracle/fmw-infrastructure:12.2.1.3, or update the samples to refer to the image you created.\n$ docker tag oracle/fmw-infrastructure:12213-update-k8s oracle/fmw-infrastructure:12.2.1.3   Download the Oracle SOA Suite installer, latest Oracle SOA bundle patch (30638100 or later) and the patch 27117282 from the Oracle Technology Network or e-delivery.\n NOTE: Copy the installer binaries to the same location as the Dockerfile and the patch ZIP files under the docker-images/OracleSOASuite/dockerfiles/12.2.1.3/patches folder.\n   Create the Oracle SOA Suite image by running the provided script:\n$ cd docker-images/OracleSOASuite/dockerfiles $ ./buildDockerImage.sh -v 12.2.1.3 -s The image produced will be named oracle/soa:12.2.1.3. The samples and instructions assume the Oracle SOA Suite image is named container-registry.oracle.com/middleware/soasuite:12.2.1.3. You will need to rename your image to match this name, or update the samples to refer to the image you created.\n$ docker tag oracle/soa:12.2.1.3 container-registry.oracle.com/middleware/soasuite:12.2.1.3 You can use this image to run the Repository Creation Utility and to run your domain using the “domain on a persistent volume” model.\n  Before creating a domain, you will need to set up the necessary schemas in your database.\nConfiguring access to your database SOA Suite domains require a database with the necessary schemas installed in them. The Repository Creation Utility (RCU) allows you to create those schemas. You must set up the database before you create your domain. There are no additional requirements added by running SOA in Kubernetes; the same existing requirements apply.\nFor testing and development, you may choose to run your database inside Kubernetes or outside of Kubernetes.\nThe Oracle Database Docker images are supported for non-production use only. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1).\n Running the database inside Kubernetes Follow these instructions to perform a basic deployment of the Oracle database in Kubernetes. For more details about database setup and configuration, refer to this page.\nWhen running the Oracle database in Kubernetes, you have an option to attach PersistentVolumes (PV) so that the database storage will be persisted across database restarts. If you prefer not to persist the database storage, follow the instructions in this document to set up a database in a container with no PersistentVolume (PV) attached.\n NOTE: start-db-service.sh creates the database in the default namespace. If you want to create the database in a different namespace, you need to manually update the value for all the occurrences of the namespace field in the provided sample file create-rcu-schema/common/oracle.db.yaml.\n These instructions will set up the database in a container with the PersistentVolume (PV) attached. If you chose not to use persistent storage, please go to the RCU creation step.\n Create the PersistentVolume and PersistentVolumeClaim for the database using the create-pv-pvc.sh sample. Refer to the instructions provided in that sample.  When creating the PV and PVC for the database, make sure that you use a different name and storage class for the PV and PVC for the domain. The name is set using the value of the baseName field in create-pv-pvc-inputs.yaml.\n  Start the database and database service using the following commands:   NOTE: Make sure you update the kubernetes/samples/scripts/create-soa-domain/domain-home-on-pv/create-database/db-with-pv.yaml file with the name of the PVC created in the previous step. Also, update the value for all the occurrences of the namespace field to the namespace where the database PVC was created.\n ```bash $ cd weblogic-kubernetes-operator/kubernetes/samples/scripts/create-soa-domain/domain-home-on-pv/create-database $ kubectl create -f db-with-pv.yaml ```  The database will take several minutes to start the first time, while it performs setup operations. You can watch the log to see its progress using this command:\n$ kubectl logs -f oracle-db -n soans A log message will indicate when the database is ready. Also, you can verify the database service status using this command:\n$ kubectl get pods,svc -n soans |grep oracle-db po/oracle-db 1/1 Running 0 6m svc/oracle-db ClusterIP None \u0026lt;none\u0026gt; 1521/TCP,5500/TCP 7m Running the Repository Creation Utility to set up your database schemas Creating schemas To create the database schemas for Oracle SOA Suite, run the create-rcu-schema.sh script as described here.\nThe following example shows commands you might use to execute create-rcu-schema.sh:\n$ cd weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-schema $ ./create-rcu-schema.sh \\  -s SOA1 \\  -t soaessosb \\  -d oracle-db.soans.svc.cluster.local:1521/devpdb.k8s \\  -i container-registry.oracle.com/middleware/soasuite:12.2.1.3 For SOA domains, the create-rcu-schema.sh script supports the following domain types soa,osb,soaosb,soaess,soaessosb. You must specify one of these using the -t flag.\nYou need to make sure that you maintain the association between the database schemas and the matching domain just like you did in a non-Kubernetes environment. There is no specific functionality provided to help with this.\nDropping schemas If you want to drop the schema, you can use the drop-rcu-schema.sh script as described here.\nThe following example shows commands you might use to execute drop-rcu-schema.sh:\n$ cd weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-schema $ ./drop-rcu-schema.sh \\  -s SOA1 \\  -t soaessosb \\  -d oracle-db.soans.svc.cluster.local:1521/devpdb.k8s For SOA domains, the drop-rcu-schema.sh script supports the domain types soa,osb,soaosb,soaess,soaessosb. You must specify one of these using the -t flag.\nCreate a Kubernetes Secret with the RCU credentials You also need to create a Kubernetes Secret containing the credentials for the database schemas. When you create your domain using the sample provided below, it will obtain the RCU credentials from this secret.\nUse the provided sample script to create the secret as shown below:\n$ cd kubernetes/samples/scripts/create-rcu-credentials $ ./create-rcu-credentials.sh \\  -u SOA1_SOAINFRA \\  -p Welcome1 \\  -a sys \\  -q Oradoc_db1 \\  -d soainfra \\  -n soans \\  -s soainfra-rcu-credentials The parameter values are as follows:\n The schema owner user name (-u) must be the schemaPrefix value followed by an underscore and a component name, for example SOA1_SOAINFRA. The schema owner password (-p) will be the password you provided for regular schema users during RCU creation. The database administration user (-u) and password (-q). The domainUID for the domain (-d). The namespace the domain is in (-n); if omitted, then default is assumed. The name of the secret (-s).  You can confirm the secret was created as expected with the kubectl get secret command. An example is shown below, including the output:\n$ kubectl get secret soainfra-rcu-credentials -o yaml -n soans apiVersion: v1 data: password: V2VsY29tZTE= sys_password: T3JhZG9jX2RiMQ== sys_username: c3lz username: U09BMQ== kind: Secret metadata: creationTimestamp: 2019-06-02T07:15:31Z labels: weblogic.domainName: soainfra weblogic.domainUID: soainfra name: soainfra-rcu-credentials namespace: soans resourceVersion: \u0026#34;11562794\u0026#34; selfLink: /api/v1/namespaces/soans/secrets/soainfra-rcu-credentials uid: 1230385e-6caa-11e9-8143-fa163efa261a type: Opaque Creating a SOA domain Now that you have your Docker images and you have created your RCU schemas, you are ready to create your domain. To continue, follow the instructions in the SOA Domain sample.\nConfiguring a load balancer for SOA Suite domains An ingress-based load balancer can be configured to access the Oracle SOA and Oracle Service Bus domain application URLs. Refer to the Ingress document for details.\nAs part of the ingress-per-domain setup for Oracle SOA and Oracle Service Bus domains, the values.yaml file (under the ingress-per-domain directory) needs to be updated with the appropriate values from your environment. A sample values.yaml file (for the Traefik load balancer) is shown below:\n# Default values for ingress-per-domain. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Load balancer type. Supported values are: TRAEFIK, VOYAGER type: TRAEFIK # WLS domain as backend to the load balancer wlsDomain: domainUID: soainfra soaClusterName: soa_cluster osbClusterName: osb_cluster soaManagedServerPort: 8001 osbManagedServerPort: 9001 adminServerName: adminserver adminServerPort: 7001 # Traefik specific values traefik: # hostname used by host-routing hostname: testhost.domain.com # Voyager specific values voyager: # web port webPort: 30305 # stats port statsPort: 30315 Below are the path-based, ingress routing rules (spec.rules section) that need to be defined for Oracle SOA and Oracle Service Bus domains. You need to update the appropriate Ingress template YAML file based on the load balancer being used. For example, the template YAML file for the Traefik load balancer is located at kubernetes/samples/charts/ingress-per-domain/templates/traefik-ingress.yaml.\nrules: - host: \u0026#39;{{ .Values.traefik.hostname }}\u0026#39; http: paths: - path: /console backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /em backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /servicebus backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: /lwpfconsole backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-{{ .Values.wlsDomain.adminServerName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.adminServerPort }} - path: backend: serviceName: \u0026#39;{{ .Values.wlsDomain.domainUID }}-cluster-{{ .Values.wlsDomain.soaClusterName | lower | replace \u0026#34;_\u0026#34; \u0026#34;-\u0026#34; }}\u0026#39; servicePort: {{ .Values.wlsDomain.soaManagedServerPort }} Now you can access the Oracle SOA Suite domain URLs, as listed below, based on the domain type you selected.\n  Oracle SOA:\nhttp://\\\u0026lt;hostname\\\u0026gt;:\\\u0026lt;port\\\u0026gt;/weblogic/ready\nhttp://\\\u0026lt;hostname\\\u0026gt;:\\\u0026lt;port\\\u0026gt;/console\nhttp://\\\u0026lt;hostname\\\u0026gt;:\\\u0026lt;port\\\u0026gt;/em\nhttp://\\\u0026lt;hostname\\\u0026gt;:\\\u0026lt;port\\\u0026gt;/soa-infra\nhttp://\\\u0026lt;hostname\\\u0026gt;:\\\u0026lt;port\\\u0026gt;/soa/composer\nhttp://\\\u0026lt;hostname\\\u0026gt;:\\\u0026lt;port\\\u0026gt;/integration/worklistapp\n  Oracle Enterprise Scheduler Service (ESS):\nhttp://\\\u0026lt;hostname\\\u0026gt;:\\\u0026lt;port\\\u0026gt;/ess\nhttp://\\\u0026lt;hostname\\\u0026gt;:\\\u0026lt;port\\\u0026gt;/EssHealthCheck\n  Oracle Service Bus (OSB):\nhttp://\\\u0026lt;hostname\\\u0026gt;:\\\u0026lt;port\\\u0026gt;/servicebus\nhttp://\\\u0026lt;hostname\\\u0026gt;:\\\u0026lt;port\\\u0026gt;/lwpfconsole\n  Monitoring a SOA domain After the SOA domain is set up, you can:\n Monitor the SOA instance using Prometheus and Grafana. See Monitor a SOA domain. Publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana. See Publish logs to Elasticsearch.  "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/cicd/why-layering-matters/",
	"title": "Why layering matters",
	"tags": [],
	"description": "Learn why Docker image layering affects CI/CD processes.",
	"content": "How does layering affect our CI/CD process? Now that we know more about layering, let’s talk about why it is important to our CI/CD process. Let\u0026rsquo;s consider the kinds of updates we might want to make to our domain:\nYou might want to update the domain by:\n Installing a patch on the operating system or a library. Updating the version of the JDK you are using. Picking up a new version of WebLogic Server. Installing patches on WebLogic Server. Updating the domain configuration, for example:  Adding or changing a resource like a data source or queue. Installing or updating applications. Changing various settings in the domain configuration.    If we just want to update the domain configuration itself, that is the top layer, then it is pretty easy. We can make the necessary changes and save a new version of that layer, and then roll the domain. We could also choose to just build another layer on top of the existing top layer that contains our delta. If the change is small, then we will just end up with another small layer, and as we have seen, the small layers are no problem.\nBut consider a more complicated scenario - let\u0026rsquo;s take updating the JDK as an example to understand the impact of layers. Say we want to update from JDK 8u201 to 8u202 as shown in the example above. If we took the \u0026ldquo;your first domain\u0026rdquo; image and updated the JDK, then we would end up with a new layer on top containing JDK 8u202. That other layer with JDK 8u201 is still there; even if we \u0026ldquo;delete\u0026rdquo; the directory, we don\u0026rsquo;t get that space back. So now our 1.5GB \u0026ldquo;image\u0026rdquo; has grown to 1.75GB. This is not ideal, and the more often we try to change lower layers, the worse it gets.\nYou might be asking, \u0026ldquo;Can\u0026rsquo;t we just swap out the JDK layer for a new one?\u0026rdquo; That is an excellent question, but the unfortunate reality today is that there is no reliable way to do that. There are various attempts to create a \u0026ldquo;rebasing\u0026rdquo; capability for Docker that would enable such an action, but some research will show you that they are mostly abandoned due to limited documentation of how the layering works at the level of detail needed to implement something like this.\nNext you might think, \u0026ldquo;Oh, that’s ok, we can just rebuild the layers above the JDK on top of this new layer.\u0026rdquo; That is very true, we can. But there is a big caveat here for Domain in Image domains. When you create a WebLogic domain, a domain encryption key is created. This key is stored in the security/SerializedSystemIni.dat file in your domain and it is used to encrypt several other things in your domain configuration, like passwords, for example. Today (in WebLogic Server 12.2.1.4.0) there is no way to conveniently \u0026ldquo;extract\u0026rdquo; or \u0026ldquo;reuse\u0026rdquo; this encryption key. So what does this mean in practice?\nIf you recreate a Domain in Image domain in your CI/CD process, even though you may end up with a domain that is for all intents and purposes identical to the previous domain, it will have a different encryption key.\n This means that technically, it is a \u0026ldquo;different\u0026rdquo; domain for Domain in Image type domains. Does this matter? Maybe, maybe not. It depends. If you want to do a rolling restart of your domain, then yes, it matters. First of all, the \u0026ldquo;new\u0026rdquo; servers will fail to start because the operator will be trying to inject credentials to start the server which were encrypted with the \u0026ldquo;old\u0026rdquo; domain encryption key.\nBut even if this did not prevent Domain in Image pods from starting, there would still be a problem. You cannot have members of a domain with different encryption keys. If WebLogic saw a new member trying to join the domain with a different key, it would consider it to be an intruder and refuse to accept it into the domain. Client HTTP sessions would not work across the two different sets of servers, so clients could see errors and need to retry. Worse, if these two different sets of servers tried to access the same resources this could lead to data corruption.\nSo what can we do? Well, we could not roll the domain, but instead completely shut down the old version first, and then start up the new one. This way we avoid any issues with incompatibilities, but we do introduce a brief outage. This may be acceptable, or it may not.\nAnother option is to find a way to keep the \u0026ldquo;same\u0026rdquo; domain, that is, the same domain encryption key, so that we can still roll the domain and there will be no conflicts.\nMutating Domain in Image domain home configuration without losing encryption keys If we want to make a change in a lower layer in Domain in Image domains without losing our domain encryption keys, then we need to find a way to \u0026ldquo;save\u0026rdquo; the domain and then put it back into a new layer, later, on top of the other new (lower) layers, as depicted in the image below:\nThe process looks like this:\n From our existing image (left), we extract the domain into some kind of archive. Then we start with the new JDK image which was built on top of the same base image (or we build it ourselves, if needed). We build a new WebLogic layer (or grab the one that Oracle built for us) on top of this new JDK. Then we need to “restore” our domain from the archive into a new layer.  "
},
{
	"uri": "/weblogic-kubernetes-operator/security/domain-security/",
	"title": "Domain security",
	"tags": [],
	"description": "WebLogic domain security and the operator",
	"content": "  Docker image protection  WebLogic domain in Docker image protection\n Channels  WebLogic channels\n "
},
{
	"uri": "/weblogic-kubernetes-operator/security/domain-security/weblogic-channels/",
	"title": "Channels",
	"tags": [],
	"description": "WebLogic channels",
	"content": "WebLogic T3 channels Oracle recommends not exposing any administrative, RMI, or T3 channels outside the Kubernetes cluster unless absolutely necessary. If exposing any of these channels, limit access using controls like security lists or set up a Bastion to provide access.\n When accessing T3 or RMI based channels, the preferred approach is to kubectl exec into the Kubernetes Pod and then run wlst, or set up Bastion access and then run wlst from the Bastion host to connect to the Kubernetes cluster.\nAlso, consider a private VPN if you need use cross-domain T3 access between clouds, data centers, and such.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/domain-lifecycle/restarting/",
	"title": "Restarting",
	"tags": [],
	"description": "This document describes when to restart servers in the Oracle WebLogic Server in Kubernetes environment.",
	"content": "This document describes when to restart servers in the Oracle WebLogic Server in Kubernetes environment.\nOverview There are many situations where changes to the Oracle WebLogic Server in Kubernetes environment require that all the servers in a domain or cluster be restarted, for example, when applying a WebLogic Server patch or when upgrading an application.\nOne of the operator\u0026rsquo;s most important jobs is to start and stop WebLogic Servers by creating and deleting their corresponding Kubernetes Pods. Sometimes, you need to make changes that make the pods obsolete, therefore the pods need to be deleted and recreated. Depending on the change, sometimes the pods can be gradually recreated, without taking the entire domain out of service (for example, rolling restarts) and sometimes all the pods need to be deleted then recreated, taking the entire domain out of service for a while (for example, full restarts).\nThe following types of server restarts are supported in Oracle WebLogic Server in Kubernetes:\n  Rolling restarts - a coordinated and controlled shut down of all of the servers in a domain or cluster while ensuring that service to the end user is not interrupted.\n  Operator initiated - where the WebLogic Server Kubernetes Operator can detect some types of changes and will automatically initiate rolling restarts of server pods in a WebLogic domain.\n  Manually initiated - required when certain changes in the Oracle WebLogic Server in Kubernetes environment cannot be detected by the operator, so a rolling restart must be manually initiated.\n    Full domain restarts - the Administration Server and all the Managed Servers in a domain are shutdown, impacting service availability to the end user, and then restarted. Unlike a rolling restart, the operator cannot detect and initiate a full domain restart; it must always be manually initiated.\n  For detailed information on how to restart servers in a Oracle WebLogic Server in Kubernetes environment, see Starting, stopping, and restarting servers.\nCommon restart scenarios This document describes what actions you need to take to properly restart your servers for a number of common scenarios:\n Modifying the WebLogic configuration Changing the custom domain configuration overrides (also called situational configuration) for Domain in PV and Domain in Image domains Changing the model files for Model in Image domains Changing the WebLogic Server credentials (the user name and password) Changing properties on the domain resource that affect server pods (such as image, volumes, and env) Applying WebLogic Server patches Updating deployed applications for domain home in image  Use cases Modifying the WebLogic Server configuration Changes to the Oracle WebLogic Server configuration may require either a rolling or full domain restart depending on the domain home location and the type of configuration change.\n  Domain in Image: For a domain home in image, any changes (dynamic or non-dynamic) to the WebLogic configuration requires a full domain restart.\n If you create a new image with a new name, then you must avoid a rolling restart, which can cause unexpected behavior for the running domain due to configuration inconsistencies as seen by the various servers, by following the steps in Avoiding a rolling restart when changing image property on a domain resource. If you create a new image with the same name, then you must manually initiate a full domain restart. See Full domain restarts.    Model in Image:\n  Any image that supplies configuration changes that are incompatible with the current running domain require a full shutdown before changing the domain resource image setting, instead of a rolling restart. For changes that support a rolling restart, see Supported and unsupported updates.\n  If you create a new image with a new name, and you want to avoid a rolling restart, see Avoiding a rolling restart when changing image property on a domain resource.\n  If you create a new image with the same name, then you must manually initiate either a full domain restart or rolling restart for pods to run with the new image. To initiate a full restart, see Full domain restarts. To initiate a rolling restart, change the value of your domain resource restartVersion field. See Restarting servers and Rolling restarts.\n  If you are supplying updated models or secrets for a running domain, and you want the configuration updates to take effect using a rolling restart:\n You must either supply a new image name in the domain resource or change the domain resource restartVersion in order to force the operator to reload the configuration. With either of these two changes, the operator will rerun the domain\u0026rsquo;s introspector job, which will verify and apply the new configuration. If the introspector job\u0026rsquo;s configuration verification succeeds, then it will subsequently roll (restart) the pods; if the job fails, then a roll will not occur. If you change other fields that typically cause a restart, such as volumes, env, and such, then the introspector job will not rerun and a rolling restart will proceed without loading the configuration changes.      Domain in PV: For a domain home on PV, the type of restart needed to apply the changes depends on the nature of the WebLogic configuration change:\n Changes to parts of the WebLogic configuration that the operator introspects, require a full restart, even if the changes are dynamic. The following are the types of changes to the WebLogic Server configuration that the operator introspects:  Adding or removing a cluster, server, dynamic server, or network access point Changing a cluster, server, dynamic server, or network access point name Enabling or disabling the listen port, SSL port, or admin port Changing any port numbers Changing a network access point\u0026rsquo;s public address   Other dynamic WebLogic configuration changes do not require a restart. For example, a change to a server\u0026rsquo;s connection timeout property is dynamic and does not require a restart. Other non-dynamic WebLogic configuration changes require either a manually initiated rolling restart or a full domain restart, depending on the nature of the change. For example, a rolling restart is applicable when changing a WebLogic Server stuck thread timer interval property. See Restart all the servers in the domain.    Changing the custom domain configuration overrides Any change to domain configuration overrides requires a full domain restart. This includes:\n Changing the domain resource\u0026rsquo;s configuration.overridesConfigMap to point to a different configuration map Changing the domain resource\u0026rsquo;s configuration.secrets to point to a different list of secrets Changing the contents of the configuration map referenced by configuration.overridesConfigMap Changing the contents to any of the secrets referenced by configuration.secrets  Changing the WebLogic Server credentials A change to the WebLogic Server credentials (the user name and password), contained in the Kubernetes Secret for the domain, requires a full domain restart. The Kubernetes Secret can be updated directly or a new secret can be created and then referenced by the webLogicCredentialsSecret property in the domain resource.\nChanging properties on the domain resource that affect server pods The operator will initiate a rolling restart of the domain when you modify any of the domain resource properties that affect the server pods configuration, such as image, volumes, and env. For a complete list, see Properties that cause servers to be restarted.\nYou can modify these properties using the kubectl command-line tool\u0026rsquo;s edit and patch commands or through the Kubernetes REST API.\nFor example, to edit the domain resource directly using the kubectl command-line tool:\nkubectl edit domain \u0026lt;domain name\u0026gt; -n \u0026lt;domain namespace\u0026gt; The edit command opens a text editor which lets you edit the domain resource in place.\nTypically, it\u0026rsquo;s better to edit the domain resource directly; otherwise, if you scaled the domain, and you edit only the original domain.yaml file and reapply it, you could go back to your old replicas count.\n Applying WebLogic Server patches Oracle provides different types of patches for WebLogic Server, such as Patch Set Updates, Security Patch Updates, and One-Off patches. Information on whether a patch is rolling-compatible or requires a manual full domain restart usually can be found in the patch\u0026rsquo;s documentation, such as the README file.\nWebLogic Server patches can be applied to either a domain home in image or a domain home on PV.\nWith rolling-compatible patches:\n If you update the image property with a new image name, then the operator will initiate a rolling restart. If you keep the same image name, then you must manually initiate a rolling restart. See Restart all the servers in the domain.  With patches that are not rolling-compatible:\n If you keep the same image name, then you must manually initiate a full domain restart. See Full domain restarts. If you update the image property with a new image name, then you must avoid the rolling restart by following the steps in Avoiding a rolling restart when changing image property on a domain resource.  Updating deployed applications Frequent updates of deployed applications using a continuous integration/continuous delivery (CI/CD) process is a very common use case. The process for applying an updated application is different for domain home in image and model in image than it is for domain home on PV. A rolling-compatible application update is where some servers are running the old version and some are running the new version of the application during the rolling restart process. On the other hand, an application update that is not rolling-compatible requires that all the servers in the domain be shut down and restarted.\nIf the application update is rolling-compatible:\n If you update the image property with a new image name, then the operator will initiate a rolling restart. If you keep the same image name, then you must manually initiate a rolling restart. See Restart all the servers in the domain.  If the application update is not rolling-compatible:\n If you keep the same image name, then you must manually initiate a full domain restart. See Full domain restarts. If you update the image property with a new image name, then you must avoid the rolling restart by following the steps in Avoiding a rolling restart when changing image property on a domain resource.  Rolling out an updated domain home in image or model in image Follow these steps to create new rolling-compatible image if you only need to patch your WebLogic Server domain or update application deployment files:\na. Select a different name for the new image.\nb. For domain home in image domains, it is important to keep your original domain home in your new image.\nUsing the same domain home-in-image Docker image as a base, create a new Docker image by copying (COPY command in a Dockerfile) the updated application deployment files or WebLogic Server patches into the Docker image during the Docker image build.\nThe key here is to make sure that you do not re-run WLST or WDT to create a new domain home even though it will have the same configuration. Creating a new domain will change the domain secret and you won\u0026rsquo;t be able to do a rolling restart.\n c. Deploy the new Docker image to your Docker repository with the new name.\nd. Update the image property of the domain resource, specifying the new image name.\nFor example:\n ``` domain: spec: image: oracle/weblogic-updated:2.5.0 ```  e. The operator will now initiate a rolling restart, which will apply the updated image, for all the server pods in the domain.\nAvoiding a rolling restart when changing the image property on a domain resource If you\u0026rsquo;ve created a new image that is not rolling-compatible, and you\u0026rsquo;ve changed the image name, then:\n  Bring the domain down (stopping all the server pods) by setting the serverStartPolicy to NEVER. See Shut down all the servers.\n  Update the image property with a new image name.\n  Start up the domain (starting all the server pods) by setting the serverStartPolicy to IF_NEEDED.\n  Other considerations for restarting a domain   Consider the order of changes:\nIf you need to make multiple changes to your domain at the same time, you\u0026rsquo;ll want to be careful about the order in which you do your changes, so that servers aren\u0026rsquo;t restarted prematurely or restarted needlessly. For example, if you want to change the readiness probe\u0026rsquo;s tuning parameters and the Java options (both of which are rolling-compatible), then you should update the domain resource once, changing both values, so that the operator rolling restarts the servers once. Or, if you want to change the readiness probe\u0026rsquo;s tuning parameters (which is rolling-compatible) and change the domain customizations (which require a full restart), then you should do a full shutdown first, then make the changes, and then restart the servers.\nAlternatively, if you know that your set of changes are not rolling-compatible, then you must avoiding a rolling restart by:\n  Bringing the domain down (stopping all the server pods) by setting the serverStartPolicy to NEVER. See Shut down all the servers.\n  Make all your changes to the Oracle WebLogic Server in Kubernetes environment.\n  Starting up the domain (starting all the server pods) by setting the serverStartPolicy to IF_NEEDED.\n    Changes that require domain knowledge.\nSometimes you need to make changes that require server restarts, yet the changes are not to the WebLogic configuration, the image, or the Kubernetes resources that register your domain with the operator. For example, your servers are caching information from an external database and you\u0026rsquo;ve modified the contents of the database.\nIn these cases, you must manually initiate a restart.\n  Managed Coherence Servers safe shut down.\nIf the domain is configured to use a Coherence cluster, then you will need to increase the Kubernetes graceful timeout value. When a server is shut down, Coherence needs time to recover partitions and rebalance the cluster before it is safe to shut down a second server. Using the Kubernetes graceful termination feature, the operator will automatically wait until the Coherence HAStatus MBean attribute indicates that it is safe to shut down the server. However, after the graceful termination timeout expires, the pod will be deleted regardless. Therefore, it is important to set the domain YAML timeoutSeconds to a large enough value to prevent the server from shutting down before Coherence is safe. Furthermore, if the operator is not able to access the Coherence MBean, then the server will not be shut down until the domain timeoutSeconds expires. To minimize any possibility of cache data loss, you should increase the timeoutSeconds value to a large number, for example, 15 minutes.\n  "
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/domains/domain-home-on-pv/",
	"title": "Domain home on a PV",
	"tags": [],
	"description": "Sample for creating a WebLogic domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated WebLogic domain.",
	"content": "The sample scripts demonstrate the creation of a WebLogic domain home on an existing Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain. Optionally, the scripts start up the domain, and WebLogic Server pods and services.\nPrerequisites Before you begin, read this document, Domain resource.\nThe following prerequisites must be met prior to running the create domain script:\n Make sure the WebLogic Server Kubernetes Operator is running. The operator requires either Oracle WebLogic Server 12.2.1.3.0 with patch 29135930 applied, or Oracle WebLogic Server 12.2.1.4.0, or Oracle WebLogic Server 14.1.1.0.0. The existing WebLogic Docker image, container-registry.oracle.com/middleware/weblogic:12.2.1.3, has all the necessary patches applied. For details on how to obtain or create the image, see WebLogic Docker images. Create a Kubernetes Namespace for the domain unless you intend to use the default namespace. In the same Kubernetes Namespace, create the Kubernetes PersistentVolume (PV) where the domain home will be hosted, and the Kubernetes PersistentVolumeClaim (PVC) for the domain. For samples to create a PV and PVC, see Create sample PV and PVC. By default, the create-domain.sh script creates a domain with the domainUID set to domain1 and expects the PVC domain1-weblogic-sample-pvc to be present. You can create domain1-weblogic-sample-pvc using create-pv-pvc.sh with an inputs file that has the domainUID set to domain1. Create the Kubernetes Secrets username and password of the administrative account in the same Kubernetes Namespace as the domain.  Please note the following important considerations about using persistent storage.\n There are a number of different Kubernetes storage providers that can be used to create persistent volumes. Depending on which variant and version of Kubernetes you are using, there will be different steps required to create persistent volumes. You must use a storage provider that supports the ReadWriteMany option.\nMany storage providers will create a file system on the persistent volume which is owned by the root user. In those cases, you will need to update the file permissions or ownership so that the oracle user (uid 1000) that is used in the standard WebLogic Server Docker images can write to the file system in the persistent volume.\nThis sample will automatically set the owner of all files on the persistent volume to uid 1000. If you want to change that behavior, please edit kubernetes/samples/scripts/create-weblogic-domain/domain-home-on-pv/create-domain-job-template.yaml and edit or remove the initContainer section.\nIn some variants of Kubernetes (for example OpenShift), you may also need to configure your pods to run with user 1000 to make sure that the WebLogic processes can access the file system on the persistent volume.\nUse the script to create a domain Make a copy of the create-domain-inputs.yaml file, and run the create script, pointing it at your inputs file and an output directory:\n$ ./create-domain.sh \\ -i create-domain-inputs.yaml \\ -o /\u0026lt;path to output-directory\u0026gt; The script will perform the following steps:\n Create a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The pathname is /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, then its contents must be removed before using this script. Create a Kubernetes Job that will start up a utility WebLogic Server container and run offline WLST scripts, or WebLogic Deploy Tool (WDT) scripts, to create the domain on the shared storage. Run and wait for the job to finish. Create a Kubernetes domain YAML file, domain.yaml, in the directory that is created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command:  $ kubectl apply -f /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml  Create a convenient utility script, delete-domain-job.yaml, to clean up the domain home created by the create script.  As a convenience, using the -e option, the script can optionally create the domain object, which in turn results in the creation of the corresponding WebLogic Server pods and services as well.\nThe usage of the create script is as follows:\n$ sh create-domain.sh -h usage: create-domain.sh -o dir -i file [-e] [-v] [-h] -i Parameter inputs file, must be specified. -o Output directory for the generated YAML files, must be specified. -e Also create the resources in the generated YAML files, optional. -v Validate the existence of persistentVolumeClaim, optional. -h Help If you copy the sample scripts to a different location, make sure that you copy everything in the \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts directory together into the target directory, maintaining the original directory hierarchy.\nThe default domain created by the script has the following characteristics:\n An Administration Server named admin-server listening on port 7001. A dynamic cluster named cluster-1 of size 5. Two Managed Servers, named managed-server1 and managed-server2, listening on port 8001. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;. No applications deployed. No data sources or JMS resources. A T3 channel.  The domain creation inputs can be customized by editing create-domain-inputs.yaml.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Definition Default     adminPort Port number of the Administration Server inside the Kubernetes cluster. 7001   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. admin-server   clusterName Name of the WebLogic cluster instance to generate for the domain. cluster-1   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. It can also be set to the relative path wdt, and then the built-in WDT scripts will be used instead. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes ConfigMap, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes Pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes Job to run the script (specified in the createDomainScriptName property) in a Kubernetes Pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes Pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes Job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the WebLogic domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /shared/domains/domain1   domainPVMountPath Mount path of the domain persistent volume. /shared   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes Service name. domain1   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   httpAccessLogInLogHome Boolean indicating if server HTTP access log files should be written to the same directory as logHome. Otherwise, server HTTP access log files will be written to the directory specified in the WebLogic domain home configuration. true   image WebLogic Docker image. The operator requires either Oracle WebLogic Server 12.2.1.3.0 with patch 29135930 applied, or Oracle WebLogic Server 12.2.1.4.0, or Oracle WebLogic Server 14.1.1.0.0. The existing WebLogic Docker image, container-registry.oracle.com/middleware/weblogic:12.2.1.3, has all the necessary patches applied. For details on how to obtain or create the image, see WebLogic Docker images. container-registry.oracle.com/middleware/weblogic:12.2.1.3   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never IfNotPresent   imagePullSecretName Name of the Kubernetes Secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified    includeServerOutInPodLog Boolean indicating whether to include the server .out in the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to start initially for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for domain log, server logs, server out, Node Manager log, and server HTTP access log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /shared/logs/domain1   managedServerNameBase Base string used to generate Managed Server names. managed-server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes Namespace in which to create the domain. default   persistentVolumeClaimName Name of the persistent volume claim. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc domain1-weblogic-sample-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Servers will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the network access point. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only, in a single server (all-in-one) Kubernetes Deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster.   weblogicCredentialsSecretName Name of the Kubernetes Secret for the Administration Server user name and password. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. domain1-weblogic-credentials   weblogicImagePullSecretName Name of the Kubernetes Secret for the Docker Store, used to pull the WebLogic Server image. docker-store-secret   serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName, and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes Service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create a WebLogic domain home and associated Kubernetes resources for a domain that only has one cluster. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain YAML file could also be modified to cover more use cases.\nVerify the results The create script will verify that the domain was created, and will report failure if there was any error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nNote that the example results below use the default Kubernetes Namespace. If you are using a different namespace, you need to replace NAMESPACE in the example kubectl commands with the actual Kubernetes Namespace.\nGenerated YAML files with the default inputs The content of the generated domain.yaml:\n# Copyright (c) 2017, 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026quot;weblogic.oracle/v2\u0026quot; kind: Domain metadata: name: domain1 namespace: default labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: # The WebLogic Domain Home domainHome: /shared/domains/domain1 # Set domain home type to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: PersistentVolume # The WebLogic Server Docker image that the operator uses to start the domain image: \u0026quot;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026quot; # imagePullPolicy defaults to \u0026quot;Always\u0026quot; if image version is :latest imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: domain1-weblogic-credentials # Whether to include the server out file into the pod's stdout, default is true includeServerOutInPodLog: true # Whether to enable log home logHomeEnabled: true # The in-pod name location for domain log, server logs, server out, and Node Manager log files logHome: /shared/logs/domain1 # serverStartPolicy legal values are \u0026quot;NEVER\u0026quot;, \u0026quot;IF_NEEDED\u0026quot;, or \u0026quot;ADMIN_ONLY\u0026quot; # This determines which WebLogic Servers the operator will start up when it discovers this Domain # - \u0026quot;NEVER\u0026quot; will not start any server in the domain # - \u0026quot;ADMIN_ONLY\u0026quot; will start up only the administration server (no managed servers will be started) # - \u0026quot;IF_NEEDED\u0026quot; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom \u0026quot; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: domain1-weblogic-sample-pvc volumeMounts: - mountPath: /shared name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026quot;RUNNING\u0026quot; or \u0026quot;ADMIN\u0026quot; # \u0026quot;RUNNING\u0026quot; means the listed server will be started up to \u0026quot;RUNNING\u0026quot; mode # \u0026quot;ADMIN\u0026quot; means the listed server will be start up to \u0026quot;ADMIN\u0026quot; mode serverStartState: \u0026quot;RUNNING\u0026quot; # adminService: # channels: # The Admin Server's NodePort # - channelName: default # nodePort: 30701 # Uncomment to export the T3Channel as a service # - channelName: T3Channel # clusters is used to configure the desired behavior for starting member servers of a cluster. # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: cluster-1 serverStartState: \u0026quot;RUNNING\u0026quot; replicas: 2 # The number of managed servers to start for unlisted clusters # replicas: 1 Verify the domain To confirm that the domain was created, use this command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\nHere is an example of the output of this command:\n$ kubectl describe domain domain1 Name: domain1 Namespace: default Labels: weblogic.domainUID=domain1 weblogic.resourceVersion=domain-v2 Annotations: kubectl.kubernetes.io/last-applied-configuration={\u0026quot;apiVersion\u0026quot;:\u0026quot;weblogic.oracle/v2\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Domain\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;labels\u0026quot;:{\u0026quot;weblogic.domainUID\u0026quot;:\u0026quot;domain1\u0026quot;,\u0026quot;weblogic.resourceVersion\u0026quot;:\u0026quot;do... API Version: weblogic.oracle/v2 Kind: Domain Metadata: Cluster Name: Creation Timestamp: 2019-01-10T14:50:52Z Generation: 1 Resource Version: 3700284 Self Link: /apis/weblogic.oracle/v2/namespaces/default/domains/domain1 UID: 2023ae0a-14e7-11e9-b751-fa163e855ac8 Spec: Admin Server: Server Pod: Annotations: Container Security Context: Env: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Clusters: Cluster Name: cluster-1 Cluster Service: Annotations: Labels: Replicas: 2 Server Pod: Annotations: Container Security Context: Env: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Domain Home: /shared/domains/domain1 Domain Home In Image: false Image: container-registry.oracle.com/middleware/weblogic:12.2.1.4 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Log Home: /shared/logs/domain1 Log Home Enabled: true Managed Servers: Server Pod: Annotations: Container Security Context: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Xms64m -Xmx256m Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Mount Path: /shared Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: domain1-weblogic-sample-pvc Server Service: Annotations: Labels: Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: domain1-weblogic-credentials Status: Conditions: Last Transition Time: 2019-01-10T14:52:33.878Z Reason: ServersReady Status: True Type: Available Servers: Health: Activation Time: 2019-01-10T14:52:07.351Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: admin-server State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-01-10T14:53:30.352Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: managed-server1 State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-01-10T14:53:26.503Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: managed-server2 State: RUNNING Start Time: 2019-01-10T14:50:52.104Z Events: \u0026lt;none\u0026gt; In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued very soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. The operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Use the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE domain1-admin-server 1/1 Running 0 1m domain1-managed-server1 1/1 Running 0 8m domain1-managed-server2 1/1 Running 0 8m Verify the services Use the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command:\n$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE domain1-admin-server ClusterIP 10.96.206.134 \u0026lt;none\u0026gt; 7001/TCP 23m domain1-admin-server-external NodePort 10.107.164.241 \u0026lt;none\u0026gt; 30012:30012/TCP 22m domain1-cluster-cluster-1 ClusterIP 10.109.133.168 \u0026lt;none\u0026gt; 8001/TCP 22m domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 22m domain1-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 22m Delete the generated domain home Sometimes in production, but most likely in testing environments, you might want to remove the domain home that is generated using the create-domain.sh script. Do this by running the generated delete domain job script in the /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt; directory.\n$ kubectl create -f delete-domain-job.yaml Troubleshooting Message: status on iteration 20 of 20 pod domain1-create-weblogic-sample-domain-job-4qwt2 status is Pending The create domain job is not showing status completed after waiting 300 seconds.\nThe most likely cause is related to the value of persistentVolumeClaimName, defined in domain-home-on-pv/create-domain-inputs.yaml.\nTo determine if this is the problem:\n* Execute `kubectl get all --all-namespaces` to find the name of the `create-weblogic-sample-domain-job`. * Execute `kubectl describe pod \u0026lt;name-of-create-weblogic-sample-domain-job\u0026gt;` to see if there is an event that has text similar to `persistentvolumeclaim \u0026quot;domain1-weblogic-sample-pvc\u0026quot; not found`. * Find the name of the PVC that was created by executing [create-pv-pvc.sh](https://github.com/oracle/weblogic-kubernetes-operator/blob/master/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc/create-pv-pvc.sh), using `kubectl describe pvc`. It is likely to be `weblogic-sample-pvc`. * Change the value of `persistentVolumeClaimName` to match the name created when you executed [create-pv-pvc.sh](https://github.com/oracle/weblogic-kubernetes-operator/blob/master/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc/create-pv-pvc.sh). * Rerun the `create-domain.sh` script with the same arguments as you did before. * Verify that the operator is deployed. Use the command:  kubectl get all --all-namespaces Look for lines similar to:\nweblogic-operator1 pod/weblogic-operator- If you do not find something similar in the output, the WebLogic Server Kubernetes Operator might not have been installed completely. Review the operator installation instructions.\nMessage: ERROR: Unable to create folder /shared/domains\nThe most common cause is a poor choice of value for weblogicDomainStoragePath in the input file used when you executed:\ncreate-pv-pvc.sh You should delete the resources for your sample domain, correct the value in that file, and rerun the commands to create the PV/PVC and the credential before you attempt to rerun:\ncreate-domain.sh A correct value for weblogicDomainStoragePath will meet the following requirements:\n Must be the name of a directory. The directory must be world writable.  Optionally, follow these steps to tighten permissions on the named directory after you run the sample the first time:\n Become the root user. ls -nd $value-of-weblogicDomainStoragePath  Note the values of the third and fourth field of the output.   chown $third-field:$fourth-field $value-of-weblogicDomainStoragePath chmod 755 $value-of-weblogicDomainStoragePath Return to your normal user ID.  Message: ERROR: The create domain job will not overwrite an existing domain. The domain folder /shared/domains/domain1 already exists\nYou will see this message if the directory domains/domain1 exists in the directory named as the value of weblogicDomainStoragePath in create-pv-pvc-inputs.yaml. For example, if the value of weblogicDomainStoragePath is /tmp/wls-op-4-k8s, you would need to remove (or move) /tmp/wls-op-4-k8s/domains/domain1.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-operators/using-the-operator/",
	"title": "Use the operator",
	"tags": [],
	"description": "",
	"content": "  Use Helm  Useful Helm operations.\n The REST API  Use the operator\u0026#39;s REST services.\n "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/domain-resource/",
	"title": "Domain resource",
	"tags": [],
	"description": "",
	"content": " This document contains details for the current production release 2.5.0 first, and then for the 3.0.0-rc1 release candidate below.\n Use this document to set up and configure your own domain resource which can be used to configure the operation of your WebLogic domain. The domain resource does not replace the traditional configuration of WebLogic domains found in the domain configuration files, but instead cooperates with those files to describe the Kubernetes artifacts of the corresponding domain. For instance, the WebLogic domain configuration will still specify deployed applications, data sources, and most other details about the domain while the domain resource will specify the number of cluster members currently running or the persistent volumes that will be mounted into the containers running WebLogic Server instances.\nMany of the samples accompanying the operator project include scripts to generate an initial domain resource from a set of simplified inputs; however, the domain resource is the actual source of truth for how the operator will manage each WebLogic domain. You are encouraged to either start with the domain resource YAML files generated by the various samples or create domain resources manually or by using other tools based on the schema referenced here or this documentation.\nSwagger documentation is available here.\nPrerequisites The following prerequisites must be fulfilled before proceeding with the creation of the resource:\n Make sure the WebLogic Kubernetes Operator is running. Create a Kubernetes Namespace for the domain resource unless the intention is to use the default namespace. Create the Kubernetes Secrets containing the username and password of the administrative account in the same Kubernetes Namespace as the domain resource.  YAML files Domain resources are defined using the domain resource YAML files. For each WLS domain you want to create and configure, you should create one domain resource YAML file and apply it. In the example referenced below, the sample script, create-domain.sh, generates a domain resource YAML file that you can use as a basis. Copy the file and override the default settings so that it matches all the WLS domain parameters that define your WLS domain.\nSee the WebLogic sample Domain home on a persistent volume README.\nKubernetes resources After you have written your YAML files, you use them to create your WLS domain artifacts using the kubectl apply -f command.\n$ kubectl apply -f domain-resource.yaml Verify the results To confirm that the domain resource was created, use this command:\n$ kubectl describe domain [domain name] -n [namespace] Domain resource overview The domain resource, like all Kubernetes objects, is described by three sections: metadata, spec, and status.\nThe metadata section names the domain resource and its namespace. The name of the domain resource is the default value for the domain UID which is used by the operator to distinguish domains running in the Kubernetes cluster that may have the same domain name. The domain resource name is required to be unique in the namespace and the domain UID should be unique across the cluster. The domain UID, domain resource name, and domain name (from the WebLogic domain configuration) may all be different.\nThe spec section describes the intended running state of the domain, including intended runtime state of server instances, number of cluster members started, and details about Kubernetes Pod or Service generation, such as resource constraints, scheduling requirements, or volume mounts.\nThe status section is updated by the operator and describes the actual running state of the domain, including WebLogic Server instance runtime states and current health.\nDomain resource spec elements The domain resource spec section contains elements for configuring the domain operation and sub-sections specific to the Administration Server, specific clusters, or specific Managed Servers.\nElements related to domain identification, Docker image, and domain home:\n domainUID: The domain unique identifier. Must be unique across the Kubernetes cluster. Not required. Defaults to the value of metadata.name. image: The WebLogic Docker image. Required when domainHomeInImage is true; otherwise, defaults to container-registry.oracle.com/middleware/weblogic:12.2.1.3. imagePullPolicy: The image pull policy for the WebLogic Docker image. Legal values are Always, Never and IfNotPresent. Defaults to Always if image ends in :latest; IfNotPresent otherwise. imagePullSecrets: A list of image pull secrets for the WebLogic Docker image. domainHome: The folder for the WebLogic domain. Not required. Defaults to /shared/domains/domains/domainUID if domainHomeInImage is false. Defaults to /u01/oracle/user_projects/domains/ if domainHomeInImage is true. domainHomeInImage: True if this domain\u0026rsquo;s home is defined in the Docker image for the domain. Defaults to true.  Elements related to logging:\n includeServerOutInPodLog: If true (the default), the server .out file will be included in the pod\u0026rsquo;s stdout. logHome: The in-pod name of the directory in which to store the domain, Node Manager, server logs, and server .out files. logHomeEnabled: Specifies whether the log home folder is enabled. Not required. Defaults to true if domainHomeInImage is false. Defaults to false if domainHomeInImage is true.  Elements related to security:\n webLogicCredentialsSecret: The name of a pre-created Kubernetes Secret, in the domain resource\u0026rsquo;s namespace, that holds the user name and password needed to boot WebLogic Server under the username and password fields.  Elements related to domain startup and shutdown:\n serverStartPolicy: The strategy for deciding whether to start a server. Legal values are ADMIN_ONLY, NEVER, or IF_NEEDED. serverStartState: The state in which the server is to be started. Use ADMIN if the server should start in the admin state. Defaults to RUNNING. restartVersion: If present, every time this value is updated, the operator will restart the required servers. replicas: The number of Managed Servers to run in any cluster that does not specify a replicas count.  Elements related to overriding WebLogic domain configuration:\n configOverrides: The name of the ConfigMap for optional WebLogic configuration overrides. configOverrideSecrets: A list of names of the secrets for optional WebLogic configuration overrides.  Elements related to Kubernetes Pod and Service generation:\n serverPod: Configuration affecting server pods for WebLogic Server instances. Most entries specify standard Kubernetes content for pods that you may want the operator to include in pods generated for WebLogic Server instances, such as labels, annotations, volumes, or scheduling constraints, including anti-affinity. serverService: Customization affecting ClusterIP Kubernetes services for WebLogic Server instances.  Sub-sections related to the Administration Server, specific clusters, or specific Managed Servers:\n adminServer: Configuration for the Administration Server. clusters: Configuration for specific clusters. managedServers: Configuration for specific Managed Servers.  The elements serverStartPolicy, serverStartState, serverPod and serverService are repeated under adminServer and under each entry of clusters or managedServers. The values directly under spec set the defaults for the entire domain. The values under a specific entry under clusters set the defaults for cluster members of that cluster. The values under adminServer or an entry under managedServers set the values for that specific server. Values from the domain scope and values from the cluster (for cluster members) are merged with or overridden by the setting for the specific server depending on the element. See the startup and shutdown documentation for details about serverStartPolicy combination.\nJVM memory and Java option environment variables You can use the following environment variables to specify JVM memory and JVM option arguments to WebLogic Server Managed Server and Node Manager instances:\n JAVA_OPTIONS : Java options for starting WebLogic Server. USER_MEM_ARGS : JVM memory arguments for starting WebLogic Server. NODEMGR_JAVA_OPTIONS : Java options for starting Node Manager instance. NODEMGR_MEM_ARGS : JVM memory arguments for starting Node Manager instance.  Note: The USER_MEM_ARGS environment variable defaults to -Djava.security.egd=file:/dev/./urandom in all WebLogic Server pods and the WebLogic introspection job. It can be explicitly set to another value in your domain resource YAML file using the env attribute under the serverPod configuration.\nThe following behavior occurs depending on whether or not NODEMGR_JAVA_OPTIONS and NODEMGR_MEM_ARGS are defined:\n If NODEMGR_JAVA_OPTIONS is not defined and JAVA_OPTIONS is defined, then the JAVA_OPTIONS value will be applied to the Node Manager instance. If NODEMGR_MEM_ARGS is not defined, then default memory and Java security property values (-Xms64m -Xmx100m -Djava.security.egd=file:/dev/./urandom) will be applied to the Node Manager instance. It can be explicitly set to another value in your domain resource YAML file using the env attribute under the serverPod configuration.  Note: Defining -Djava.security.egd=file:/dev/./urandom in the NODEMGR_MEM_ARGS environment variable helps to speed up the Node Manager startup on systems with low entropy.\nThis example snippet illustrates how to add the above environment variables using the env attribute under the serverPod configuration in your domain resource YAML file.\n# Copyright 2017, 2019, Oracle Corporation and/or its affiliates. All rights reserved. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. # apiVersion: \u0026quot;weblogic.oracle/v6\u0026quot; kind: Domain metadata: name: domain1 namespace: domains23 labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false \u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom \u0026quot; - name: NODEMGR_JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false \u0026quot; - name: NODEMGR_MEM_ARGS value: \u0026quot;-Xms64m -Xmx100m -Djava.security.egd=file:/dev/./urandom \u0026quot; Pod generation The operator creates a pod for each running WebLogic Server instance. This pod will have a container based on the Docker image specified by the image field. Additional pod or container content can be specified using the elements under serverPod. This includes Kubernetes sidecar and init containers, labels, annotations, volumes, volume mounts, scheduling constraints, including anti-affinity, resource requirements, or security context.\nPrior to creating a pod, the operator replaces variable references allowing the pod content to be templates. The format of these variable references is $(VARIABLE_NAME) where VARIABLE_NAME is one of the variable names available in the container for the WebLogic Server instance. The default set of environment variables includes:\n DOMAIN_NAME: The WebLogic domain name DOMAIN_UID: The domain unique identifier DOMAIN_HOME: The domain home location as a file system path within the container SERVER_NAME: The WebLogic Server name CLUSTER_NAME: The WebLogic cluster name, if this is a cluster member LOG_HOME: The WebLogic log location as a file system path within the container  This example domain YAML file specifies that pods for WebLogic Server instances in the cluster-1 cluster will have a per-managed server volume and volume mount (similar to a Kubernetes StatefulSet), an init container to initialize some files in that volume, and anti-affinity scheduling so that the server instances are scheduled as much as possible on different nodes:\n# Copyright 2017, 2019, Oracle Corporation and/or its affiliates. All rights reserved. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. # apiVersion: \u0026quot;weblogic.oracle/v6\u0026quot; kind: Domain metadata: name: domain1 namespace: domains23 labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: domainHome: /u01/oracle/user_projects/domains/domain1 domainHomeInImage: true image: \u0026quot;phx.ocir.io/weblogick8s/my-domain-home-in-image:12.2.1.3\u0026quot; imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; imagePullSecrets: - name: ocirsecret webLogicCredentialsSecret: name: domain1-weblogic-credentials includeServerOutInPodLog: true serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; serverPod: env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom \u0026quot; adminServer: serverStartState: \u0026quot;RUNNING\u0026quot; clusters: - clusterName: cluster-1 serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - cluster-1 topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; volumes: - name: $(SERVER_NAME)-volume emptyDir: {} volumeMounts: - mountPath: /server-volume name: $(SERVER_NAME)-volume initContainers: - name: volumeinit image: \u0026quot;oraclelinux:7-slim\u0026quot; imagePullPolicy: IfNotPresent command: [\u0026quot;/usr/bin/sh\u0026quot;] args: [\u0026quot;echo\u0026quot;, \u0026quot;Replace with command to initialize files in /init-volume\u0026quot;] volumeMounts: - mountPath: /init-volume name: $(SERVER_NAME)-volume replicas: 2 The operator uses an \u0026ldquo;introspection\u0026rdquo; job to discover details about the WebLogic domain configuration, such as the list of clusters and network access points. The job pod for the introspector is generated using the serverPod entries for the administration server. Because the administration server name is not known until the introspection step is complete, the value of the $(SERVER_NAME) variable for the introspection job will be \u0026ldquo;introspector\u0026rdquo;.\nThis document contains details for the current production release 2.5.0 above, and then for the 3.0.0-rc1 release candidate below.\n Details for 3.0.0-rc1 Use this document to set up and configure your own domain resource which can be used to configure the operation of your WebLogic domain. The domain resource does not replace the traditional configuration of WebLogic domains found in the domain configuration files, but instead cooperates with those files to describe the Kubernetes artifacts of the corresponding domain. For instance, the WebLogic domain configuration will still specify deployed applications, data sources, and most other details about the domain while the domain resource will specify the number of cluster members currently running or the persistent volumes that will be mounted into the containers running WebLogic Server instances.\nMany of the samples accompanying the operator project include scripts to generate an initial domain resource from a set of simplified inputs; however, the domain resource is the actual source of truth for how the operator will manage each WebLogic domain. You are encouraged to either start with the domain resource YAML files generated by the various samples or create domain resources manually or by using other tools based on the schema referenced here or this documentation.\nSwagger documentation is available here.\nPrerequisites The following prerequisites must be fulfilled before proceeding with the creation of the resource:\n Make sure the WebLogic Server Kubernetes Operator is running. Create a Kubernetes Namespace for the domain resource unless the intention is to use the default namespace. Create the Kubernetes Secrets containing the username and password of the administrative account in the same Kubernetes Namespace as the domain resource.  YAML files Domain resources are defined using the domain resource YAML files. For each WLS domain you want to create and configure, you should create one domain resource YAML file and apply it. In the example referenced below, the sample scripts generate a domain resource YAML file that you can use as a basis. Copy the file and override the default settings so that it matches all the WLS domain parameters that define your WLS domain.\nSee the WebLogic samples, Domain home on a PV, Domain home in Image, and Model in Image.\nKubernetes resources After you have written your YAML files, you use them to create your WLS domain artifacts using the kubectl apply -f command.\n$ kubectl apply -f domain-resource.yaml Verify the results To confirm that the domain resource was created, use this command:\n$ kubectl describe domain [domain name] -n [namespace] Domain resource overview The domain resource, like all Kubernetes objects, is described by three sections: metadata, spec, and status.\nThe metadata section names the domain resource and its namespace. The name of the domain resource is the default value for the domain UID which is used by the operator to distinguish domains running in the Kubernetes cluster that may have the same domain name. The domain resource name is required to be unique in the namespace and the domain UID should be unique across the cluster. The domain UID, domain resource name, and domain name (from the WebLogic domain configuration) may all be different.\nThe spec section describes the intended running state of the domain, including intended runtime state of server instances, number of cluster members started, and details about Kubernetes Pod or Service generation, such as resource constraints, scheduling requirements, or volume mounts.\nThe status section is updated by the operator and describes the actual running state of the domain, including WebLogic Server instance runtime states and current health.\nDomain resource spec elements The domain resource spec section contains elements for configuring the domain operation and sub-sections specific to the Administration Server, specific clusters, or specific Managed Servers.\nElements related to domain identification, Docker image, and domain home:\n domainUID: The domain unique identifier. Must be unique across the Kubernetes cluster. Not required. Defaults to the value of metadata.name. image: The WebLogic Docker image. Required when domainHomeSourceType is Image; otherwise, defaults to container-registry.oracle.com/middleware/weblogic:12.2.1.4. imagePullPolicy: The image pull policy for the WebLogic Docker image. Legal values are Always, Never, and IfNotPresent. Defaults to Always if image ends in :latest; IfNotPresent otherwise. imagePullSecrets: A list of image pull secrets for the WebLogic Docker image. domainHome: The folder for the WebLogic domain. Not required. Defaults to /shared/domains/domains/domainUID if domainHomeSourceType is PersistentVolume. Defaults to /u01/oracle/user_projects/domains/ if domainHomeSourceType is Image. Defaults to /u01/domains/domainUID if domainHomeSourceType is FromModel. domainHomeSourceType: The source for the domain home. Legal values are Image (for Domain in Image), PersistentVolume (for Domain in PV), and FromModel (for Model in Image). Defaults to Image.  Elements related to logging:\n includeServerOutInPodLog: If true (the default), the server .out file will be included in the pod\u0026rsquo;s stdout. logHome: The in-pod name of the directory in which to store the domain, Node Manager, server logs, and server .out files. Defaults to /shared/logs/\u0026lt;domainUID\u0026gt;. Ignored if logHomeEnabled is false. logHomeEnabled: Specifies whether the log home folder is enabled. Not required. Defaults to true if domainHomeSourceType is PersistentVolume. Defaults to false if domainHomeSourceType is Image or FromModel.  Elements related to security:\n webLogicCredentialsSecret: The name of a pre-created Kubernetes Secret, in the domain resource\u0026rsquo;s namespace, that holds the user name and password needed to boot WebLogic Server under the username and password fields. See also elements under configuration below.  Elements related to domain startup and shutdown:\n serverStartPolicy: The strategy for deciding whether to start a server. Legal values are ADMIN_ONLY, NEVER, or IF_NEEDED. serverStartState: The state in which the server is to be started. Use ADMIN if the server should start in the admin state. Defaults to RUNNING. restartVersion: If present, every time this value is updated, the operator will restart the required servers. replicas: The number of Managed Servers to run in any cluster that does not specify a replicas count.  Elements related to specifying and overriding WebLogic domain configuration:\n  These elements are under configuration.\n overridesConfigMap: The name of the ConfigMap for optional Configuration overrides. The value only applies if the domainHomeSourceType is Image or PersistentVolume. Do not set this value if the domainHomeSourceType is FromModel. secrets: A list of secret names for optional Configuration overrides macros or Model in Image Model files macros. Often used for specifying data source URLs, user names, and passwords. introspectorJobActiveDeadlineSeconds: Time in seconds before timing out the introspector job. Default is 120 seconds.    These elements are under configuration.model, only apply if the domainHomeSourceType is FromModel, and are discussed in Model in Image.\n configMap: Optional configuration map for supplying runtime model file updates to Model in Image model configuration. domainType: Must be one of WLS, JRF, or RestrictedJRF. Default is WLS. runtimeEncryptionSecret: Required. Expected field is password. This is used by Model in Image to encrypt data while the data is passed from the introspector job to WebLogic pods. The password can be arbitrary: the only requirement is that it must stay the same for the life of a domain resource. If a domain resource is deleted then redeployed, it\u0026rsquo;s fine to change the password during the interim. wdtEncryptionSecret: Optional. Rarely used. See Model in Image for details.    These elements are under configuration.opss, and only apply if the domainHomeSourceType is FromModel and the domainType is JRF.\n walletPasswordSecret: The expected secret field is walletPassword. Used to encrypt/decrypt the wallet that\u0026rsquo;s used for accessing the domain\u0026rsquo;s entries in its RCU database. walletFileSecret: Optional. The expected secret field is walletFile. Use this to allow a JRF domain to reuse its entries in the RCU database (specify a wallet file that was obtained from the domain home while the domain was booted for the first time).    Elements related to Kubernetes Pod and Service generation:\n serverPod: Configuration affecting server pods for WebLogic Server instances. Most entries specify standard Kubernetes content for pods that you may want the operator to include in pods generated for WebLogic Server instances, such as labels, annotations, volumes, or scheduling constraints, including anti-affinity. serverService: Customization affecting ClusterIP Kubernetes services for WebLogic Server instances.  Sub-sections related to the Administration Server, specific clusters, or specific Managed Servers:\n adminServer: Configuration for the Administration Server. clusters: Configuration for specific clusters. managedServers: Configuration for specific Managed Servers.  The elements serverStartPolicy, serverStartState, serverPod and serverService are repeated under adminServer and under each entry of clusters or managedServers. The values directly under spec, set the defaults for the entire domain. The values under a specific entry under clusters, set the defaults for cluster members of that cluster. The values under adminServer or an entry under managedServers, set the values for that specific server. Values from the domain scope and values from the cluster (for cluster members) are merged with or overridden by the setting for the specific server depending on the element. See Startup and shutdown for details about serverStartPolicy combinations.\nJVM memory and Java option environment variables You can use the following environment variables to specify JVM memory and JVM option arguments to WebLogic Server Managed Server and Node Manager instances:\n JAVA_OPTIONS : Java options for starting WebLogic Server. USER_MEM_ARGS : JVM memory arguments for starting WebLogic Server. NODEMGR_JAVA_OPTIONS : Java options for starting a Node Manager instance. NODEMGR_MEM_ARGS : JVM memory arguments for starting a Node Manager instance.  Note: The USER_MEM_ARGS environment variable defaults to -Djava.security.egd=file:/dev/./urandom in all WebLogic Server pods and the WebLogic introspection job. It can be explicitly set to another value in your domain resource YAML file using the env attribute under the serverPod configuration.\nThe following behavior occurs depending on whether or not NODEMGR_JAVA_OPTIONS and NODEMGR_MEM_ARGS are defined:\n If NODEMGR_JAVA_OPTIONS is not defined and JAVA_OPTIONS is defined, then the JAVA_OPTIONS value will be applied to the Node Manager instance. If NODEMGR_MEM_ARGS is not defined, then default memory and Java security property values (-Xms64m -Xmx100m -Djava.security.egd=file:/dev/./urandom) will be applied to the Node Manager instance. It can be explicitly set to another value in your domain resource YAML file using the env attribute under the serverPod configuration.  Note: Defining -Djava.security.egd=file:/dev/./urandom in the NODEMGR_MEM_ARGS environment variable helps to speed up the Node Manager startup on systems with low entropy.\nThis example snippet illustrates how to add the above environment variables using the env attribute under the serverPod configuration in your domain resource YAML file.\n# Copyright (c) 2017, 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # apiVersion: \u0026quot;weblogic.oracle/v7\u0026quot; kind: Domain metadata: name: domain1 namespace: domains23 labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false \u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom \u0026quot; - name: NODEMGR_JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false \u0026quot; - name: NODEMGR_MEM_ARGS value: \u0026quot;-Xms64m -Xmx100m -Djava.security.egd=file:/dev/./urandom \u0026quot; Pod generation The operator creates a pod for each running WebLogic Server instance. This pod will have a container based on the Docker image specified by the image field. Additional pod or container content can be specified using the elements under serverPod. This includes Kubernetes sidecar and init containers, labels, annotations, volumes, volume mounts, scheduling constraints, including anti-affinity, resource requirements, or security context.\nPrior to creating a pod, the operator replaces variable references allowing the pod content to be templates. The format of these variable references is $(VARIABLE_NAME) where VARIABLE_NAME is one of the variable names available in the container for the WebLogic Server instance. The default set of environment variables includes:\n DOMAIN_NAME: The WebLogic domain name. DOMAIN_UID: The domain unique identifier. DOMAIN_HOME: The domain home location as a file system path within the container. SERVER_NAME: The WebLogic Server name. CLUSTER_NAME: The WebLogic cluster name, if this is a cluster member. LOG_HOME: The WebLogic log location as a file system path within the container.  This example domain YAML file specifies that pods for WebLogic Server instances in the cluster-1 cluster will have a per-Managed Server volume and volume mount (similar to a Kubernetes StatefulSet), an init container to initialize some files in that volume, and anti-affinity scheduling so that the server instances are scheduled as much as possible on different nodes:\n# Copyright (c) 2019, 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # apiVersion: \u0026quot;weblogic.oracle/v7\u0026quot; kind: Domain metadata: name: domain1 namespace: domains23 labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: domainHome: /u01/oracle/user_projects/domains/domain1 domainHomeSourceType: Image image: \u0026quot;phx.ocir.io/weblogick8s/my-domain-home-in-image:12.2.1.4\u0026quot; imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; imagePullSecrets: - name: ocirsecret webLogicCredentialsSecret: name: domain1-weblogic-credentials includeServerOutInPodLog: true serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; serverPod: env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom \u0026quot; adminServer: serverStartState: \u0026quot;RUNNING\u0026quot; clusters: - clusterName: cluster-1 serverStartState: \u0026quot;RUNNING\u0026quot; serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: \u0026quot;weblogic.clusterName\u0026quot; operator: In values: - cluster-1 topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; volumes: - name: $(SERVER_NAME)-volume emptyDir: {} volumeMounts: - mountPath: /server-volume name: $(SERVER_NAME)-volume initContainers: - name: volumeinit image: \u0026quot;oraclelinux:7-slim\u0026quot; imagePullPolicy: IfNotPresent command: [\u0026quot;/usr/bin/sh\u0026quot;] args: [\u0026quot;echo\u0026quot;, \u0026quot;Replace with command to initialize files in /init-volume\u0026quot;] volumeMounts: - mountPath: /init-volume name: $(SERVER_NAME)-volume replicas: 2 The operator uses an \u0026ldquo;introspection\u0026rdquo; job to discover details about the WebLogic domain configuration, such as the list of clusters and network access points. The job pod for the introspector is generated using the serverPod entries for the Administration Server. Because the Administration Server name is not known until the introspection step is complete, the value of the $(SERVER_NAME) variable for the introspection job will be \u0026ldquo;introspector\u0026rdquo;.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "Learn about the operator&#39;s design, architecture, important terms, prerequisites, and supported environments.",
	"content": "  Get started  Review the operator prerequisites and supported environments.\n Architecture  The operator consists of several parts: the operator runtime, the model for a Kubernetes custom resource definition (CRD), a Helm chart for installing the operator, a variety of sample shell scripts for preparing or packaging WebLogic domains for running in Kubernetes, and sample Helm charts or shell scripts for conditionally exposing WebLogic endpoints outside the Kubernetes cluster.\n Design philosophy  The Oracle WebLogic Server Kubernetes Operator (the “operator”) is designed to fulfill a similar role to that which a human operator would fill in a traditional data center deployment. It contains a set of useful built-in knowledge about how to perform various life cycle operations on a domain correctly.\n This guide provides detailed user information for the Oracle WebLogic Server Kubernetes Operator. It provides instructions on how to install the operator in your Kubernetes cluster and how to use it to manage WebLogic domains. If you are looking for information about how the operator is designed, implemented, built, and such, then you should refer to the Developer guide.\nImportant terms This documentation uses several important terms which are intended to have a specific meaning.\n   Term Definition     Cluster Because this term is ambiguous, it will be prefixed to indicate which type of cluster is meant. A WebLogic cluster is a group of Managed Servers that together host some application or component and which are able to share load and state between them. A Kubernetes cluster is a group of machines (“Nodes”) that all host Kubernetes resources, like Pods and Services, and which appear to the external user as a single entity. If the term “cluster” is not prefixed, it should be assumed to mean a Kubernetes cluster.   Domain A WebLogic domain is a group of related applications and resources along with the configuration information necessary to run them.   Ingress A Kubernetes Ingress provides access to applications and services in a Kubernetes environment to external clients. An Ingress may also provide additional features like load balancing.   Namespace A Kubernetes Namespace is a named entity that can be used to group together related objects, for example, Pods and Services.   Operator A Kubernetes operator is software that performs management of complex applications.   Pod A Kubernetes Pod contains one or more containers and is the object that provides the execution environment for an instance of an application component, such as a web server or database.   Job A Kubernetes Job is a type of controller that creates one or more Pods that run to completion to complete a specific task.   Secret A Kubernetes Secret is a named object that can store secret information like user names, passwords, X.509 certificates, or any other arbitrary data.   Service A Kubernetes Service exposes application endpoints inside a Pod to other Pods, or outside the Kubernetes cluster. A Service may also provide additional features like load balancing.    Additional reading Before using the operator, you might want to read the design philosophy to develop an understanding of the operator\u0026rsquo;s design, and the architectural overview to understand its architecture, including how WebLogic domains are deployed in Kubernetes using the operator. Also worth reading are the details of the Kubernetes RBAC definitions required by the operator.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/overview/",
	"title": "Overview",
	"tags": [],
	"description": "",
	"content": "The information in the following sections is organized in the order that you would most likely need to use it. If you want to set up an operator and use it to create and manage WebLogic domains, then you should follow the User Guide sections from top to bottom, and the necessary information will be presented in the correct order.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/overview/k8s-setup/",
	"title": "Set up Kubernetes",
	"tags": [],
	"description": "",
	"content": "Cheat sheet for setting up Kubernetes If you need some help setting up a Kubernetes environment to experiment with the operator, please read on! The supported environments are either an on-premises installation of Kubernetes, for example, on bare metal, or on a cloud provider like Oracle Cloud, Google, or Amazon. Cloud providers allow you to provision a managed Kubernetes environment from their management consoles. You could also set up Kubernetes manually using compute resources on a cloud. There are also a number of ways to run a Kubernetes single-node cluster that are suitable for development or testing purposes. Your options include:\n\u0026ldquo;Production\u0026rdquo; options:\n Set up your own Kubernetes environment on bare compute resources on a cloud. Use your cloud provider\u0026rsquo;s management console to provision a managed Kubernetes environment. Install Kubernetes on your own compute resources (for example, \u0026ldquo;real\u0026rdquo; computers, outside a cloud).  \u0026ldquo;Development/test\u0026rdquo; options:\n Install Docker for Mac and enable its embedded Kubernetes cluster (or register for the Docker for Windows beta and wait until Kubernetes is available there). We do not recommend or support other development/test options like Minikube, Minishift, kind, and so on.  We have provided our hints and tips for several of these options in the sections below.\nSet up Kubernetes on bare compute resources in a cloud Follow the basic steps from the Terraform Kubernetes installer for Oracle Cloud Infrastructure.\nPrerequisites  Download and install Terraform (v0.10.3 or later). Download and install the OCI Terraform Provider (v2.0.0 or later). Create an Terraform configuration file at ~/.terraformrc that specifies the path to the OCI provider: providers { oci = \u0026quot;\u0026lt;path_to_provider_binary\u0026gt;/terraform-provider-oci\u0026quot; }  Ensure that you have kubectl installed if you plan to interact with the cluster locally.  Quick Start   Do a git clone of the Terraform Kubernetes installer project:\ngit clone https://github.com/oracle/terraform-kubernetes-installer.git   Initialize your project:\ncd terraform-kubernetes-installer terraform init   Copy the example terraform.tvfars:\ncp terraform.example.tfvars terraform.tfvars   Edit the terraform.tvfars file to include values for your tenancy, user, and compartment. Optionally, edit the variables to change the Shape of the VMs for your Kubernetes master and workers, and your etcd cluster. For example:\n#give a label to your cluster to help identify it if you have multiple label_prefix=\u0026quot;weblogic-operator-1-\u0026quot; #identification/authorization info tenancy_ocid = \u0026quot;ocid1.tenancy....\u0026quot; compartment_ocid = \u0026quot;ocid1.compartment....\u0026quot; fingerprint = \u0026quot;...\u0026quot; private_key_path = \u0026quot;/Users/username/.oci/oci_api_key.pem\u0026quot; user_ocid = \u0026quot;ocid1.user...\u0026quot; #shapes for your VMs etcdShape = \u0026quot;VM.Standard1.2\u0026quot; k8sMasterShape = \u0026quot;VM.Standard1.8\u0026quot; k8sWorkerShape = \u0026quot;VM.Standard1.8\u0026quot; k8sMasterAd1Count = \u0026quot;1\u0026quot; k8sWorkerAd1Count = \u0026quot;2\u0026quot; #this ingress is set to wide-open for testing **not secure** etcd_ssh_ingress = \u0026quot;0.0.0.0/0\u0026quot; master_ssh_ingress = \u0026quot;0.0.0.0/0\u0026quot; worker_ssh_ingress = \u0026quot;0.0.0.0/0\u0026quot; master_https_ingress = \u0026quot;0.0.0.0/0\u0026quot; worker_nodeport_ingress = \u0026quot;0.0.0.0/0\u0026quot; #create iscsi volumes to store your etcd and /var/lib/docker info worker_iscsi_volume_create = true worker_iscsi_volume_size = 100 etcd_iscsi_volume_create = true etcd_iscsi_volume_size = 50   Test and apply your changes:\nterraform plan terraform apply   Test your cluster using the built-in script scripts/cluster-check.sh:\nscripts/cluster-check.sh   Output the SSH private key:\n# output the ssh private key for use later $ rm -f generated/instances_id_rsa \u0026amp;\u0026amp; terraform output ssh_private_key \u0026gt; generated/instances_id_rsa \u0026amp;\u0026amp; chmod 600 generated/instances_id_rsa   If you need shared storage between your Kubernetes worker nodes, enable and configure NFS:\n  In the current GA version, the OCI Container Engine for Kubernetes supports network block storage that can be shared across nodes with access permission RWOnce (meaning that only one can write, others can read only). If you choose to place your domain in a persistent volume, you must use a shared file system to store the WebLogic domain configuration, which MUST be accessible from all the pods across the nodes. Oracle recommends that you use the Oracle Cloud Infrastructure File Storage Service (or equivalent on other cloud providers). Alternatively, you may install an NFS server on one node and share the file system across all the nodes.\nCurrently, we recommend that you use NFS version 3.0 for running WebLogic Server on OCI Container Engine for Kubernetes. During certification, we found that when using NFS 4.0, the servers in the WebLogic domain went into a failed state intermittently. Because multiple threads use NFS (default store, diagnostics store, Node Manager, logging, and domain_home), there are issues when accessing the file store. These issues are removed by changing the NFS to version 3.0.\n $ terraform output worker_public_ips IP1, IP2 $ terraform output worker_private_ips PRIVATE_IP1, PRIVATE_IP2 $ ssh -i `pwd`/generated/instances_id_rsa opc@IP1 worker-1$ sudo su - worker-1# yum install -y nfs-utils worker-1# mkdir /scratch worker-1# echo \u0026quot;/scratch PRIVATE_IP2(rw)\u0026quot; \u0026gt;\u0026gt; /etc/exports worker-1# systemctl restart nfs worker-1# exit worker-1$ exit # configure worker-2 to mount the share from worker-1 $ ssh -i `pwd`/generated/instances_id_rsa opc@IP2 worker-2$ sudo su - worker-2# yum install -y nfs-utils worker-2# mkdir /scratch worker-2# echo \u0026quot;PRIVATE_IP1:/scratch /scratch nfs nfsvers=3 0 0\u0026quot; \u0026gt;\u0026gt; /etc/fstab worker-2# mount /scratch worker-2# exit worker-2$ exit $ Install Kubernetes on your own compute resources (for example, Oracle Linux servers outside a cloud) These instructions are for Oracle Linux 7u2+. If you are using a different flavor of Linux, you will need to adjust them accordingly.\nThese steps must be run with the root user, until specified otherwise! Any time you see YOUR_USERID in a command, you should replace it with your actual userid.\n   Choose the directories where your Docker and Kubernetes files will be stored. The Docker directory should be on a disk with a lot of free space (more than 100GB) because it will be used for the /var/lib/docker file system, which contains all of your images and containers. The Kubernetes directory will be used for the /var/lib/kubelet file system and persistent volume storage.\nexport docker_dir=/scratch/docker export k8s_dir=/scratch/k8s_dir   Create a shell script that sets up the necessary environment variables. You should probably just append this to the user\u0026rsquo;s .bashrc so that it will get executed at login. You will also need to configure your proxy settings here if you are behind an HTTP proxy:\nexport PATH=$PATH:/sbin:/usr/sbin pod_network_cidr=\u0026quot;10.244.0.0/16\u0026quot; k8s_dir=$k8s_dir ## grab my IP address to pass into kubeadm init, and to add to no_proxy vars # assume ipv4 and eth0 ip_addr=`ip -f inet addr show eth0 | egrep inet | awk '{print $2}' | awk -F/ '{print $1}'\\` export HTTPS_PROXY=http://proxy:80 export https_proxy=http://proxy:80 export NO_PROXY=localhost,127.0.0.1,.my.domain.com,/var/run/docker.sock,$pod_network_cidr,$ip_addr export no_proxy=localhost,127.0.0.1,.my.domain.com,/var/run/docker.sock,$pod_network_cidr,$ip_addr export HTTP_PROXY=http://proxy:80 export http_proxy=http://proxy:80 export KUBECONFIG=$k8s_dir/admin.conf Source that script to set up your environment variables:\n. ~/.bashrc If you want command completion, you can add the following to the script:\n[ -f /usr/share/bash-completion/bash_completion ] \u0026amp;\u0026amp; . /usr/share/bash-completion/bash_completion source \u0026lt;(kubectl completion bash)   Create the directories you need:\nmkdir -p $docker_dir $k8s_dir/kubelet ln -s $k8s_dir/kubelet /var/lib/kubelet   Set an environment variable with the Docker version you want to install:\ndocker_version=\u0026quot;17.03.1.ce\u0026quot;   Install Docker, removing any previously installed version:\n### install docker and curl-devel (for git if needed) yum-config-manager --enable ol7_addons ol7_latest # we are going to just uninstall any docker-engine that is installed yum -y erase docker-engine docker-engine-selinux # now install the docker-engine at our specified version yum -y install docker-engine-$docker_version curl-devel   Update the Docker options:\n# edit /etc/sysconfig/docker to add custom OPTIONS cat /etc/sysconfig/docker | sed \u0026quot;s#^OPTIONS=.*#OPTIONS='--selinux-enabled --group=docker -g $docker_dir'#g\u0026quot; \u0026gt; /tmp/docker.out diff /etc/sysconfig/docker /tmp/docker.out mv /tmp/docker.out /etc/sysconfig/docker   Set up the Docker network, including the HTTP proxy configuration, if you need it:\n# generate a custom /setc/sysconfig/docker-network cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysconfig/docker-network # /etc/sysconfig/docker-network DOCKER_NETWORK_OPTIONS=\u0026quot;-H tcp://0.0.0.0:4243 -H unix:///var/run/docker.sock\u0026quot; HTTP_PROXY=\u0026quot;http://proxy:80\u0026quot; HTTPS_PROXY=\u0026quot;http://proxy:80\u0026quot; NO_PROXY=\u0026quot;localhost,127.0.0.0/8,.my.domain.com,/var/run/docker.sock\u0026quot; EOF   Add your user to the docker group:\nusermod -aG docker YOUR_USERID   Enable and start the Docker service that you just installed and configured:\nsystemctl enable docker \u0026amp;\u0026amp; systemctl start docker   Install the Kubernetes packages:\n# generate the yum repo config cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF setenforce 0 # install kube* packages v=${1:-1.8.4-0} old_ver=`echo $v | egrep \u0026quot;^1.7\u0026quot;` yum install -y kubelet-$v kubeadm-$v kubectl-$v kubernetes-cni # change the cgroup-driver to match what docker is using cgroup=`docker info 2\u0026gt;\u0026amp;1 | egrep Cgroup | awk '{print $NF}'` [ \u0026quot;$cgroup\u0026quot; == \u0026quot;\u0026quot; ] \u0026amp;\u0026amp; echo \u0026quot;cgroup not detected!\u0026quot; \u0026amp;\u0026amp; exit 1 cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf | sed \u0026quot;s#KUBELET_CGROUP_ARGS=--cgroup-driver=.*#KUBELET_CGROUP_ARGS=--cgroup-driver=$cgroup\\\u0026quot;#\u0026quot;\u0026gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.out diff /etc/systemd/system/kubelet.service.d/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.out mv /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.out /etc/systemd/system/kubelet.service.d/10-kubeadm.conf if [ \u0026quot;$old_ver\u0026quot; = \u0026quot;\u0026quot; ] ; then # run with swap if not in version 1.7* (starting in 1.8, kubelet # fails to start with swap enabled) # cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/kubelet.service.d/90-local-extras.conf [Service] Environment=\u0026quot;KUBELET_EXTRA_ARGS=--fail-swap-on=false\u0026quot; EOF fi   Enable and start the Kubernetes Service:\nsystemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet   Install and use Flannel for CNI:\n# run kubeadm init as root echo Running kubeadm init --skip-preflight-checks --apiserver-advertise-address=$ip_addr --pod-network-cidr=$pod_network_cidr echo \u0026quot; see /tmp/kubeadm-init.out for output\u0026quot; kubeadm init --skip-preflight-checks --apiserver-advertise-address=$ip_addr --pod-network-cidr=$pod_network_cidr \u0026gt; /tmp/kubeadm-init.out 2\u0026gt;\u0026amp;1 if [ $? -ne 0 ] ; then echo \u0026quot;ERROR: kubeadm init returned non 0\u0026quot; chmod a+r /tmp/kubeadm-init.out exit 1 else echo; echo \u0026quot;kubeadm init complete\u0026quot; ; echo # tail the log to get the \u0026quot;join\u0026quot; token tail -6 /tmp/kubeadm-init.out fi cp /etc/kubernetes/admin.conf $KUBECONFIG chown YOUR_USERID:YOUR_GROUP $KUBECONFIG chmod 644 $KUBECONFIG  The following steps should be run with your normal (non-root) user.\n   Configure CNI:\nsudo -u YOUR_USERID kubectl create clusterrolebinding permissive-binding --clusterrole=cluster-admin --user=admin --user=kubelet --group=system:serviceaccounts sudo -u YOUR_USERID kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml Wait for kubectl get nodes to show Ready for this host:\nhost=`hostname | awk -F. '{print $1}'` status=\u0026quot;NotReady\u0026quot; max=10 count=1 while [ ${status:=Error} != \u0026quot;Ready\u0026quot; -a $count -lt $max ] ; do sleep 30 status=`sudo -u YOUR_USERID kubectl get nodes | egrep $host | awk '{print $2}'` echo \u0026quot;kubectl status is ${status:=Error}, iteration $count of $max\u0026quot; count=`expr $count + 1` done status=`sudo -u YOUR_USERID kubectl get nodes | egrep $host | awk '{print $2}'` if [ ${status:=Error} != \u0026quot;Ready\u0026quot; ] ; then echo \u0026quot;ERROR: kubectl get nodes reports status=${status:=Error} after configuration, exiting!\u0026quot; exit 1 fi   Taint the nodes:\nsudo -u YOUR_USERID kubectl taint nodes --all node-role.kubernetes.io/master- sudo -u YOUR_USERID kubectl get nodes sudo -u YOUR_USERID kubeadm version Congratulations! Docker and Kubernetes are installed and configured!\n  Install Docker for Mac with Kubernetes Docker for Mac 18+ provides an embedded Kubernetes environment that is a quick and easy way to get a simple test environment set up on your Mac. To set it up, follow these instructions:\n  Install \u0026ldquo;Docker for Mac\u0026rdquo; https://download.docker.com/mac/edge/Docker.dmg. Then start up the Docker application (press Command-Space bar, type in Docker and run it). After it is running you will see the Docker icon appear in your status bar:\n  Click the Docker icon and select \u0026ldquo;Preferences\u0026hellip;\u0026rdquo; from the drop down menu. Go to the \u0026ldquo;Advanced\u0026rdquo; tab and give Docker a bit more memory if you have enough to spare:\n  Go to the \u0026ldquo;Kubernetes\u0026rdquo; tab and click on the option to enable Kubernetes:\nIf you are behind an HTTP proxy, then you should also go to the \u0026ldquo;Proxies\u0026rdquo; tab and enter your proxy details.\n Docker will download the Kubernetes components and start them up for you. When it is done, you will see the Kubernetes status go to green/running in the menu:\n  Ensure that kubectl on your Mac, is pointing to the correct cluster and context.\n$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE docker-for-desktop docker-for-desktop-cluster docker-for-desktop kubernetes-admin@kubernetes kubernetes kubernetes-admin $ kubectl config use-context docker-for-desktop Switched to context \u0026quot;docker-for-desktop\u0026quot;. $ kubectl config get-clusters NAME kubernetes docker-for-desktop-cluster $ kubectl config set-cluster docker-for-desktop-cluster Cluster \u0026quot;docker-for-desktop-cluster\u0026quot; set.   You should add docker-for-desktop to your /etc/hosts file entry for 127.0.0.1, as shown in this example, and you must be an admin user to edit this file:\n## # Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 127.0.0.1\tlocalhost docker-for-desktop 255.255.255.255\tbroadcasthost ::1 localhost   You may also have to tell kubectl to ignore the certificate by entering this command:\nkubectl config set-cluster docker-for-desktop --insecure-skip-tls-verify=true   Then validate you are talking to the Kubernetes in Docker by entering these commands:\n$ kubectl cluster-info Kubernetes master is running at https://docker-for-desktop:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Important note about persistent volumes   Docker for Mac has some restrictions on where you can place a directory that can be used as a HostPath for a persistent volume. To keep it simple, place your directory somewhere under /Users.\n "
},
{
	"uri": "/weblogic-kubernetes-operator/reference/swagger/",
	"title": "Swagger",
	"tags": [],
	"description": "Swagger REST API documentation.",
	"content": "View the Swagger REST API documentation here.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/quickstart/prerequisites/",
	"title": "Before you begin",
	"tags": [],
	"description": "",
	"content": "For this exercise, you’ll need a Kubernetes cluster. If you need help setting one up, check out our cheat sheet. This guide assumes a single node cluster.\nThe operator uses Helm to create and deploy the necessary resources and then run the operator in a Kubernetes cluster. For Helm installation and usage information, see Install Helm and Tiller.\nYou should clone this repository to your local machine so that you have access to the various sample files mentioned throughout this guide:\n$ git clone https://github.com/oracle/weblogic-kubernetes-operator "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/",
	"title": "User Guide",
	"tags": [],
	"description": "",
	"content": "The User Guide provides detailed information about all aspects of using the operator.\n Introduction  Learn about the operator\u0026#39;s design, architecture, important terms, prerequisites, and supported environments.\n Overview  The information in the following sections is organized in the order that you would most likely need to use it. If you want to set up an operator and use it to create and manage WebLogic domains, then you should follow the User Guide sections from top to bottom, and the necessary information will be presented in the correct order.\n Manage operators  Helm is used to create and deploy necessary operator resources and to run the operator in a Kubernetes cluster. Use the operator\u0026#39;s Helm chart to install and manage the operator.\n Manage WebLogic domains  Important considerations for WebLogic domains in Kubernetes.\n Manage FMW Domains  Use the operator to manage Oracle Fusion Middleware domains. Manage FMW Infrastructure domains FMW Infrastructure domains contain the Java Required Files (JRF) feature and are the prerequisite for upper stack products like Oracle SOA Suite. Manage SOA domains SOA domains include the deployment of various Oracle Service-Oriented Architecture (SOA) Suite components, such as SOA, Oracle Service Bus (OSB), and Oracle Enterprise Scheduler (ESS).  CI/CD considerations  Learn about managing domain images with continuous integration and continuous delivery (CI/CD).\n Experimental features  Learn about experimental features included in the operator.\n "
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/elastic-stack/soa-domain/",
	"title": "SOA domain",
	"tags": [],
	"description": "Samples for publishing logs to Elasticsearch and monitoring a SOA instance.",
	"content": "  Publish logs to Elasticsearch  Use the WebLogic Logging Exporter to publish the WebLogic Server logs to Elasticsearch.\n Monitor a SOA domain  Use the WebLogic Monitoring Exporter to monitor a SOA instance using Prometheus and Grafana.\n "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/cicd/choose-an-approach/",
	"title": "Choose an approach",
	"tags": [],
	"description": "How to choose an approach.",
	"content": "Let\u0026rsquo;s review what we have discussed and talk about when we might want to use various approaches. We can start by asking ourselves questions like these:\n  Can you make the desired change with a configuration override or Model in Image ConfigMap?\nWhen your domain home source type is Domain in PV or Domain in Image, the operator allows you to inject a number of configuration overrides into your pods before starting any servers in the domain.\nWhen your domain home source type is Model in Image, you can inject model updates before starting any servers in the domain for any type of update, or even while your domain is running for most types of updates. Model in Image runtime model updates are propagated by restarting (rolling) the running WebLogic Servers.\nA good example of a change would be changing the settings for a data source. For example, you may wish to have a larger connection pool in your production environment than you do in your development/test environments. You probably also want to have different credentials. You may want to change the service name, and so on. All of these kinds of updates can be made with configuration overrides for Domain in PV and Domain in Image, and with model updates for Model in Image. These are placed in a Kubernetes ConfigMap, meaning that they are outside of the image, so they do not require rebuilding the Docker image. If all of your changes fit into this category, it is probably much better to just use configuration overrides for Domain in PV and Domain in Image, and use model updates for Model in Image.\n  Are you only changing the WebLogic configuration, for example, deploying or updating an application, changing a resource configuration in a way that is not supported by configuration overrides, Model in Image model updates, and such?\nIf your changes fit into this category, and you have used the \u0026ldquo;domain-in-image\u0026rdquo; approach and the Docker layering model, then you only need to update the top layer of your image. This is relatively easy compared to making changes in lower layers. You could create a new layer with the changes, or you could rebuild/replace the existing top layer with a new one. Which approach you choose depends mainly on whether you need to maintain the same domain encryption keys or not.\n  Do you need to be able to do a rolling restart?\nIf you need to do a rolling restart with Domain in Image, for example, to maintain the availability of your applications, then you need to make sure that a new domain layer has the same domain encryption keys. You cannot perform a rolling restart of a domain if the new members have a different encryption key.\nIf you need to do a rolling restart with Model in Image or Domain in PV, the domain encryption keys are not a concern. In Model in Image, the keys are generated by the operator at runtime before the first WebLogic Server pod is started. In Domain in PV, the keys are generated once, the first time the domain is started, and remain in the domain home within the PV.\n  Do you need to mutate something in a lower layer, for example, patch WebLogic, the JDK, or Linux?\nIf you need to make an update in a lower layer, then you will need to rebuild that layer and all of the layers above it. This means that you will need to rebuild the domain layer. In the case of Model in Image, you will need to determine if you need to keep the same domain encryption keys.\n  The diagram below summarizes these concerns in a decision tree for the “domain-in-image” case:\nIf you are using the \u0026ldquo;domain-on-PV\u0026rdquo; or \u0026ldquo;model-in-image\u0026rdquo; approach, many of these concerns become moot because you have an effective separation between your domain and the Docker image. There is still the possibility that an update in the Docker image could affect your domain; for example, if you updated the JDK, you may need to update some of your domain scripts to reflect the new JDK path.\nHowever, in this scenario, your environment is much closer to what you are probably used to in a traditional (non-Kubernetes) environment, and you will probably find that all of the practices you used from that pre-Kubernetes environment are directly applicable here too, with just some small modifications. For example, applying a WebLogic patch would now involve building a new Docker image.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/introduction/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "The operator consists of several parts: the operator runtime, the model for a Kubernetes custom resource definition (CRD), a Helm chart for installing the operator, a variety of sample shell scripts for preparing or packaging WebLogic domains for running in Kubernetes, and sample Helm charts or shell scripts for conditionally exposing WebLogic endpoints outside the Kubernetes cluster.",
	"content": "The operator consists of the following parts:\n The operator runtime, a process that runs in a Docker container deployed into a Kubernetes Pod and which performs the actual management tasks. The model for a Kubernetes custom resource definition (CRD) that when installed in a Kubernetes cluster allows the Kubernetes API server to manage instances of this new type representing the operational details and status of WebLogic domains. A Helm chart for installing the operator runtime and related resources. A variety of sample shell scripts for preparing or packaging WebLogic domains for running in Kubernetes. A variety of sample Helm charts or shell scripts for conditionally exposing WebLogic endpoints outside the Kubernetes cluster.  The operator is packaged in a Docker image which you can access using the following docker pull commands:\n$ docker login $ docker pull oracle/weblogic-kubernetes-operator:2.5.0 For more details on acquiring the operator image and prerequisites for installing the operator, consult the Quick Start guide.\nThe operator registers a Kubernetes custom resource definition called domain.weblogic.oracle (shortname domain, plural domains). More details about the domain resource type defined by this CRD, including its schema, are available here.\nThe diagram below shows the general layout of high-level components, including optional components, in a Kubernetes cluster that is hosting WebLogic domains and the operator:\nThe Kubernetes cluster has several namespaces. Components may be deployed into namespaces as follows:\n The operator is deployed into its own namespace. If the Elastic Stack integration option is configured, then a Logstash pod will also be deployed in the operator’s namespace. WebLogic domains will be deployed into various namespaces. There can be more than one domain in a namespace, if desired. There is no limit on the number of domains or namespaces that an operator can manage. Note that there can be more than one operator in a Kubernetes cluster, but each operator is configured with a list of the specific namespaces that it is responsible for. The operator will not take any action on any domain that is not in one of the namespaces the operator is configured to manage. Customers are responsible for load balancer configuration, which will typically be in the same namespace with domains or in a system, shared namespace such as the kube-system namespace. Customers are responsible for Elasticsearch and Kibana deployment, which are typically deployed in the default namespace.  Domain architecture The diagram below shows how the various parts of a WebLogic domain are manifest in Kubernetes by the operator.\nThis diagram shows the following details:\n An optional, persistent volume is created by the customer using one of the available providers. If the persistent volume is shared across the domain or members of a cluster, then the chosen provider must support “Read Write Many” access mode. The shared state on the persistent volume may include the domain directory, the applications directory, a directory for storing logs, and a directory for any file-based persistence stores. A pod is created for the WebLogic Server Administration Server. This pod is labeled with weblogic.domainUID, weblogic.serverName, and weblogic.domainName. One container runs in this pod. WebLogic Node Manager and Administration Server processes are run inside this container. The Node Manager process is used as an internal implementation detail for the liveness probe, for patching, and to provide monitoring and control capabilities to the Administration Console. It is not intended to be used for other purposes, and it may be removed in some future release. A ClusterIP type service is created for the Administration Server pod. This service provides a stable, well-known network (DNS) name for the Administration Server. This name is derived from the domainUID and the Administration Server name, and it is known before starting up any pod. The Administration Server ListenAddress is set to this well-known name. ClusterIP type services are only visible inside the Kubernetes cluster. They are used to provide the well-known names that all of the servers in a domain use to communicate with each other. This service is labeled with weblogic.domainUID and weblogic.domainName. A NodePort type service is optionally created for the Administration Server pod. This service provides HTTP access to the Administration Server to clients that are outside the Kubernetes cluster. This service is intended to be used to access the WebLogic Server Administration Console or for the T3 protocol for WLST connections. This service is labeled with weblogic.domainUID and weblogic.domainName. A pod is created for each WebLogic Server Managed Server. These pods are labeled with weblogic.domainUID, weblogic.serverName, and weblogic.domainName. One container runs in each pod. WebLogic Node Manager and Managed Server processes are run inside each of these containers. The Node Manager process is used as an internal implementation detail for the liveness probe. It is not intended to be used for other purposes, and it may be removed in some future release. A ClusterIP type service is created for each Managed Server pod that contains a Managed Server that is not part of a WebLogic cluster. These services are intended to be used to access applications running on the Managed Servers. These services are labeled with weblogic.domainUID and weblogic.domainName. Customers must expose these services using a load balancer or NodePort type service to expose these endpoints outside the Kubernetes cluster. An Ingress may optionally be created by the customer for each WebLogic cluster. An Ingress provides load balanced HTTP access to all Managed Servers in that WebLogic cluster. The load balancer updates its routing table for an Ingress every time a Managed Server in the WebLogic cluster becomes “ready” or ceases to be able to service requests, such that the Ingress always points to just those Managed Servers that are able to handle user requests.  The diagram below shows the components inside the containers running WebLogic Server instances:\nThe domain resource specifies a Docker image, defaulting to container-registry.oracle.com/middleware/weblogic:12.2.1.4. All containers running WebLogic Server use this same Docker image. Depending on the use case, this image could contain the WebLogic Server product binaries or also include the domain directory. During a rolling event caused by a change to the domain resource\u0026rsquo;s image field, containers will be using a mix of the updated value of the image field and its previous value.\n Within the container, the following aspects are configured by the operator:\n The ENTRYPOINT is configured by a script that starts up a Node Manager process, and then uses WLST to request that Node Manager start the server. Node Manager is used to start servers so that the socket connection to the server will be available to obtain server status even when the server is unresponsive. This is used by the liveness probe. The liveness probe is configured to check that the server is alive by querying the Node Manager process. The liveness probe is by default configured to check liveness every 15 seconds, and to timeout after 5 seconds. If a pod fails the liveness probe, Kubernetes will restart that container. The readiness probe is configured to use the WebLogic Server ReadyApp framework. The readiness probe is used to determine if the server is ready to accept user requests. The readiness is used to determine when a server should be included in a load balancer\u0026rsquo;s endpoints, when a restarted server is fully started in the case of a rolling restart, and for various other purposes. A shutdown hook is configured that will execute a script that performs a graceful shutdown of the server. This ensures that servers have an opportunity to shut down cleanly before they are killed.  Domain state stored outside Docker images The operator expects (and requires) that all state be stored outside of the Docker images that are used to run the domain. This means either in a persistent file system, or in a database. The WebLogic configuration, that is, the domain directory and the applications directory may come from the Docker image or a persistent volume. However, other state, such as file-based persistent stores, and such, must be stored on a persistent volume or in a database. All of the containers that are participating in the WebLogic domain use the same image, and take on their personality; that is, which server they execute, at startup time. Each pod mounts storage, according to the domain resource, and has access to the state information that it needs to fulfill its role in the domain.\nIt is worth providing some background information on why this approach was adopted, in addition to the fact that this separation is consistent with other existing operators (for other products) and the Kubernetes “cattle, not pets” philosophy when it comes to containers.\nThe external state approach allows the operator to treat the Docker images as essentially immutable, read-only, binary images. This means that the image needs to be pulled only once, and that many domains can share the same image. This helps to minimize the amount of bandwidth and storage needed for WebLogic Server Docker images.\nThis approach also eliminates the need to manage any state created in a running container, because all of the state that needs to be preserved is written into either the persistent volume or a database back end. The containers and pods are completely throwaway and can be replaced with new containers and pods, as necessary. This makes handling failures and rolling restarts much simpler because there is no need to preserve any state inside a running container.\nWhen users wish to apply a binary patch to WebLogic Server, it is necessary to create only a single new, patched Docker image. If desired, any domains that are running may be updated to this new patched image with a rolling restart, because there is no state in the containers.\nIt is envisaged that in some future release of the operator, it will be desirable to be able to “move” or “copy” domains in order to support scenarios like Kubernetes federation, high availability, and disaster recovery. Separating the state from the running containers is seen as a way to greatly simplify this feature, and to minimize the amount of data that would need to be moved over the network, because the configuration is generally much smaller than the size of WebLogic Server Docker images.\nThe team developing the operator felt that these considerations provided adequate justification for adopting the external state approach.\nNetwork name predictability The operator uses services to provide stable, well-known DNS names for each server. These names are known in advance of starting up a pod to run a server, and are used in the ListenAddress fields in the WebLogic Server configuration to ensure that servers will always be able to find each other. This also eliminates the need for pod names or the actual WebLogic Server instance names to be the same as the DNS addresses.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/accessing-the-domain/wlst/",
	"title": "Use WLST",
	"tags": [],
	"description": "You can use the WebLogic Scripting Tool (WLST) to manage a domain running in Kubernetes.",
	"content": "You can use the WebLogic Scripting Tool (WLST) to manage a domain running in Kubernetes. If the Administration Server was configured to expose a T3 channel using the exposeAdminT3Channel setting when creating the domain, then the matching T3 service can be used to connect. For example, if the domainUID is domain1, and the Administration Server name is admin-server, then the service would be called:\ndomain1-admin-server-external This service will be in the same namespace as the domain. The external port number can be obtained by checking this service’s nodePort:\n$ kubectl get service domain1-admin-server-external -n domain1 -o jsonpath='{.spec.ports[0].nodePort}' 30012 In this example, the nodePort is 30012. If the Kubernetes server’s address was kubernetes001, then WLST can connect to t3://kubernetes001:30012 as shown below:\n$ ~/wls/oracle_common/common/bin/wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt; connect('weblogic','*password*','t3:// kubernetes001:30012') Connecting to t3:// kubernetes001:30012 with userid weblogic ... Successfully connected to Admin Server \u0026quot;admin-server\u0026quot; that belongs to domain \u0026quot;base_domain\u0026quot;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/base_domain/serverConfig/\u0026gt; exit() Exiting WebLogic Scripting Tool. "
},
{
	"uri": "/weblogic-kubernetes-operator/security/encryption/",
	"title": "Encryption",
	"tags": [],
	"description": "WebLogic domain encryption and the operator",
	"content": "Contents  Introspector encryption Encryption of Kubernetes Secrets Additional reading  Introspector encryption The operator has an introspection job that handles WebLogic domain encryption. The introspection job also addresses the use of Kubernetes Secrets with configuration overrides. For additional information on the configuration handling, see Configuration overrides.\nThe introspection job also creates a boot.properties file that is made available to the pods in the WebLogic domain. The credential used for the WebLogic domain is kept in a Kubernetes Secret which follows the naming pattern \u0026lt;domainUID\u0026gt;-weblogic-credentials, where \u0026lt;domainUID\u0026gt; is the unique identifier of the domain, for example, mydomain-weblogic-credentials.\nFor more information about the WebLogic credentials secret, see Secrets.\n Encryption of Kubernetes Secrets To better protect your credentials and private keys, the Kubernetes cluster should be set up with encryption. Please see the Kubernetes documentation about encryption at rest for secret data and using a KMS provider for data encryption.\n Additional reading  Encryption of values for WebLogic configuration overrides  "
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/domains/domain-home-in-image/",
	"title": "Domain home in image",
	"tags": [],
	"description": "Sample for creating a WebLogic domain home inside a Docker image, and the domain resource YAML file for deploying the generated WebLogic domain.",
	"content": "The sample scripts demonstrate the creation of a WebLogic domain home in a Docker image using one of the domain home in image samples in the Oracle WebLogic Docker images GitHub project. The sample scripts have the option of putting the WebLogic domain log, server logs, server output files, and the Node Manager logs on an existing Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC). The scripts also generate the domain YAML file, which can then be used by the scripts or used manually to start the Kubernetes artifacts of the corresponding domain, including the WebLogic Server pods and services.\nPrerequisites Before you begin, read this document, Domain resource.\nThe following prerequisites must be met prior to running the create domain script:\n The WebLogic Deploy Tooling (WDT) sample requires that JAVA_HOME is set to a Java JDK version 1.8 or later. The operator requires either Oracle WebLogic Server 12.2.1.3.0 with patch 29135930 applied, or Oracle WebLogic Server 12.2.1.4.0, or Oracle WebLogic Server 14.1.1.0.0. The existing WebLogic Docker image, container-registry.oracle.com/middleware/weblogic:12.2.1.3, has all the necessary patches applied. For details on how to obtain or create the image, see WebLogic Docker images. Create a Kubernetes Namespace for the domain unless you intend to use the default namespace. If logHomeOnPV is enabled, create the Kubernetes PersistentVolume where the log home will be hosted, and the Kubernetes PersistentVolumeClaim for the domain in the same Kubernetes Namespace. For samples to create a PV and PVC, see Create sample PV and PVC. Create a Kubernetes Secret for the WebLogic administrator credentials that contains the fields username and password, and make sure that the secret name matches the value specified for weblogicCredentialsSecretName; see Configuration parameters below. For example:  $ cd ./kubernetes/samples/scripts/create-weblogic-domain-credentials $ create-weblogic-credentials.sh -u \u0026lt;username\u0026gt; -p \u0026lt;password\u0026gt; -d domain1 -n default -s domain1-weblogic-credentials NOTE: Using this example, you would configure weblogicCredentialsSecretName to be domain1-weblogic-credentials.\nUse the script to create a domain The create-domain.sh script generates a new Docker image on each run with a new domain home and a different internal domain secret in it. To prevent having disparate images with different domain secrets in the same domain, we strongly recommend that a new domain uses a domainUID that is different from any of the active domains, or that you delete the existing domain resource using the following command and wait until all the server pods are terminated before you create a domain with the same domainUID: $ kubectl delete domain [domainUID] -n [domainNamespace]\n The sample for creating domains is in this directory:\n$ cd kubernetes/samples/scripts/create-weblogic-domain/domain-home-in-image Make a copy of the create-domain-inputs.yaml file, update it with the correct values, and run the create script, pointing it at your inputs file and an output directory:\n$ ./create-domain.sh \\ -u \u0026lt;username\u0026gt; \\ -p \u0026lt;password\u0026gt; \\ -i create-domain-inputs.yaml \\ -o /\u0026lt;path to output-directory\u0026gt; The script will perform the following steps:\n Create a directory for the generated properties and Kubernetes YAML files for this domain if it does not already exist. The pathname is /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, its contents will be removed. Create a properties file, domain.properties, in the directory that is created above. This properties file will be used to create a sample WebLogic Server domain. Clone the WebLogic docker-images project into the directory that is derived from the domainHomeImageBuildPath property using git clone https://github.com/oracle/docker-images.git. By default, the script always cleans up the directory and clones it again on every run. You need to specify the -k option if you want to use a previously cloned project. Note that if the specified domainHomeImageBuildPath is empty, the script will still clone the project even if the -k option is specified. Replace the built-in user name and password in the properties/docker-build/domain_security.properties file with the username and password that are supplied on the command line using the -u and -p options. These credentials need to match the WebLogic domain administrator credentials in the secret that is specified using the weblogicCredentialsSecretName property in the create-domain-inputs.yaml file. Build a Docker image based on the Docker sample, Example Image with a WebLogic Server Domain using the Oracle WebLogic Scripting Tooling (WLST) or Example Image with a WebLogic Server Domain using the Oracle WebLogic Deploy Tooling (WDT). It will create a sample WebLogic Server domain in the Docker image. Oracle strongly recommends storing the image containing the domain home as private in the registry (for example, Oracle Cloud Infrastructure Registry, Docker Hub, and such) because this image contains sensitive information about the domain, including keys and credentials that are used to access external resources (for example, the data source password). For more information, see WebLogic domain in Docker image protection.\n  Create a tag that refers to the generated Docker image. Create a Kubernetes domain YAML file, domain.yaml, in the directory that is created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command.  $ kubectl apply -f /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml As a convenience, using the -e option, the script can optionally create the domain object, which in turn results in the creation of the corresponding WebLogic Server pods and services. This option should be used in a single node Kubernetes cluster only.\nFor a multi-node Kubernetes cluster, make sure that the generated image is available on all nodes before creating the domain resource using the kubectl apply -f command.\nThe usage of the create script is as follows:\n$ sh create-domain.sh -h usage: create-domain.sh -o dir -i file -u username -p password [-k] [-e] [-h] -i Parameter inputs file, must be specified. -o Ouput directory for the generated properties and YAML files, must be specified. -u User name used in building the Docker image for WebLogic domain in image. -p Password used in building the Docker image for WebLogic domain in image. -e Also create the resources in the generated YAML files, optional. -v Validate the existence of persistentVolumeClaim, optional. -k Keep what has been previously cloned from https://github.com/oracle/docker-images.git, optional. If not specified, this script will always remove the existing project and clone again. -h Help If you copy the sample scripts to a different location, make sure that you copy everything in the \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts directory together into the target directory, maintaining the original directory hierarchy.\nThe default domain created by the script has the following characteristics:\n An Administration Server named admin-server listening on port 7001. A dynamic cluster named cluster-1 of size 5. Two Managed Servers, named managed-server1 and managed-server2, listening on port 8001. No applications deployed. A T3 channel.  If you run the sample from a machine that is remote to the Kubernetes cluster, and you need to push the new image to a registry that is local to the cluster, you need to do the following (also, see the image property in the Configuration parameters table.):\n Set the image property in the inputs file to the target image name (including the registry hostname, port, and the tag, if needed). If you want Kubernetes to pull the image from a private registry, create a Kubernetes Secret to hold your credentials and set the imagePullSecretName property in the inputs file to the name of the secret. The Kubernetes Secret must be in the same namespace where the domain will be running. For more information, see WebLogic domain in Docker image protection.\n  Run the create-domain.sh script without the -e option. Push the image to the target registry. Run the following command to create the domain:  $ kubectl apply -f /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml The domain creation inputs can be customized by editing create-domain-inputs.yaml.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Definition Default     sslEnabled Boolean indicating whether to enable SSL for each WebLogic Server instance. false   adminPort Port number of the Administration Server inside the Kubernetes cluster. 7001   adminServerSSLPort SSL port number of the Administration Server inside the Kubernetes cluster. 7002   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. admin-server   clusterName Name of the WebLogic cluster instance to generate for the domain. cluster-1   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   domainHomeImageBase Base WebLogic binary image used to build the WebLogic domain image. The operator requires either Oracle WebLogic Server 12.2.1.3.0 with patch 29135930 applied, or Oracle WebLogic Server 12.2.1.4.0, or Oracle WebLogic Server 14.1.1.0.0. The existing WebLogic Docker image, container-registry.oracle.com/middleware/weblogic:12.2.1.3, has all the necessary patches applied. For details on how to obtain or create the image, see WebLogic Docker images. container-registry.oracle.com/middleware/weblogic:12.2.1.3   domainHomeImageBuildPath Location of the WebLogic \u0026ldquo;domain home in image\u0026rdquo; Docker image in the https://github.com/oracle/docker-images.git project. If not specified, use ./docker-images/OracleWebLogic/samples/12213-domain-home-in-image. Another possible value is ./docker-images/OracleWebLogic/samples/12213-domain-home-in-image-wdt which uses WDT, instead of WLST, to generate the domain configuration. ./docker-images/OracleWebLogic/samples/12213-domain-home-in-image   domainPVMountPath Mount path of the domain persistent volume. This parameter is required if logHomeOnPV is true. Otherwise, it is ignored. /shared   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes Service name. domain1   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   httpAccessLogInLogHome Boolean indicating if server HTTP access log files should be written to the same directory as logHome if logHomeOnPV is true. Otherwise, server HTTP access log files will be written to the directory specified in the WebLogic domain home configuration. true   image WebLogic Server Docker image that the operator uses to start the domain. The create domain scripts generate a WebLogic Server Docker image with a domain home in it. By default, the scripts tag the generated WebLogic Server Docker image as either domain-home-in-image or domain-home-in-image-wdt based on the domainHomeImageBuildPath property, and use it plus the tag that is obtained from the domainHomeImageBase to set the image element in the generated domain YAML file. If this property is set, the create domain scripts will use the value specified, instead of the default value, to tag the generated image and set the image in the domain YAML file. A unique value is required for each domain that is created using the scripts. If you are running the sample scripts from a machine that is remote to the Kubernetes cluster where the domain is going to be running, you need to set this property to the image name that is intended to be used in a registry local to that Kubernetes cluster. You also need to push the image to that registry before starting the domain using the kubectl create -f or kubectl apply -f command.    imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never. IfNotPresent   imagePullSecretName Name of the Kubernetes Secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server .out int the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to initially start for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). If sslEnabled is set to true and the WebLogic demo certificate is used, add -Dweblogic.security.SSL.ignoreHostnameVerification=true to allow the managed servers to connect to the Administration Server while booting up. The WebLogic generated demo certificate in this environment typically contains a host name that is different from the runtime container\u0026rsquo;s host name. -Dweblogic.StdoutDebugEnabled=false   logHomeOnPV Specifies whether the log home is stored on the persistent volume. If set to true, then you must specify the logHome, persistentVolumeClaimName, and domainPVMountPath parameters. false   logHome The in-pod location for domain log, server logs, server out, Node Manager log, and server HTTP access log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. This parameter is required if logHomeOnPV is true. Otherwise, it is ignored. /shared/logs/domain1   managedServerNameBase Base string used to generate Managed Server names. managed-server   managedServerPort Port number for each Managed Server. 8001   managedServerSSLPort SSL port number for each Managed Server. 8002   namespace Kubernetes Namespace in which to create the domain. default   persistentVolumeClaimName Name of the persistent volume claim. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. This parameter is required if logHomeOnPV is true. Otherwise, it is ignored. domain1-weblogic-sample-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the network access point. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only, in a single server (all-in-one) Kubernetes Deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster.   weblogicCredentialsSecretName Name of the Kubernetes Secret for the Administration Server user name and password. domain1-weblogic-credentials   serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName, and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes Service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create a WebLogic domain home and associated Kubernetes resources for a domain that has only one cluster. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. Also, the generated domain YAML file can be modified to cover more use cases.\nVerify the results The create script will verify that the domain was created, and will report failure if there was any error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nNote that the example results below use the default Kubernetes Namespace. If you are using a different namespace, you need to replace NAMESPACE in the example kubectl commands with the actual Kubernetes Namespace.\nGenerated YAML files with the default inputs The content of the generated domain.yaml:\n# Copyright (c) 2017, 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026quot;weblogic.oracle/v2\u0026quot; kind: Domain metadata: name: domain1 namespace: default labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: # The WebLogic Domain Home domainHome: /u01/oracle/user_projects/domains/domain1 # Set domain home type to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: Image # The WebLogic Server Docker image that the operator uses to start the domain image: \u0026quot;domain-home-in-image:12.2.1.4\u0026quot; # imagePullPolicy defaults to \u0026quot;Always\u0026quot; if image version is :latest imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: domain1-weblogic-credentials # Whether to include the server out file into the pod's stdout, default is true includeServerOutInPodLog: true # Whether to enable log home # logHomeEnabled: false # The in-pod location for domain log, server logs, server out, and Node Manager log files # logHome: /shared/logs/domain1 # serverStartPolicy legal values are \u0026quot;NEVER\u0026quot;, \u0026quot;IF_NEEDED\u0026quot;, or \u0026quot;ADMIN_ONLY\u0026quot; # This determines which WebLogic Servers the operator will start up when it discovers this Domain # - \u0026quot;NEVER\u0026quot; will not start any server in the domain # - \u0026quot;ADMIN_ONLY\u0026quot; will start up only the administration server (no managed servers will be started) # - \u0026quot;IF_NEEDED\u0026quot; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom \u0026quot; # volumes: # - name: weblogic-domain-storage-volume # persistentVolumeClaim: # claimName: domain1-weblogic-sample-pvc # volumeMounts: # - mountPath: /shared # name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026quot;RUNNING\u0026quot; or \u0026quot;ADMIN\u0026quot; # \u0026quot;RUNNING\u0026quot; means the listed server will be started up to \u0026quot;RUNNING\u0026quot; mode # \u0026quot;ADMIN\u0026quot; means the listed server will be start up to \u0026quot;ADMIN\u0026quot; mode serverStartState: \u0026quot;RUNNING\u0026quot; # adminService: # channels: # The Admin Server's NodePort # - channelName: default # nodePort: 30701 # Uncomment to export the T3Channel as a service # - channelName: T3Channel # clusters is used to configure the desired behavior for starting member servers of a cluster. # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: cluster-1 serverStartState: \u0026quot;RUNNING\u0026quot; replicas: 2 # The number of managed servers to start for unlisted clusters # replicas: 1 Verify the domain To confirm that the domain was created, use this command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\nHere is an example of the output of this command:\n$ kubectl describe domain domain1 Name: domain1 Namespace: default Labels: weblogic.domainUID=domain1 weblogic.resourceVersion=domain-v2 Annotations: \u0026lt;none\u0026gt; API Version: weblogic.oracle/v2 Kind: Domain Metadata: Cluster Name: Creation Timestamp: 2019-01-10T14:29:37Z Generation: 1 Resource Version: 3698533 Self Link: /apis/weblogic.oracle/v2/namespaces/default/domains/domain1 UID: 28655979-14e4-11e9-b751-fa163e855ac8 Spec: Admin Server: Server Pod: Annotations: Container Security Context: Env: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Clusters: Cluster Name: cluster-1 Cluster Service: Annotations: Labels: Replicas: 2 Server Pod: Annotations: Container Security Context: Env: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Domain Home: /u01/oracle/user_projects/domains/domain1 Domain Home In Image: true Image: domain-home-in-image:12.2.1.4 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Managed Servers: Server Pod: Annotations: Container Security Context: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Xms64m -Xmx256m Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: domain1-weblogic-credentials Status: Conditions: Last Transition Time: 2019-01-10T14:31:10.681Z Reason: ServersReady Status: True Type: Available Servers: Health: Activation Time: 2019-01-10T14:30:47.432Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: admin-server State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-01-10T14:32:01.467Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: managed-server1 State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-01-10T14:32:04.532Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: managed-server2 State: RUNNING Start Time: 2019-01-10T14:29:37.455Z Events: \u0026lt;none\u0026gt; In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued very soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. The operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Use the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE domain1-admin-server 1/1 Running 0 30m domain1-managed-server1 1/1 Running 0 29m domain1-managed-server2 1/1 Running 0 29m Verify the services Use the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command:\n$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE domain1-admin-server ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 32m domain1-cluster-cluster-1 ClusterIP 10.99.151.142 \u0026lt;none\u0026gt; 8001/TCP 31m domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 31m domain1-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 22m Delete the domain The generated YAML file in the /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt; directory can be used to delete the Kubernetes resource. Use the following command to delete the domain:\n$ kubectl delete -f domain.yaml "
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/domains/",
	"title": "Domains",
	"tags": [],
	"description": "These samples show various choices for working with domains.",
	"content": "These samples show various choices for working with domains.\n Manually  Sample for creating the domain custom resource manually.\n Domain home on a PV  Sample for creating a WebLogic domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated WebLogic domain.\n Domain home in image  Sample for creating a WebLogic domain home inside a Docker image, and the domain resource YAML file for deploying the generated WebLogic domain.\n Model in image  Sample for supplying a WebLogic Deploy Tooling (WDT) model that the operator expands into a full domain home during runtime.\n FMW Infrastructure domain  Sample for creating an FMW Infrastructure domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated WebLogic domain.\n SOA domain  Sample for creating a SOA Suite domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated SOA domain.\n Delete domain resources  Delete the domain resources created while executing the samples.\n "
},
{
	"uri": "/weblogic-kubernetes-operator/developerguide/building/",
	"title": "Building",
	"tags": [],
	"description": "",
	"content": "The operator is built using Apache Maven. The build machine will also need to have Docker installed.\nTo build the operator, issue the following command in the project directory:\n$ mvn clean install This will compile the source files, build JAR files containing the compiled classes and libraries needed to run the operator, and will also execute all of the unit tests.\nContributions must conform to coding and formatting standards.\nBuilding Javadoc To build the Javadoc for the operator, issue the following command:\n$ mvn javadoc:aggregate The Javadoc is also available in the GitHub repository here.\nBuilding the operator Docker image Log in to the Docker Store so that you will be able to pull the base image and create the Docker image as follows. These commands should be executed in the project root directory:\n$ docker login $ docker build --build-arg VERSION=\u0026lt;version\u0026gt; -t weblogic-kubernetes-operator:some-tag --no-cache=true . Replace \u0026lt;version\u0026gt; with the version of the project found in the pom.xml file in the project root directory.\nWe recommend that you use a tag other than latest, to make it easy to distinguish your image. In the example above, the tag could be the GitHub ID of the developer.\nRunning the operator from an IDE The operator can be run from an IDE, which is useful for debugging. In order to do so, the machine running the IDE must be configured with a Kubernetes configuration file in ~/.kube/config or in a location pointed to by the KUBECONFIG environment variable.\nConfigure the IDE to run the class oracle.kubernetes.operator.Main.\nYou may need to create a directory called /operator on your machine. Please be aware that the operator code is targeted to Linux, and although it will run fine on macOS, it will probably not run on other operating systems. If you develop on another operating system, you should deploy the operator to a Kubernetes cluster and use remote debugging instead.\nRunning the operator in a Kubernetes cluster If you\u0026rsquo;re not running Kubernetes on your development machine, you\u0026rsquo;ll need to make the Docker image available to a registry visible to your Kubernetes cluster. Either docker push the image to a private registry or upload your image to a machine running Docker and Kubernetes as follows:\n# on your build machine $ docker save weblogic-kubernetes-operator:some-tag \u0026gt; operator.tar $ scp operator.tar YOUR_USER@YOUR_SERVER:/some/path/operator.tar # on the Kubernetes server $ docker load \u0026lt; /some/path/operator.tar Use the Helm charts to install the operator.\nIf the operator\u0026rsquo;s behavior or pod log is insufficient to diagnose and resolve failures, then you can connect a Java debugger to the operator using the debugging options.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-operators/using-the-operator/the-rest-api/",
	"title": "The REST API",
	"tags": [],
	"description": "Use the operator&#39;s REST services.",
	"content": "Use the operator\u0026rsquo;s REST services The operator provides a REST server which you can use to get a list of WebLogic domains and clusters and to initiate scaling operations. Swagger documentation for the REST API is available here.\nYou can access most of the REST services using GET, for example:\n To obtain a list of domains, send a GET request to the URL /operator/latest/domains To obtain a list of clusters in a domain, send a GET request to the URL /operator/latest/domains/\u0026lt;domainUID\u0026gt;/clusters  All of the REST services require authentication. Callers must pass in a valid token header and a CA certificate file. Callers should pass in the Accept:/application/json header.\nTo protect against Cross Site Request Forgery (CSRF) attacks, the operator REST API requires that you send in a X-Requested-By header when you invoke a REST endpoint that makes a change (for example, when you POST to the /scale endpoint). The value is an arbitrary name such as MyClient. For example, when using curl:\ncurl ... -H X-Requested-By:MyClient ... -X POST .../scaling If you do not pass in the X-Requested-By header, then you\u0026rsquo;ll get a 400 (bad request) response without any details explaining why the request is bad. The X-Requested-By header is not needed for requests that only read, for example, when you GET any of the operator\u0026rsquo;s REST endpoints.\nBefore using the sample script below, you must:\n Update it to ensure it has the correct service account, namespaces, and such, and it points to the values.yaml that you used to install the operator (so that it can get the certificates). Add your operator\u0026rsquo;s certificate to your operating system\u0026rsquo;s trust store (see below). If you are using a self-signed certificate and your client is macOS, you may need to update the version of curl you have installed. The version of CURL that ships with macOS High Sierra (curl 7.54.0 (x86_64-apple-darwin17.0) libcurl/7.54.0 LibreSSL/2.0.20 zlib/1.2.11 nghttp2/1.24.0) has known issues with self-signed certificates. Oracle recommends curl 7.63.0 (x86_64-apple-darwin17.7.0) libcurl/7.63.0 SecureTransport zlib/1.2.11, which can be installed with brew install curl.  How to add your certificate to your operating system trust store For macOS, find the certificate in Finder, and double-click on it. This will add it to your keystore and open Keychain Access. Find the certificate in Keychain Access and double-click on it to open the details. Open the \u0026ldquo;Trust\u0026rdquo; pull-down menu and set the value of \u0026ldquo;When using this certificate\u0026rdquo; to \u0026ldquo;Always Trust\u0026rdquo;, then close the detail window. Enter your password when prompted.\nFor Oracle Linux, run the script below, once to copy the certificate into /tmp/operator.cert.pem, then run these commands to add the certificate to the trust store:\n$ sudo cp /tmp/operator.cert.pem /etc/pki/ca-trust/source/anchors/ $ sudo update-ca-trust enable; sudo update-ca-trust extract $ openssl x509 -noout -hash -in /tmp/operator.cert.pem $ sudo ln -s /etc/pki/ca-trust/source/anchors/operator.cert.pem /etc/pki/tls/certs/e242d2da.0 In the final command, the file name e242d2da.0 should be the output of the previous command plus the suffix .0.\nFor other operating systems, consult your operating system\u0026rsquo;s documentation (or Google).\nSample operator REST client script Here is a small, sample BASH script that may help to prepare the necessary token, certificates, and such, to call the operator\u0026rsquo;s REST services. Please read the important caveats above before using this script:\n#!/bin/bash KUBERNETES_SERVER=$1 URL_TAIL=$2 REST_PORT=`kubectl get services -n weblogic-operator -o jsonpath='{.items[?(@.metadata.name == \u0026quot;external-weblogic-operator-svc\u0026quot;)].spec.ports[?(@.name == \u0026quot;rest\u0026quot;)].nodePort}'` REST_ADDR=\u0026quot;https://${KUBERNETES_SERVER}:${REST_PORT}\u0026quot; SECRET=`kubectl get serviceaccount weblogic-operator -n weblogic-operator -o jsonpath='{.secrets[0].name}'` ENCODED_TOKEN=`kubectl get secret ${SECRET} -n weblogic-operator -o jsonpath='{.data.token}'` TOKEN=`echo ${ENCODED_TOKEN} | base64 --decode` OPERATOR_CERT_DATA=`kubectl get secret -n weblogic-operator weblogic-operator-external-rest-identity -o jsonpath='{.data.tls\\.crt}'` OPERATOR_CERT_FILE=\u0026quot;/tmp/operator.cert.pem\u0026quot; echo ${OPERATOR_CERT_DATA} | base64 --decode \u0026gt; ${OPERATOR_CERT_FILE} cat ${OPERATOR_CERT_FILE} echo \u0026quot;Ready to call operator REST APIs\u0026quot; STATUS_CODE=`curl \\ -v \\ --cacert ${OPERATOR_CERT_FILE} \\ -H \u0026quot;Authorization: Bearer ${TOKEN}\u0026quot; \\ -H Accept:application/json \\ -X GET ${REST_ADDR}/${URL_TAIL} \\ -o curl.out \\ --stderr curl.err \\ -w \u0026quot;%{http_code}\u0026quot;` cat curl.err cat curl.out | jq .  You can use the -k option to bypass the check to verify that the operator\u0026rsquo;s certificate is trusted (instead of curl --cacert), but this is insecure.\n To use this script, pass in the Kubernetes server address and then the URL you want to call. The script assumes jq is installed and uses it to format the response. This can be removed if desired. The script also prints out quite a bit of useful debugging information, in addition to the response. Here is an example of the output of this script:\n$ ./rest.sh kubernetes001 operator/latest/domains/domain1/clusters Ready to call operator REST APIs Note: Unnecessary use of -X or --request, GET is already inferred. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0* Trying 10.139.151.214... * TCP_NODELAY set * Connected to kubernetes001 (10.1.2.3) port 31001 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH * error setting certificate verify locations, continuing anyway: * CAfile: /tmp/operator.cert.pem CApath: none * TLSv1.2 (OUT), TLS handshake, Client hello (1): } [512 bytes data] * TLSv1.2 (IN), TLS handshake, Server hello (2): { [81 bytes data] * TLSv1.2 (IN), TLS handshake, Certificate (11): { [799 bytes data] * TLSv1.2 (IN), TLS handshake, Server key exchange (12): { [413 bytes data] * TLSv1.2 (IN), TLS handshake, Server finished (14): { [4 bytes data] * TLSv1.2 (OUT), TLS handshake, Client key exchange (16): } [150 bytes data] * TLSv1.2 (OUT), TLS change cipher, Client hello (1): } [1 bytes data] * TLSv1.2 (OUT), TLS handshake, Finished (20): } [16 bytes data] * TLSv1.2 (IN), TLS change cipher, Client hello (1): { [1 bytes data] * TLSv1.2 (IN), TLS handshake, Finished (20): { [16 bytes data] * SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256 * ALPN, server did not agree to a protocol * Server certificate: * subject: CN=weblogic-operator * start date: Jan 18 16:30:01 2018 GMT * expire date: Jan 16 16:30:01 2028 GMT * issuer: CN=weblogic-operator * SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway. \u0026gt; GET /operator/latest/domains/domain1/clusters HTTP/1.1 \u0026gt; Host: kubernetes001:31001 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJ3ZWJsb2dpYy1vcGVyYXRvciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQ (truncated) 1vcGVyYXRvcjp3ZWJsb2dpYy1vcGVyYXRvciJ9.NgaGR0NbzbJpVXguQDjRKyDBnNTqwgwPEXv3NjWwMcaf0OlN54apHubdrIx6KYz9ONGz-QeTLnoMChFY7oWA6CBfbvjt-GQX6JvdoJYxsQo1pt-E6sO2YvqTFE4EG-gpEDaiCE_OjZ_bBpJydhIiFReToA3-mxpDAUK2_rUfkWe5YEaLGMWoYQfXPAykzFiH4vqIi_tzzyzNnGxI2tUcBxNh3tzWFPGXKhzG18HswiwlFU5pe7XEYv4gJbvtV5tlGz7YdmH74Rc0dveV-54qHD_VDC5M7JZVh0ZDlyJMAmWe4YcdwNQQNGs91jqo1-JEM0Wj8iQSDE3cZj6MB0wrdg \u0026gt; Accept:application/json \u0026gt; 0 0 0 0 0 0 0 0 --:--:-- 0:00:01 --:--:-- 0\u0026lt; HTTP/1.1 200 OK \u0026lt; Content-Type: application/json \u0026lt; Content-Length: 463 \u0026lt; { [463 bytes data] 100 463 100 463 0 0 205 0 0:00:02 0:00:02 --:--:-- 205 * Connection #0 to host kubernetes001 left intact { \u0026quot;links\u0026quot;: [ { \u0026quot;rel\u0026quot;: \u0026quot;self\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;href\u0026quot;: \u0026quot;/operator/latest/domains/domain1/clusters\u0026quot; }, { \u0026quot;rel\u0026quot;: \u0026quot;canonical\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;href\u0026quot;: \u0026quot;/operator/latest/domains/domain1/clusters\u0026quot; }, { \u0026quot;rel\u0026quot;: \u0026quot;parent\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;href\u0026quot;: \u0026quot;/operator/latest/domains/domain1\u0026quot; } ], \u0026quot;items\u0026quot;: [ { \u0026quot;links\u0026quot;: [ { \u0026quot;rel\u0026quot;: \u0026quot;self\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;href\u0026quot;: \u0026quot;/operator/latest/domains/domain1/clusters/cluster-1\u0026quot; }, { \u0026quot;rel\u0026quot;: \u0026quot;canonical\u0026quot;, \u0026quot;title\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;href\u0026quot;: \u0026quot;/operator/latest/domains/domain1/clusters/cluster-1\u0026quot; } ], \u0026quot;cluster\u0026quot;: \u0026quot;cluster-1\u0026quot; } ] } "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/domain-lifecycle/scaling/",
	"title": "Scaling",
	"tags": [],
	"description": "The operator provides several ways to initiate scaling of WebLogic clusters.",
	"content": "WebLogic Server supports two types of clustering configurations, configured and dynamic. Configured clusters are created by manually configuring each individual Managed Server instance. In dynamic clusters, the Managed Server configurations are generated from a single, shared template. With dynamic clusters, when additional server capacity is needed, new server instances can be added to the cluster without having to manually configure them individually. Also, unlike configured clusters, scaling up of dynamic clusters is not restricted to the set of servers defined in the cluster but can be increased based on runtime demands. For more information on how to create, configure, and use dynamic clusters in WebLogic Server, see Dynamic Clusters.\nThe following blogs provide more in-depth information on support for scaling WebLogic clusters in Kubernetes:\n Automatic Scaling of WebLogic Clusters on Kubernetes WebLogic Dynamic Clusters on Kubernetes  The operator provides several ways to initiate scaling of WebLogic clusters, including:\n On-demand, updating the domain resource directly (using kubectl). Calling the operator\u0026rsquo;s REST scale API, for example, from curl. Using a WLDF policy rule and script action to call the operator\u0026rsquo;s REST scale API. Using a Prometheus alert action to call the operator\u0026rsquo;s REST scale API.  On-demand, updating the domain resource directly The easiest way to scale a WebLogic cluster in Kubernetes is to simply edit the replicas property within a domain resource. This can be done by using the kubectl command-line interface for running commands against Kubernetes clusters. More specifically, you can modify the domain resource directly by using the kubectl edit command. For example:\n$ kubectl edit domain domain1 -n [namespace] Here we are editing a domain resource named domain1. The kubectl edit command will open the domain resource definition in an editor and allow you to modify the replicas value directly. Once committed, the operator will be notified of the change and will immediately attempt to scale the corresponding dynamic cluster by reconciling the number of running pods/Managed Server instances with the replicas value specification.\nspec: ... clusters: - clusterName: cluster-1 replicas: 1 ... Alternatively, you can specify a default replicas value for all the clusters. If you do this, then you don\u0026rsquo;t need to list the cluster in the domain resource (unless you want to customize another property of the cluster).\nspec: ... replicas: 1 ... Calling the operator\u0026rsquo;s REST scale API Scaling up or scaling down a WebLogic cluster provides increased reliability of customer applications as well as optimization of resource usage. In Kubernetes cloud environments, scaling WebLogic clusters involves scaling the corresponding pods in which WebLogic Server Managed Server instances are running. Because the operator manages the life cycle of a WebLogic domain, the operator exposes a REST API that allows an authorized actor to request scaling of a WebLogic cluster.\nThe following URL format is used for describing the resources for scaling (up and down) a WebLogic cluster:\nhttp(s)://${OPERATOR_ENDPOINT}/operator/\u0026lt;version\u0026gt;/domains/\u0026lt;domainUID\u0026gt;/clusters/\u0026lt;clusterName\u0026gt;/scale For example:\nhttp(s)://${OPERATOR_ENDPOINT}/operator/v1/domains/domain1/clusters/cluster-1/scale In this URL format:\n OPERATOR_ENDPOINT is the host and port of the operator REST endpoint (internal or external). \u0026lt;version\u0026gt; denotes the version of the REST resource. \u0026lt;domainUID\u0026gt; is the unique identifier of the WebLogic domain. \u0026lt;clusterName\u0026gt; is the name of the WebLogic cluster to be scaled.  The /scale REST endpoint accepts an HTTP POST request and the request body supports the JSON \u0026quot;application/json\u0026quot; media type. The request body will be a simple name-value item named managedServerCount; for example:\n{ \u0026quot;managedServerCount\u0026quot;: 3 } The managedServerCount value designates the number of WebLogic Server instances to scale to. Note that the scale resource is implemented using the JAX-RS framework, and so a successful scaling request will return an HTTP response code of 204 (“No Content”) because the resource method’s return type is void and does not return a message body.\nWhen you POST to the /scale REST endpoint, you must send the following headers:\n X-Requested-By request value. The value is an arbitrary name such as MyClient. Authorization: Bearer request value. The value of the Bearer token is the WebLogic domain service account token.  For example, when using curl:\ncurl -v -k -H X-Requested-By:MyClient -H Content-Type:application/json -H Accept:application/json -H \u0026quot;Authorization:Bearer ...\u0026quot; -d '{ \u0026quot;managedServerCount\u0026quot;: 3 }' https://.../scaling If you omit the header, you\u0026rsquo;ll get a 400 (bad request) response without any details explaining why the request was bad. If you omit the Bearer Authentication header, then you\u0026rsquo;ll get a 401 (Unauthorized) response.\nOperator REST endpoints The WebLogic Server Kubernetes Operator can expose both an internal and external REST HTTPS endpoint. The internal REST endpoint is only accessible from within the Kubernetes cluster. The external REST endpoint is accessible from outside the Kubernetes cluster. The internal REST endpoint is enabled by default and thus always available, whereas the external REST endpoint is disabled by default and only exposed if explicitly configured. Detailed instructions for configuring the external REST endpoint are available here.\nRegardless of which endpoint is being invoked, the URL format for scaling is the same.\n What does the operator do in response to a scaling request? When the operator receives a scaling request, it will:\n Perform an authentication and authorization check to verify that the specified user is allowed to perform the specified operation on the specified resource. Validate that the specified domain, identified by domainUID, exists. Validate that the WebLogic cluster, identified by clusterName, exists. Verify that the specified WebLogic cluster has a sufficient number of configured servers to satisfy the scaling request. Initiate scaling by setting the replicas property within the corresponding domain resource, which can be done in either:  A cluster entry, if defined within its cluster list. At the domain level, if not defined in a cluster entry.    In response to a change to either replicas property, in the domain resource, the operator will increase or decrease the number of pods (Managed Servers) to match the desired replica count.\nUsing a WLDF policy rule and script action to call the operator\u0026rsquo;s REST scale API The WebLogic Diagnostics Framework (WLDF) is a suite of services and APIs that collect and surface metrics that provide visibility into server and application performance. To support automatic scaling of WebLogic clusters in Kubernetes, WLDF provides the Policies and Actions component, which lets you write policy expressions for automatically executing scaling operations on a cluster. These policies monitor one or more types of WebLogic Server metrics, such as memory, idle threads, and CPU load. When the configured threshold in a policy is met, the policy is triggered, and the corresponding scaling action is executed. The WebLogic Server Kubernetes Operator project provides a shell script, scalingAction.sh, for use as a Script Action, which illustrates how to issue a request to the operator’s REST endpoint.\nConfigure automatic scaling of WebLogic clusters in Kubernetes with WLDF The following steps are provided as a guideline on how to configure a WLDF Policy and Script Action component for issuing scaling requests to the operator\u0026rsquo;s REST endpoint:\n  Copy the scalingAction.sh script to $DOMAIN_HOME/bin/scripts so that it\u0026rsquo;s accessible within the Administration Server pod. For more information, see Configuring Script Actions in Configuring and Using the Diagnostics Framework for Oracle WebLogic Server.\n  Configure a WLDF policy and action as part of a diagnostic module targeted to the Administration Server. For information about configuring the WLDF Policies and Actions component, see Configuring Policies and Actions in Configuring and Using the Diagnostics Framework for Oracle WebLogic Server.\na. Configure a WLDF policy with a rule expression for monitoring WebLogic Server metrics, such as memory, idle threads, and CPU load for example.\nb. Configure a WLDF script action and associate the scalingAction.sh script.\n  Important notes about the configuration properties for the Script Action:\nThe scalingAction.sh script requires access to the SSL certificate of the operator’s endpoint and this is provided through the environment variable INTERNAL_OPERATOR_CERT.\nThe operator’s SSL certificate can be found in the internalOperatorCert entry of the operator’s ConfigMap weblogic-operator-cm:\nFor example:\n#\u0026gt; kubectl describe configmap weblogic-operator-cm -n weblogic-operator ... Data ==== internalOperatorCert: ---- LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR3akNDQXFxZ0F3SUJBZ0lFRzhYT1N6QU... ... The scalingAction.sh script accepts a number of customizable parameters:\n  action - scaleUp or scaleDown (Required)\n  domain_uid - WebLogic domain unique identifier (Required)\n  cluster_name - WebLogic cluster name (Required)\n  kubernetes_master - Kubernetes master URL, default=https://kubernetes\n  Set this to https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT} when invoking scalingAction.sh from the Administration Server pod.\n   access_token - Service Account Bearer token for authentication and authorization for access to REST Resources\n  wls_domain_namespace - Kubernetes Namespace in which the WebLogic domain is defined, default=default\n  operator_service_name - WebLogic Server Kubernetes Operator Service name of the REST endpoint, default=internal-weblogic-operator-service\n  operator_service_account - Kubernetes Service Account name for the operator, default=weblogic-operator\n  operator_namespace – Namespace in which the operator is deployed, default=weblogic-operator\n  scaling_size – Incremental number of WebLogic Server instances by which to scale up or down, default=1\n  You can use any of the following tools to configure policies for diagnostic system modules:\n WebLogic Server Administration Console WLST REST JMX application  A more in-depth description and example on using WLDF\u0026rsquo;s Policies and Actions component for initiating scaling requests through the operator\u0026rsquo;s REST endpoint can be found in the blogs:\n Automatic Scaling of WebLogic Clusters on Kubernetes WebLogic Dynamic Clusters on Kubernetes  Create ClusterRoleBindings to allow a namespace user to query WLS Kubernetes cluster information The script scalingAction.sh, specified in the WLDF script action above, needs the appropriate RBAC permissions granted for the service account user (in the namespace in which the WebLogic domain is deployed) in order to query the Kubernetes API server for both configuration and runtime information of the domain resource. The following is an example YAML file for creating the appropriate Kubernetes ClusterRole bindings:\nIn the example ClusterRoleBinding definition below, the WebLogic domain is deployed to a namespace weblogic-domain. Replace the namespace value with the name of the namespace in which the WebLogic domain is deployed in your Kubernetes environment.\n kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: weblogic-domain-cluster-role rules: - apiGroups: [\u0026quot;weblogic.oracle\u0026quot;] resources: [\u0026quot;domains\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;update\u0026quot;] - apiGroups: [\u0026quot;apiextensions.k8s.io\u0026quot;] resources: [\u0026quot;customresourcedefinitions\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;] --- # # creating role-bindings for cluster role # kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: domain-cluster-rolebinding subjects: - kind: ServiceAccount name: default namespace: weblogic-domain apiGroup: \u0026quot;\u0026quot; roleRef: kind: ClusterRole name: weblogic-domain-cluster-role apiGroup: \u0026quot;rbac.authorization.k8s.io\u0026quot; --- # # creating role-bindings # kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: weblogic-domain-operator-rolebinding namespace: weblogic-operator subjects: - kind: ServiceAccount name: default namespace: weblogic-domain apiGroup: \u0026quot;\u0026quot; roleRef: kind: ClusterRole name: cluster-admin apiGroup: \u0026quot;rbac.authorization.k8s.io\u0026quot; --- Using a Prometheus alert action to call the operator\u0026rsquo;s REST scale API In addition to using the WebLogic Diagnostic Framework for automatic scaling of a dynamic cluster, you can use a third-party monitoring application like Prometheus. Please read the following blog for details about Using Prometheus to Automatically Scale WebLogic Clusters on Kubernetes.\nHelpful Tips Debugging scalingAction.sh The scalingAction.sh script was designed to be executed within the Administration Server pod because the associated diagnostic module is targed to the Administration Server.\nThe easiest way to verify and debug the scalingAction.sh script is to open a shell on the running Administration Server pod and execute the script on the command line.\nThe following example illustrates how to open a bash shell on a running Administration Server pod named domain1-admin-server and execute the scriptAction.sh script. It assumes that:\n The domain home is in /u01/oracle/user-projects/domains/domain1 (that is, the domain home is inside a Docker image). The Dockerfile copied scalingAction.sh to /u01/oracle/user-projects/domains/domain1/bin/scripts/scalingAction.sh.  \u0026gt; kubectl exec -it domain1-admin-server /bin/bash # bash\u0026gt; cd /u01/oracle/user-projects/domains/domain1/bin/scripts # bash\u0026gt; ./scalingAction.sh A log, scalingAction.log, will be generated in the same directory in which the script was executed and can be examined for errors.\nExample on accessing the external REST endpoint The easiest way to test scaling using the external REST endpoint is to use a command-line tool like curl. Using curl to issue an HTTPS scale request requires these mandatory header properties:\n Bearer Authorization token SSL certificate for the operator\u0026rsquo;s external REST endpoint X-Requested-By header value  The following shell script is an example of how to issue a scaling request, with the necessary HTTP request header values, using curl. This example assumes the operator and domain resource are configured with the following properties in Kubernetes:\n Operator properties:  externalRestEnabled: true externalRestHttpsPort: 31001 operator\u0026rsquo;s namespace: weblogic-operator operator\u0026rsquo;s hostname is the same as the host shell script is executed on.   Domain resource properties:  WebLogic cluster name: DockerCluster Domain UID: domain1    #!/bin/sh # Setup properties ophost=`uname -n` opport=31001 #externalRestHttpsPort cluster=cluster-1 size=3 #New cluster size ns=weblogic-operator # Operator NameSpace sa=weblogic-operator # Operator ServiceAccount domainuid=domain1 # Retrieve service account name for given namespace sec=`kubectl get serviceaccount ${sa} -n ${ns} -o jsonpath='{.secrets[0].name}'` #echo \u0026quot;Secret [${sec}]\u0026quot; # Retrieve base64 encoded secret for the given service account enc_token=`kubectl get secret ${sec} -n ${ns} -o jsonpath='{.data.token}'` #echo \u0026quot;enc_token [${enc_token}]\u0026quot; # Decode the base64 encoded token token=`echo ${enc_token} | base64 --decode` #echo \u0026quot;token [${token}]\u0026quot; # clean up any temporary files rm -rf operator.rest.response.body operator.rest.stderr operator.cert.pem # Retrieve SSL certificate from the Operator's external REST endpoint `openssl s_client -showcerts -connect ${ophost}:${opport} \u0026lt;/dev/null 2\u0026gt;/dev/null | openssl x509 -outform PEM \u0026gt; operator.cert.pem` echo \u0026quot;Rest EndPoint url https://${ophost}:${opport}/operator/v1/domains/${domainuid}/clusters/${cluster}/scale\u0026quot; # Issue 'curl' request to external REST endpoint curl --noproxy '*' -v --cacert operator.cert.pem \\ -H \u0026quot;Authorization: Bearer ${token}\u0026quot; \\ -H Accept:application/json \\ -H \u0026quot;Content-Type:application/json\u0026quot; \\ -H \u0026quot;X-Requested-By:WLDF\u0026quot; \\ -d \u0026quot;{\\\u0026quot;managedServerCount\\\u0026quot;: $size}\u0026quot; \\ -X POST https://${ophost}:${opport}/operator/v1/domains/${domainuid}/clusters/${cluster}/scale \\ -o operator.rest.response.body \\ --stderr operator.rest.stderr "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/persistent-storage/",
	"title": "Persistent storage",
	"tags": [],
	"description": "",
	"content": "This document outlines how to set up a Kubernetes PersistentVolume and PersistentVolumeClaim which can be used as storage for WebLogic domain homes and log files. A PersistentVolume can be shared by multiple WebLogic domains or dedicated to a particular domain.\nPrerequisites The following prerequisites must be fulfilled before proceeding with the creation of the volume:\n Create a Kubernetes Namespace for the PersistentVolumeClaim unless the intention is to use the default namespace. Note that a PersistentVolumeClaim has to be in the same namespace as the domain resource that uses it. Make sure that all the servers in the WebLogic domain are able to reach the storage location. Make sure that the host directory that will be used, already exists and has the appropriate file permissions set.  Storage locations PersistentVolumes can point to different storage locations, for example NFS servers or a local directory path. The list of available options is listed in the Kubernetes documentation.\nNote regarding HostPath: In a single-node Kubernetes cluster, such as may be used for testing or proof of concept activities, HOST_PATH provides the simplest configuration. In a multinode Kubernetes cluster, a HOST_PATH that is located on shared storage mounted by all nodes in the Kubernetes cluster is the simplest configuration. If nodes do not have shared storage, then NFS is probably the most widely available option. There are other options listed in the referenced table.\nThe PersistentVolume for the domain must be created using the appropriate tools before running the script to create the domain. In the simplest case, namely the HOST_PATH provider, this means creating a directory on the Kubernetes master and ensuring that it has the correct permissions:\n$ mkdir -m 777 -p /path/to/domain1PersistentVolume Note regarding NFS: In the current GA version, the OCI Container Engine for Kubernetes supports network block storage that can be shared across nodes with access permission RWOnce (meaning that only one can write, others can read only). At this time, the WebLogic on Kubernetes domain created by the WebLogic Server Kubernetes Operator, requires a shared file system to store the WebLogic domain configuration, which MUST be accessible from all the pods across the nodes. As a workaround, you need to install an NFS server on one node and share the file system across all the nodes.\nCurrently, we recommend that you use NFS version 3.0 for running WebLogic Server on OCI Container Engine for Kubernetes. During certification, we found that when using NFS 4.0, the servers in the WebLogic domain went into a failed state intermittently. Because multiple threads use NFS (default store, diagnostics store, Node Manager, logging, and domain_home), there are issues when accessing the file store. These issues are removed by changing the NFS to version 3.0.\nPersistentVolume GID annotation The HOST_PATH directory permissions can be made more secure by using a Kubernetes annotation on the PersistentVolume that provides the group identifier (GID) which will be added to pods using the PersistentVolume.\nFor example, if the GID of the directory is 6789, then the directory can be updated to remove permissions other than for the user and group along with the PersistentVolume being annotated with the specified GID:\n$ chmod 770 /path/to/domain1PersistentVolume $ kubectl annotate pv domain1-weblogic-sample-pv pv.beta.kubernetes.io/gid=6789 After the domain is created and servers are running, the group ownership of the PersistentVolume files can be updated to the specified GID which will provide read access to the group members. Typically, files created from a pod onto the PersistentVolume will have UID 1000 and GID 1000 which is the oracle user from the WebLogic Docker image.\nAn example of updating the group ownership on the PersistentVolume would be as follows:\n$ cd /path/to/domain1PersistentVolume $ sudo chgrp 6789 applications domains logs stores $ sudo chgrp -R 6789 domains/ $ sudo chgrp -R 6789 logs/ YAML files Persistent volumes and claims are described in YAML files. For each PersistentVolume, you should create one PersistentVolume YAML file and one PersistentVolumeClaim YAML file. In the example below, you will find two YAML templates, one for the volume and one for the claim. As stated above, they either can be dedicated to a specific domain, or shared across multiple domains. For the use cases where a volume will be dedicated to a particular domain, it is a best practice to label it with weblogic.domainUID=[domain name]. This makes it easy to search for, and clean up resources associated with that particular domain.\nFor sample YAML templates, refer to the PersistentVolumes example.\nKubernetes resources After you have written your YAML files, use them to create the PersistentVolume by creating Kubernetes resources using the kubectl create -f command:\n$ kubectl create -f pv.yaml $ kubectl create -f pvc.yaml Verify the results To confirm that the PersistentVolume was created, use these commands:\n$ kubectl describe pv [persistent volume name] $ kubectl describe pvc -n NAMESPACE [persistent volume claim name] Common problems This section provides details of common problems that might occur while running the script and how to resolve them.\nPersistentVolume provider not configured correctly Possibly the most common problem experienced during testing was the incorrect configuration of the PersistentVolume provider. The PersistentVolume must be accessible to all Kubernetes Nodes, and must be able to be mounted as Read/Write/Many. If this is not the case, the PersistentVolume creation will fail.\nThe simplest case is where the HOST_PATH provider is used. This can be either with one Kubernetes Node, or with the HOST_PATH residing in shared storage available at the same location on every node (for example, on an NFS mount). In this case, the path used for the PersistentVolume must have its permission bits set to 777.\nFurther reading  See the blog, How to run WebLogic clusters on the Oracle Cloud Infrastructure Container Engine for Kubernetes.  "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-operators/",
	"title": "Manage operators",
	"tags": [],
	"description": "Helm is used to create and deploy necessary operator resources and to run the operator in a Kubernetes cluster. Use the operator&#39;s Helm chart to install and manage the operator.",
	"content": "Overview Helm is a framework that helps you manage Kubernetes applications, and Helm charts help you define and install Helm applications into a Kubernetes cluster. The operator\u0026rsquo;s Helm chart is located in the kubernetes/charts/weblogic-operator directory.\nImportant note for users of operator releases before 2.0\nIf you have an older version of the operator installed on your cluster, for example, a 1.x version or one of the 2.0 release candidates, then you must remove it before installing this version. This includes the 2.0-rc1 version; it must be completely removed. You should remove the deployment (for example, kubectl delete deploy weblogic-operator -n your-namespace) and the custom resource definition (for example, kubectl delete crd domain). If you do not remove the custom resource definition, then you might see errors like this:\nError from server (BadRequest): error when creating \u0026quot;/scratch/output/uidomain/weblogic-domains/uidomain/domain.yaml\u0026quot;: the API version in the data (weblogic.oracle/v2) does not match the expected API version (weblogic.oracle/v1  Install Helm and Tiller Helm has two parts: a client (Helm) and a server (Tiller). Tiller runs inside of your Kubernetes cluster, and manages releases (installations) of your charts. For detailed instructions on installing Helm and Tiller, see https://github.com/helm/helm.\nIn order to use Helm to install and manage the operator, you need to ensure that the service account that Tiller uses has the cluster-admin role. The default would be default in namespace kube-system. You can give that service account the necessary permissions with this command:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: helm-user-cluster-admin-role roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: default namespace: kube-system EOF  Oracle strongly recommends that you create a new service account to be used exclusively by Tiller and grant cluster-admin to that service account, rather than using the default one.\n Operator\u0026rsquo;s Helm Chart Configuration The operator Helm chart is pre-configured with default values for the configuration of the operator.\nYou can override these values by doing one of the following:\n Creating a custom YAML file with only the values to be overridden, and specifying the --value option on the Helm command line. Overriding individual values directly on the Helm command line, using the --set option.  You can find out the configuration values that the Helm chart supports, as well as the default values, using this command:\n$ helm inspect values kubernetes/charts/weblogic-operator The available configuration values are explained by category in Operator Helm configuration values.\nHelm commands are explained in more detail in Useful Helm operations.\nOptional: Configure the operator\u0026rsquo;s external REST HTTPS interface The operator can expose an external REST HTTPS interface which can be accessed from outside the Kubernetes cluster. As with the operator\u0026rsquo;s internal REST interface, the external REST interface requires an SSL/TLS certificate and private key that the operator will use as the identity of the external REST interface (see below).\nTo enable the external REST interface, configure these values in a custom configuration file, or on the Helm command line:\n Set externalRestEnabled to true. Set externalRestIdentitySecret to the name of the Kubernetes tls secret that contains the certificates and private key. Optionally, set externalRestHttpsPort to the external port number for the operator REST interface (defaults to 31001).  For more detailed information, see the REST interface configuration values.\nSample SSL certificate and private key for the REST interface For testing purposes, the WebLogic Server Kubernetes Operator project provides a sample script that generates a self-signed certificate and private key for the operator external REST interface. The generated certificate and key are stored in a Kubernetes tls secret and the sample script outputs the corresponding configuration values in YAML format. These values can be added to your custom YAML configuration file, for use when the operator\u0026rsquo;s Helm chart is installed.\nThe sample script should not be used in a production environment because typically a self-signed certificate for external communication is not considered safe. A certificate signed by a commercial certificate authority is more widely accepted and should contain valid host names, expiration dates, and key constraints.\n For more detailed information about the sample script and how to run it, see the REST APIs.\nOptional: Elastic Stack (Elasticsearch, Logstash, and Kibana) integration The operator Helm chart includes the option of installing the necessary Kubernetes resources for Elastic Stack integration.\nYou are responsible for configuring Kibana and Elasticsearch, then configuring the operator Helm chart to send events to Elasticsearch. In turn, the operator Helm chart configures Logstash in the operator deployment to send the operator\u0026rsquo;s log contents to that Elasticsearch location.\nElastic Stack per-operator configuration As part of the Elastic Stack integration, Logstash configuration occurs for each deployed operator instance. You can use the following configuration values to configure the integration:\n Set elkIntegrationEnabled is true to enable the integration. Set logStashImage to override the default version of Logstash to be used (logstash:6.2). Set elasticSearchHost and elasticSearchPort to override the default location where Elasticsearch is running (elasticsearch2.default.svc.cluster.local:9201). This will configure Logstash to send the operator\u0026rsquo;s log contents there.  For more detailed information, see the Operator Helm configuration values.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/overview/database/",
	"title": "Run a database",
	"tags": [],
	"description": "",
	"content": "Run the Oracle database in Kubernetes If you want to run the Oracle database inside your Kubernetes cluster, in order to place your state store, leasing tables, and such, in that database, then you can use this sample to install the database.\nThe Oracle Database Docker images are only supported for non-production use. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1)\n You must configure your database to store its DB files on persistent storage. Refer to your cloud vendor\u0026rsquo;s documentation for details of available storage providers and how to create a persistent volume and attach it to a pod.\nFirst, create a namespace for the database:\nkubectl create namespace database-namespace Next, create a file called database.yml with the following content. Make sure you update the password field with your chosen administrator password for the database.\napiVersion: extensions/v1beta1 kind: Deployment metadata: name: database namespace: database-namespace labels: app: database version: 12.1.0.2 spec: replicas: 1 selector: matchLabels: app: database version: 12.1.0.2 template: metadata: name: database labels: app: database version: 12.1.0.2 spec: volumes: - name: dshm emptyDir: medium: Memory # add your volume mount for your persistent storage here containers: - name: database command: - /home/oracle/setup/dockerInit.sh image: container-registry.oracle.com/database/enterprise:12.1.0.2 imagePullPolicy: IfNotPresent resources: requests: memory: 10Gi ports: - containerPort: 1521 hostPort: 1521 volumeMounts: - mountPath: /dev/shm name: dshm # add your persistent storage for DB files here env: - name: DB_SID value: OraDoc - name: DB_PDB value: OraPdb - name: DB_PASSWD value: *password* - name: DB_DOMAIN value: my.domain.com - name: DB_BUNDLE value: basic - name: DB_MEMORY value: 8g imagePullSecrets: - name: regsecret --- apiVersion: v1 kind: Service metadata: name: database namespace: database-namespace spec: selector: app: database version: 12.1.0.2 ports: - protocol: TCP port: 1521 targetPort: 1521 If you have not previously done so, you will need to go to the Oracle Container Registry and accept the license for the Oracle database image.\nCreate a Docker registry secret so that Kubernetes can pull the database image:\nkubectl create secret docker-registry regsecret \\ --docker-server=container-registry.oracle.com \\ --docker-username=your.email@some.com \\ --docker-password=your-password \\ --docker-email=your.email@some.com \\ -n database-namespace Now, use the following command to install the database:\nkubectl apply -f database.yml This will start up the database and expose it in the cluster at the following address:\ndatabase.database-namespace.svc.cluster.local:1521 "
},
{
	"uri": "/weblogic-kubernetes-operator/reference/domain-resource/",
	"title": "Domain resource",
	"tags": [],
	"description": "Use this document to set up and configure your own domain resource.",
	"content": "View the domain resource reference document here.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/quickstart/get-images/",
	"title": "Get images",
	"tags": [],
	"description": "",
	"content": "Get these images and put them into your local registry.   If you don\u0026rsquo;t already have one, obtain a Docker store account, log in to the Docker store, and accept the license agreement for the WebLogic Server image.\n  Log in to the Docker store from your Docker client:\n$ docker login   Pull the operator image:\n$ docker pull oracle/weblogic-kubernetes-operator:2.5.0  If you are here because you are following the Model In Image sample, change the image to oracle/weblogic-kubernetes-operator:3.0.0-rc1 in the previous command.\n   Pull the Traefik load balancer image:\n$ docker pull traefik:1.7.12   Obtain the WebLogic image from the Oracle Container Registry.\na. First time users, follow these directions.\nb. Find and then pull the WebLogic 12.2.1.4 install image:\n$ docker pull container-registry.oracle.com/middleware/weblogic:12.2.1.4  The WebLogic Docker image, weblogic:12.2.1.3, has all the necessary patches applied. The WebLogic Docker image, weblogic:12.2.1.4, does not require any additional patches.\n   Copy the image to all the nodes in your cluster, or put it in a Docker registry that your cluster can access.\n  "
},
{
	"uri": "/weblogic-kubernetes-operator/samples/",
	"title": "Samples",
	"tags": [],
	"description": "",
	"content": "The samples provide demonstrations of how to accomplish common tasks. These samples are provided for educational and demonstration purposes only; they are not intended to be used in production deployments or to be depended upon to create production environments.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/elastic-stack/soa-domain/weblogic-logging-exporter-setup/",
	"title": "Publish logs to Elasticsearch",
	"tags": [],
	"description": "Use the WebLogic Logging Exporter to publish the WebLogic Server logs to Elasticsearch.",
	"content": "The WebLogic Logging Exporter adds a log event handler to WebLogic Server. WebLogic Server logs can be pushed to Elasticsearch in Kubernetes directly by using the Elasticsearch REST API. For more details, refer to the WebLogic Logging Exporter project.\nThis sample shows you how to publish WebLogic Server logs to Elasticsearch and view them in Kibana. For publishing operator logs, see this sample.\nPrerequisites This document assumes that you have already set up Elasticsearch and Kibana for logs collection. If you have not, please refer this document.\n Download the WebLogic Logging Exporter binaries The pre-built binaries are available on the WebLogic Logging Exporter Releases page.\nDownload:\n weblogic-logging-exporter-0.1.1.jar from the Releases page. snakeyaml-1.23.jar from Maven Central.  These identifiers are used in the sample commands in this document.\n soans: SOA domain namespace soainfra: domainUID soainfra-adminserver: Administration Server pod name   Copy the JAR files to the WebLogic domain home Copy the weblogic-logging-exporter-0.1.1.jar and snakeyaml-1.23.jar files to the domain home directory in the Administration Server pod.\n$ kubectl cp \u0026lt;file-to-copy\u0026gt; \u0026lt;namespace\u0026gt;/\u0026lt;Administration-Server-pod\u0026gt;:\u0026lt;domainhome\u0026gt; $ kubectl cp snakeyaml-1.23.jar soans/soainfra-adminserver:/u01/oracle/user_projects/domains/soainfra/ $ kubectl cp weblogic-logging-exporter-0.1.1.jar soans/soainfra-adminserver:/u01/oracle/user_projects/domains/soainfra/ Add a startup class to the domain configuration   Using the WebLogic Server Administration Console, in the left-side navigation pane, expand Environment, and then select Startup and Shutdown Classes.\n  Add a new startup class. You may choose any descriptive name, however, the class name must be weblogic.logging.exporter.Startup.\n  Target the startup class to each server from which you want to export logs.\n  You can verify this step by looking for the update in your config.xml file, which should look similar to the following example:\n\u0026lt;startup-class\u0026gt; \u0026lt;name\u0026gt;weblogic-logging-exporter\u0026lt;/name\u0026gt; \u0026lt;target\u0026gt;AdminServer,soa_cluster\u0026lt;/target\u0026gt; \u0026lt;class-name\u0026gt;weblogic.logging.exporter.Startup\u0026lt;/class-name\u0026gt; \u0026lt;/startup-class\u0026gt; Update the WebLogic Server CLASSPATH Copy the setDomainEnv.sh file from the pod to a local folder.\n$ kubectl cp soans/soainfra-adminserver:/u01/oracle/user_projects/domains/soainfra/bin/setDomainEnv.sh . Modify setDomainEnv.sh to update the server class path.\nCLASSPATH=/u01/oracle/user_projects/domains/soainfra/weblogic-logging-exporter-0.1.1.jar:/u01/oracle/user_projects/domains/soainfra/snakeyaml-1.23.jar:${CLASSPATH} export CLASSPATH Copy back the modified setDomainEnv.sh file to the pod.\n$ kubectl cp setDomainEnv.sh soans/soainfra-adminserver:/u01/oracle/user_projects/domains/soainfra/bin/setDomainEnv.sh Create a configuration file for the WebLogic Logging Exporter Create a file named WebLogicLoggingExporter.yaml. Specify the Elasticsearch server host and port number.\nweblogicLoggingIndexName: wls publishHost: elasticsearch.default.svc.cluster.local publishPort: 9200 domainUID: soainfra weblogicLoggingExporterEnabled: true weblogicLoggingExporterSeverity: TRACE weblogicLoggingExporterBulkSize: 1 Copy the WebLogicLoggingExporter.yaml file to the domain home directory in the WebLogic Server Administration Server pod.\n$ kubectl cp WebLogicLoggingExporter.yaml soans/soainfra-adminserver:/u01/oracle/user_projects/domains/soainfra/config/ Restart all the servers in the domain To restart the servers, stop and then start them using the following commands.\nTo stop the servers:\n$ kubectl patch domain soainfra -n soans --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/serverStartPolicy\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;NEVER\u0026quot; }]' To start the servers:\n$ kubectl patch domain soainfra -n soans --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/serverStartPolicy\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;IF_NEEDED\u0026quot; }]' After all the servers are restarted, in all the server logs, you will see the weblogic-logging-exporter class being called, as shown below.\n======================= WebLogic Logging Exporter Startup class called Reading configuration from file name: /u01/oracle/user_projects/domains/soainfra/config/WebLogicLoggingExporter.yaml Config{weblogicLoggingIndexName='wls', publishHost='domain.host.com', publishPort=9200, weblogicLoggingExporterSeverity='Notice', weblogicLoggingExporterBulkSize='2', enabled=true, weblogicLoggingExporterFilters=FilterConfig{expression='NOT(MSGID = 'BEA-000449')', servers=[]}], domainUID='soainfra'} Create an index pattern in Kibana Create an index pattern wls* in Kibana \u0026gt; Management. After the servers are started, you will see the log data in the Kibana dashboard.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/cicd/mutate-the-domain-layer/",
	"title": "Mutate the domain layer",
	"tags": [],
	"description": "How to mutate the domain layer.",
	"content": "If you need to mutate the domain layer, and keep the same domain encryption keys, then there are some choices about how to implement that, as alluded to previously. Let\u0026rsquo;s explore those in some more detail now.\nThe first option is to implement each mutation as a delta to the previous state. This is conceptually similar to how immutable objects (like Java Strings) are implemented, a \u0026ldquo;copy on write\u0026rdquo; approach applied to the domain configuration as a unit. This does have the advantage that it is simple to implement, but the disadvantage that your builds would depend on the previous good build and this is somewhat contrary to typical CI/CD practices. You also have to work out what to do with the bad builds, or \u0026ldquo;holes\u0026rdquo; in the sequence.\nAn alternative is to capture a \u0026ldquo;primordial state\u0026rdquo; of the domain before starting the sequence. In practical terms, this might mean creating a very simple domain with no applications or resources in it, and \u0026ldquo;saving\u0026rdquo; it before ever starting any servers. This primordial domain (let’s call it t=0) would then be used to build each mutation. So each state is built from t=0, plus all of the changes up to that point.\nSaid another way, each build would start with t=0 as the base image and extend it. This eliminates the need to keep each intermediate state, and would also likely have benefits when you remove things from the domain, because you would not have \u0026ldquo;lost\u0026rdquo; (\u0026ldquo;whited out\u0026rdquo; is the Docker layer term) space in the intermediate layers. Although, these layers tend to be relatively small, so this is possibly not a big issue.\nThis approach is probably an improvement. It does get interesting though when you update a lower layer, for example when you patch WebLogic or update the JDK. When this happens, you need to create another base image, shown in the diagram as v2 t-0. All of the mutations in this new chain are based on this new base image. So that still leaves us with the problem of how to take the domain from the first series (v1 t=0 to t=3) and \u0026ldquo;copy\u0026rdquo; it across to the second series (v2).\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/introduction/design/",
	"title": "Design philosophy",
	"tags": [],
	"description": "The Oracle WebLogic Server Kubernetes Operator (the “operator”) is designed to fulfill a similar role to that which a human operator would fill in a traditional data center deployment.  It contains a set of useful built-in knowledge about how to perform various life cycle operations on a domain correctly.",
	"content": "The Oracle WebLogic Server Kubernetes Operator (the “operator”) is designed to fulfill a similar role to that which a human operator would fill in a traditional data center deployment. It contains a set of useful built-in knowledge about how to perform various life cycle operations on a domain correctly.\nHuman operators are typically responsible for starting and stopping environments, initiating backups, performing scaling operations, performing manual tasks associated with disaster recovery and high availability needs and coordinating actions with other operators in other data centers. It is envisaged that the operator will have similar responsibilities in a Kubernetes environment.\nIt is important to note the distinction between an operator and an administrator. A WebLogic Server administrator typically has different responsibilities centered around managing the detailed configuration of the WebLogic domains. The operator has only limited interest in the domain configuration, with its main concern being the high-level topology of the domain; for example, how many clusters and servers, and information about network access points, such as channels.\nHuman operators may manage more than one domain, and the operator is also designed to be able to manage more than one domain. Like its human counterpart, the operator will only take actions against domains that it is told to manage, and will ignore any other domains that may be present in the same environment.\nLike a human operator, the operator is designed to be event-based. It waits for a significant event to occur, or for a scheduled time to perform some action, and then takes the appropriate action. Examples of significant events include being made aware of a new domain that needs to be managed, receiving a request to scale up a WebLogic cluster, or receiving a request to perform a backup of a domain.\nThe operator is designed with security in mind from the outset. Some examples of the specific security practices we follow are:\n During the deployment of the operator, Kubernetes Roles are defined and assigned to the operator. These roles are designed to give the operator the minimum amount of privileges that it requires to perform its tasks. The code base is regularly scanned with security auditing tools and any issues that are identified are promptly resolved. All HTTP communications – between the operator and an external client, between the operator and WebLogic Server Administration Servers, and so on – are configured to require SSL and TLS 1.2. Unused code is pruned from the code base regularly. Dependencies are kept as up-to-date as possible and are regularly reviewed for security vulnerabilities.  The operator is designed to avoid imposing any arbitrary restriction on how WebLogic Server may be configured or used in Kubernetes. Where there are restrictions, these are based on the availability of some specific feature in Kubernetes; for example, multicast support.\nThe operator learns of WebLogic domains through instances of a domain Kubernetes resource. When the operator is installed, it creates a Kubernetes Custom Resource Definition. This custom resource definition defines the domain resource type. After this type is defined, you can manage domain resources using kubectl just like any other resource type. For instance, kubectl get domain or kubectl edit domain domain1.\nSchema for domain resources is here.\nThe schema for the domain resource is designed to be as sparse as possible. It includes the connection details for the Administration Server, but all of the other content is operational details about which servers should be started, environment variables, and details about what should be exposed outside the Kubernetes cluster. This way, the WebLogic domain\u0026rsquo;s configuration remains the normative configuration.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/security/service-accounts/",
	"title": "Service accounts",
	"tags": [],
	"description": "Kubernetes ServiceAccounts for the operator",
	"content": "WebLogic Server Kubernetes Operator ServiceAccounts When the operator is installed, the Helm chart property, serviceAccount, can be specified where the value contains the name of the Kubernetes ServiceAccount in the namespace in which the operator will be installed. For more information about the Helm chart, see the Operator Helm configuration values.\nThe operator will use this ServiceAccount when calling the Kubernetes API server and the appropriate access controls will be created for this ServiceAccount by the operator\u0026rsquo;s Helm chart.\nFor more information about access controls, see RBAC.\n If the operator\u0026rsquo;s service account cannot have the privileges to access the cluster-level resources, such as CustomResourceDefinitions, Namespaces and PersistentVolumes, consider using a dedicated namespace for each operator and the domains that the operator manages. See the dedicated setting in Operator Helm configuration values.\n In order to display the ServiceAccount used by the operator, where the operator was installed using the Helm release name weblogic-operator, look for the serviceAccount value using the Helm command:\n$ helm get values --all weblogic-operator Additional reading  Helm service account Operator Helm chart service account configuration  "
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/domains/model-in-image/",
	"title": "Model in image",
	"tags": [],
	"description": "Sample for supplying a WebLogic Deploy Tooling (WDT) model that the operator expands into a full domain home during runtime.",
	"content": " This feature is supported only in 3.0.0-rc1.\n Contents  Introduction  Model in Image domain types (WLS, JRF, and Restricted JRF) Use cases Sample directory structure   Prerequisites for all domain types Additional prerequisites for JRF domains Initial use case: An initial WebLogic domain Update1 use case: Dynamically adding a data source using a model ConfigMap Cleanup References  Introduction This sample demonstrates deploying a Model in Image domain home source type. Unlike Domain in PV and Domain in Image, Model in Image eliminates the need to pre-create your WebLogic domain home prior to deploying your domain resource. Instead, Model in Image uses a WebLogic Deploy Tooling (WDT) model to specify your WebLogic configuration.\nWDT models are a convenient and simple alternative to WebLogic WLST configuration scripts and templates. They compactly define a WebLogic domain using YAML files and support including application archives in a ZIP file. The WDT model format is described in the open source, WebLogic Deploy Tooling GitHub project, and the required directory structure for a WDT archive is specifically discussed here.\nFor more information on Model in Image, see the Model in Image user guide. For a comparison of Model in Image to other domain home source types, see Choose a domain home source type.\nModel in Image domain types (WLS, JRF, and Restricted JRF) There are three types of domains supported by Model in Image: a standard WLS domain, an Oracle Fusion Middleware Infrastructure Java Required Files (JRF) domain, and a RestrictedJRF domain. This sample demonstrates the WLS and JRF types.\nThe JRF domain path through the sample includes additional steps required for JRF: deploying an infrastructure database, initializing the database using the Repository Creation Utility (RCU) tool, referencing the infrastructure database from the WebLogic configuration, setting an Oracle Platform Security Services (OPSS) wallet password, and exporting/importing an OPSS wallet file. JRF domains may be used by Oracle products that layer on top of WebLogic Server, such as SOA and OSB. Similarly, RestrictedJRF domains may be used by Oracle layered products, such as Oracle Communications products.\nUse cases This sample demonstrates two Model in Image use cases:\n  Initial: An initial WebLogic domain with the following characteristics:\n Image model-in-image:WLS-v1 with:  A WebLogic installation A WebLogic Deploy Tooling (WDT) installation A WDT archive with version v1 of an exploded Java EE web application A WDT model with:  A WebLogic Administration Server A WebLogic cluster A reference to the web application     Kubernetes Secrets:  WebLogic credentials Required WDT runtime password   A domain resource with:  spec.domainHomeSourceType: FromModel spec.image: model-in-image:WLS-v1 References to the secrets      Update1: Demonstrates udpating the initial domain by dynamically adding a data source using a model ConfigMap:\n Image model-in-image:WLS-v1:  Same image as Initial use case   Kubernetes Secrets:  Same as Initial use case plus secrets for data source credentials and URL   Kubernetes ConfigMap with:  A WDT model for a data source targeted to the cluster   A domain resource with:  Same as Initial use case plus:  spec.model.configMap referencing the ConfigMap References to data source secrets        Sample directory structure The sample contains the following files and directories:\n   Location Description     domain-resources JRF and WLS domain resources.   archives Source code location for WebLogic Deploy Tooling application ZIP archives.   model-images Staging for each model image\u0026rsquo;s WDT YAML, WDT properties, and WDT archive ZIP files. The directories in model images are named for their respective images.   model-configmaps Staging files for a model ConfigMap that configures a data source.   ingresses Ingress resources.   utils/wl-pod-wait.sh Utility for watching the pods in a domain reach their expected restartVersion, image name, and ready state.   utils/patch-restart-version.sh Utility for updating a running domain spec.restartVersion field (which causes it to \u0026lsquo;re-instrospect\u0026rsquo; and \u0026lsquo;roll\u0026rsquo;).   utils/opss-wallet.sh Utility for exporting or importing a JRF domain OPSS wallet file.    Prerequisites for all domain types   Choose the type of domain you\u0026rsquo;re going to use throughout the sample, WLS or JRF.\n The first time you try this sample, we recommend that you choose WLS even if you\u0026rsquo;re familiar with JRF. This is because WLS is simpler and will more easily familiarize you with Model in Image concepts. We recommend choosing JRF only if you are already familiar with JRF, you have already tried the WLS path through this sample, and you have a definite use case where you need to use JRF.    The JAVA_HOME environment variable must be set and must reference a valid JDK 8 or 11 installation.\n  Get the operator source from the release/3.0.0-rc1 branch and put it in /tmp/operator-source.\nFor example:\n$ mkdir /tmp/operator-source $ cd /tmp/operator-source $ git clone https://github.com/oracle/weblogic-kubernetes-operator.git $ git checkout release/3.0.0-rc1  Note: We will refer to the top directory of the operator source tree as /tmp/operator-source; however, you can use a different location.\n For additional information about obtaining the operator source, see the Developer Guide Requirements.\n  Copy the sample to a new directory; for example, use directory /tmp/mii-sample.\n$ mkdir /tmp/mii-sample $ cp -r /tmp/operator-source/kubernetes/samples/scripts/create-weblogic-domain/model-in-image/* /tmp/mii-sample  Note: We will refer to this working copy of the sample as /tmp/mii-sample; however, you can use a different location.     Make sure an operator is set up to manage namespace sample-domain1-ns. Also, make sure a Traefik ingress controller is managing the same namespace and listening on port 30305.\nFor example, follow the same steps as the Quick Start guide from the beginning through to the Prepare for a domain step.\nMake sure you stop when you complete the \u0026ldquo;Prepare for a domain\u0026rdquo; step and then resume following these instructions.\n   Set up ingresses that will redirect HTTP from Traefik port 30305 to the clusters in this sample\u0026rsquo;s WebLogic domains.\n  Option 1: To create the ingresses, use the following YAML to create a file called /tmp/mii-sample/ingresses/myingresses.yaml and then call kubectl apply -f /tmp/mii-sample/ingresses/myingresses.yaml:\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: traefik-ingress-sample-domain1-admin-server namespace: sample-domain1-ns labels: weblogic.domainUID: sample-domain1 annotations: kubernetes.io/ingress.class: traefik spec: rules: - host: http: paths: - path: /console backend: serviceName: sample-domain1-admin-server servicePort: 7001 --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: traefik-ingress-sample-domain1-cluster-cluster-1 namespace: sample-domain1-ns labels: weblogic.domainUID: sample-domain1 annotations: kubernetes.io/ingress.class: traefik spec: rules: - host: sample-domain1-cluster-cluster-1.mii-sample.org http: paths: - path: backend: serviceName: sample-domain1-cluster-cluster-1 servicePort: 8001 --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: traefik-ingress-sample-domain1-cluster-cluster-2 namespace: sample-domain1-ns labels: weblogic.domainUID: sample-domain1 annotations: kubernetes.io/ingress.class: traefik spec: rules: - host: sample-domain1-cluster-cluster-2.mii-sample.org http: paths: - path: backend: serviceName: sample-domain1-cluster-cluster-2 servicePort: 8001 --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: traefik-ingress-sample-domain2-cluster-cluster-1 namespace: sample-domain1-ns labels: weblogic.domainUID: sample-domain2 annotations: kubernetes.io/ingress.class: traefik spec: rules: - host: sample-domain2-cluster-cluster-1.mii-sample.org http: paths: - path: backend: serviceName: sample-domain2-cluster-cluster-1 servicePort: 8001   Option 2: Run kubectl apply -f on each of the ingress YAML files that are already included in the sample source /tmp/mii-sample/ingresses directory:\n  $ cd /tmp/mii-sample/ingresses $ kubectl apply -f traefik-ingress-sample-domain1-admin-server.yaml $ kubectl apply -f traefik-ingress-sample-domain1-cluster-cluster-1.yaml $ kubectl apply -f traefik-ingress-sample-domain1-cluster-cluster-2.yaml $ kubectl apply -f traefik-ingress-sample-domain2-cluster-cluster-1.yaml $ kubectl apply -f traefik-ingress-sample-domain2-cluster-cluster-2.yaml  NOTE: We give each cluster ingress a different host name that is decorated using both its operator domain UID and its cluster name. This makes each cluster uniquely addressable even when cluster names are the same across different clusters. When using curl to access the WebLogic domain through the ingress, you will need to supply a host name header that matches the host names in the ingress.\n For more on information ingresses and load balancers, see Ingress.\n  Obtain the WebLogic 12.2.1.4 image that is required to create the sample\u0026rsquo;s model images.\na. Use a browser to access Oracle Container Registry.\nb. Choose an image location: for JRF domains, select Middleware, then fmw-infrastructure; for WLS domains, select Middleware, then weblogic.\nc. Select Sign In and accept the license agreement.\nd. Use your terminal to log in to Docker locally: docker login container-registry.oracle.com.\ne. Later in this sample, when you run WebLogic Image Tool commands, the tool will use the image as a base image for creating model images. Specifically, the tool will implicitly call docker pull for one of the above licensed images as specified in the tool\u0026rsquo;s command line using the --fromImage parameter. For JRF, this sample specifies container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4, and for WLS, the sample specifies container-registry.oracle.com/middleware/weblogic:12.2.1.4.\nIf you prefer, you can create your own base image and then substitute this image name in the WebLogic Image Tool --fromImage parameter throughout this sample. See Preparing a Base Image.\n   Download the latest WebLogic Deploying Tooling and WebLogic Image Tool installer ZIP files to your /tmp/mii-sample/model-images directory.\nBoth WDT and WIT are required to create your Model in Image Docker images. Download the latest version of each tool\u0026rsquo;s installer ZIP file to the /tmp/mii-sample/model-images directory.\nFor example, visit the GitHub WebLogic Deploy Tooling Releses and WebLogic Image Tool Releases web pages to determine the latest release version for each, and then, assuming the version numbers are 1.8.0 and 1.8.4 respectively, call:\n$ curl -m 30 -fL https://github.com/oracle/weblogic-deploy-tooling/releases/download/weblogic-deploy-tooling-1.8.0/weblogic-deploy.zip \\ -o /tmp/mii-sample/model-images/weblogic-deploy.zip $ curl -m 30 -fL https://github.com/oracle/weblogic-image-tool/releases/download/release-1.8.4/imagetool.zip \\ -o /tmp/mii-sample/model-images/imagetool.zip   Set up the WebLogic Image Tool.\nRun the following commands:\n$ cd /tmp/mii-sample/model-images $ unzip imagetool.zip $ ./imagetool/bin/imagetool.sh cache addInstaller \\ --type wdt \\ --version latest \\ --path /tmp/mii-sample/model-images/weblogic-deploy.zip These steps will install WIT to the /tmp/mii-sample/model-images/imagetool directory, plus put a wdt_latest entry in the tool\u0026rsquo;s cache which points to the WDT ZIP installer. We will use WIT later in the sample for creating model images.\n  Additional prerequisites for JRF domains  NOTE: If you\u0026rsquo;re using a WLS domain type, skip this section and continue here.\n JRF Prerequisites Contents  Introduction to JRF setups Set up and initialize an infrastructure database Increase introspection job timeout Important considerations for RCU model attributes, domain resource attributes, and secrets  Introduction to JRF setups  NOTE: The requirements in this section are in addition to Prerequisites for all domain types.\n A JRF domain requires an infrastructure database, initializing this database with RCU, and configuring your domain to access this database. All of these steps must occur before you create your domain.\nSet up and initialize an infrastructure database A JRF domain requires an infrastructure database and also requires initializing this database with a schema and a set of tables. The following example shows how to set up a database and use the RCU tool to create the infrastructure schema for a JRF domain. The database is set up with the following attributes:\n   Attribute Value     database Kubernetes namespace default   database Kubernetes pod oracle-db   database image container-registry.oracle.com/database/enterprise:12.2.0.1-slim   database password Oradoc_db1   infrastructure schema prefix FMW1   infrastructure schema password Oradoc_db1   database URL oracle-db.default.svc.cluster.local:1521/devpdb.k8s      Ensure that you have access to the database image, and then create a deployment using it:\n  Use a browser to log in to https://container-registry.oracle.com, select database-\u0026gt;enterprise and accept the license agreement.\n  Get the database image:\n In the local shell, docker login container-registry.oracle.com. In the local shell, docker pull container-registry.oracle.com/database/enterprise:12.2.0.1-slim.    Use the sample script in /tmp/operator-source/kubernetes/samples/scripts/create-oracle-db-service to create an Oracle database running in the pod, oracle-db.\n$ cd /tmp/operator-source/kubernetes/samples/scripts/create-oracle-db-service $ start-db-service.sh This script will deploy a database in the default namespace with the connect string oracle-db.default.svc.cluster.local:1521/devpdb.k8s, and administration password Oradoc_db1.\nThis step is based on the steps documented in Run a Database.\nWARNING: The Oracle Database Docker images are supported only for non-production use. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1).\n    Use the sample script in /tmp/operator-source/kubernetes/samples/scripts/create-rcu-schema to create the RCU schema with the schema prefix FMW1.\nNote that this script assumes Oradoc_db1 is the DBA password, Oradoc_db1 is the schema password, and that the database URL is oracle-db.default.svc.cluster.local:1521/devpdb.k8s.\n$ cd /tmp/operator-source/kubernetes/samples/scripts/create-rcu-schema $ ./create-rcu-schema.sh -s FMW1 -i container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 NOTE: If you need to drop the repository, use this command:\n$ drop-rcu-schema.sh -s FMW1   Increase introspection job timeout The JRF domain home creation can take more time than the introspection job\u0026rsquo;s default timeout. You should increase the timeout for the introspection job. Use the configuration.introspectorJobActiveDeadlineSeconds in your domain resource to override the default with a value of at least 300 seconds (the default is 120 seconds). Note that the JRF versions of the domain resource files that are provided in /tmp/mii-sample/domain-resources already set this value.\nImportant considerations for RCU model attributes, domain resource attributes, and secrets To allow Model in Image to access the database and OPSS wallet, you must create an RCU access secret containing the database connect string, user name, and password that\u0026rsquo;s referenced from your model and an OPSS wallet password secret that\u0026rsquo;s referenced from your domain resource before deploying your domain. It\u0026rsquo;s also necessary to define an RCUDbInfo stanza in your model.\nThe sample includes examples of JRF models and domain resources in the /tmp/mii-sample/model-images and /tmp/mii-sample/domain-resources directories, and instructions in the following sections will describe setting up the RCU and OPSS secrets.\nWhen you follow the instructions later in this sample, avoid instructions that are WLS only, and substitute JRF for WLS in the corresponding model image tags and domain resource file names.\nFor example:\n  JRF domain resources in this sample have an opss.walletPasswordSecret field that references a secret named sample-domain1-opss-wallet-password-secret, with password=welcome1.\n  JRF image models in this sample have a domainInfo -\u0026gt; RCUDbInfo stanza that reference a sample-domain1-rcu-access secret with appropriate values for attributes rcu_prefix, rcu_schema_password, and rcu_db_conn_string for accessing the Oracle database that you deployed to the default namespace as one of the prerequisite steps.\n  Important considerations for reusing or sharing OPSS tables We do not recommend that most users share OPSS tables. Extreme caution is required when sharing OPSS tables between domains.\n When you successfully deploy your JRF domain resource for the first time, the introspector job will initialize the OPSS tables for the domain using the domainInfo -\u0026gt; RCUDbInfo stanza in the WDT model plus the configuration.opss.walletPasswordSecret specified in the domain resource. The job will also create a new domain home. Finally, the operator will also capture an OPSS wallet file from the new domain\u0026rsquo;s local directory and place this file in a new Kubernetes ConfigMap.\nThere are scenarios when the domain needs to be recreated between updates, such as when WebLogic credentials are changed, security roles defined in the WDT model have been changed, or you want to share the same infrastructure tables with different domains. In these scenarios, the operator needs the walletPasswordSecret as well as the OPSS wallet file, together with the exact information in domainInfo -\u0026gt; RCUDbInfo so that the domain can be recreated and access the same set of tables. Without the wallet file and wallet password, you will not be able to recreate a domain accessing the same set of tables, therefore we strongly recommend that you back up the wallet file.\nTo recover a domain\u0026rsquo;s OPSS tables between domain restarts or to share an OPSS schema between different domains, it is necessary to extract this wallet file from the domain\u0026rsquo;s automatically deployed introspector ConfigMap and save the OPSS wallet password secret that was used for the original domain. The wallet password and wallet file are needed again when you recreate the domain or share the database with other domains.\nTo save the wallet file, assuming that your namespace is sample-domain1-ns and your domain UID is sample-domain1:\n $ kubectl -n sample-domain1-ns \\ get configmap sample-domain1-weblogic-domain-introspect-cm \\ -o jsonpath='{.data.ewallet\\.p12}' \\ \u0026gt; ./ewallet.p12 Alternatively, you can save the file using the sample\u0026rsquo;s wallet utility:\n $ /tmp/mii-sample/utils/opss-wallet.sh -n sample-domain1-ns -d sample-domain1 -wf ./ewallet.p12 # For help: /tmp/mii-sample/utils/opss-wallet.sh -? Important! Back up your wallet file to a safe location that can be retrieved later.\nTo reuse the wallet file in subsequent redeployments or to share the domain\u0026rsquo;s OPSS tables between different domains:\n Load the saved wallet file into a secret with a key named walletFile (again, assuming that your domain UID is sample-domain1 and your namespace is sample-domain1-ns):   $ kubectl -n sample-domain1-ns create secret generic sample-domain1-opss-walletfile-secret \\ --from-file=walletFile=./ewallet.p12 $ kubectl -n sample-domain1-ns label secret sample-domain1-opss-walletfile-secret \\ weblogic.domainUID=`sample-domain1` Alternatively, use the sample\u0026rsquo;s wallet utility:\n $ /tmp/mii-sample/utils/opss-wallet.sh -n sample-domain1-ns -d sample-domain1 -wf ./ewallet.p12 -ws sample-domain1-opss-walletfile-secret # For help: /tmp/mii-sample/utils/opss-wallet.sh -? Modify your domain resource JRF YAML files to provide the wallet file secret name, for example:   configuration: opss: # Name of secret with walletPassword for extracting the wallet walletPasswordSecret: sample-domain1-opss-wallet-password-secret # Name of secret with walletFile containing base64 encoded opss wallet walletFileSecret: sample-domain1-opss-walletfile-secret  Note: The sample JRF domain resource files included in /tmp/mii-sample/domain-resources already have the above YAML stanza.\n Initial use case Contents  Overview Image creation  Image creation - Introduction Understanding our first archive Staging a ZIP file of the archive Staging model files Creating the image with WIT   Deploy resources  Deploy resources - Introduction Secrets Domain resource    Overview In this use case, we set up an initial WebLogic domain. This involves:\n A WDT archive ZIP file that contains your applications. A WDT model that describes your WebLogic configuration. A Docker image that contains your WDT model files and archive. Creating secrets for the domain. Creating a domain resource for the domain that references your secrets and image.  After the domain resource is deployed, the WebLogic operator will start an \u0026lsquo;introspector job\u0026rsquo; that converts your models into a WebLogic configuration, and then the operator will pass this configuration to each WebLogic Server in the domain.\nPerform the steps in Prerequisites for all domain types before performing the steps in this use case.\nIf you are taking the JRF path through the sample, then substitute JRF for WLS in your image names and directory paths. Also note that the JRF-v1 model YAML differs from the WLS-v1 YAML file (it contains an additional domainInfo -\u0026gt; RCUDbInfo stanza).\n Image creation - Introduction The goal of the initial use case \u0026lsquo;image creation\u0026rsquo; is to demonstrate using the WebLogic Image Tool to create an image named model-in-image:WLS-v1 from files that we will stage to /tmp/mii-sample/model-images/model-in-image:WLS-v1/. The staged files will contain a web application in a WDT archive, and WDT model configuration for a WebLogic Administration Server called admin-server and a WebLogic cluster called cluster-1.\nOverall, a Model in Image image must contain a WebLogic installation and also a WebLogic Deploy Tooling installation in its /u01/wdt/weblogic-deploy directory. In addition, if you have WDT model archive files, then the image must also contain these files in its /u01/wdt/models directory. Finally, an image may optionally also contain your WDT model YAML and properties files in the same /u01/wdt/models directory. If you do not specify WDT model YAML in your /u01/wdt/models directory, then the model YAML must be supplied dynamically using a Kubernetes ConfigMap that is referenced by your domain resource spec.model.configMap attribute. We will provide an example of using a model ConfigMap later in this sample.\nLet\u0026rsquo;s walk through the steps for creating the image model-in-image:WLS-v1:\n Understanding our first archive Staging a ZIP file of the archive Staging model files Creating the image with WIT  Understanding our first archive The sample includes a predefined archive directory in /tmp/mii-sample/archives/archive-v1 that we will use to create an archive ZIP file for the image.\nThe archive top directory, named wlsdeploy, contains a directory named applications, which includes an \u0026lsquo;exploded\u0026rsquo; sample JSP web application in the directory, myapp-v1. Three useful aspects to remember about WDT archives are:\n A model image can contain multiple WDT archives. WDT archives can contain multiple applications, libraries, and other components. WDT archives have a well defined directory structure, which always has wlsdeploy as the top directory.    If you are interested in the web application source, click here to see the JSP code.   \u0026lt;%-- Copyright (c) 2019, 2020, Oracle Corporation and/or its affiliates. --%\u0026gt; \u0026lt;%-- Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. --%\u0026gt; \u0026lt;%@ page import=\u0026quot;javax.naming.InitialContext\u0026quot; %\u0026gt; \u0026lt;%@ page import=\u0026quot;javax.management.*\u0026quot; %\u0026gt; \u0026lt;%@ page import=\u0026quot;java.io.*\u0026quot; %\u0026gt; \u0026lt;% InitialContext ic = null; try { ic = new InitialContext(); String srName=System.getProperty(\u0026quot;weblogic.Name\u0026quot;); String domainUID=System.getenv(\u0026quot;DOMAIN_UID\u0026quot;); String domainName=System.getenv(\u0026quot;CUSTOM_DOMAIN_NAME\u0026quot;); out.println(\u0026quot;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt;\u0026quot;); out.println(\u0026quot;*****************************************************************\u0026quot;); out.println(); out.println(\u0026quot;Hello World! This is version 'v1' of the mii-sample JSP web-app.\u0026quot;); out.println(); out.println(\u0026quot;Welcome to WebLogic server '\u0026quot; + srName + \u0026quot;'!\u0026quot;); out.println(); out.println(\u0026quot; domain UID = '\u0026quot; + domainUID +\u0026quot;'\u0026quot;); out.println(\u0026quot; domain name = '\u0026quot; + domainName +\u0026quot;'\u0026quot;); out.println(); MBeanServer mbs = (MBeanServer)ic.lookup(\u0026quot;java:comp/env/jmx/runtime\u0026quot;); // display the current server's cluster name Set\u0026lt;ObjectInstance\u0026gt; clusterRuntimes = mbs.queryMBeans(new ObjectName(\u0026quot;*:Type=ClusterRuntime,*\u0026quot;), null); out.println(\u0026quot;Found \u0026quot; + clusterRuntimes.size() + \u0026quot; local cluster runtime\u0026quot; + (String)((clusterRuntimes.size()!=1)?\u0026quot;s:\u0026quot;:\u0026quot;:\u0026quot;)); for (ObjectInstance clusterRuntime : clusterRuntimes) { String cName = (String)mbs.getAttribute(clusterRuntime.getObjectName(), \u0026quot;Name\u0026quot;); out.println(\u0026quot; Cluster '\u0026quot; + cName + \u0026quot;'\u0026quot;); } out.println(); // display local data sources ObjectName jdbcRuntime = new ObjectName(\u0026quot;com.bea:ServerRuntime=\u0026quot; + srName + \u0026quot;,Name=\u0026quot; + srName + \u0026quot;,Type=JDBCServiceRuntime\u0026quot;); ObjectName[] dataSources = (ObjectName[])mbs.getAttribute(jdbcRuntime, \u0026quot;JDBCDataSourceRuntimeMBeans\u0026quot;); out.println(\u0026quot;Found \u0026quot; + dataSources.length + \u0026quot; local data source\u0026quot; + (String)((dataSources.length!=1)?\u0026quot;s:\u0026quot;:\u0026quot;:\u0026quot;)); for (ObjectName dataSource : dataSources) { String dsName = (String)mbs.getAttribute(dataSource, \u0026quot;Name\u0026quot;); String dsState = (String)mbs.getAttribute(dataSource, \u0026quot;State\u0026quot;); out.println(\u0026quot; Datasource '\u0026quot; + dsName + \u0026quot;': State='\u0026quot; + dsState +\u0026quot;'\u0026quot;); } out.println(); out.println(\u0026quot;*****************************************************************\u0026quot;); } catch (Throwable t) { t.printStackTrace(new PrintStream(response.getOutputStream())); } finally { out.println(\u0026quot;\u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026quot;); if (ic != null) ic.close(); } %\u0026gt;    The application displays important details about the WebLogic Server that it\u0026rsquo;s running on: namely its domain name, cluster name, and server name, as well as the names of any data sources that are targeted to the server. You can also see that application output reports that it\u0026rsquo;s at version v1; we will update this to v2 in a future use case to demonstrate upgrading the application.\nStaging a ZIP file of the archive When we create our image, we will use the files in staging directory /tmp/mii-sample/model-in-image__WLS-v1. In preparation, we need it to contain a ZIP file of the WDT application archive.\nRun the following commands to create your application archive ZIP file and put it in the expected directory:\n# Delete existing archive.zip in case we have an old leftover version $ rm -f /tmp/mii-sample/model-images/model-in-image__WLS-v1/archive.zip # Move to the directory which contains the source files for our archive $ cd /tmp/mii-sample/archives/archive-v1 # Zip the archive to the location will later use when we run the WebLogic Image Tool $ zip -r /tmp/mii-sample/model-images/model-in-image__WLS-v1/archive.zip wlsdeploy Staging model files In this step, we explore the staged WDT model YAML file and properties in directory /tmp/mii-sample/model-in-image__WLS-v1. The model in this directory references the web application in our archive, configures a WebLogic Administration Server, and configures a WebLogic cluster. It consists of only two files, model.10.properties, a file with a single property, and, model.10.yaml, a YAML file with our WebLogic configuration model.10.yaml.\nCLUSTER_SIZE=5 Here is the WLS model.10.yaml:\ndomainInfo: AdminUserName: '@@SECRET:__weblogic-credentials__:username@@' AdminPassword: '@@SECRET:__weblogic-credentials__:password@@' ServerStartMode: 'prod' topology: Name: '@@ENV:CUSTOM_DOMAIN_NAME@@' AdminServerName: 'admin-server' Cluster: 'cluster-1': DynamicServers: ServerTemplate: 'cluster-1-template' ServerNamePrefix: 'managed-server' DynamicClusterSize: '@@PROP:CLUSTER_SIZE@@' MaxDynamicClusterSize: '@@PROP:CLUSTER_SIZE@@' MinDynamicClusterSize: '0' CalculatedListenPorts: false Server: 'admin-server': ListenPort: 7001 ServerTemplate: 'cluster-1-template': Cluster: 'cluster-1' ListenPort: 8001 appDeployments: Application: myapp: SourcePath: 'wlsdeploy/applications/myapp-v1' ModuleType: ear Target: 'cluster-1'    Click here to expand the JRF `model.10.yaml`, and note the RCUDbInfo stanza and its references to a DOMAIN_UID-rcu-access secret.   domainInfo: AdminUserName: '@@SECRET:__weblogic-credentials__:username@@' AdminPassword: '@@SECRET:__weblogic-credentials__:password@@' ServerStartMode: 'prod' RCUDbInfo: rcu_prefix: '@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_prefix@@' rcu_schema_password: '@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_schema_password@@' rcu_db_conn_string: '@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_db_conn_string@@' topology: AdminServerName: 'admin-server' Name: '@@ENV:CUSTOM_DOMAIN_NAME@@' Cluster: 'cluster-1': Server: 'admin-server': ListenPort: 7001 'managed-server1-c1-': Cluster: 'cluster-1' ListenPort: 8001 'managed-server2-c1-': Cluster: 'cluster-1' ListenPort: 8001 'managed-server3-c1-': Cluster: 'cluster-1' ListenPort: 8001 'managed-server4-c1-': Cluster: 'cluster-1' ListenPort: 8001 appDeployments: Application: myapp: SourcePath: 'wlsdeploy/applications/myapp-v1' ModuleType: ear Target: 'cluster-1'    The model files:\n  Define a WebLogic domain with:\n Cluster cluster-1 Administration Server admin-server A cluster-1 targeted ear application that\u0026rsquo;s located in the WDT archive ZIP file at wlsdeploy/applications/myapp-v1    Leverage macros to inject external values:\n The property file CLUSTER_SIZE property is referenced in the model YAML DynamicClusterSize and MaxDynamicClusterSize fields using a PROP macro. The model file domain name is injected using a custom environment variable named CUSTOM_DOMAIN_NAME using an ENV macro.  We set this environment variable later in this sample using an env field in its domain resource. This conveniently provides a simple way to deploy multiple differently named domains using the same model image.   The model file administrator user name and password are set using a weblogic-credentials secret macro reference to the WebLogic credential secret.  This secret is in turn referenced using the weblogicCredentialsSecret field in the domain resource. The weblogic-credentials is a reserved name that always dereferences to the owning domain resource actual WebLogic credentials secret name.      A Model in Image image can contain multiple properties files, archive ZIP files, and YAML files, but in this sample we use just one of each. For a full discussion of Model in Images model file naming conventions, file loading order, and macro syntax, see Model files in the Model in Image user documentation.\nCreating the image with WIT  Note: If you are using JRF in this sample, substitute JRF for each occurrence of WLS in the imagetool command line below, plus substitute container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 for the --fromImage value.\n At this point, we have staged all of the files needed for image model-in-image:WLS-v1, they include:\n /tmp/mii-sample/model-images/weblogic-deploy.zip /tmp/mii-sample/model-images/model-in-image__WLS-v1/model.10.yaml /tmp/mii-sample/model-images/model-in-image__WLS-v1/model.10.properties /tmp/mii-sample/model-images/model-in-image__WLS-v1/archive.zip  If you don\u0026rsquo;t see the weblogic-deploy.zip file, then it means that you missed a step in the prerequisites.\nNow let\u0026rsquo;s use the Image Tool to create an image named model-in-image:WLS-v1 that\u0026rsquo;s layered on a base WebLogic image. We\u0026rsquo;ve already set up this tool during the prerequisite steps at the beginning of this sample.\nRun the following commands to create the model image and verify that it worked:\n$ cd /tmp/mii-sample/model-images $ ./imagetool/bin/imagetool.sh update \\ --tag model-in-image:WLS-v1 \\ --fromImage container-registry.oracle.com/middleware/weblogic:12.2.1.4 \\ --wdtModel ./model-in-image__WLS-v1/model.10.yaml \\ --wdtVariables ./model-in-image__WLS-v1/model.10.properties \\ --wdtArchive ./model-in-image__WLS-v1/archive.zip \\ --wdtModelOnly \\ --wdtDomainType WLS If you don\u0026rsquo;t see the imagetool directory, then it means that you missed a step in the prerequisites.\nThis command runs the WebLogic Image Tool in its Model in Image mode, and does the following:\n Builds the final Docker image as a layer on the container-registry.oracle.com/middleware/weblogic:12.2.1.4 base image. Copies the WDT ZIP file that\u0026rsquo;s referenced in the WIT cache into the image.  Note that we cached WDT in WIT using the keyword latest when we set up the cache during the sample prerequisites steps. This lets WIT implicitly assume its the desired WDT version and removes the need to pass a -wdtVersion flag.   Copies the specified WDT model, properties, and application archives to image location /u01/wdt/models.  When the command succeeds, it should end with output like:\n[INFO ] Build successful. Build time=36s. Image tag=model-in-image:WLS-v1 Also, if you run the docker images command, then you should see a Docker image named model-in-image:WLS-v1.\nDeploy resources - Introduction In this section we will deploy our new image to namespace sample-domain1-ns, including the following steps:\n Create a secret containing your WebLogic administrator user name and password. Create a secret containing your Model in Image runtime encryption password:  All Model in Image domains must supply a runtime encryption secret with a password value. It is used to encrypt configuration that is passed around internally by the operator. The value must be kept private but can be arbitrary; you can optionally supply a different secret value every time you restart the domain.   If your domain type is JRF, create secrets containing your RCU access URL, credentials, and prefix. Deploy a domain resource YAML file that references the new image. Wait for the domain\u0026rsquo;s pods to start and reach their ready state.  Secrets First, create the secrets needed by both WLS and JRF type model domains. In this case, we have two secrets.\nRun the following kubectl commands to deploy the required secrets:\n$ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-weblogic-credentials \\ --from-literal=username=weblogic --from-literal=password=welcome1 $ kubectl -n sample-domain1-ns label secret \\ sample-domain1-weblogic-credentials \\ weblogic.domainUID=sample-domain1 $ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-runtime-encryption-secret \\ --from-literal=password=my_runtime_password $ kubectl -n sample-domain1-ns label secret \\ sample-domain1-runtime-encryption-secret \\ weblogic.domainUID=sample-domain1 Some important details about these secrets:\n  The WebLogic credentials secret:\n It is required and must contain username and password fields. It must be referenced by the spec.weblogicCredentialsSecret field in your domain resource. It also must be referenced by macros in the domainInfo.AdminUserName and domainInfo.AdminPassWord fields in your model YAML file.    The Model WDT runtime secret:\n This is a special secret required by Model in Image. It must contain a password field. It must be referenced using the spec.model.runtimeEncryptionSecret attribute in its domain resource. It must remain the same for as long as the domain is deployed to Kubernetes, but can be changed between deployments. It is used to encrypt data as it\u0026rsquo;s internally passed using log files from the domain\u0026rsquo;s introspector job and on to its WebLogic Server pods.    Deleting and recreating the secrets:\n We delete a secret before creating it, otherwise the create command will fail if the secret already exists. This allows us to change the secret when using the kubectl create secret command.    We name and label secrets using their associated domain UID for two reasons:\n To make it obvious which secrets belong to which domains. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all resources associated with a domain.    If you\u0026rsquo;re following the JRF path through the sample, then you also need to deploy the additional secret referenced by macros in the JRF model RCUDbInfo clause, plus an OPSS wallet password secret. For details about the uses of these secrets, see the Model in Image user documentation.\n  Click here for the commands for deploying additional secrets for JRF.   $ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-rcu-access \\ --from-literal=rcu_prefix=FMW1 \\ --from-literal=rcu_schema_password=Oradoc_db1 \\ --from-literal=rcu_db_conn_string=oracle-db.default.svc.cluster.local:1521/devpdb.k8s $ kubectl -n sample-domain1-ns label secret \\ sample-domain1-rcu-access \\ weblogic.domainUID=sample-domain1 $ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-opss-wallet-password-secret \\ --from-literal=walletPassword=welcome1 $ kubectl -n sample-domain1-ns label secret \\ sample-domain1-opss-wallet-password-secret \\ weblogic.domainUID=sample-domain1    Domain resource Now let\u0026rsquo;s create a domain resource. A domain resource is the key resource that tells the operator how to deploy a WebLogic domain.\nCopy the following to a file called /tmp/mii-sample/mii-initial.yaml or similar, or use the file /tmp/mii-sample/domain-resources/WLS/mii-initial-d1-WLS-v1.yaml that is included in the sample source.\n  Click here to expand the WLS domain resource YAML.    # # This is an example of how to define a Domain resource. # # If you are using 3.0.0-rc1, then the version on the following line # should be `v7` not `v6`. apiVersion: \u0026quot;weblogic.oracle/v6\u0026quot; kind: Domain metadata: name: sample-domain1 namespace: sample-domain1-ns labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: sample-domain1 spec: # Set to 'FromModel' to indicate 'Model in Image'. domainHomeSourceType: FromModel # The WebLogic Domain Home, this must be a location within # the image for 'Model in Image' domains. domainHome: /u01/domains/sample-domain1 # The WebLogic Server Docker image that the Operator uses to start the domain image: \u0026quot;model-in-image:WLS-v1\u0026quot; # Defaults to \u0026quot;Always\u0026quot; if image tag (version) is ':latest' imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: regsecret # Identify which Secret contains the WebLogic Admin credentials, # the secret must contain 'username' and 'password' fields. webLogicCredentialsSecret: name: sample-domain1-weblogic-credentials # Whether to include the WebLogic server stdout in the pod's stdout, default is true includeServerOutInPodLog: true # Whether to enable overriding your log file location, see also 'logHome' #logHomeEnabled: false # The location for domain log, server logs, server out, and Node Manager log files # see also 'logHomeEnabled', 'volumes', and 'volumeMounts'. #logHome: /shared/logs/sample-domain1 # Set which WebLogic servers the Operator will start # - \u0026quot;NEVER\u0026quot; will not start any server in the domain # - \u0026quot;ADMIN_ONLY\u0026quot; will start up only the administration server (no managed servers will be started) # - \u0026quot;IF_NEEDED\u0026quot; will start all non-clustered servers, including the administration server, and clustered servers up to their replica count. serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; # Settings for all server pods in the domain including the introspector job pod serverPod: # Optional new or overridden environment variables for the domain's pods # - This sample uses CUSTOM_DOMAIN_NAME in its image model file # to set the Weblogic domain name env: - name: CUSTOM_DOMAIN_NAME value: \u0026quot;domain1\u0026quot; - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-XX:+UseContainerSupport -Djava.security.egd=file:/dev/./urandom \u0026quot; # Optional volumes and mounts for the domain's pods. See also 'logHome'. #volumes: #- name: weblogic-domain-storage-volume # persistentVolumeClaim: # claimName: sample-domain1-weblogic-sample-pvc #volumeMounts: #- mountPath: /shared # name: weblogic-domain-storage-volume # The desired behavior for starting the domain's administration server. adminServer: # The serverStartState legal values are \u0026quot;RUNNING\u0026quot; or \u0026quot;ADMIN\u0026quot; # \u0026quot;RUNNING\u0026quot; means the listed server will be started up to \u0026quot;RUNNING\u0026quot; mode # \u0026quot;ADMIN\u0026quot; means the listed server will be start up to \u0026quot;ADMIN\u0026quot; mode serverStartState: \u0026quot;RUNNING\u0026quot; # Setup a Kubernetes node port for the administration server default channel #adminService: # channels: # - channelName: default # nodePort: 30701 # The number of managed servers to start for unlisted clusters replicas: 1 # The desired behavior for starting a specific cluster's member servers clusters: - clusterName: cluster-1 serverStartState: \u0026quot;RUNNING\u0026quot; replicas: 2 # Change the `restartVersion` to force the introspector job to rerun # and apply any new model configuration, to also force a subsequent # roll of your domain's WebLogic pods. restartVersion: '1' configuration: # Settings for domainHomeSourceType 'FromModel' model: # Valid model domain types are 'WLS', 'JRF', and 'RestrictedJRF', default is 'WLS' domainType: \u0026quot;WLS\u0026quot; # Optional configmap for additional models and variable files #configMap: sample-domain1-wdt-config-map # All 'FromModel' domains require a runtimeEncryptionSecret with a 'password' field runtimeEncryptionSecret: sample-domain1-runtime-encryption-secret # Secrets that are referenced by model yaml macros # (the model yaml in the optional configMap or in the image) #secrets: #- sample-domain1-datasource-secret      Click here to expand the JRF domain resource YAML.   # Copyright (c) 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # # If you are using 3.0.0-rc1, then the version on the following line # should be `v7` not `v6`. apiVersion: \u0026quot;weblogic.oracle/v6\u0026quot; kind: Domain metadata: name: sample-domain1 namespace: sample-domain1-ns labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: sample-domain1 spec: # Set to 'FromModel' to indicate 'Model in Image'. domainHomeSourceType: FromModel # The WebLogic Domain Home, this must be a location within # the image for 'Model in Image' domains. domainHome: /u01/domains/sample-domain1 # The WebLogic Server Docker image that the Operator uses to start the domain image: \u0026quot;model-in-image:JRF-v1\u0026quot; # Defaults to \u0026quot;Always\u0026quot; if image tag (version) is ':latest' imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: regsecret # Identify which Secret contains the WebLogic Admin credentials, # the secret must contain 'username' and 'password' fields. webLogicCredentialsSecret: name: sample-domain1-weblogic-credentials # Whether to include the WebLogic server stdout in the pod's stdout, default is true includeServerOutInPodLog: true # Whether to enable overriding your log file location, see also 'logHome' #logHomeEnabled: false # The location for domain log, server logs, server out, and Node Manager log files # see also 'logHomeEnabled', 'volumes', and 'volumeMounts'. #logHome: /shared/logs/sample-domain1 # Set which WebLogic servers the Operator will start # - \u0026quot;NEVER\u0026quot; will not start any server in the domain # - \u0026quot;ADMIN_ONLY\u0026quot; will start up only the administration server (no managed servers will be started) # - \u0026quot;IF_NEEDED\u0026quot; will start all non-clustered servers, including the administration server, and clustered servers up to their replica count. serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; # Settings for all server pods in the domain including the introspector job pod serverPod: # Optional new or overridden environment variables for the domain's pods # - This sample uses CUSTOM_DOMAIN_NAME in its image model file # to set the Weblogic domain name env: - name: CUSTOM_DOMAIN_NAME value: \u0026quot;domain1\u0026quot; - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-XX:+UseContainerSupport -Djava.security.egd=file:/dev/./urandom \u0026quot; # Optional volumes and mounts for the domain's pods. See also 'logHome'. #volumes: #- name: weblogic-domain-storage-volume # persistentVolumeClaim: # claimName: sample-domain1-weblogic-sample-pvc #volumeMounts: #- mountPath: /shared # name: weblogic-domain-storage-volume # The desired behavior for starting the domain's administration server. adminServer: # The serverStartState legal values are \u0026quot;RUNNING\u0026quot; or \u0026quot;ADMIN\u0026quot; # \u0026quot;RUNNING\u0026quot; means the listed server will be started up to \u0026quot;RUNNING\u0026quot; mode # \u0026quot;ADMIN\u0026quot; means the listed server will be start up to \u0026quot;ADMIN\u0026quot; mode serverStartState: \u0026quot;RUNNING\u0026quot; # Setup a Kubernetes node port for the administration server default channel #adminService: # channels: # - channelName: default # nodePort: 30701 # The number of managed servers to start for unlisted clusters replicas: 1 # The desired behavior for starting a specific cluster's member servers clusters: - clusterName: cluster-1 serverStartState: \u0026quot;RUNNING\u0026quot; replicas: 2 # Change the restartVersion to force the introspector job to rerun # and apply any new model configuration, to also force a subsequent # roll of your domain's WebLogic pods. restartVersion: '1' configuration: # Settings for domainHomeSourceType 'FromModel' model: # Valid model domain types are 'WLS', 'JRF', and 'RestrictedJRF', default is 'WLS' domainType: \u0026quot;JRF\u0026quot; # Optional configmap for additional models and variable files #configMap: sample-domain1-wdt-config-map # All 'FromModel' domains require a runtimeEncryptionSecret with a 'password' field runtimeEncryptionSecret: sample-domain1-runtime-encryption-secret # Secrets that are referenced by model yaml macros # (the model yaml in the optional configMap or in the image) secrets: #- sample-domain1-datasource-secret - sample-domain1-rcu-access # Increase the introspector job active timeout value for JRF use cases introspectorJobActiveDeadlineSeconds: 300 opss: # Name of secret with walletPassword for extracting the wallet, used for JRF domains walletPasswordSecret: sample-domain1-opss-wallet-password-secret # Name of secret with walletFile containing base64 encoded opss wallet, used for JRF domains #walletFileSecret: sample-domain1-opss-walletfile-secret    Run the following command to create the domain custom resource:\n$ kubectl apply -f /tmp/mii-sample/domain-resources/WLS/mii-initial-d1-WLS-v1.yaml  Note: If you are choosing not to use the predefined domain resource YAML file and instead created your own domain resource file earlier, then substitute your custom file name in the above command. You might recall that we suggested naming it /tmp/mii-sample/mii-initial.yaml.\n If you run kubectl get pods -n sample-domain1-ns --watch, then you should see the introspector job run and your WebLogic Server pods start. The output should look something like this:\n  Click here to expand.   $ kubectl get pods -n sample-domain1-ns --watch NAME READY STATUS RESTARTS AGE sample-domain1-introspect-domain-job-lqqj9 0/1 Pending 0 0s sample-domain1-introspect-domain-job-lqqj9 0/1 ContainerCreating 0 0s sample-domain1-introspect-domain-job-lqqj9 1/1 Running 0 1s sample-domain1-introspect-domain-job-lqqj9 0/1 Completed 0 65s sample-domain1-introspect-domain-job-lqqj9 0/1 Terminating 0 65s sample-domain1-admin-server 0/1 Pending 0 0s sample-domain1-admin-server 0/1 ContainerCreating 0 0s sample-domain1-admin-server 0/1 Running 0 1s sample-domain1-admin-server 1/1 Running 0 32s sample-domain1-managed-server1 0/1 Pending 0 0s sample-domain1-managed-server2 0/1 Pending 0 0s sample-domain1-managed-server1 0/1 ContainerCreating 0 0s sample-domain1-managed-server2 0/1 ContainerCreating 0 0s sample-domain1-managed-server1 0/1 Running 0 2s sample-domain1-managed-server2 0/1 Running 0 2s sample-domain1-managed-server1 1/1 Running 0 43s sample-domain1-managed-server2 1/1 Running 0 42s    Alternatively, you can run /tmp/mii-sample/utils/wl-pod-wait.sh -p 3. This is a utility script that provides useful information about a domain\u0026rsquo;s pods and waits for them to reach a ready state, reach their target restartVersion, and reach their target image before exiting.\n  Click here to expand the `wl-pod-wait.sh` usage.   $ ./wl-pod-wait.sh -? Usage: wl-pod-wait.sh [-n mynamespace] [-d mydomainuid] \\ [-p expected_pod_count] \\ [-t timeout_secs] \\ [-q] Exits non-zero if 'timeout_secs' is reached before 'pod_count' is reached. Parameters: -d \u0026lt;domain_uid\u0026gt; : Defaults to 'sample-domain1'. -n \u0026lt;namespace\u0026gt; : Defaults to 'sample-domain1-ns'. pod_count \u0026gt; 0 : Wait until exactly 'pod_count' WebLogic server pods for a domain all (a) are ready, (b) have the same 'domainRestartVersion' label value as the current domain resource's 'spec.restartVersion, and (c) have the same image as the current domain resource's image. pod_count = 0 : Wait until there are no running WebLogic server pods for a domain. The default. -t \u0026lt;timeout\u0026gt; : Timeout in seconds. Defaults to '600'. -q : Quiet mode. Show only a count of wl pods that have reached the desired criteria. -? : This help.      Click here to expand sample output from `wl-pod-wait.sh`.   @@ [2020-04-30T13:50:42][seconds=0] Info: Waiting up to 600 seconds for exactly '3' WebLogic server pods to reach the following criteria: @@ [2020-04-30T13:50:42][seconds=0] Info: ready='true' @@ [2020-04-30T13:50:42][seconds=0] Info: image='model-in-image:WLS-v1' @@ [2020-04-30T13:50:42][seconds=0] Info: domainRestartVersion='1' @@ [2020-04-30T13:50:42][seconds=0] Info: namespace='sample-domain1-ns' @@ [2020-04-30T13:50:42][seconds=0] Info: domainUID='sample-domain1' @@ [2020-04-30T13:50:42][seconds=0] Info: '0' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:50:42][seconds=0] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------------------- ------- ----- ----- --------- 'sample-domain1-introspect-domain-job-rkdkg' '' '' '' 'Pending' @@ [2020-04-30T13:50:45][seconds=3] Info: '0' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:50:45][seconds=3] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------------------- ------- ----- ----- --------- 'sample-domain1-introspect-domain-job-rkdkg' '' '' '' 'Running' @@ [2020-04-30T13:51:50][seconds=68] Info: '0' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:51:50][seconds=68] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE ---- ------- ----- ----- ----- @@ [2020-04-30T13:51:59][seconds=77] Info: '0' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:51:59][seconds=77] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE ----------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'false' 'Pending' @@ [2020-04-30T13:52:02][seconds=80] Info: '0' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:52:02][seconds=80] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE ----------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'false' 'Running' @@ [2020-04-30T13:52:32][seconds=110] Info: '1' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:52:32][seconds=110] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'false' 'Pending' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'false' 'Pending' @@ [2020-04-30T13:52:34][seconds=112] Info: '1' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:52:34][seconds=112] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'false' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'false' 'Running' @@ [2020-04-30T13:53:14][seconds=152] Info: '3' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:53:14][seconds=152] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:53:14][seconds=152] Info: Success!    If you see an error, then consult Debugging in the Model in Image user guide.\nInvoke the web application Now that all the initial use case resources have been deployed, you can invoke the sample web application through the Traefik ingress controller\u0026rsquo;s NodePort. Note: The web application will display a list of any data sources it finds, but we don\u0026rsquo;t expect it to find any because the model doesn\u0026rsquo;t contain any at this point.\nSend a web application request to the load balancer:\n$ curl -s -S -m 10 -H 'host: sample-domain1-cluster-cluster-1.mii-sample.org' \\ http://localhost:30305/myapp_war/index.jsp Or, if Traefik is unavailable and your Administration Server pod is running, you can use kubectl exec:\n$ kubectl exec -n sample-domain1-ns sample-domain1-admin-server -- bash -c \\ \u0026quot;curl -s -S -m 10 http://sample-domain1-cluster-cluster-1:8001/myapp_war/index.jsp\u0026quot; You should see output like the following:\n$ curl -s -S -m 10 -H 'host: sample-domain1-cluster-cluster-1.mii-sample.org' \\ http://localhost:30305/myapp_war/index.jsp \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version 'v1' of the mii-sample JSP web-app. Welcome to WebLogic server 'managed-server2'! domain UID = 'sample-domain1' domain name = 'domain1' Found 1 local cluster runtime: Cluster 'cluster-1' Found 0 local data sources: ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; Note: If you\u0026rsquo;re running your curl commands on a remote machine, then substitute localhost with an external address suitable for contacting your Kubernetes cluster. A Kubernetes cluster address that often works can be obtained by using the address just after https:// in the KubeDNS line of the output from the kubectl cluster-info command.\nIf you want to continue to the next use case, then leave your domain running.\nUpdate1 use case This use case demonstrates dynamically adding a data source to your running domain. It demonstrates several features of WDT and Model in Image:\n The syntax used for updating a model is exactly the same syntax you use for creating the original model. A domain\u0026rsquo;s model can be updated dynamically by supplying a model update in a file in a Kubernetes ConfigMap. Model updates can be as simple as changing the value of a single attribute, or more complex, such as adding a JMS Server.  For a detailed discussion of model updates, see Runtime Updates in the Model in Image user guide.\nThe operator does not support all possible dynamic model updates. For model update limitations, consult Runtime Updates in the Model in Image user docs, and carefully test any model update before attempting a dynamic update in production.\n Here are the steps:\n  Ensure that you have a running domain.\nMake sure you have deployed the domain from the Initial use case.\n  Create a data source model YAML file.\nCreate a WDT model snippet for a data source (or use the example provided). Make sure that its target is set to cluster-1, and that its initial capacity is set to 0.\nThe reason for the latter is to prevent the data source from causing a WebLogic Server startup failure if it can\u0026rsquo;t find the database, which would be likely to happen because we haven\u0026rsquo;t deployed one (unless you\u0026rsquo;re using the JRF path through the sample).\nHere\u0026rsquo;s an example data source model configuration that meets these criteria:\nresources: JDBCSystemResource: mynewdatasource: Target: 'cluster-1' JdbcResource: JDBCDataSourceParams: JNDIName: [ jdbc/mydatasource1, jdbc/mydatasource2 ] GlobalTransactionsProtocol: TwoPhaseCommit JDBCDriverParams: DriverName: oracle.jdbc.xa.client.OracleXADataSource URL: '@@SECRET:@@ENV:DOMAIN_UID@@-datasource-secret:url@@' PasswordEncrypted: '@@SECRET:@@ENV:DOMAIN_UID@@-datasource-secret:password@@' Properties: user: Value: 'sys as sysdba' oracle.net.CONNECT_TIMEOUT: Value: 5000 oracle.jdbc.ReadTimeout: Value: 30000 JDBCConnectionPoolParams: InitialCapacity: 0 MaxCapacity: 1 TestTableName: SQL ISVALID TestConnectionsOnReserve: true Place the above model snippet in a file named /tmp/mii-sample/mydatasource.yaml and then use it in the later step where we deploy the model ConfigMap, or alternatively, use the same data source that\u0026rsquo;s provided in /tmp/mii-sample/model-configmaps/datasource/model.20.datasource.yaml.\n  Create the data source secret.\nThe data source references a new secret that needs to be created. Run the following commands to create the secret:\n$ kubectl -n sample-domain1-ns create secret generic \\ sample-domain1-datasource-secret \\ --from-literal=password=Oradoc_db1 \\ --from-literal=url=jdbc:oracle:thin:@oracle-db.default.svc.cluster.local:1521/devpdb.k8s $ kubectl -n sample-domain1-ns label secret \\ sample-domain1-datasource-secret \\ weblogic.domainUID=sample-domain1 We name and label secrets using their associated domain UID for two reasons:\n To make it obvious which secret belongs to which domains. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all the resources associated with a domain.    Create a ConfigMap with the WDT model that contains the data source definition.\nRun the following commands:\n$ kubectl -n sample-domain1-ns create configmap sample-domain1-wdt-config-map \\ --from-file=/tmp/mii-sample/model-configmaps/datasource $ kubectl -n sample-domain1-ns label configmap sample-domain1-wdt-config-map \\ weblogic.domainUID=sample-domain1  If you\u0026rsquo;ve created your own data source file, then substitute the file name in the --from-file= parameter (we suggested /tmp/mii-sample/mydatasource.yaml earlier). Note that the -from-file= parameter can reference a single file, in which case it puts the designated file in the ConfigMap, or it can reference a directory, in which case it populates the ConfigMap with all of the files in the designated directory.  We name and label ConfigMap using their associated domain UID for two reasons:\n To make it obvious which ConfigMap belong to which domains. To make it easier to cleanup a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all resources associated with a domain.    Update your domain resource to refer to the ConfigMap and secret.\n  Option 1: Update your current domain resource file from the \u0026ldquo;Initial\u0026rdquo; use case.\n  Add the secret to its spec.configuration.secrets stanza:\nspec: ... configuration: ... secrets: - sample-domain1-datasource-secret (Leave any existing secrets in place.)\n  Change its spec.configuration.model.configMap to look like:\nspec: ... configuration: ... model: ... configMap: sample-domain1-wdt-config-map   Apply your changed domain resource:\n$ kubectl apply -f your-domain-resource.yaml     Option 2: Use the updated domain resource file that is supplied with the sample:\n$ kubectl apply -f /tmp/miisample/domain-resources/mii-update1-d1-WLS-v1-ds.yaml     Restart (\u0026lsquo;roll\u0026rsquo;) the domain.\nNow that the data source is deployed in a ConfigMap and its secret is also deployed, and we have applied an updated domain resource with its spec.configuration.model.configMap and spec.configuration.secrets referencing the ConfigMap and secret, let\u0026rsquo;s tell the operator to roll the domain.\nWhen a model domain restarts, it will rerun its introspector job in order to regenerate its configuration, and it will also pass the configuration changes found by the introspector to each restarted server. One way to cause a running domain to restart is to change the domain\u0026rsquo;s spec.restartVersion. To do this:\n  Option 1: Edit your domain custom resource.\n Call kubectl -n sample-domain1-ns edit domain sample-domain1. Edit the value of the spec.restartVersion field and save.  The field is a string; typically, you use a number in this field and increment it with each restart.      Option 2: Dynamically change your domain using kubectl patch.\n  To get the current restartVersion call:\n$ kubectl -n sample-domain1-ns get domain sample-domain1 '-o=jsonpath={.spec.restartVersion}'   Choose a new restart version that\u0026rsquo;s different from the current restart version.\n The field is a string; typically, you use a number in this field and increment it with each restart.    Use kubectl patch to set the new value. For example, assuming the new restart version is 2:\n$ kubectl -n sample-domain1-ns patch domain sample-domain1 --type=json '-p=[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/restartVersion\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;2\u0026quot; }]'     Option 3: Use the sample helper script.\n Call /tmp/mii-sample/utils/patch-restart-version.sh -n sample-domain1-ns -d sample-domain1. This will perform the same kubectl get and kubectl patch commands as Option 2.      Wait for the roll to complete.\nNow that you\u0026rsquo;ve started a domain roll, you\u0026rsquo;ll need to wait for it to complete if you want to verify that the data source was deployed.\n  One way to do this is to call kubectl get pods -n sample-domain1-ns --watch and wait for the pods to cycle back to their ready state.\n  Alternatively, you can run /tmp/mii-sample/utils/wl-pod-wait.sh -p 3. This is a utility script that provides useful information about a domain\u0026rsquo;s pods and waits for them to reach a ready state, reach their target restartVersion, and reach their target image before exiting.\n  Click here to expand the `wl-pod-wait.sh` usage.    $ ./wl-pod-wait.sh -? Usage: wl-pod-wait.sh [-n mynamespace] [-d mydomainuid] \\ [-p expected_pod_count] \\ [-t timeout_secs] \\ [-q] Exits non-zero if 'timeout_secs' is reached before 'pod_count' is reached. Parameters: -d \u0026lt;domain_uid\u0026gt; : Defaults to 'sample-domain1'. -n \u0026lt;namespace\u0026gt; : Defaults to 'sample-domain1-ns'. pod_count \u0026gt; 0 : Wait until exactly 'pod_count' WebLogic server pods for a domain all (a) are ready, (b) have the same 'domainRestartVersion' label value as the current domain resource's 'spec.restartVersion, and (c) have the same image as the current domain resource's image. pod_count = 0 : Wait until there are no running WebLogic server pods for a domain. The default. -t \u0026lt;timeout\u0026gt; : Timeout in seconds. Defaults to '600'. -q : Quiet mode. Show only a count of wl pods that have reached the desired criteria. -? : This help.      Click here to expand sample output from `wl-pod-wait.sh` that shows a rolling domain.    @@ [2020-04-30T13:53:19][seconds=0] Info: Waiting up to 600 seconds for exactly '3' WebLogic server pods to reach the following criteria: @@ [2020-04-30T13:53:19][seconds=0] Info: ready='true' @@ [2020-04-30T13:53:19][seconds=0] Info: image='model-in-image:WLS-v1' @@ [2020-04-30T13:53:19][seconds=0] Info: domainRestartVersion='2' @@ [2020-04-30T13:53:19][seconds=0] Info: namespace='sample-domain1-ns' @@ [2020-04-30T13:53:19][seconds=0] Info: domainUID='sample-domain1' @@ [2020-04-30T13:53:19][seconds=0] Info: '0' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:53:19][seconds=0] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-introspect-domain-job-wlkpr' '' '' '' 'Pending' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:53:20][seconds=1] Info: '0' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:53:20][seconds=1] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-introspect-domain-job-wlkpr' '' '' '' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:54:18][seconds=59] Info: '0' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:54:18][seconds=59] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------------------- ------- ----------------------- ------ ----------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-introspect-domain-job-wlkpr' '' '' '' 'Succeeded' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:54:19][seconds=60] Info: '0' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:54:19][seconds=60] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:54:31][seconds=72] Info: '0' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:54:31][seconds=72] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'false' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:54:40][seconds=81] Info: '0' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:54:40][seconds=81] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:54:52][seconds=93] Info: '0' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:54:52][seconds=93] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:54:58][seconds=99] Info: '0' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:54:58][seconds=99] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'false' 'Pending' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:55:00][seconds=101] Info: '0' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:55:00][seconds=101] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'false' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:55:12][seconds=113] Info: '0' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:55:12][seconds=113] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'false' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:55:24][seconds=125] Info: '0' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:55:24][seconds=125] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'false' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:55:33][seconds=134] Info: '1' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:55:33][seconds=134] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:55:34][seconds=135] Info: '1' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:55:34][seconds=135] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'false' 'Pending' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:55:40][seconds=141] Info: '1' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:55:40][seconds=141] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:55:44][seconds=145] Info: '1' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:55:44][seconds=145] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'false' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:56:25][seconds=186] Info: '2' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:56:25][seconds=186] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:56:26][seconds=187] Info: '2' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:56:26][seconds=187] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'false' 'Pending' @@ [2020-04-30T13:56:30][seconds=191] Info: '2' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:56:30][seconds=191] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:56:34][seconds=195] Info: '2' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:56:34][seconds=195] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '2' 'model-in-image:WLS-v1' 'false' 'Pending' @@ [2020-04-30T13:57:09][seconds=230] Info: '3' WebLogic pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:57:09][seconds=230] Info: Introspector and WebLogic pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '2' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:57:09][seconds=230] Info: Success!        After your domain is running, you can call the sample web application to determine if the data source was deployed.\nSend a web application request to the ingress controller:\n$ curl -s -S -m 10 -H 'host: sample-domain1-cluster-cluster-1.mii-sample.org' \\ http://localhost:30305/myapp_war/index.jsp Or, if Traefik is unavailable and your Administration Server pod is running, you can run kubectl exec:\n$ kubectl exec -n sample-domain1-ns sample-domain1-admin-server -- bash -c \\ \u0026quot;curl -s -S -m 10 http://sample-domain1-cluster-cluster-1:8001/myapp_war/index.jsp\u0026quot; You should see something like the following:\n  Click here to see the expected web application output.   $ curl -s -S -m 10 -H 'host: sample-domain1-cluster-cluster-1.mii-sample.org' \\ http://localhost:30305/myapp_war/index.jsp \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version 'v1' of the mii-sample JSP web-app. Welcome to WebLogic server 'managed-server1'! domain UID = 'sample-domain1' domain name = 'domain1' Found 1 local cluster runtime: Cluster 'cluster-1' Found 1 local data source: Datasource 'mynewdatasource': State='Running' ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;      If you see an error, then consult Debugging in the Model in Image user guide.\nThis completes the sample scenarios.\nCleanup To remove the resources you have created in these samples:\n  Delete the domain resources.\n$ /tmp/operator-source/kubernetes/samples/scripts/delete-domain/delete-weblogic-domain-resources.sh -d sample-domain1 $ /tmp/operator-source/kubernetes/samples/scripts/delete-domain/delete-weblogic-domain-resources.sh -d sample-domain2 This deletes the domain and any related resources that are labeled with the domain UID sample-domain1 and sample-domain2.\nIt leaves the namespace intact, the operator running, the load balancer running (if installed), and the database running (if installed).\n Note: When you delete a domain, the operator should detect your domain deletion and shut down its pods. Wait for these pods to exit before deleting the operator that monitors the sample-domain1-ns namespace. You can monitor this process using the command kubectl get pods -n sample-domain1-ns --watch (ctrl-c to exit).\n   If you set up the Traefik ingress controller:\n$ helm delete --purge traefik-operator $ kubectl delete namespace traefik   If you set up a database for JRF:\n$ /tmp/operator-source/kubernetes/samples/scripts/create-oracle-db-service/stop-db-service.sh   Delete the operator and its namespace:\n$ helm delete --purge sample-weblogic-operator $ kubectl delete namespace sample-weblogic-operator-ns   Delete the domain\u0026rsquo;s namespace:\n$ kubectl delete namespace sample-domain1-ns   Delete the images you may have created in this sample:\n$ docker image rm model-in-image:WLS-v1 $ docker image rm model-in-image:WLS-v2 $ docker image rm model-in-image:JRF-v1 $ docker image rm model-in-image:JRF-v2   References For references to the relevant user documentation, see:\n Model in Image user documentation Oracle WebLogic Server Deploy Tooling Oracle WebLogic Image Tool  "
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/rest/",
	"title": "REST APIs",
	"tags": [],
	"description": "Sample for generating a self-signed certificate and private key that can be used for the operator&#39;s external REST API.",
	"content": "Sample to create certificate and key When a user enables the operator\u0026rsquo;s external REST API (by setting externalRestEnabled to true when installing or upgrading the operator Helm chart), the user also needs to provide the certificates and private key used for the SSL/TLS identity on the external REST API endpoint by creating a kubernetes tls secret and using that secret\u0026rsquo;s name with the operator Helm chart values.\nThis sample script generates a self-signed certificate and private key that can be used for the operator\u0026rsquo;s external REST API when experimenting with the operator.\nThe certificate and key generated with this script should not be used in a production environment.\n The syntax of the script is:\n$ kubernetes/samples/scripts/rest/generate-external-rest-identity.sh \\ -a \u0026lt;SANs\u0026gt; -n \u0026lt;operator-namespace\u0026gt; [-s \u0026lt;secret-name\u0026gt;] Where \u0026lt;SANs\u0026gt; lists the subject alternative names to put into the generated self-signed certificate for the external operator REST HTTPS interface, \u0026lt;operator-namespace\u0026gt; should match the namespace where the operator will be installed, and optionally the secret name, which defaults to weblogic-operator-external-rest-identity.\nYou should include the addresses of all masters and load balancers (for example, what a client specifies to access the external REST endpoint) in the subject alternative name list. In addition, each name must be prefaced by DNS: for a host name, or IP: for an address, as in this example:\n-a \u0026quot;DNS:myhost,DNS:localhost,IP:127.0.0.1\u0026quot; The external certificate and key can be changed after installation of the operator. For more information, see Updating operator external certificates.\nThe following script will create the tls secret named weblogic-operator-identity in the namespace weblogic-operator-ns, using a self-signed certificate and private key:\n$ echo \u0026quot;externalRestEnabled: true\u0026quot; \u0026gt; my_values.yaml $ generate-external-rest-identity.sh \\ -a \u0026quot;DNS:${HOSTNAME},DNS:localhost,IP:127.0.0.1\u0026quot; \\ -n weblogic-operator-ns -s weblogic-operator-identity \u0026gt;\u0026gt; my_values.yaml # $ kubectl -n weblogic-operator-ns describe secret weblogic-operator-identity # $ helm install my_operator kubernetes/charts/weblogic-operator \\ --namespace weblogic-operator-ns --values my_values.yaml --wait "
},
{
	"uri": "/weblogic-kubernetes-operator/developerguide/integration-tests/",
	"title": "Integration tests",
	"tags": [],
	"description": "",
	"content": "The project includes integration tests that can be run against a Kubernetes cluster. If you want to use these tests, you will need to provide your own Kubernetes cluster. The Kubernetes cluster must meet the version number requirements and have Helm installed. Ensure that the operator Docker image is in a Docker registry visible to the Kubernetes cluster.\nYou will need to obtain the kube.config file for an administrative user and make it available on the machine running the build. To run the tests, update the KUBECONFIG environment variable to point to your config file and then execute:\n$ mvn clean verify -P java-integration-tests For more detailed information, see How to run the Java integration tests .\nWhen you run the integrations tests, they do a cleanup of any operator or domains on that cluster.\n "
},
{
	"uri": "/weblogic-kubernetes-operator/developerguide/branching/",
	"title": "Branching",
	"tags": [],
	"description": "",
	"content": "The master branch is protected and contains source for the most recently published release, including release candidates.\nThe develop branch is protected and contains source for the latest completed features and bug fixes. While this branch contains active work, we expect to keep it always \u0026ldquo;ready to release.\u0026rdquo; Therefore, longer running feature work will be performed on specific branches, such as feature/dynamic-clusters.\nBecause we want to balance separating destabilizing work into feature branches against the possibility of later difficult merges, we encourage developers working on features to pull out any necessary refactoring or improvements that are general purpose into their own shorter-lived branches and create pull requests to develop when these smaller work items are completed.\nAll commits to develop must pass the integration test suite. Please run these tests locally before submitting a pull request. Additionally, each push to a branch in our GitHub repository triggers a run of a subset of the integration tests with the results visible here.\nPlease submit pull requests to the develop branch unless you are collaborating on a feature and have another target branch. Please see details on the Oracle Contributor Agreement (OCA) and guidelines for pull requests on the README.\nWe will create git tags for each release candidate and generally available (GA) release of the operator.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/domain-in-image/",
	"title": "Domain in Image",
	"tags": [],
	"description": "",
	"content": "  Base images  Creating or obtaining WebLogic Docker images.\n "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/",
	"title": "Manage WebLogic domains",
	"tags": [],
	"description": "Important considerations for WebLogic domains in Kubernetes.",
	"content": "Contents  Important considerations for WebLogic domains in Kubernetes Creating and managing WebLogic domains Modifying domain configurations About the domain resource Managing life cycle operations Scaling clusters  Important considerations for WebLogic domains in Kubernetes Be aware of the following important considerations for WebLogic domains running in Kubernetes:\n  Domain Home Location: The WebLogic domain home location is determined by the domain resource domainHome and domainHomeInImage fields. If you are using 3.0.0-rc1, these same fields are used, if specified; otherwise, a default location is determined by the domainHomeSourceType setting. The domainHomeSourceType field is not available in releases before 3.0.0-rc1.\n If the domain resource domainHome field is not specified and domainHomeSourceType is Image (the default), then the operator will assume that the domain home is a directory under /u01/oracle/user_projects/domains/, and report an error if no domain is found or more than one domain is found. If the domain resource domainHome field is not specified and domainHomeSourceType is PersistentVolume, then the operator will assume that the domain home is /shared/domains/DOMAIN_UID. Finally, if the domain resource domainHome field is not specified and the domainHomeSourceType is FromModel, then the operator will assume that the domain home is /u01/domains/DOMAIN_UID.  Oracle strongly recommends storing an image containing a WebLogic domain home (domainHomeSourceType is Image) as private in the registry (for example, Oracle Cloud Infrastructure Registry, Docker Hub, and such). A Docker image that contains a WebLogic domain has sensitive information including keys and credentials that are used to access external resources (for example, the data source password). For more information, see WebLogic domain in Docker image protection.\n   Log File Locations: The operator can automatically override WebLogic domain and server log locations using configuration overrides. This occurs if the domain resource logHomeEnabled field is explicitly set to true, or if logHomeEnabled isn\u0026rsquo;t set and domainHomeSourceType is set to PersistentVolume. When overriding, the log location will be the location specified by the logHome setting.\n  Listen Address Overrides: The operator will automatically override all WebLogic domain default, SSL, admin, or custom channel listen addresses (using situational configuration overrides). These will become domainUID followed by a hyphen and then the server name, all lower case, and underscores converted to hyphens. For example, if domainUID=domain1 and the WebLogic Server name is Admin_Server, then its listen address becomes domain1-admin-server.\n  Domain, Cluster, Server, and Network-Access-Point Names: WebLogic domain, cluster, server, and network-access-point (channel) names must contain only the characters A-Z, a-z, 0-9, -, or _. This ensures that they can be converted to meet Kubernetes resource and DNS1123 naming requirements. (When generating pod and service names, the operator will convert configured names to lower case and substitute a hyphen (-) for each underscore (_).)\n  Node Ports: If you choose to expose any WebLogic channels outside the Kubernetes cluster using a NodePort, for example, the administration port or a T3 channel to allow WLST access, you need to ensure that you allocate each channel a unique port number across the entire Kubernetes cluster. If you expose the administration port in each WebLogic domain in the Kubernetes cluster, then each one must have a different port number. This is required because NodePorts are used to expose channels outside the Kubernetes cluster.\nExposing administrative, RMI, or T3 capable channels using a Kubernetes NodePort can create an insecure configuration. In general, only HTTP protocols should be made available externally and this exposure is usually accomplished by setting up an external load balancer that can access internal (non-NodePort) services. For more information, see T3 channels.\n   Host Path Persistent Volumes: If using a hostPath persistent volume, then it must be available on all worker nodes in the cluster and have read/write/many permissions for all container/pods in the WebLogic Server deployment. Be aware that many cloud provider\u0026rsquo;s volume providers may not support volumes across availability zones. You may want to use NFS or a clustered file system to work around this limitation.\n  Security Note: The USER_MEM_ARGS environment variable defaults to -Djava.security.egd=file:/dev/./urandom in all WebLogic Server pods and the WebLogic introspection job. It can be explicitly set to another value in your domain resource YAML file using the env attribute under the serverPod configuration.\n  JVM Memory and Java Option Arguments: The following environment variables can be used to customize the JVM memory and Java options for both the WebLogic Server Managed Servers and Node Manager instances:\n JAVA_OPTIONS - Java options for starting WebLogic Server USER_MEM_ARGS - JVM memory arguments for starting WebLogic Server NODEMGR_JAVA_OPTIONS - Java options for starting a Node Manager instance NODEMGR_MEM_ARGS - JVM memory arguments for starting a Node Manager instance  For more information, see Domain resource.\n  The following features are not certified or supported in this release:\n Whole server migration Consensus leasing Node Manager (although it is used internally for the liveness probe and to start WebLogic Server instances) Multicast Multitenancy Production redeployment  For up-to-date information about the features of WebLogic Server that are supported in Kubernetes environments, see My Oracle Support Doc ID 2349228.1.\nCreating and managing WebLogic domains You can locate a WebLogic domain either in a persistent volume (PV) or in a Docker image. For examples of each, see the WebLogic Server Kubernetes Operator samples.\nIf you want to create your own Docker images, for example, to choose a specific set of patches or to create a domain with a specific configuration or applications deployed, then you can create the domain custom resource manually to deploy your domain. This process is documented in this sample.\nModifying domain configurations You can modify the WebLogic domain configuration for Domain in PV, Domain in Image, and Model in Image before deploying a domain resource:\nWhen the domain is in a persistent volume, you can use WLST or WDT to change the configuration.\nFor Domain in Image and Domain in PV you can use configuration overrides.\nConfiguration overrides allow changing a configuration without modifying its original config.xml or system resource XML files, and supports parameterizing overrides so that you can inject values into them from Kubernetes Secrets. For example, you can inject database user names, passwords, and URLs that are stored in a secret.\nFor Model in Image you use Runtime Updates.\nAbout the domain resource For more information, see Domain resource.\nManaging lifecycle operations You can perform lifecycle operations on WebLogic Servers, clusters, or domains. See Starting and stopping and Restarting servers.\nScaling clusters The operator let\u0026rsquo;s you initiate scaling of clusters in various ways:\n Using kubectl to edit the domain resource Using the operator\u0026rsquo;s REST APIs Using WLDF policies Using a Prometheus action  "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-fmw-domains/",
	"title": "Manage FMW Domains",
	"tags": [],
	"description": "",
	"content": "Use the operator to manage Oracle Fusion Middleware domains.\n Manage FMW Infrastructure domains  FMW Infrastructure domains contain the Java Required Files (JRF) feature and are the prerequisite for upper stack products like Oracle SOA Suite.\n Manage SOA domains  SOA domains include the deployment of various Oracle Service-Oriented Architecture (SOA) Suite components, such as SOA, Oracle Service Bus (OSB), and Oracle Enterprise Scheduler (ESS).\n "
},
{
	"uri": "/weblogic-kubernetes-operator/quickstart/install/",
	"title": "Install the operator and load balancer",
	"tags": [],
	"description": "",
	"content": "Grant the Helm service account the cluster-admin role. $ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: helm-user-cluster-admin-role roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: default namespace: kube-system EOF Use Helm to install the operator and Traefik load balancer. First, set up Helm:\n$ helm repo add stable https://kubernetes-charts.storage.googleapis.com/ Create a Traefik (ingress-based) load balancer. Create a namespace for the load balancer.\n$ kubectl create namespace traefik Use the values.yaml in the sample but set kubernetes.namespaces specifically.\n$ helm install traefik-operator stable/traefik \\  --namespace traefik \\  --values kubernetes/samples/charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --wait Install the operator.   Create a namespace for the operator:\n$ kubectl create namespace sample-weblogic-operator-ns   Create a service account for the operator in the operator\u0026rsquo;s namespace:\n$ kubectl create serviceaccount -n sample-weblogic-operator-ns sample-weblogic-operator-sa   Use helm to install and start the operator from the directory you just cloned:\n$ helm install sample-weblogic-operator kubernetes/charts/weblogic-operator \\  --namespace sample-weblogic-operator-ns \\  --set image=oracle/weblogic-kubernetes-operator:2.5.0 \\  --set serviceAccount=sample-weblogic-operator-sa \\  --set \u0026#34;domainNamespaces={}\u0026#34; \\  --wait   Verify that the operator\u0026rsquo;s pod is running, by listing the pods in the operator\u0026rsquo;s namespace. You should see one for the operator.\n$ kubectl get pods -n sample-weblogic-operator-ns   Verify that the operator is up and running by viewing the operator pod\u0026rsquo;s log:\n$ kubectl logs -n sample-weblogic-operator-ns -c weblogic-operator deployments/weblogic-operator   "
},
{
	"uri": "/weblogic-kubernetes-operator/developerguide/",
	"title": "Developer Guide",
	"tags": [],
	"description": "",
	"content": "Developer Guide The Developer Guide provides information for developers who want to understand or contribute to the code.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/model-in-image/",
	"title": "Model in Image",
	"tags": [],
	"description": "",
	"content": "  Overview  Introduction to Model in Image, description of its runtime behavior, and references.\n Usage  Steps for creating and deploying Model in Image images and their associated domain resources.\n Model files  Model file requirements, macros, and loading order.\n Runtime updates  Updating a running Model in Image domain\u0026#39;s images and model files.\n Debugging  Debugging a deployed Model in Image domain.\n "
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/elastic-stack/soa-domain/weblogic-monitoring-exporter-setup/",
	"title": "Monitor a SOA domain",
	"tags": [],
	"description": "Use the WebLogic Monitoring Exporter to monitor a SOA instance using Prometheus and Grafana.",
	"content": "Monitor a SOA domain using Prometheus and Grafana by exporting the metrics from the domain instance using the WebLogic Monitoring Exporter. This sample shows you how to set up the WebLogic Monitoring Exporter to push the data to Prometheus.\nPrerequisites This document assumes that the Prometheus Operator is deployed on the Kubernetes cluster. If it is not already deployed, follow the steps below for deploying the Prometheus Operator.\nClone the kube-prometheus project $ git clone https://github.com/coreos/kube-prometheus.git Create the kube-prometheus resources Change to the kube-prometheus directory and execute the following commands to create the namespace and CRDs. Wait for their availability before creating the remaining resources.\n$ cd kube-prometheus $ kubectl create -f manifests/setup $ until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo \u0026#34;\u0026#34;; done $ kubectl create -f manifests/ Label the nodes Kube-Prometheus requires all the exporter nodes to be labelled with kubernetes.io/os=linux. If a node is not labelled, then you must label it using the following command:\n$ kubectl label nodes --all kubernetes.io/os=linux Provide external access To provide external access for Grafana, Prometheus, and Alertmanager, execute the commands below:\n$ kubectl patch svc grafana -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32100 }]\u0026#39; $ kubectl patch svc prometheus-k8s -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32101 }]\u0026#39; $ kubectl patch svc alertmanager-main -n monitoring --type=json -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;NodePort\u0026#34; },{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/ports/0/nodePort\u0026#34;, \u0026#34;value\u0026#34;: 32102 }]\u0026#39; NOTE:\n 32100 is the external port for Grafana 32101 is the external port for Prometheus 32102 is the external port for Alertmanager   Use the following instructions to set up the WebLogic Monitoring Exporter to collect WebLogic Server metrics and monitor a SOA domain.\nDownload the WebLogic Monitoring Exporter Download these WebLogic Monitoring Exporter files from the Releases page:\n wls-exporter.war getX.X.X.sh  Create a configuration file for the WebLogic Monitoring Exporter The configuration file will have the server port of the WebLogic Server instance where the monitoring exporter application will be deployed.\nSee the following sample snippet of the configuration:\nmetricsNameSnakeCase: true restPort: 7001 queries: - key: name keyName: location prefix: wls_server_ applicationRuntimes: key: name keyName: app componentRuntimes: prefix: wls_webapp_config_ type: WebAppComponentRuntime key: name values: [deploymentState, contextRoot, sourceInfo, openSessionsHighCount, openSessionsCurrentCount, sessionsOpenedTotalCount, sessionCookieMaxAgeSecs, sessionInvalidationIntervalSecs, sessionTimeoutSecs, singleThreadedServletPoolSize, sessionIDLength, servletReloadCheckSecs, jSPPageCheckSecs] servlets: prefix: wls_servlet_ key: servletName - JVMRuntime: prefix: wls_jvm_ key: name - executeQueueRuntimes: prefix: wls_socketmuxer_ key: name values: [pendingRequestCurrentCount] - workManagerRuntimes: prefix: wls_workmanager_ key: name values: [stuckThreadCount, pendingRequests, completedRequests] - threadPoolRuntime: prefix: wls_threadpool_ key: name values: [executeThreadTotalCount, queueLength, stuckThreadCount, hoggingThreadCount] - JMSRuntime: key: name keyName: jmsruntime prefix: wls_jmsruntime_ JMSServers: prefix: wls_jms_ key: name keyName: jmsserver destinations: prefix: wls_jms_dest_ key: name keyName: destination - persistentStoreRuntimes: prefix: wls_persistentstore_ key: name - JDBCServiceRuntime: JDBCDataSourceRuntimeMBeans: prefix: wls_datasource_ key: name - JTARuntime: prefix: wls_jta_ key: name Generate the deployment package You must generate two separate deployment packages with the restPort as 7001 or 8001 in the config.yaml file. Two packages are required because the listening ports are different for the Administration Server and Managed Servers.\nUse the getX.X.X.sh script to update the configuration file into the wls-exporter package.\nSee the following sample usage:\n$ ./get1.1.0.sh config-admin.yaml % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 607 0 607 0 0 915 0 --:--:-- --:--:-- --:--:-- 915 100 2016k 100 2016k 0 0 839k 0 0:00:02 0:00:02 --:--:-- 1696k created /tmp/ci-H1SNbxKo1b /tmp/ci-H1SNbxKo1b ~/weblogic-monitor/tmp in temp dir adding: config.yml (deflated 63%) ~/weblogic-monitor/tmp $ ls config-admin.yaml get1.1.0.sh wls-exporter.war Similarly, you must generate the deployment package for the Managed Servers with a different configuration file.\nDeploy the WebLogic Monitoring Exporter Follow these steps to deploy the package in the WebLogic Server instances.\n  Deploy the WebLogic Monitoring Exporter (wls-exporter.war) in the Administration Server and Managed Servers separately using the Oracle Enterprise Manager.\n  Select the servers to deploy.\n  Set the application name. The application name has to be different if it is deployed separately in the Administration Server and Managed Servers.\n  Set the context-root to wls-exporter for both the deployments.\n  Select Install and start application.\n  Then deploy the WebLogic Monitoring Exporter application (wls-exporter.war).\n  Activate the changes to start the application. If the application is started and the port is exposed, then you can access the WebLogic Monitoring Exporter console using this URL, http://\u0026lt;server:port\u0026gt;/wls-exporter.\n  Prometheus Operator configuration You must configure Prometheus to collect the metrics from the WebLogic Monitoring Exporter. The Prometheus Operator identifies the targets using service discovery. To get the WebLogic Monitoring Exporter end point discovered as a target, you must create a service monitor pointing to the service.\nSee the following sample service monitor deployment YAML configuration file.\nServiceMonitor for wls-exporter:\napiVersion: v1 kind: Secret metadata: name: basic-auth namespace: monitoring data: password: V2VsY29tZTE= # Welcome1 i.e.'WebLogic password' user: d2VibG9naWM= # weblogic i.e. 'WebLogic username' type: Opaque --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: wls-exporter-soainfra namespace: monitoring labels: k8s-app: wls-exporter spec: namespaceSelector: matchNames: - soans selector: matchLabels: weblogic.domainName: soainfra endpoints: - basicAuth: password: name: basic-auth key: password username: name: basic-auth key: user port: default relabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) interval: 10s honorLabels: true path: /wls-exporter/metrics The exporting of metrics from wls-exporter requires basicAuth so a Kubernetes Secret is created with the user name and password that are base64 encoded. This Secret will be used in the ServiceMonitor deployment.\nBe careful in the generation of the base64 encoded strings for the user name and password. A new line character might get appended in the encoded string and cause an authentication failure. To avoid a new line string, use the following example:\n$ echo -n \u0026quot;Welcome1\u0026quot; | base64 V2VsY29tZTE=  In the deployment YAML configuration for wls-exporter shown above, weblogic.domainName: soainfra is used as a label under spec.selector.matchLabels, so all the server services will be selected for the service monitor. Otherwise, you may have to create separate service monitors for each server if the server name is used as matching labels in spec.selector.matchLabels. The relabeling of the configuration is required, because Prometheus, by default, ignores the labels provided in the wls-exporter.\nBy default, Prometheus will not store all the labels provided by the target. In the service monitor deployment YAML configuration, it is required to mention the relabeling configuration (spec.endpoints.relabelings), so that certain labels provided by weblogic-monitoring-exporter (required for the Grafana dashboard) are stored in Prometheus. Do not delete the following section from the configuration YAML file.\nrelabelings: - action: labelmap regex: __meta_kubernetes_service_label_(.+) Add RoleBinding and Role for the WebLogic domain namespace You need to add RoleBinding for the namespace under which the WebLogic Servers pods are running in the Kubernetes cluster. This RoleBinding is required for Prometheus to access the endpoints provided by the WebLogic Monitoring Exporter. Edit the prometheus-roleBindingSpecificNamespaces.yaml file in the Prometheus Operator deployment manifests and add the RoleBinding for the namespace (soans) similar to the following example:\n- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: soans roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring Similarly, you need to add the Role for the namespace under which the WebLogic Servers pods are running in the Kubernetes cluster. Edit prometheus-roleSpecificNamespaces.yaml in the Prometheus Operator deployment manifests and add the Role for the namespace (soans) similar to the following example:\n- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: prometheus-k8s namespace: soans rules: - apiGroups: - \u0026quot;\u0026quot; resources: - services - endpoints - pods verbs: - get - list - watch Then apply prometheus-roleBindingSpecificNamespaces.yaml and prometheus-roleSpecificNamespaces.yaml for the RoleBinding and Role to take effect in the cluster.\n$ kubectl apply -f prometheus-roleBindingSpecificNamespaces.yaml $ kubectl apply -f prometheus-roleSpecificNamespaces.yaml Deploy the service monitor After creating the deployment YAML for the service monitor, deploy the service monitor using the following command:\n$ kubectl create -f wls-exporter.yaml Service discovery After the deployment of the service monitor, wls-exporter should be discovered by Prometheus and able to export metrics.\nGrafana dashboard Deploy the Grafana dashboard provided in the WebLogic Monitoring Exporter to view the domain metrics.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/domains/fmw-domain/",
	"title": "FMW Infrastructure domain",
	"tags": [],
	"description": "Sample for creating an FMW Infrastructure domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated WebLogic domain.",
	"content": "The sample scripts demonstrate the creation of an FMW Infrastructure domain home on an existing Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain. Optionally, the scripts start up the domain, and WebLogic Server pods and services.\nPrerequisites Before you begin, read this document, Domain resource.\nThe following prerequisites must be met prior to running the create domain script:\n Make sure the WebLogic Server Kubernetes Operator is running. The operator requires FMW Infrastructure 12.2.1.3.0 with patch 29135930 applied or FMW Infrastructure 12.2.1.4.0. For details on how to obtain or create the image, refer to FMW Infrastructure domains. Create a Kubernetes Namespace for the domain unless you intend to use the default namespace. In the same Kubernetes Namespace, create the Kubernetes PersistentVolume (PV) where the domain home will be hosted, and the Kubernetes PersistentVolumeClaim (PVC) for the domain. For samples to create a PV and PVC, see Create sample PV and PVC. By default, the create-domain.sh script creates a domain with the domainUID set to domain1 and expects the PVC domain1-weblogic-sample-pvc to be present. You can create domain1-weblogic-sample-pvc using create-pv-pvc.sh with an inputs file that has the domainUID set to domain1. Create the Kubernetes Secrets username and password of the administrative account in the same Kubernetes namespace as the domain. Configure access to your database. For details, see here. Create a Kubernetes Secret with the RCU credentials. For details, refer to this document.  Use the script to create a domain Make a copy of the create-domain-inputs.yaml file, and run the create script, pointing it at your inputs file and an output directory:\n$ ./create-domain.sh \\ -i create-domain-inputs.yaml \\ -o /\u0026lt;path to output-directory\u0026gt; The script will perform the following steps:\n  Create a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The path name is /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, its contents must be removed before using this script.\n  Create a Kubernetes Job that will start up a utility FMW Infrastructure container and run offline WLST scripts to create the domain on the shared storage.\n  Run and wait for the job to finish.\n  Create a Kubernetes domain YAML file, domain.yaml, in the directory that is created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command:\n$ kubectl apply -f /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml   Create a convenient utility script, delete-domain-job.yaml, to clean up the domain home created by the create script.\n  As a convenience, using the -e option, the script can optionally create the domain object, which in turn results in the creation of the corresponding WebLogic Server pods and services as well.\nThe usage of the create script is as follows:\n$ sh create-domain.sh -h usage: create-domain.sh -o dir -i file [-e] [-v] [-h] -i Parameter inputs file, must be specified. -o Output directory for the generated YAML files, must be specified. -e Also create the resources in the generated YAML files, optional. -v Validate the existence of persistentVolumeClaim, optional. -h Help If you copy the sample scripts to a different location, make sure that you copy everything in the \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts directory together into the target directory, maintaining the original directory hierarchy.\nThe default domain created by the script has the following characteristics:\n An Administration Server named admin-server listening on port 7001. A configured cluster named cluster-1 of size 5. Five Managed Servers, named managed-server1, managed-server2, and so on, listening on port 8001. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;. No applications deployed. No data sources or JMS resources. A T3 channel.  The domain creation inputs can be customized by editing create-domain-inputs.yaml.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Definition Default     adminPort Port number of the Administration Server inside the Kubernetes cluster. 7001   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. admin-server   clusterName Name of the WebLogic cluster instance to generate for the domain. cluster-1   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. It can also be set to the relative path wdt, and then the built-in WDT scripts will be used instead. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes ConfigMap, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes Pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes Job to run the script (specified in the createDomainScriptName property) in a Kubernetes Pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes Pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes Job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the WebLogic domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /shared/domains/domain1   domainPVMountPath Mount path of the domain persistent volume. /shared   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes Service name. domain1   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   httpAccessLogInLogHome Boolean indicating if server HTTP access log files should be written to the same directory as logHome. Otherwise, server HTTP access log files will be written to the directory specified in the WebLogic domain home configuration. true   image WebLogic Docker image. The operator requires FMW Infrastructure 12.2.1.3.0 with patch 29135930 applied or FMW Infrastructure 12.2.1.4.0. For details on how to obtain or create the image, see FMW Infrastructure domains. container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never. IfNotPresent   imagePullSecretName Name of the Kubernetes Secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server .out in the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to start initially for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, Node Manager log, and server HTTP access log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /shared/logs/domain1   managedServerNameBase Base string used to generate Managed Server names. managed-server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes Namespace in which to create the domain. default   persistentVolumeClaimName Name of the persistent volume claim. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. domain1-weblogic-sample-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the network access point. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only, in a single server (all-in-one) Kubernetes Deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster.   weblogicCredentialsSecretName Name of the Kubernetes Secret for the Administration Server user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. domain1-weblogic-credentials   weblogicImagePullSecretName Name of the Kubernetes Secret for the Docker Store, used to pull the WebLogic Server image. docker-store-secret   serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.   rcuSchemaPrefix The schema prefix to use in the database, for example SOA1. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. domain1   rcuDatabaseURL The database URL. database:1521/service   rcuCredentialsSecret The Kubernetes Secret containing the database credentials. domain1-rcu-credentials    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes Service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create a WebLogic domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain YAML file could also be modified to cover more use cases.\nVerify the results The create script will verify that the domain was created, and will report failure if there was any error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nNote that the example results below use the default Kubernetes Namespace. If you are using a different namespace, you need to replace NAMESPACE in the example kubectl commands with the actual Kubernetes Namespace.\nGenerated YAML files with the default inputs The content of the generated domain.yaml:\n# Copyright (c) 2017, 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026quot;weblogic.oracle/v4\u0026quot; kind: Domain metadata: name: fmw-domain namespace: default labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: fmw-domain spec: # The WebLogic Domain Home domainHome: /shared/domains/fmw-domain # Set domain home type to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: PersistentVolume # The WebLogic Server Docker image that the Operator uses to start the domain image: \u0026quot;container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4\u0026quot; # imagePullPolicy defaults to \u0026quot;Always\u0026quot; if image version is :latest imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: fmw-domain-weblogic-credentials # Whether to include the server out file into the pod's stdout, default is true includeServerOutInPodLog: true # Whether to enable log home logHomeEnabled: true # The in-pod location for domain log, server logs, server out, and Node Manager log files logHome: /shared/logs/fmw-domain # serverStartPolicy legal values are \u0026quot;NEVER\u0026quot;, \u0026quot;IF_NEEDED\u0026quot;, or \u0026quot;ADMIN_ONLY\u0026quot; # This determines which WebLogic Servers the Operator will start up when it discovers this Domain # - \u0026quot;NEVER\u0026quot; will not start any server in the domain # - \u0026quot;ADMIN_ONLY\u0026quot; will start up only the administration server (no managed servers will be started) # - \u0026quot;IF_NEEDED\u0026quot; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom \u0026quot; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: fmw-domain-weblogic-pvc volumeMounts: - mountPath: /shared name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026quot;RUNNING\u0026quot; or \u0026quot;ADMIN\u0026quot; # \u0026quot;RUNNING\u0026quot; means the listed server will be started up to \u0026quot;RUNNING\u0026quot; mode # \u0026quot;ADMIN\u0026quot; means the listed server will be start up to \u0026quot;ADMIN\u0026quot; mode serverStartState: \u0026quot;RUNNING\u0026quot; adminService: channels: # The Admin Server's NodePort - channelName: default nodePort: 30731 # Uncomment to export the T3Channel as a service # - channelName: T3Channel # clusters is used to configure the desired behavior for starting member servers of a cluster. # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: cluster-1 serverStartState: \u0026quot;RUNNING\u0026quot; replicas: 2 # The number of managed servers to start for unlisted clusters # replicas: 1 Verify the domain To confirm that the domain was created, use this command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\nHere is an example of the output of this command:\nName: fmw-domain Namespace: default Labels: weblogic.domainUID=fmw-domain weblogic.resourceVersion=domain-v2 Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026quot;apiVersion\u0026quot;:\u0026quot;weblogic.oracle/v4\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Domain\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;labels\u0026quot;:{\u0026quot;weblogic.domainUID\u0026quot;:\u0026quot;fmw-domain\u0026quot;,\u0026quot;weblogic.reso... API Version: weblogic.oracle/v4 Kind: Domain Metadata: Creation Timestamp: 2019-04-18T00:11:15Z Generation: 23 Resource Version: 5904947 Self Link: /apis/weblogic.oracle/v4/namespaces/default/domains/fmw-domain UID: 7b3477e2-616e-11e9-ab7b-000017024fa2 Spec: Admin Server: Admin Service: Annotations: Channels: Channel Name: default Node Port: 30731 Labels: Server Pod: Annotations: Container Security Context: Containers: Env: Init Containers: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Clusters: Cluster Name: cluster-1 Cluster Service: Annotations: Labels: Replicas: 4 Server Pod: Annotations: Container Security Context: Containers: Env: Init Containers: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Domain Home: /shared/domains/fmw-domain Domain Home In Image: false Image: container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Log Home: /shared/logs/fmw-domain Log Home Enabled: true Managed Servers: Server Pod: Annotations: Container Security Context: Containers: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom Init Containers: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Mount Path: /shared Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: fmw-domain-weblogic-pvc Server Service: Annotations: Labels: Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: fmw-domain-weblogic-credentials Status: Conditions: Last Transition Time: 2019-04-18T00:14:34.322Z Reason: ServersReady Status: True Type: Available Modified: true Replicas: 4 Servers: Cluster Name: cluster-1 Health: Activation Time: 2019-04-18T00:18:46.787Z Overall Health: ok Subsystems: Node Name: mark Server Name: managed-server4 State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-04-18T00:18:46.439Z Overall Health: ok Subsystems: Node Name: mark Server Name: managed-server3 State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-04-18T00:14:19.227Z Overall Health: ok Subsystems: Node Name: mark Server Name: managed-server2 State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-04-18T00:14:17.747Z Overall Health: ok Subsystems: Node Name: mark Server Name: managed-server1 State: RUNNING Health: Activation Time: 2019-04-18T00:13:13.836Z Overall Health: ok Subsystems: Node Name: mark Server Name: admin-server State: RUNNING Start Time: 2019-04-18T00:11:15.306Z Events: \u0026lt;none\u0026gt; In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued very soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. The operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Use the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE fmw-domain-admin-server 1/1 Running 0 15h fmw-domain-managed-server1 1/1 Running 0 15h fmw-domain-managed-server2 1/1 Running 0 15h fmw-domain-managed-server3 1/1 Running 0 15h fmw-domain-managed-server4 1/1 Running 0 15h Verify the services Use the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command:\n$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE fmw-domain-admin-server ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 15h fmw-domain-admin-server-external NodePort 10.101.26.42 \u0026lt;none\u0026gt; 7001:30731/TCP 15h fmw-domain-cluster-cluster-1 ClusterIP 10.107.55.188 \u0026lt;none\u0026gt; 8001/TCP 15h fmw-domain-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 15h fmw-domain-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 15h fmw-domain-managed-server3 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 15h fmw-domain-managed-server4 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 15h Delete the generated domain home Sometimes in production, but most likely in testing environments, you might want to remove the domain home that is generated using the create-domain.sh script. Do this by running the generated delete domain job script in the /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt; directory.\n$ kubectl create -f delete-domain-job.yaml "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/cicd/how-to-copy-domains/",
	"title": "Copy domains",
	"tags": [],
	"description": "How to copy domains.",
	"content": "The recommended approach to save a copy of a Domain in Image or Domain in PV domain is to simply ZIP (or tar) the domain directory. However, there is a very important caveat with this recommendation - when you unzip the domain, it must go back into exactly the same location (Domain Home) in the (new) file system. Using this approach will maintain the same domain encryption key.\nThe best practice/recommended approach is to create a \u0026ldquo;primordial domain\u0026rdquo; which does not contain any applications or resources, and to create a ZIP file of this domain before starting any servers.\n The domain ZIP must be created before starting servers.\n When servers are started the first time, they will encrypt various other data. Make sure that you create the ZIP file before starting servers for the first time. The primordial domain ZIP file should be stored in a safe place where the CI/CD can get it when needed, for example in a secured Artifactory repository (or something similar).\nRemember, anyone who gets access to this ZIP file can get access to the domain encryption key, so it needs to be protected appropriately.\n Every time you run your CI/CD pipeline to create a new mutation of the domain, it should retrieve and unzip the primordial domain first, and then apply changes to that domain using tools like WDT or WLST (see here).\n Always use external state.\n You should always keep state outside the Docker image. This means that you should use JDBC stores for leasing tables, JMS and Transaction stores, EJB timers, JMS queues, and so on. This ensures that data will not be lost when a container is destroyed.\nWe recommend that state be kept in a database to take advantage of built-in database server HA, and the fact that disaster recovery of sites across all but the shortest distances almost always requires using a single database server to consolidate and replicate data (DataGuard).\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/cicd/",
	"title": "CI/CD considerations",
	"tags": [],
	"description": "Learn about managing domain images with continuous integration and continuous delivery (CI/CD).",
	"content": "Overview In this section, we will discuss the recommended techniques for managing the evolution and mutation of Docker images to run WebLogic in Kubernetes. There are several approaches and techniques available, and the choice of which to use depends very much on your particular requirements. We will start with a review of the \u0026ldquo;problem space,\u0026rdquo; and then talk about the considerations that would lead us to choose various approaches. We will provide details about several approaches to implementing CI/CD and links to examples.\nReview of the problem space Kubernetes makes a fundamental assumption that Docker images are immutable, that they contain no state, and that updating them is as simple as throwing away a pod/container and replacing it with a new one that uses a newer version of the Docker image. These assumptions work very well for microservices applications, but for more traditional workloads, we need to do some extra thinking and some extra work to get the behavior we want.\nCI/CD is an area where the standard assumptions aren\u0026rsquo;t always suitable. In the microservices architecture, you typically minimize dependencies and build images from scratch with all of the dependencies in them. You also typically keep all of the configuration outside of the image, for example, in Kubernetes config maps or secrets, and all of the state outside of the image too. This makes it very easy to update running pods with a new image.\nLet\u0026rsquo;s consider how a WebLogic image is different. There will, of course, be a base layer with the operating system; let\u0026rsquo;s assume it is Oracle Linux \u0026ldquo;slim\u0026rdquo;. Then you need a JDK and this is very commonly in another layer. Many people will use the officially supported JDK images from the Docker Store, like the Server JRE image, for example. On top of this, you need the WebLogic Server binaries (the \u0026ldquo;Oracle Home\u0026rdquo;). On top of that, you may wish to have some patches or updates installed. And then you need your domain, that is the configuration.\nThere is also other information associated with a domain that needs to live somewhere, for example leasing tables, message and transaction stores, and so on. We recommend that these be kept in a database to take advantage of built-in database server HA, and the fact that disaster recovery of sites across all but the shortest distances, almost always requires using a single database server to consolidate and replicate data (DataGuard).\nThere are three common approaches on how to structure these components:\n The first, \u0026ldquo;domain on a persistent volume\u0026rdquo; or Domain in PV, places the JDK and WebLogic binaries in the Docker image, but the domain home is kept on a separate persistent storage outside of the image. The second, Domain in Image, puts the JDK, WebLogic binaries, and the domain home all in the Docker image. The third approach, Model in Image, puts the JDK, WebLogic binaries, and a domain model in the Docker image, and generates the domain home at runtime from the domain model.  All of these approaches are perfectly valid (and fully supported) and they have various advantages and disadvantages. We have listed the relative advantages of these approaches here.\nOne of the key differences between these approaches is how many Docker images you have, and therefore, how you build and maintain them - your image CI/CD process. Let\u0026rsquo;s take a short detour and talk about Docker image layering.\n Docker image layering  Learn about Docker image layering and why it is important.\n Why layering matters  Learn why Docker image layering affects CI/CD processes.\n Choose an approach  How to choose an approach.\n Mutate the domain layer  How to mutate the domain layer.\n Copy domains  How to copy domains.\n Tools  Tools that are available to build CI/CD pipelines.\n "
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/ingress/",
	"title": "Ingress",
	"tags": [],
	"description": "Load balancer sample scripts.",
	"content": "The Oracle WebLogic Server Kubernetes Operator supports three load balancers: Traefik, Voyager, and Apache. We provide samples that demonstrate how to install and configure each one. The samples are located in following folders:\n  Traefik Traefik is recommended for development and test environments only. For production environments, we recommend Apache or Voyager ingress controllers, or the load balancer provided by your cloud provider.\n  Voyager\n  Apache-samples/custom-sample\n  Apache-samples/default-sample\n  Ingress-per-domain\n  Apache-webtier\n  The Apache-webtier script contains a Helm chart that is used in the Apache samples.\n "
},
{
	"uri": "/weblogic-kubernetes-operator/developerguide/coding-standards/",
	"title": "Coding standards",
	"tags": [],
	"description": "",
	"content": "This project has adopted the following coding standards:\n Code will be formated using Oracle / WebLogic standards, which are identical to the Google Java Style. Javadoc must be provided for all public packages, classes, and methods, and must include all parameters and returns. Javadoc is not required for methods that override or implement methods that are already documented. All non-trivial methods should include LOGGER.entering() and LOGGER.exiting() calls. The LOGGER.exiting() call should include the value that is going to be returned from the method, unless that value includes a credential or other sensitive information. All logged messages must be internationalized using the resource bundle src/main/resources/Operator.properties and using a key itemized in src/main/java/oracle/kubernetes/operator/logging/MessageKeys.java. After operator initialization, all operator work must be implemented using the asynchronous call model (described below). In particular, worker threads must not use sleep() or IO or lock-based blocking methods.  Code formatting plugins The following IDE plugins are available to assist with following the code formatting standards\nIntelliJ An IntelliJ plugin is available from the plugin repository.\nThe plugin will be enabled by default. To disable it in the current project, go to File \u0026gt; Settings... \u0026gt; google-java-format Settings (or IntelliJ IDEA \u0026gt; Preferences... \u0026gt; Other Settings \u0026gt; google-java-format Settings on macOS) and uncheck the \u0026ldquo;Enable google-java-format\u0026rdquo; checkbox.\nTo disable it by default in new projects, use File \u0026gt; Other Settings \u0026gt; Default Settings....\nWhen enabled, it will replace the normal \u0026ldquo;Reformat Code\u0026rdquo; action, which can be triggered from the \u0026ldquo;Code\u0026rdquo; menu or with the Ctrl-Alt-L (by default) keyboard shortcut.\nThe import ordering is not handled by this plugin, unfortunately. To fix the import order, download the IntelliJ Java Google Style file and import it into File→Settings→Editor→Code Style.\nEclipse An Eclipse plugin can be downloaded from the releases page. Drop it into the Eclipse drop-ins folder to activate the plugin.\nThe plugin adds a google-java-format formatter implementation that can be configured in Eclipse \u0026gt; Preferences \u0026gt; Java \u0026gt; Code Style \u0026gt; Formatter \u0026gt; Formatter Implementation.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/security/rbac/",
	"title": "RBAC",
	"tags": [],
	"description": "Operator role-based authorization",
	"content": "Contents  Overview Operator RBAC definitions  Role and RoleBinding naming conventions ClusterRole and ClusterRoleBinding naming conventions   RoleBindings ClusterRoleBindings  Overview The operator assumes that certain Kubernetes Roles are created in the Kubernetes cluster. The operator Helm chart creates the required ClusterRoles, ClusterRoleBindings, Roles, and RoleBindings for the ServiceAccount that is used by the operator. The operator will also attempt to verify that the RBAC settings are correct when the operator starts running.\nFor more information about the Kubernetes ServiceAccount used by the operator, see Service Accounts.\n The general design goal is to provide the operator with the minimum amount of permissions that the operator requires and to favor built-in roles over custom roles where it make sense to use the Kubernetes built-in roles.\nFor more information about Kubernetes Roles, see the Kubernetes RBAC documentation.\n Operator RBAC definitions To display the Kubernetes Roles and related Bindings used by the operator where the operator was installed using the Helm release name weblogic-operator, look for the Kubernetes objects, Role, RoleBinding, ClusterRole, and ClusterRoleBinding, when using the Helm status command:\n$ helm status weblogic-operator Assuming the operator was installed into the namespace weblogic-operator-ns with a target namespaces of domain1-ns, the following commands can be used to display a subset of the Kubernetes Roles and related RoleBindings:\n$ kubectl describe clusterrole \\  weblogic-operator-ns-weblogic-operator-clusterrole-general $ kubectl describe clusterrolebinding \\  weblogic-operator-ns-weblogic-operator-clusterrolebinding-general $ kubectl -n weblogic-operator-ns \\  describe role weblogic-operator-role $ kubectl -n domain1-ns \\  describe rolebinding weblogic-operator-rolebinding-namespace \\ Kubernetes Role and RoleBinding naming conventions The following naming pattern is used for the Role and RoleBinding objects:\n weblogic-operator-\u0026lt;type\u0026gt;-\u0026lt;optional-role-name\u0026gt;  Using:\n \u0026lt;type\u0026gt; as the kind of Kubernetes object:  role rolebinding   \u0026lt;optional-role-name\u0026gt; as an optional name given to the Role or RoleBinding  For example: namespace    A complete name for an operator created Kubernetes RoleBinding would be:\n weblogic-operator-rolebinding-namespace\n Kubernetes ClusterRole and ClusterRoleBinding naming conventions The following naming pattern is used for the ClusterRole and ClusterRoleBinding objects:\n \u0026lt;operator-ns\u0026gt;-weblogic-operator-\u0026lt;type\u0026gt;-\u0026lt;role-name\u0026gt;  Using:\n \u0026lt;operator-ns\u0026gt; as the namespace in which the operator is installed  For example: weblogic-operator-ns   \u0026lt;type\u0026gt; as the kind of Kubernetes object:  clusterrole clusterrolebinding   \u0026lt;role-name\u0026gt; as the name given to the Role or RoleBinding  For example: general    A complete name for an operator created Kubernetes ClusterRoleBinding would be:\n weblogic-operator-ns-weblogic-operator-clusterrolebinding-general\n RoleBindings Assuming that the operator was installed into the Kubernetes Namespace weblogic-operator-ns, and a target namespace for the operator is domain1-ns, the following RoleBinding entries are mapped to a Role or ClusterRole granting permission to the operator.\n   RoleBinding Mapped to Role Resource Access Notes     weblogic-operator-rolebinding weblogic-operator-role Edit: secrets, configmaps, events The RoleBinding is created in the namespace weblogic-operator-ns 1   weblogic-operator-rolebinding-namespace Operator Cluster Role namespace Read: secrets, pods/log The RoleBinding is created in the namespace domain1-ns 2     Edit: configmaps, events, pods, services, jobs.batch      Create: pods/exec     ClusterRoleBindings Assuming that the operator was installed into the Kubernetes Namespace weblogic-operator-ns, the following ClusterRoleBinding entries are mapped to a ClusterRole granting permission to the operator.\nNote: The operator names in table below represent the \u0026lt;role-name\u0026gt; from the cluster names section.\n   ClusterRoleBinding Mapped to Cluster Role Resource Access Notes     Operator general Operator general Read: namespaces 3     Edit: customresourcedefinitions      Update: domains (weblogic.oracle), domains/status      Create: tokenreviews, selfsubjectrulesreviews    Operator nonresource Operator nonresource Get: /version/* 1   Operator discovery Kubernetes system:discovery See: Kubernetes Discovery Roles 3   Operator auth-delegator Kubernetes system:auth-delegator See: Kubernetes Component Roles 3      The binding is assigned to the operator ServiceAccount. \u0026#x21a9;\u0026#xfe0e;\n The binding is assigned to the operator ServiceAccount in each namespace listed with the domainNamespaces setting. The domainNamespaces setting contains the list of namespaces that the operator is configured to manage. \u0026#x21a9;\u0026#xfe0e;\n The binding is assigned to the operator ServiceAccount. In addition, the Kubernetes RBAC resources that the operator installation actually set up will be adjusted based on the value of the dedicated setting. By default, the dedicated value is set to false, those security resources are created as ClusterRole and ClusterRoleBindings. If the dedicated value is set to true, those resources will be created as Roles and RoleBindings in the namespace of the operator. \u0026#x21a9;\u0026#xfe0e;\n   "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/configoverrides/",
	"title": "Configuration overrides",
	"tags": [],
	"description": "",
	"content": "Contents  Overview Prerequisites Typical overrides Unsupported overrides Override template names and syntax  Override template names Override template schemas Override template macros Override template syntax special requirements Override template samples   Step-by-step guide Debugging Internal design flow   Overview Configuration overrides can only be used in combination with Domain in Image and Domain in PV domains. For Model in Image domains, use Model in Image Runtime Updates instead.\n Use configuration overrides (also called situational configuration) to customize a Domain in Image or Domain in PV domain\u0026rsquo;s WebLogic domain home configuration without modifying the domain\u0026rsquo;s actual config.xml or system resource files. For example, you may want to override a JDBC data source XML module user name, password, and URL so that it references a local database.\nYou can use overrides to customize domains as they are moved from QA to production, are deployed to different sites, or are even deployed multiple times at the same site.\nHow do you specify overrides?  Make sure your domain home meets the prerequisites. See Prerequisites. Make sure your overrides are supported. See Typical overrides and Unsupported overrides. Create a Kubernetes configuration map that contains:  Override templates (also known as situational configuration templates), with names and syntax as described in Override template names and syntax. A file named version.txt that contains the exact string 2.0.   Set your domain resource configuration.overridesConfigMap to the name of this configuration map. If templates leverage secret macros:  Create Kubernetes Secrets that contain template macro values. Set your domain configuration.secrets to reference the aforementioned secrets.   Stop all running WebLogic Server pods in your domain. (See Starting and stopping servers.) Start or restart your domain. (See Starting and stopping servers and Restarting servers.) Verify your overrides are taking effect. (See Debugging).  For a detailed walk-through of these steps, see the Step-by-step guide.\nHow do overrides work during runtime?  When a domain is first deployed, or is restarted after shutting down all the WebLogic Server pods, the operator will:  Resolve any macros in your override templates. Place expanded override templates in the optconfig directory located in each WebLogic domain home directory.   When the WebLogic Servers start, they will:  Automatically load the override files from the optconfig directory. Use the override values in the override files instead of the values specified in their config.xml or system resource XML files.    For a detailed walk-through of the runtime flow, see the Internal design flow.\n Prerequisites   Configuration overrides can be used in combination with Domain in Image and Domain in PV domains in releases before 3.0.0-rc1. In release candidate 3.0.0-rc1, configuration overrides can be used in combination with Domain in Image and Domain in PV domains (the domainHomeSourceType must be either PersistentVolume or Image). For Model in Image domains (introduced in 3.0.0-rc1) (domainHomeSourceType is FromModel), use Model in Image Runtime Updates instead.\n  A WebLogic domain home must not contain any situational configuration XML file in its optconfig directory that was not placed there by the operator. Any existing situational configuration XML files in this directory will be deleted and replaced by your operator override templates (if any).\n  If you want to override a JDBC, JMS, or WLDF (diagnostics) module, then the original module must be located in your domain home config/jdbc, config/jms, and config/diagnostics directory, respectively. These are the default locations for these types of modules.\n   Typical overrides Typical attributes for overrides include:\n User names, passwords, and URLs for:  JDBC data sources JMS bridges, foreign servers, and SAF   Network channel external/public addresses  For remote RMI clients (T3, JMS, EJB) For remote WLST clients   Network channel external/public ports  For remote RMI clients (T3, JMS, EJB)   Debugging Tuning (MaxMessageSize, and such)   Unsupported overrides IMPORTANT: The operator does not support custom overrides in the following areas.\n Domain topology (cluster members) Network channel listen address, port, and enabled configuration Server and domain log locations Node Manager related configuration Changing any existing MBean name Adding or removing a module (for example, a JDBC module)  Specifically, do not use custom overrides for:\n Adding or removing:  Servers Clusters Network Access Points (custom channels) Modules   Changing any of the following:  Dynamic cluster size Default, SSL, and Admin channel Enabled, listen address, and port Network Access Point (custom channel), listen address, or port Server and domain log locations \u0026ndash; use the logHome domain setting instead Node Manager access credentials Any existing MBean name (for example, you cannot change the domain name)    Note that it\u0026rsquo;s OK, even expected, to override network access point public or external addresses and ports. Also note that external access to JMX (MBean) or online WLST requires that the network access point internal port and external port match (external T3 or HTTP tunneling access to JMS, RMI, or EJBs don\u0026rsquo;t require port matching).\nThe behavior when using an unsupported override is undefined.\n Override template names and syntax Overrides leverage a built-in WebLogic feature called \u0026ldquo;Configuration Overriding\u0026rdquo; which is often informally called \u0026ldquo;Situational Configuration.\u0026rdquo; Situational configuration consists of XML formatted files that closely resemble the structure of WebLogic config.xml and system resource module XML files. In addition, the attribute fields in these files can embed add, replace, and delete verbs to specify the desired override action for the field.\nOverride template names The operator requires a different file name format for override templates than WebLogic\u0026rsquo;s built-in situational configuration feature. It converts the names to the format required by situational configuration when it moves the templates to the domain home optconfig directory. The following table describes the format:\n   Original Configuration Required Override Name     config.xml config.xml   JMS module jms-MODULENAME.xml   JDBC module jdbc-MODULENAME.xml   Diagnostics module diagnostics-MODULENAME.xml    A MODULENAME must correspond to the MBean name of a system resource defined in your original config.xml file. It\u0026rsquo;s not possible to add a new module by using overrides. If you need your overrides to set up a new module, then have your original configuration specify \u0026lsquo;skeleton\u0026rsquo; modules that can be overridden.\nOverride template schemas An override template must define the exact schemas required by the situational configuration feature. The schemas vary based on the file type you wish to override.\nconfig.xml\n\u0026lt;?xml version='1.0' encoding='UTF-8'?\u0026gt; \u0026lt;d:domain xmlns:d=\u0026quot;http://xmlns.oracle.com/weblogic/domain\u0026quot; xmlns:f=\u0026quot;http://xmlns.oracle.com/weblogic/domain-fragment\u0026quot; xmlns:s=\u0026quot;http://xmlns.oracle.com/weblogic/situational-config\u0026quot;\u0026gt; ... \u0026lt;/d:domain\u0026gt; jdbc-MODULENAME.xml\n\u0026lt;?xml version='1.0' encoding='UTF-8'?\u0026gt; \u0026lt;jdbc:jdbc-data-source xmlns:jdbc=\u0026quot;http://xmlns.oracle.com/weblogic/jdbc-data-source\u0026quot; xmlns:f=\u0026quot;http://xmlns.oracle.com/weblogic/jdbc-data-source-fragment\u0026quot; xmlns:s=\u0026quot;http://xmlns.oracle.com/weblogic/situational-config\u0026quot;\u0026gt; ... \u0026lt;/jdbc:jdbc-data-source\u0026gt; jms-MODULENAME.xml\n\u0026lt;?xml version='1.0' encoding='UTF-8'?\u0026gt; \u0026lt;jms:weblogic-jms xmlns:jms=\u0026quot;http://xmlns.oracle.com/weblogic/weblogic-jms\u0026quot; xmlns:f=\u0026quot;http://xmlns.oracle.com/weblogic/weblogic-jms-fragment\u0026quot; xmlns:s=\u0026quot;http://xmlns.oracle.com/weblogic/situational-config\u0026quot; \u0026gt; ... \u0026lt;/jms:weblogic-jms\u0026gt; diagnostics-MODULENAME.xml\n\u0026lt;?xml version='1.0' encoding='UTF-8'?\u0026gt; \u0026lt;wldf:wldf-resource xmlns:wldf=\u0026quot;http://xmlns.oracle.com/weblogic/weblogic-diagnostics\u0026quot; xmlns:f=\u0026quot;http://xmlns.oracle.com/weblogic/weblogic-diagnostics-fragment\u0026quot; xmlns:s=\u0026quot;http://xmlns.oracle.com/weblogic/situational-config\u0026quot; \u0026gt; ... \u0026lt;/wldf:wldf-resource\u0026gt; Override template macros The operator supports embedding macros within override templates. This helps make your templates flexibly handle multiple use cases, such as specifying a different URL, user name, and password for a different deployment.\nTwo types of macros are supported, environment variable macros and secret macros:\n  Environment variable macros have the syntax ${env:ENV-VAR-NAME}, where the supported environment variables include DOMAIN_UID, DOMAIN_NAME, DOMAIN_HOME, and LOG_HOME.\n  Secret macros have the syntax ${secret:SECRETNAME.SECRETKEY} and ${secret:SECRETNAME.SECRETKEY:encrypt}.\n  The secret macro SECRETNAME field must reference the name of a Kubernetes Secret, and the SECRETKEY field must reference a key within that secret. For example, if you have created a secret named dbuser with a key named username that contains the value scott, then the macro ${secret:dbuser.username} will be replaced with the word scott before the template is copied into its WebLogic Server pod.\nSECURITY NOTE: Use the :encrypt suffix in a secret macro to encrypt its replacement value with the WebLogic WLST encrypt command (instead of leaving it at its plain text value). This is useful for overriding MBean attributes that expect encrypted values, such as the password-encrypted field of a data source, and is also useful for ensuring that a custom override situational configuration file the operator places in the domain home does not expose passwords in plain-text.\nOverride template syntax special requirements Check each item below for best practices and to ensure custom situational configuration takes effect:\n Reference the name of the current bean and each parent bean in any hierarchy you override.  Note that the combine-mode verbs (add and replace) should be omitted for beans that are already defined in your original domain home configuration.  See Override template samples for examples.     Use situational config replace and add verbs as follows:  If you are adding a new bean that doesn\u0026rsquo;t already exist in your original domain home config.xml, then specify add on the MBean itself and on each attribute within the bean.  See the server-debug stanza in Override template samples for an example.   If you are adding a new attribute to an existing bean in the domain home config.xml, then the attribute needs an add verb.  See the max-message-size stanza in Override template samples for an example.   If you are changing the value of an existing attribute within a domain home config.xml, then the attribute needs a replace verb.  See the public-address stanza in Override template samples for an example.     When overriding config.xml:  The XML namespace (xmlns: in the XML) must be exactly as specified in Override template schemas.  For example, use d: to reference config.xml beans and attributes, f: for add and replace domain-fragment verbs, and s: to reference the situational configuration schema.   Avoid specifying the domain name stanza, as this may cause some overrides to be ignored (for example, server-template scoped overrides).   When overriding modules:  It is a best practice to use XML namespace abbreviations jms:, jdbc:, and wldf: respectively for JMS, JDBC, and WLDF (diagnostics) module override files. A module must already exist in your original configuration if you want to override it; it\u0026rsquo;s not possible to add a new module by using overrides. If you need your overrides to set up a new module, then have your original configuration specify \u0026lsquo;skeleton\u0026rsquo; modules that can be overridden. See Overriding a data source module for best practice advice. Note that similar advice applies generally to other module types.   Consider having your original configuration reference invalid user names, passwords, and URLs:  If your original (non-overridden) configuration references non-working user names, passwords, and URLS, then this helps guard against accidentally deploying a working configuration that\u0026rsquo;s invalid for the intended environment. For example, if your base configuration references a working QA database, and there is some mistake in setting up overrides, then it\u0026rsquo;s possible the running servers will connect to the QA database when you deploy to your production environment.    Override template samples Here are some sample template override files.\nOverriding config.xml The following config.xml override file demonstrates:\n Setting the max-message-size field on a WebLogic Server named admin-server. It assumes the original config.xml does not define this value, and so uses add instead of replace. Sets the public-address and public-port fields with values obtained from a secret named test-host with keys hostname and port. It assumes the original config.xml already sets these fields, and so uses replace instead of add. Sets two debug settings. It assumes the original config.xml does not have a server-debug stanza, so it uses add throughout the entire stanza.  \u0026lt;?xml version='1.0' encoding='UTF-8'?\u0026gt; \u0026lt;d:domain xmlns:d=\u0026quot;http://xmlns.oracle.com/weblogic/domain\u0026quot; xmlns:f=\u0026quot;http://xmlns.oracle.com/weblogic/domain-fragment\u0026quot; xmlns:s=\u0026quot;http://xmlns.oracle.com/weblogic/situational-config\u0026quot; \u0026gt; \u0026lt;d:server\u0026gt; \u0026lt;d:name\u0026gt;admin-server\u0026lt;/d:name\u0026gt; \u0026lt;d:max-message-size f:combine-mode=\u0026quot;add\u0026quot;\u0026gt;78787878\u0026lt;/d:max-message-size\u0026gt; \u0026lt;d:server-debug f:combine-mode=\u0026quot;add\u0026quot;\u0026gt; \u0026lt;d:debug-server-life-cycle f:combine-mode=\u0026quot;add\u0026quot;\u0026gt;true\u0026lt;/d:debug-server-life-cycle\u0026gt; \u0026lt;d:debug-jmx-core f:combine-mode=\u0026quot;add\u0026quot;\u0026gt;true\u0026lt;/d:debug-jmx-core\u0026gt; \u0026lt;/d:server-debug\u0026gt; \u0026lt;d:network-access-point\u0026gt; \u0026lt;d:name\u0026gt;T3Channel\u0026lt;/d:name\u0026gt; \u0026lt;d:public-address f:combine-mode=\u0026quot;replace\u0026quot;\u0026gt;${secret:test-host.hostname}\u0026lt;/d:public-address\u0026gt; \u0026lt;d:public-port f:combine-mode=\u0026quot;replace\u0026quot;\u0026gt;${secret:test-host.port}\u0026lt;/d:public-port\u0026gt; \u0026lt;/d:network-access-point\u0026gt; \u0026lt;/d:server\u0026gt; \u0026lt;/d:domain\u0026gt; Overriding a data source module The following jdbc-testDS.xml override template demonstrates setting the URL, user name, and password-encrypted fields of a JDBC module named testDS by using secret macros. The generated situational configuration that replaces the macros with secret values will be located in the DOMAIN_HOME/optconfig/jdbc directory. The password-encrypted field will be populated with an encrypted value because it uses a secret macro with an :encrypt suffix. The secret is named dbsecret and contains three keys: url, username, and password.\nBest practices for data source modules and their overrides:\n A data source module must already exist in your original configuration if you want to override it; it\u0026rsquo;s not possible to add a new module by using overrides. If you need your overrides to set up a new module, then have your original configuration specify \u0026lsquo;skeleton\u0026rsquo; modules that can be overridden. See the next two bulleted items for the typical contents of a skeleton data source module. Set your original (non-overridden) URL, username, and password to invalid values. This helps prevent accidentally starting a server without overrides, and then having the data source successfully connect to a database that\u0026rsquo;s wrong for the current environment. For example, if these attributes are set to reference a QA database in your original configuration, then a mistake configuring overrides in your production Kubernetes Deployment could cause your production applications to use your QA database. Set your original (non-overridden) JDBCConnectionPoolParams MinCapacity and InitialCapacity to 0, and set your original DriverName to a reference an existing JDBC Driver. This ensures that you can still successfully boot a server even when you have configured invalid URL/username/password values, your database isn\u0026rsquo;t running, or you haven\u0026rsquo;t specified your overrides yet.  \u0026lt;?xml version='1.0' encoding='UTF-8'?\u0026gt; \u0026lt;jdbc:jdbc-data-source xmlns:jdbc=\u0026quot;http://xmlns.oracle.com/weblogic/jdbc-data-source\u0026quot; xmlns:f=\u0026quot;http://xmlns.oracle.com/weblogic/jdbc-data-source-fragment\u0026quot; xmlns:s=\u0026quot;http://xmlns.oracle.com/weblogic/situational-config\u0026quot;\u0026gt; \u0026lt;jdbc:name\u0026gt;testDS\u0026lt;/jdbc:name\u0026gt; \u0026lt;jdbc:jdbc-driver-params\u0026gt; \u0026lt;jdbc:url f:combine-mode=\u0026quot;replace\u0026quot;\u0026gt;${secret:dbsecret.url}\u0026lt;/jdbc:url\u0026gt; \u0026lt;jdbc:properties\u0026gt; \u0026lt;jdbc:property\u0026gt; \u0026lt;jdbc:name\u0026gt;user\u0026lt;/jdbc:name\u0026gt; \u0026lt;jdbc:value f:combine-mode=\u0026quot;replace\u0026quot;\u0026gt;${secret:dbsecret.username}\u0026lt;/jdbc:value\u0026gt; \u0026lt;/jdbc:property\u0026gt; \u0026lt;/jdbc:properties\u0026gt; \u0026lt;jdbc:password-encrypted f:combine-mode=\u0026quot;replace\u0026quot;\u0026gt;${secret:dbsecret.password:encrypt}\u0026lt;/jdbc:password-encrypted\u0026gt; \u0026lt;/jdbc:jdbc-driver-params\u0026gt; \u0026lt;/jdbc:jdbc-data-source\u0026gt;  Step-by-step guide  Make sure your domain home meets the prerequisites. See Prerequisites. Make sure your overrides are supported. See Typical overrides and Unsupported overrides. Create a directory containing (A) a set of situational configuration templates for overriding the MBean properties you want to replace and (B) a version.txt file.  This directory must not contain any other files. The version.txt file must contain exactly the string 2.0.  Note: This version.txt file must stay 2.0 even when you are updating your templates from a previous deployment.   Templates must not override the settings listed in Unsupported overrides. Templates must be formatted and named as per Override template names and syntax. Templates can embed macros that reference environment variables or Kubernetes Secrets. See Override template macros.   Create a Kubernetes configuration map from the directory of templates.   The configuration map must be in the same Kubernetes Namespace as the domain.\n  If the configuration map is going to be used by a single DOMAIN_UID, then we recommend adding the weblogic.domainUID=\u0026lt;mydomainuid\u0026gt; label to help track the resource.\n  For example, assuming ./mydir contains your version.txt and situation configuration template files:\nkubectl -n MYNAMESPACE create cm MYCMNAME --from-file ./mydir kubectl -n MYNAMESPACE label cm MYCMNAME weblogic.domainUID=DOMAIN_UID    Create any Kubernetes Secrets referenced by a template \u0026lsquo;secret macro\u0026rsquo;.   Secrets can have multiple keys (files) that can hold either cleartext or base64 values. We recommend that you use base64 values for passwords by using Opaque type secrets in their data field, so that they can\u0026rsquo;t be easily read at a casual glance. For more information, see https://kubernetes.io/docs/concepts/configuration/secret/.\n  Secrets must be in the same Kubernetes Namespace as the domain.\n  If a secret is going to be used by a single DOMAIN_UID, then we recommend adding the weblogic.domainUID=\u0026lt;mydomainuid\u0026gt; label to help track the resource.\n  For example:\nkubectl -n MYNAMESPACE create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret kubectl -n MYNAMESPACE label secret my-secret weblogic.domainUID=DOMAIN_UID    Configure the name of the configuration map in the domain CR configuration.overridesConfigMap field. Configure the names of each secret in domain CR.  If the secret contains the WebLogic admin username and password keys, then set the domain CR webLogicCredentialsSecret field. For all other secrets, add them to the domain CR configuration.secrets field. Note: This must be in an array format even if you only add one secret (see the sample domain resource YAML below).   Any override changes require stopping all WebLogic pods, applying your domain resource (if it changed), and restarting the WebLogic pods before they can take effect.  Custom override changes on an existing running domain, such as updating an override configuration map, a secret, or a domain resource, will not take effect until all running WebLogic Server pods in your domain are shutdown (so no servers are left running), and the domain is subsequently restarted with your new domain resource (if it changed), or with your existing domain resource (if you haven\u0026rsquo;t changed it). To stop all running WebLogic Server pods in your domain, apply a changed resource, and then start/restart the domain:  Set your domain resource serverStartPolicy to NEVER, wait, and apply your latest domain resource with the serverStartPolicy restored back to ALWAYS or IF_NEEDED (See Starting and stopping servers.) Or delete your domain resource, wait, and apply your (potentially changed) domain resource.     See Debugging for ways to check if the situational configuration is taking effect or if there are errors.  Example domain resource YAML:\napiVersion: \u0026quot;weblogic.oracle/v2\u0026quot; kind: Domain metadata: name: domain1 namespace: default labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: [ ... ] webLogicCredentialsSecret: name: domain1-wl-credentials-secret configuration: overridesConfigMap: domain1-overrides-config-map secrets: [my-secret, my-other-secret] [ ... ]  Debugging Incorrectly formatted override files may be accepted without warnings or errors and may not prevent WebLogic pods from booting. So, it is important to make sure that the template files are correct in a QA environment, otherwise your WebLogic Servers may start even though critically required overrides are failing to take effect.\nOn WebLogic Servers that support the weblogic.SituationalConfig.failBootOnError system property ( Note: It is not supported in WebLogic Server 12.2.1.3.0 ), by default the WebLogic Server will fail to boot if any situational configuration files are invalid, or if it encounters an error while loading situational configuration files. By setting the FAIL_BOOT_ON_SITUATIONAL_CONFIG_ERROR environment variable in the Kubernetes containers for the WebLogic Servers to false, you can start up the WebLogic Servers even with incorrectly formatted override files.\n  Make sure you\u0026rsquo;ve followed each step in the Step-by-step guide.\n  If WebLogic pods do not come up at all, then:\n In the domain\u0026rsquo;s namespace, see if you can find a job named DOMAIN_UID-introspect-domain-job and a corresponding pod named something like DOMAIN_UID-introspect-domain-job-xxxx. If so, examine:  kubectl -n MYDOMAINNAMESPACE describe job INTROSPECTJOBNAME kubectl -n MYDOMAINNAMESPACE logs INTROSPECTPODNAME   Check your operator log for Warning/Error/Severe messages.  kubectl -n MYOPERATORNAMESPACE logs OPERATORPODNAME      If WebLogic pods do start, then:\n Search your Administration Server pod\u0026rsquo;s kubectl log for the keyword situational, for example kubectl logs MYADMINPOD | grep -i situational.  The only WebLogic Server log lines that match should look something like:  \u0026lt;Dec 14, 2018 12:20:47 PM UTC\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Management\u0026gt; \u0026lt;BEA-141330\u0026gt; \u0026lt;Loading situational configuration file: /shared/domains/domain1/optconfig/custom-situational-config.xml\u0026gt; This line indicates a situational configuration file has been loaded.   If the search yields Warning or Error lines, then the format of the custom situational configuration template is incorrect, and the Warning or Error text should describe the problem. Note: The following exception may show up in your server logs when overriding JDBC modules. It is not expected to affect runtime behavior, and can be ignored (a fix is pending for them): java.lang.NullPointerException at weblogic.management.provider.internal.situationalconfig.SituationalConfigManagerImpl.registerListener(SituationalConfigManagerImpl.java:227) at weblogic.management.provider.internal.situationalconfig.SituationalConfigManagerImpl.start(SituationalConfigManagerImpl.java:319) ... at weblogic.management.configuration.DomainMBeanImpl.setJDBCSystemResources(DomainMBeanImpl.java:11444) ...    Look in your DOMAIN_HOME/optconfig directory.  This directory, or a subdirectory within this directory, should contain each of your custom situational configuration files. If it doesn\u0026rsquo;t, then this likely indicates your domain resource configuration.overridesConfigMap was not set to match your custom override configuration map name, or that your custom override configuration map does not contain your override files.      If the Administration Server pod does start but fails to reach ready state or tries to restart:\n Check for this message  WebLogic Server failed to start due to missing or invalid situational configuration files in the Administration Server pod\u0026rsquo;s kubectl log  This suggests that the Administration Server failure to start may have been caused by errors found in a configuration override file.  Lines containing the String situational may be found in the Administration Server pod log to provide more hints. For example:  \u0026lt;Jun 20, 2019 3:48:45 AM GMT\u0026gt; \u0026lt;Warning\u0026gt; \u0026lt;Management\u0026gt; \u0026lt;BEA-141323\u0026gt; \u0026lt;The situational config file has an invalid format, it is being ignored: XMLSituationalConfigFile[/shared/domains/domain1/optconfig/jdbc/testDS-0527-jdbc-situational-config.xml] because org.xml.sax.SAXParseException; lineNumber: 8; columnNumber: 3; The element type \u0026quot;jdbc:jdbc-driver-params\u0026quot; must be terminated by the matching end-tag \u0026quot;\u0026lt;/jdbc:jdbc-driver-params\u0026gt;\u0026quot;.   The warning message suggests a syntax error is found in the provided configuration override file for the testDS JDBC datasource.        If you\u0026rsquo;d like to verify that the situational configuration is taking effect in the WebLogic MBean tree, then one way to do this is to compare the server config and domain config MBean tree values.\n The domain config value should reflect the original value in your domain home configuration. The server config value should reflect the overridden value. For example, assuming your DOMAIN_UID is domain1, and your domain contains a WebLogic Server named admin-server, then:  kubectl exec -it domain1-admin-server /bin/bash $ wlst.sh \u0026gt; connect(MYADMINUSERNAME, MYADMINPASSWORD, 't3://domain1-admin-server:7001') \u0026gt; domainConfig() \u0026gt; get('/Servers/admin-server/MaxMessageSize') \u0026gt; serverConfig() \u0026gt; get('/Servers/admin-server/MaxMessageSize') \u0026gt; exit()   To cause the WebLogic situational configuration feature to produce additional debugging information in the WebLogic Server logs, configure the JAVA_OPTIONS environment variable in your domain resource with:\n  -Dweblogic.debug.DebugSituationalConfig=true -Dweblogic.debug.DebugSituationalConfigDumpXml=true  NOTE: The WebLogic Server Administration Console will not reflect any override changes. You cannot use the Console to verify overrides are taking effect.   Internal design flow  When a domain is first deployed, or is restarted, the operator runtime creates a Kubernetes introspector job named DOMAIN_UID-introspect-domain-job. The introspector job\u0026rsquo;s pod:  Mounts the Kubernetes configuration map and secrets specified by using the operator domain resource configuration.overridesConfigMap, webLogicCredentialsSecret, and configuration.secrets fields. Reads the mounted situational configuration templates from the configuration map and expands them to create the actual situational configuration files for the domain:  It expands some fixed replaceable values (for example, ${env:DOMAIN_UID}). It expands referenced secrets by reading the value from the corresponding mounted secret file (for example, ${secret:mysecret.mykey}). It optionally encrypts secrets using offline WLST to encrypt the value - useful for passwords (for example, ${secret:mysecret.mykey:encrypt}). It returns expanded situational configuration files to the operator. It reports any errors when attempting expansion to the operator.     The operator runtime:  Reads the expanded situational configuration files or errors from the introspector. And, if the introspector reported no errors, it:  Puts situational configuration files in a configuration map named DOMAIN_UID-weblogic-domain-introspect-cm. Mounts this configuration map into the WebLogic Server pods. Starts the WebLogic Server pods.   Otherwise, if the introspector reported errors, it:  Logs warning, error, or severe messages. Will not start WebLogic Server pods.     The startServer.sh script in the WebLogic Server pods:  Copies the expanded situational configuration files to a special location where the WebLogic runtime can find them:  config.xml overrides are copied to the optconfig directory in its domain home. Module overrides are copied to the optconfig/jdbc, optconfig/jms, or optconfig/diagnostics directory.   Deletes any situational configuration files in the optconfig directory that do not have corresponding template files in the configuration map.   WebLogic Servers read their overrides from their domain home\u0026rsquo;s optconfig directory.  "
},
{
	"uri": "/weblogic-kubernetes-operator/reference/",
	"title": "Reference",
	"tags": [],
	"description": "",
	"content": "See the following reference documentation.\n Javadoc  Java API documentation.\n Swagger  Swagger REST API documentation.\n Domain resource  Use this document to set up and configure your own domain resource.\n "
},
{
	"uri": "/weblogic-kubernetes-operator/quickstart/prepare/",
	"title": "Prepare for a domain",
	"tags": [],
	"description": "",
	"content": "  Create a namespace that can host one or more domains:\n$ kubectl create namespace sample-domain1-ns   Use helm to configure the operator to manage domains in this namespace:\n$ helm upgrade sample-weblogic-operator kubernetes/charts/weblogic-operator \\  --namespace sample-weblogic-operator-ns \\  --reuse-values \\  --set \u0026#34;domainNamespaces={sample-domain1-ns}\u0026#34; \\  --wait   Configure Traefik to manage ingresses created in this namespace:\n$ helm upgrade traefik-operator stable/traefik \\  --namespace traefik \\  --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik,sample-domain1-ns}\u0026#34; \\  --wait   If you have reached this point while following the \u0026ldquo;Model in Image\u0026rdquo; sample, please stop here and return to the sample instructions.\n "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/experimental/",
	"title": "Experimental features",
	"tags": [],
	"description": "Learn about experimental features included in the operator.",
	"content": "This section provides details of experimental features in the operator. These features are not considered \u0026ldquo;complete\u0026rdquo; but are included as a \u0026ldquo;preview\u0026rdquo; to allow users to experiment with them and give feedback.\nExperimental features are activated using the experimental keyword in the domain custom resource. These features are only documented in this section.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/domains/soa-domain/",
	"title": "SOA domain",
	"tags": [],
	"description": "Sample for creating a SOA Suite domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated SOA domain.",
	"content": " Oracle SOA Suite is currently supported only for non-production use in Docker and Kubernetes. The information provided in this document is a preview for early adopters who wish to experiment with Oracle SOA Suite in Kubernetes before it is supported for production use.\n The sample scripts demonstrate the creation of a SOA Suite domain home on an existing Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC). The scripts also generate the domain YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain.\nPrerequisites Before you begin, perform the following steps:\n  Review the Domain resource documentation.\n  Review the system requirements for deploying Oracle SOA Suite domains on Kubernetes.\n  Make sure that Kubernetes is set up in the environment. For details, see the Kubernetes setup guide.\n  Make sure that the WebLogic Kubernetes operator is running. See Manage operators for operator infrastructure setup and Install the operator for operator installation. Make sure you install the operator version 2.4.0.\n  Create a Kubernetes Namespace (for example, soans) for the domain unless you intend to use the default namespace. Use the newly created namespace in all the other steps. For details, see Prepare to run a domain.\n $ kubectl create namespace soans   In the Kubernetes Namespace created above, create the PV and PVC for the database by running the create-pv-pvc.sh script. Follow the instructions for using the scripts to create a PV and PVC.\n  Change the values in the create-pv-pvc-inputs.yaml file based on your requirements.\n  Ensure that the path mentioned for the weblogicDomainStoragePath property does exists (if not, you need to create it), has read and write access permissions, and it must be an empty directory.\n    Create the Kubernetes Secrets username and password of the administrative account in the same Kubernetes namespace as the domain. For details, see this document.\n$ cd kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p Welcome1 -n soans -d soainfra -s soainfra-domain-credentials You can check the secret with the kubectl get secret command. See the following example, including the output:\n$ kubectl get secret soainfra-domain-credentials -o yaml -n soans apiVersion: v1 data: password: V2VsY29tZTE= username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: 2019-06-02T07:05:25Z labels: weblogic.domainName: soainfra weblogic.domainUID: soainfra name: soainfra-domain-credentials namespace: soans resourceVersion: \u0026#34;11561988\u0026#34; selfLink: /api/v1/namespaces/soans/secrets/soainfra-domain-credentials uid: a91ef4e1-6ca8-11e9-8143-fa163efa261a type: Opaque   Complete the other preliminary required steps documented here.\n  Prepare to use the create domain script The sample scripts for Oracle SOA Suite domain deployment are available at \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts/create-soa-domain.\nYou must edit create-domain-inputs.yaml (or a copy of it) to provide the details for your domain. Please refer to the configuration parameters below to understand the information that you must provide in this file.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Definition Default     adminPort Port number of the Administration Server inside the Kubernetes cluster. 7001   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. AdminServer   clusterName Name of the WebLogic cluster instance to generate for the domain. By default the cluster name is soa_cluster for the SOA domain. You can update this to osb_cluster for an OSB domain type or soa_cluster for SOAESS or SOAOSB or SOAESSOSB domain types. soa_cluster   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. It can also be set to the relative path wdt, and then the built-in WDT scripts will be used instead. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes ConfigMap, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes Pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes Job to run the script (specified in the createDomainScriptName property) in a Kubernetes Pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes Pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes Job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the SOA domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/soainfra   domainPVMountPath Mount path of the domain persistent volume. /u01/oracle/user_projects   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Kubernetes domain resource. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes Service name. soainfra   domainType Type of the domain. Mandatory input for SOA Suite domains. You must provide one of the supported domain type values: soa (deploys a SOA domain),osb (deploys an OSB (Oracle Service Bus) domain),soaess (deploys a SOA domain with Enterprise Scheduler (ESS)),soaosb (deploys a domain with SOA and OSB), and soaessosb (deploys a domain with SOA, OSB, and ESS). soa   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   httpAccessLogInLogHome Boolean indicating if server HTTP access log files should be written to the same directory as logHome. Otherwise, server HTTP access log files will be written to the directory specified in the WebLogic domain home configuration. true   image SOA Suite Docker image. The operator requires SOA Suite 12.2.1.3.0 with patch 29135930 applied. Refer to SOA domains for details on how to obtain or create the image. container-registry.oracle.com/middleware/soasuite:12.2.1.3   imagePullPolicy WebLogic Docker image pull policy. Legal values are IfNotPresent, Always, or Never IfNotPresent   imagePullSecretName Name of the Kubernetes Secret to access the Docker Store to pull the WebLogic Server Docker image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server .out in the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to start initially for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, Node Manager log, and server HTTP access log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /u01/oracle/user_projects/domains/logs/soainfra   managedServerNameBase Base string used to generate Managed Server names. soa_server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes Namespace in which to create the domain. soans   persistentVolumeClaimName Name of the persistent volume claim created to host the domain home. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. soainfra-domain-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the network access point. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only, in a single server (all-in-one) Kubernetes Deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster.   weblogicCredentialsSecretName Name of the Kubernetes Secret for the Administration Server user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. soainfra-domain-credentials   weblogicImagePullSecretName Name of the Kubernetes Secret for the Docker Store, used to pull the WebLogic Server image.    serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.   rcuSchemaPrefix The schema prefix to use in the database, for example SOA1. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. SOA1   rcuDatabaseURL The database URL. oracle-db.default.svc.cluster.local:1521/devpdb.k8s   rcuCredentialsSecret The Kubernetes Secret containing the database credentials. soainfra-rcu-credentials    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes Service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create a SOA Suite domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain YAML file could also be modified to cover more use cases.\nRun the create domain script Run the create domain script, specifying your inputs file and an output directory to store the generated artifacts:\n$ ./create-domain.sh \\ -i create-domain-inputs.yaml \\ -o /\u0026lt;path to output-directory\u0026gt; The script will perform the following steps:\n  Create a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The path name is /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, its contents must be removed before using this script.\n  Create a Kubernetes Job that will start up a utility SOA Suite container and run offline WLST scripts to create the domain on the shared storage.\n  Run and wait for the job to finish.\n  Create a Kubernetes domain YAML file, domain.yaml, in the \u0026ldquo;output\u0026rdquo; directory that was created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command:\n$ kubectl apply -f /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml   Create a convenient utility script, delete-domain-job.yaml, to clean up the domain home created by the create script.\n  The default domain created by the script has the following characteristics:\n An Administration Server named AdminServer listening on port 7001. A configured cluster named soa_cluster-1 of size 5. Two Managed Servers, named soa_server1 and soa_server2, listening on port 8001. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;. SOA Infra, SOA composer and WorklistApp applications deployed. No data sources or JMS resources. A T3 channel.  Verify the results The create domain script will verify that the domain was created, and will report failure if there was any error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nNote that the example results below use the default Kubernetes Namespace. If you are using a different namespace, you need to replace NAMESPACE in the example kubectl commands with the actual Kubernetes Namespace.\nGenerated YAML files with the default inputs The content of the generated domain.yaml:\n$ cat output/weblogic-domains/soainfra/domain.yaml # Copyright (c) 2019, 2020, Oracle Corporation and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # # If you are using 3.0.0-rc1, then the version on the following line # should be `v7` not `v6`. apiVersion: \u0026quot;weblogic.oracle/v6\u0026quot; kind: Domain metadata: name: soainfra namespace: soans labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: soainfra spec: # The WebLogic Domain Home domainHome: /u01/oracle/user_projects/domains/soainfra # Set domain home type to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: PersistentVolume # The WebLogic Server Docker image that the Operator uses to start the domain image: \u0026quot;container-registry.oracle.com/middleware/soasuite:12.2.1.3\u0026quot; # imagePullPolicy defaults to \u0026quot;Always\u0026quot; if image version is :latest imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: soainfra-domain-credentials # Whether to include the server out file into the pod's stdout, default is true includeServerOutInPodLog: true # Whether to enable log home logHomeEnabled: true # The in-pod location for domain log, server logs, server out, and Node Manager log files logHome: /u01/oracle/user_projects/domains/logs/soainfra # An (optional) in-pod location for data storage of default and custom file stores. # If not specified or the value is either not set or empty (e.g. dataHome: \u0026quot;\u0026quot;) then the # data storage directories are determined from the WebLogic domain home configuration. dataHome: \u0026quot;\u0026quot; # serverStartPolicy legal values are \u0026quot;NEVER\u0026quot;, \u0026quot;IF_NEEDED\u0026quot;, or \u0026quot;ADMIN_ONLY\u0026quot; # This determines which WebLogic Servers the Operator will start up when it discovers this Domain # - \u0026quot;NEVER\u0026quot; will not start any server in the domain # - \u0026quot;ADMIN_ONLY\u0026quot; will start up only the administration server (no managed servers will be started) # - \u0026quot;IF_NEEDED\u0026quot; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; serverService: precreateService: true serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false\u0026quot; - name: USER_MEM_ARGS value: \u0026quot;-Djava.security.egd=file:/dev/./urandom \u0026quot; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: soainfra-domain-pvc volumeMounts: - mountPath: /u01/oracle/user_projects name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026quot;RUNNING\u0026quot; or \u0026quot;ADMIN\u0026quot; # \u0026quot;RUNNING\u0026quot; means the listed server will be started up to \u0026quot;RUNNING\u0026quot; mode # \u0026quot;ADMIN\u0026quot; means the listed server will be start up to \u0026quot;ADMIN\u0026quot; mode serverStartState: \u0026quot;RUNNING\u0026quot; # adminService: # channels: # The Admin Server's NodePort # - channelName: default # nodePort: 30701 # Uncomment to export the T3Channel as a service # - channelName: T3Channel # clusters is used to configure the desired behavior for starting member servers of a cluster. # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: osb_cluster serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; replicas: 2 # The number of managed servers to start for unlisted clusters # replicas: 1 - clusterName: soa_cluster serverService: precreateService: true serverStartState: \u0026quot;RUNNING\u0026quot; replicas: 2 # The number of managed servers to start for unlisted clusters # replicas: 1 Verify the domain To confirm that the domain was created, use this command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\nHere is an example of the output of this command:\n$ kubectl describe domain soainfra -n soans Name: soainfra Namespace: soans Labels: weblogic.domainUID=soainfra weblogic.resourceVersion=domain-v2 Annotations: \u0026lt;none\u0026gt; API Version: weblogic.oracle/v7 Kind: Domain Metadata: Creation Timestamp: 2020-01-27T10:04:11Z Generation: 6 Resource Version: 18537800 Self Link: /apis/weblogic.oracle/v7/namespaces/soans/domains/soainfra UID: 5dcb76e4-40ec-11ea-b332-020017041cc2 Spec: Admin Server: Admin Service: Annotations: Channels: Labels: Server Pod: Annotations: Container Security Context: Containers: Env: Init Containers: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Gates: Readiness Probe: Resources: Limits: Requests: Shutdown: Tolerations: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Clusters: Cluster Name: osb_cluster Cluster Service: Annotations: Labels: Replicas: 2 Server Pod: Annotations: Container Security Context: Containers: Env: Init Containers: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Gates: Readiness Probe: Resources: Limits: Requests: Shutdown: Tolerations: Volume Mounts: Volumes: Server Service: Annotations: Labels: Precreate Service: true Server Start State: RUNNING Cluster Name: soa_cluster Cluster Service: Annotations: Labels: Replicas: 2 Server Pod: Annotations: Container Security Context: Containers: Env: Init Containers: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Gates: Readiness Probe: Resources: Limits: Requests: Shutdown: Tolerations: Volume Mounts: Volumes: Server Service: Annotations: Labels: Precreate Service: true Server Start State: RUNNING Data Home: Domain Home: /u01/oracle/user_projects/domains/soainfra Domain Home In Image: false Image: container-registry.oracle.com/middleware/soasuite:12.2.1.3 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Log Home: /u01/oracle/user_projects/domains/logs/soainfra Log Home Enabled: true Managed Servers: Server Pod: Annotations: Container Security Context: Containers: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom Init Containers: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Gates: Readiness Probe: Resources: Limits: Requests: Shutdown: Tolerations: Volume Mounts: Mount Path: /u01/oracle/user_projects Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: soainfra-domain-pvc Server Service: Annotations: Labels: Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: soainfra-domain-credentials Status: Clusters: Cluster Name: soa_cluster Maximum Replicas: 5 Cluster Name: osb_cluster Maximum Replicas: 5 Conditions: Servers: Health: Activation Time: 2020-01-27T10:08:18.876Z Overall Health: ok Subsystems: Node Name: MyNode Server Name: AdminServer State: RUNNING Start Time: 2020-01-27T10:04:11.853Z Events: \u0026lt;none\u0026gt; In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued very soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. The operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Use the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command. You can verify that an Administration Server and two Managed Servers for each cluster (SOA and OSB) are running for soaessosb domain type.\n$ kubectl get pods -n soans NAME READY STATUS RESTARTS AGE soainfra-adminserver 1/1 Running 0 20h soainfra-osb-server1 1/1 Running 0 20h soainfra-osb-server2 1/1 Running 0 20h soainfra-soa-server1 1/1 Running 0 20h soainfra-soa-server2 1/1 Running 0 20h Verify the services Use the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command. You can verify that services for Administration Server and Managed Servers (for SOA and OSB clusters) are created for soaessosb domain type.\n$ kubectl get services -n soans NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE soainfra-adminserver ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 20h soainfra-cluster-osb-cluster ClusterIP 10.110.6.107 \u0026lt;none\u0026gt; 9001/TCP 20h soainfra-cluster-soa-cluster ClusterIP 10.100.165.105 \u0026lt;none\u0026gt; 8001/TCP 20h soainfra-osb-server1 ClusterIP None \u0026lt;none\u0026gt; 9001/TCP 20h soainfra-osb-server2 ClusterIP None \u0026lt;none\u0026gt; 9001/TCP 20h soainfra-osb-server3 ClusterIP 10.99.1.111 \u0026lt;none\u0026gt; 9001/TCP 20h soainfra-osb-server4 ClusterIP 10.106.178.175 \u0026lt;none\u0026gt; 9001/TCP 20h soainfra-osb-server5 ClusterIP 10.97.65.163 \u0026lt;none\u0026gt; 9001/TCP 20h soainfra-soa-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 20h soainfra-soa-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 20h soainfra-soa-server3 ClusterIP 10.104.189.192 \u0026lt;none\u0026gt; 8001/TCP 20h soainfra-soa-server4 ClusterIP 10.100.168.31 \u0026lt;none\u0026gt; 8001/TCP 20h soainfra-soa-server5 ClusterIP 10.101.171.78 \u0026lt;none\u0026gt; 8001/TCP 20h Delete the generated domain home Sometimes in production, but most likely in testing environments, you might want to remove the domain home that is generated using the create-domain.sh script. Do this by running the generated delete domain job script in the /\u0026lt;path to weblogic-operator-output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt; directory.\n$ kubectl create -f delete-domain-job.yaml "
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/cicd/tools/",
	"title": "Tools",
	"tags": [],
	"description": "Tools that are available to build CI/CD pipelines.",
	"content": "WebLogic Deploy Tooling (WDT) You can use several of the WDT tools in a CI/CD pipeline. For example, the createDomain tool creates a new domain based on a simple model, and updateDomain (and deployApps) uses the same model concept to update an existing domain (preserving the same domain encryption key). The deployApps tool is very similar to the updateDomain tool, but limits what can be updated to application-related configuration attributes such as data sources and application archives. The model used by these tools is a sparse set of attributes needed to create or update the domain. A model can be as sparse as providing only the WebLogic Server administrative password, although not very interesting. A good way to get a jumpstart on a model is to use the discoverDomain tool in WDT which builds a model based on an existing domain.\nA Model in Image domain takes advantage of WDT by letting you specify an operator domain directly with a model instead of requiring that you supply a domain home.\n Other than the tools themselves, there are three components to the WDT tools:\n The Domain Model - Metadata model describing the desired domain.\nThe metadata domain model can be written in YAML or JSON and is documented here. The Archive ZIP - Binaries to supplement the model.\nAll binaries needed to supplement the model must be specified in an archive file, which is just a ZIP file with a specific directory structure. Optionally, the model can be stored inside the ZIP file, if desired. Any binaries not already on the target system must be in the ZIP file so that the tooling can extract them in the target domain. The Properties File - A standard Java properties file.\nA property file used to provide values to placeholders in the model.  WDT Create Domain Samples  (Docker) A sample for creating a domain in a Docker image with WDT can be found here. (Kubernetes) A similar sample of creating a domain in a Docker image with WDT can be found in the WebLogic Server Kubernetes Operator project for creating a domain-in-image with WDT. (Kubernetes) A Model in Image sample for supplying an image that contains a WDT model only, instead of a domain home. In this case, the operator generates the domain home for you at runtime.  WebLogic Scripting Tool (WLST) You can use WLST scripts to create and update domain homes in a CI/CD pipeline for Domain in Image and Domain in PV domains. We recommend that you use offline WLST for this purpose. There may be some scenarios where it is necessary to use WLST online, but we recommend that you do that as an exception only, and when absolutely necessary.\nIf you do not already have WLST scripts, we recommend that you consider using WebLogic Deploy Tooling (WDT) instead. It provides a more declarative approach to domain creation, whereas WLST is more of an imperative scripting language. WDT provides advantages like being able to use the same model with different versions of WebLogic, whereas you may need to update WLST scripts manually when migrating to a new version of WebLogic for example.\nWebLogic pack and unpack tools WebLogic Server provides tools called \u0026ldquo;pack\u0026rdquo; and \u0026ldquo;unpack\u0026rdquo; that can be used to \u0026ldquo;clone\u0026rdquo; a domain home. These tools do not preserve the domain encryption key. You can use these tools to make copies of Domain in PV and Domain in Image domain homes in scenarios when you do not need the same domain encryption key. See Creating Templates and Domains Using the Pack and Unpack Commands.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/ingress/",
	"title": "Ingress",
	"tags": [],
	"description": "",
	"content": "Ingresses are one approach provided by Kubernetes to configure load balancers. Depending on the version of Kubernetes you are using, and your cloud provider, you may need to use Ingresses. For more information about Ingresses, see the Ingress documentation.\nWebLogic clusters as backends of an Ingress In an Ingress object, a list of backends are provided for each target that will be load balanced. Each backend is typically a Kubernetes Service, more specifically, a combination of a serviceName and a servicePort.\nWhen the operator creates a WebLogic domain, it also creates a service for each WebLogic cluster in the domain. The operator defines the service such that its selector will match all WebLogic Server pods within the WebLogic cluster which are in the \u0026ldquo;ready\u0026rdquo; state.\nThe name of the service created for a WebLogic cluster follows the pattern \u0026lt;domainUID\u0026gt;-cluster-\u0026lt;clusterName\u0026gt;. For example, if the domainUID is domain1 and the cluster name is cluster-1, the corresponding service will be named domain1-cluster-cluster-1.\nThe service name must comply with standard Kubernetes rules for naming of objects and in particular with DNS-1035:\n A DNS-1035 label must consist of lower case alphanumeric characters or \u0026lsquo;-', start with an alphabetic character, and end with an alphanumeric character (e.g. my-name, or abc-123, regex used for validation is [a-z]([-a-z0-9]*[a-z0-9])?).\n To comply with these requirements, if the domainUID or the cluster name contains some upper-case characters or underscores, then in the service name the upper-case characters will be converted to lower-case and underscores will be converted to hyphens. For example, if the domainUID is myDomain_1 and the cluster name is myCluster_1, the corresponding service will be named mydomain-1-cluster-mycluster-1.\nThe service, serviceName and servicePort, of a WebLogic cluster will be used in the routing rules defined in the Ingress object and the load balancer will route traffic to the WebLogic Servers within the cluster based on the rules.\nMost common ingress controllers, for example Traefik, Voyager, and nginx, understand that there are zero or more actual pods behind the service, and they actually build their backend list and route requests to those backends directly, not through the service. This means that requests are properly balanced across the pods, according to the load balancing algorithm in use. Most ingress controllers also subscribe to updates on the service and adjust their internal backend sets when additional pods become ready, or pods enter a non-ready state.\n Steps to set up an ingress load balancer   Install the ingress controller.\nAfter the ingress controller is running, it monitors Ingress resources in a given namespace and acts accordingly.\n  Create Ingress resources.\nIngress resources contain routing rules to one or more backends. An ingress controller is responsible to apply the rules to the underlying load balancer. There are two approaches to create the Ingress resource:\n  Use the Helm chart ingress-per-domain.\nEach ingress provider supports a number of annotations in Ingress resources. This Helm chart allows you to define the routing rules without dealing with the detailed provider-specific annotations. Currently we support two ingress providers: Traefik and Voyager.\n  Create the Ingress resource manually from a YAML file.\nManually create an Ingress YAML file and then apply it to the Kubernetes cluster.\n    Guide and samples for Traefik and Voyager/HAProxy Traefik and Voyager/HAProxy are both popular ingress controllers. Information about how to install and configure these to load balance WebLogic clusters is provided here:\n Traefik guide Voyager guide  Traefik is recommended for development and test environments only. For production environments, we recommend Apache or Voyager ingress controllers, or the load balancer provided by your cloud provider.\n Samples are also provided for these two ingress controllers, showing how to manage multiple WebLogic clusters as the backends, using different routing rules, host-routing and path-routing; and TLS termination:\n Traefik samples Voyager samples  "
},
{
	"uri": "/weblogic-kubernetes-operator/security/secrets/",
	"title": "Secrets",
	"tags": [],
	"description": "Kubernetes Secrets for the operator",
	"content": "Contents  Domain credentials secret Domain image pull secret Operator image pull secret Operator configuration override secrets Operator external REST interface secret Operator internal REST interface secret  Domain credentials secret The credentials for the WebLogic domain are kept in a Kubernetes Secret where the name of the secret is specified using webLogicCredentialsSecret in the WebLogic Domain resource. Also, the domain credentials secret must be created in the namespace where the Domain will be running.\nFor an example of a WebLogic domain resource using webLogicCredentialsSecret, see Docker Image Protection.\n The samples supplied with the operator use a naming convention that follows the pattern \u0026lt;domainUID\u0026gt;-weblogic-credentials, where \u0026lt;domainUID\u0026gt; is the unique identifier of the domain, for example, domain1-weblogic-credentials.\nIf the WebLogic domain will be started in domain1-ns and the \u0026lt;domainUID\u0026gt; is domain1, an example of creating a Kubernetes generic secret is as follows:\n$ kubectl -n domain1-ns create secret generic domain1-weblogic-credentials \\  --from-file=username --from-file=password $ kubectl -n domain1-ns label secret domain1-weblogic-credentials \\  weblogic.domainUID=domain1 weblogic.domainName=domain1  Oracle recommends that you not include unencrypted passwords on command lines. Passwords and other sensitive data can be prompted for or looked up by shell scripts or tooling. For more information about creating Kubernetes Secrets, see the Kubernetes Secrets documentation.\n The operator\u0026rsquo;s introspector job will expect the secret key names to be:\n username password  For example, here is the result when describing the Kubernetes Secret:\n$ kubectl -n domain1-ns describe secret domain1-weblogic-credentials Name: domain1-weblogic-credentials Namespace: domain1-ns Labels: weblogic.domainName=domain1 weblogic.domainUID=domain1 Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== password: 8 bytes username: 8 bytes Domain image pull secret The WebLogic domain that the operator manages can have images that are protected in the registry. The imagePullSecrets setting on the Domain can be used to specify the Kubernetes Secret that holds the registry credentials.\nFor more information, see Docker Image Protection.\n Operator image pull secret The Helm chart for installing the operator has an option to specify the image pull secret used for the operator\u0026rsquo;s image when using a private registry. The Kubernetes Secret of type docker-registry should be created in the namespace where the operator is deployed.\nHere is an example of using the helm install command to set the image name and image pull secret:\n$ helm install my-weblogic-operator kubernetes/charts/weblogic-operator \\  --set \u0026#34;image=my.io/my-operator-image:1.0\u0026#34; \\  --set \u0026#34;imagePullSecrets[0].name=my-operator-image-pull-secret\u0026#34; \\  --namespace weblogic-operator-ns \\  --wait  For more information, see Install the operator Helm chart.\n Operator configuration override secrets The operator supports embedding macros within configuration override templates that reference Kubernetes Secrets. These Kubernetes Secrets can be created with any name in the namespace where the Domain will be running. The Kubernetes Secret names are specified using configuration.secrets in the WebLogic Domain resource.\nFor more information, see Configuration overrides.\n Operator external REST interface secret The operator can expose an external REST HTTPS interface which can be accessed from outside the Kubernetes cluster. A Kubernetes tls secret is used to hold the certificates and private key.\nFor more information, see Certificates.\n Operator internal REST interface secret The operator exposes an internal REST HTTPS interface with a self-signed certificate. The certificate is kept in a Kubernetes ConfigMap with the name weblogic-operator-cm using the key internalOperatorCert. The private key is kept in a Kubernetes Secret with the name weblogic-operator-secrets using the key internalOperatorKey. These Kubernetes objects are managed by the operator\u0026rsquo;s Helm chart and are part of the namespace where the operator is installed.\nFor example, to see all the operator\u0026rsquo;s ConfigMaps and secrets when installed into the Kubernetes Namespace weblogic-operator-ns, use:\n$ kubectl -n weblogic-operator-ns get cm,secret "
},
{
	"uri": "/weblogic-kubernetes-operator/security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": "  Certificates  Operator SSL/TLS certificate handling\n Domain security  WebLogic domain security and the operator\n Encryption  WebLogic domain encryption and the operator\n Service accounts  Kubernetes ServiceAccounts for the operator\n RBAC  Operator role-based authorization\n Secrets  Kubernetes Secrets for the operator\n OpenShift  OpenShift information for the operator\n "
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/elastic-stack/",
	"title": "Elastic Stack",
	"tags": [],
	"description": "",
	"content": "  Operator  Sample for configuring the Elasticsearch and Kibana deployments and services for the operator\u0026#39;s logs.\n WebLogic domain  Sample for using Fluentd for WebLogic domain and operator\u0026#39;s logs.\n SOA domain  Samples for publishing logs to Elasticsearch and monitoring a SOA instance.\n "
},
{
	"uri": "/weblogic-kubernetes-operator/developerguide/code-structure/",
	"title": "Code structure",
	"tags": [],
	"description": "",
	"content": "This project has the following directory structure:\n docs: Generated Javadoc and Swagger integration-tests: Integration test suite json-schema: Java model to JSON schema generator json-schema-maven-plugin: Maven plugin for schema generator kubernetes/charts: Helm charts kubernetes/samples: All samples, including for WebLogic domain creation model: Domain resource Java model operator: Operator runtime site: This documentation src/scripts: Scripts operator injects into WebLogic Server instance Pods swagger: Swagger files for the Kubernetes API server and domain resource  Watch package The Watch API in the Kubernetes Java client provides a watch capability across a specific list of resources for a limited amount of time. As such, it is not ideally suited for our use case, where a continuous stream of watches is desired, with watch events generated in real time. The watch-wrapper in this repository extends the default Watch API to provide a continuous stream of watch events until the stream is specifically closed. It also provides resourceVersion tracking to exclude events that have already been seen. The watch-wrapper provides callbacks so events, as they occur, can trigger actions.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/quickstart/create-domain/",
	"title": "Create a domain",
	"tags": [],
	"description": "",
	"content": "  For use in the following steps:\n Select a user name and password, following the required rules for password creation (at least 8 alphanumeric characters with at least one number or special character). Pick or create a directory to which you can write output.    Create a Kubernetes Secret for the WebLogic administrator credentials containing the username and password for the domain, using the create-weblogic-credentials script:\n$ kubernetes/samples/scripts/create-weblogic-domain-credentials/create-weblogic-credentials.sh \\  -u \u0026lt;username\u0026gt; -p \u0026lt;password\u0026gt; -n sample-domain1-ns -d sample-domain1 The sample will create a secret named domainUID-weblogic-credentials where the domainUID is replaced with the value you provided. For example, the command above would create a secret named sample-domain1-weblogic-credentials.\n  Create a new image with a domain home by running the create-domain script. First, copy the sample create-domain-inputs.yaml file and update your copy with:\n domainUID: sample-domain1 image: Leave empty unless you need to tag the new image that the script builds to a different name. For example if you are using a remote cluster that will need to pull the image from a Docker registry, then you should set this value to the fully qualified image name. Note that you will need to push the image manually. weblogicCredentialsSecretName: sample-domain1-weblogic-credentials namespace: sample-domain1-ns domainHomeImageBase: container-registry.oracle.com/middleware/weblogic:12.2.1.4  For example, assuming you named your copy my-inputs.yaml:\n$ cd kubernetes/samples/scripts/create-weblogic-domain/domain-home-in-image $ ./create-domain.sh -i my-inputs.yaml -o /\u0026lt;your output directory\u0026gt; -u \u0026lt;username\u0026gt; -p \u0026lt;password\u0026gt; -e  You need to provide the same WebLogic domain administrator user name and password in the -u and -p options respectively, as you provided when creating the Kubernetes Secret in Step 2.\n   Confirm that the operator started the servers for the domain:\na. Use kubectl to show that the domain resource was created:\n$ kubectl describe domain sample-domain1 -n sample-domain1-ns b. After a short time, you will see the Administration Server and Managed Servers running.\n$ kubectl get pods -n sample-domain1-ns c. You should also see all the Kubernetes Services for the domain.\n$ kubectl get services -n sample-domain1-ns   Create an ingress for the domain, in the domain namespace, by using the sample Helm chart:\n$ helm install sample-domain1-ingress kubernetes/samples/charts/ingress-per-domain \\  --namespace sample-domain1-ns \\  --set wlsDomain.domainUID=sample-domain1 \\  --set traefik.hostname=sample-domain1.org   To confirm that the load balancer noticed the new ingress and is successfully routing to the domain\u0026rsquo;s server pods, you can send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;, as shown in the example below, which will return a HTTP 200 status code.\n$ curl -v -H 'host: sample-domain1.org' http://localhost:30305/weblogic/ready About to connect() to localhost port 30305 (#0) Trying 10.196.1.64... Connected to localhost (10.196.1.64) port 30305 (#0) \u0026gt; GET /weblogic/ HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Accept: */* \u0026gt; host: domain1.org \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Content-Length: 0 \u0026lt; Date: Thu, 20 Dec 2018 14:52:22 GMT \u0026lt; Vary: Accept-Encoding \u0026lt; Connection #0 to host localhost left intact  Depending on where your Kubernetes cluster is running, you may need to open firewall ports or update security lists to allow ingress to this port.\n   To access the WebLogic Server Administration Console:\na. Edit the my-inputs.yaml file (assuming that you named your copy my-inputs.yaml) to set exposedAdminNodePort: true.\nb. Open a browser to http://localhost:30701.\nDo not use the WebLogic Server Administration Console to start or stop servers. See Starting and stopping servers.\n   "
},
{
	"uri": "/weblogic-kubernetes-operator/security/openshift/",
	"title": "OpenShift",
	"tags": [],
	"description": "OpenShift information for the operator",
	"content": "OpenShift anyuid security context The Docker images that Oracle publishes default to the container user as oracle, which is UID 1000 and GID 1000. When running the Oracle images or layered images that retain the default user as oracle with OpenShift, the anyuid security context constraint is required to ensure proper access to the file system within the Docker image. This means that the administrator must:\n Ensure the anyuid security content is granted Ensure that WebLogic containers are annotated with openshift.io/scc: anyuid  For example, to update the OpenShift policy, use:\n$ oc adm policy add-scc-to-user anyuid -z default To annotate the WebLogic containers, update the WebLogic Domain resource to include annotations for the serverPod. For example:\nkind: Domain metadata: name: domain1 spec: domainUID: domain1 serverPod: env: - name: var1 value: value1 annotations: openshift.io/scc: anyuid  For additional information about OpenShift requirements and the operator, see the OpenShift section in the User Guide.\n Using a dedicated namespace When the user that installs an individual instance of the operator does not have the required privileges to create resources at the Kubernetes cluster level, a dedicated namespace can be used for the operator instance and all the WebLogic domains that it manages. For more details about the dedicated setting, please refer to Operator Helm configuration values.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/accessing-the-domain/",
	"title": "Access the domain",
	"tags": [],
	"description": "",
	"content": "  Use WLST  You can use the WebLogic Scripting Tool (WLST) to manage a domain running in Kubernetes.\n "
},
{
	"uri": "/weblogic-kubernetes-operator/samples/simple/domains/delete-domain/",
	"title": "Delete domain resources",
	"tags": [],
	"description": "Delete the domain resources created while executing the samples.",
	"content": "After running the samples, you will need to release domain resources that can then be used for other purposes. The script in this sample demonstrates one approach to releasing domain resources.\nUse this script to delete domain resources $ ./delete-weblogic-domain-resources.sh \\ -d domain-uid[,domain-uid...] \\ [-s max-seconds] \\ [-t] The required option -d takes domain-uid values (separated by commas and no spaces) to identify the domain resources that should be deleted.\nTo limit the amount of time spent on attempting to delete domain resources, use -s. The option must be followed by an integer that represents the total number of seconds that will be spent attempting to delete resources. The default number of seconds is 120.\nThe optional -t option shows what the script will delete without executing the deletion.\nTo see the help associated with the script:\n$ ./delete-weblogic-domain-resources.sh -h "
},
{
	"uri": "/weblogic-kubernetes-operator/developerguide/asynchronous-call-model/",
	"title": "Asynchronous call model",
	"tags": [],
	"description": "",
	"content": "Our expectation is that customers will task the operator with managing hundreds of WebLogic domains across dozens of Kubernetes Namespaces. Therefore, we have designed the operator with an efficient user-level threads pattern. We\u0026rsquo;ve used that pattern to implement an asynchronous call model for Kubernetes API requests. This call model has built-in support for timeouts, retries with exponential back-off, and lists that exceed the requested maximum size using the continuance functionality.\nUser-level thread pattern The user-level thread pattern is implemented by the classes in the oracle.kubernetes.operator.work package.\n Engine: The executor service and factory for Fibers. Fiber: The user-level thread. Fibers represent the execution of a single processing flow through a series of Steps. Fibers may be suspended and later resumed, and do not consume a Thread while suspended. Step: Individual CPU-bound activity in a processing flow. Packet: Context of the processing flow. NextAction: Used by a Step when it returns control to the Fiber to indicate what should happen next. Common \u0026lsquo;next actions\u0026rsquo; are to execute another Step or to suspend the Fiber. Component: Provider of SPI\u0026rsquo;s that may be useful to the processing flow. Container: Represents the containing environment and is a Component.  Each Step has a reference to the next Step in the processing flow; however, Steps are not required to indicate that the next Step be invoked by the Fiber when the Step returns a NextAction to the Fiber. This leads to common use cases where Fibers invoke a series of Steps that are linked by the \u0026lsquo;is-next\u0026rsquo; relationship, but just as commonly, use cases where the Fiber will invoke sets of Steps along a detour before returning to the normal flow.\nIn this sample, the caller creates an Engine, Fiber, linked set of Step instances, and Packet. The Fiber is then started. The Engine would typically be a singleton, since it\u0026rsquo;s backed by a ScheduledExecutorService. The Packet would also typically be pre-loaded with values that the Steps would use in their apply() methods.\nstatic class SomeClass { public static void main(String[] args) { Engine engine = new Engine(\u0026#34;worker-pool\u0026#34;); Fiber fiber = engine.createFiber(); Step step = new StepOne(new StepTwo(new StepThree(null))); Packet packet = new Packet(); fiber.start( step, packet, new CompletionCallback() { @Override public void onCompletion(Packet packet) { // Fiber has completed successfully  } @Override public void onThrowable(Packet packet, Throwable throwable) { // Fiber processing was terminated with an exception  } }); } } Steps must not invoke sleep or blocking calls from within apply(). This prevents the worker threads from serving other Fibers. Instead, use asynchronous calls and the Fiber suspend/resume pattern. Step provides a method, doDelay(), which creates a NextAction to drive Fiber suspend/resume that is a better option than sleep precisely because the worker thread can serve other Fibers during the delay. For asynchronous IO or similar patterns, suspend the Fiber. In the callback as the Fiber suspends, initiate the asynchronous call. Finally, when the call completes, resume the Fiber. The suspend/resume functionality handles the case where resumed before the suspending callback completes.\nIn this sample, the step uses asynchronous file IO and the suspend/resume Fiber pattern.\nstatic class StepTwo extends Step { public StepTwo(Step next) { super(next); } @Override public NextAction apply(Packet packet) { return doSuspend((fiber) -\u0026gt; { // The Fiber is now suspended  // Start the asynchronous call  try { Path path = Paths.get(URI.create(this.getClass().getResource(\u0026#34;/somefile.dat\u0026#34;).toString())); AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.READ); ByteBuffer buffer = ByteBuffer.allocate(1024); fileChannel.read(buffer, 0, buffer, new CompletionHandler\u0026lt;Integer, ByteBuffer\u0026gt;() { @Override void completed(Integer result, ByteBuffer attachment) { // Store data in Packet and resume Fiber  packet.put(\u0026#34;DATA_SIZE_READ\u0026#34;, result); packet.put(\u0026#34;DATA_FROM_SOMEFILE\u0026#34;, attachment); fiber.resume(packet); } @Override public void failed(Throwable exc, ByteBuffer attachment) { // log exc  completed(0, null); } }); } catch (IOException e) { // log exception  // If not resumed here, Fiber will never be resumed  } }); } } Call builder pattern The asynchronous call model is implemented by classes in the oracle.kubernetes.operator.helpers package, including CallBuilder and ResponseStep. The model is based on the Fiber suspend/resume pattern described above. CallBuilder provides many methods having names ending with \u0026ldquo;Async\u0026rdquo;, such as listPodAsync() or deleteServiceAsync(). These methods return a Step that can be returned as part of a NextAction. When creating these Steps, the developer must provide a ResponseStep. Only ResponseStep.onSuccess() must be implemented; however, it is often useful to override onFailure() as Kubernetes treats 404 (Not Found) as a failure.\nIn this sample, the developer is using the pattern to list pods from the default namespace that are labeled as part of cluster-1.\nstatic class StepOne extends Step { public StepOne(Step next) { super(next); } @Override public NextAction apply(Packet packet) { String namespace = \u0026#34;default\u0026#34;; Step step = CallBuilder.create().with($ -\u0026gt; { $.labelSelector = \u0026#34;weblogic.clusterName=cluster-1\u0026#34;; $.limit = 50; $.timeoutSeconds = 30; }).listPodAsync(namespace, new ResponseStep\u0026lt;V1PodList\u0026gt;(next) { @Override public NextAction onFailure(Packet packet, ApiException e, int statusCode, Map\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; responseHeaders) { if (statusCode == CallBuilder.NOT_FOUND) { return onSuccess(packet, null, statusCode, responseHeaders); } return super.onFailure(packet, e, statusCode, responseHeaders); } @Override NextAction onSuccess(Packet packet, V1PodList result, int statusCode, Map\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; responseHeaders) { // do something with the result Pod, if not null  return doNext(packet); } }); return doNext(step, packet); } } Notice that the required parameters, such as namespace, are method arguments, but optional parameters are designated using a simplified builder pattern using with() and a lambda.\nThe default behavior of onFailure() will retry with an exponential backoff the request on status codes 429 (TooManyRequests), 500 (InternalServerError), 503 (ServiceUnavailable), 504 (ServerTimeout) or a simple timeout with no response from the server.\nIf the server responds with status code 409 (Conflict), then this indicates an optimistic locking failure. Common use cases are that the code read a Kubernetes object in one asynchronous step, modified the object, and attempted to replace the object in another asynchronous step; however, another activity replaced that same object in the interim. In this case, retrying the request would give the same result. Therefore, developers may provide an \u0026ldquo;on conflict\u0026rdquo; step when calling super.onFailure(). The conflict step will be invoked after an exponential backoff delay. In this example, that conflict step should be the step that reads the existing Kubernetes object.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/faq/",
	"title": "Frequently asked questions",
	"tags": [],
	"description": "",
	"content": "Chapter 7 Frequently Asked Questions This section provides answers to frequently asked questions.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/quickstart/cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Remove the domain.   Remove the domain\u0026rsquo;s ingress by using helm:\n$ helm uninstall sample-domain1-ingress -n sample-domain1-ns   Remove the domain resources by using the sample delete-weblogic-domain-resources script:\n$ kubernetes/samples/scripts/delete-domain/delete-weblogic-domain-resources.sh -d sample-domain1   Use kubectl to confirm that the server pods and domain resource are gone:\n$ kubectl get pods -n sample-domain1-ns $ kubectl get domains -n sample-domain1-ns   Remove the domain namespace.   Configure the Traefik load balancer to stop managing the ingresses in the domain namespace:\n$ helm upgrade traefik-operator stable/traefik \\  --namespace traefik \\  --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; \\  --wait   Configure the operator to stop managing the domain:\n$ helm upgrade sample-weblogic-operator \\  kubernetes/charts/weblogic-operator \\  --namespace sample-weblogic-operator-ns \\  --reuse-values \\  --set \u0026#34;domainNamespaces={}\u0026#34; \\  --wait \\   Delete the domain namespace:\n$ kubectl delete namespace sample-domain1-ns   Remove the operator.   Remove the operator:\n$ helm uninstall sample-weblogic-operator -n sample-weblogic-operator-ns   Remove the operator\u0026rsquo;s namespace:\n$ kubectl delete namespace sample-weblogic-operator-ns   Remove the load balancer.   Remove the Traefik load balancer:\n$ helm uninstall traefik-operator -n traefik   Remove the Traefik namespace:\n$ kubectl delete namespace traefik   "
},
{
	"uri": "/weblogic-kubernetes-operator/developerguide/domain-processing/",
	"title": "Domain processing",
	"tags": [],
	"description": "",
	"content": "When the operator starts, it lists all existing Domain resources and processes these domains to create the necessary Kubernetes resources, such as Pods and Services, if they don\u0026rsquo;t already exist. This initialization also includes looking for any stranded resources that, while created by the operator, no longer correlate with a Domain resource.\nAfter this, the operator starts watches for changes to Domain resources and any changes to other resources created by the operator. When a watch event is received, the operator processes the modified Domain resource to again bring the runtime presence in to alignment with the desired state.\nThe operator ensures that at most one Fiber is running for any given Domain. For instance, if the customer modifies a Domain resource to trigger a rolling restart, then the operator will create a Fiber to process this activity. However, if while the rolling restart is in process, the customer makes another change to the Domain resource, such as to increase the replicas field for a cluster, then the operator will cancel the in-flight Fiber and replace it with a new Fiber. This replacement processing must be able to handle taking over for the cancelled work regardless of where the earlier processing may have been in its flow. Therefore, domain processing always starts at the beginning of the \u0026ldquo;make right\u0026rdquo; flow without any state other than the current Domain resource.\nFinally, the operator periodically lists all Domains and rechecks them. This is a backstop against the possibility that a watch event is missed, such as because of a temporary network outage. Recheck activities will not interrupt already running processes for a given Domain.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/domain-lifecycle/",
	"title": "Domain life cycle",
	"tags": [],
	"description": "",
	"content": "Learn how to start, stop, restart, and scale the domain\u0026rsquo;s servers.\n Startup and shutdown  There are properties on the domain resource that specify which servers should be running and which servers should be restarted. To start, stop, or restart servers, modify these properties on the domain resource.\n Restarting  This document describes when to restart servers in the Oracle WebLogic Server in Kubernetes environment.\n Scaling  The operator provides several ways to initiate scaling of WebLogic clusters.\n "
},
{
	"uri": "/weblogic-kubernetes-operator/developerguide/documentation/",
	"title": "Documentation",
	"tags": [],
	"description": "",
	"content": "This documentation is produced using Hugo. To make an update to the documentation, follow this process:\n  If you have not already done so, clone the repository.\ngit clone https://github.com/oracle/weblogic-kubernetes-operator   Create a new branch from master.\ngit checkout master git pull origin master git checkout -b your-branch   Make your documentation updates by editing the source files in docs-source/content. Make sure you check in the changes from the docs-source/content area only; do not build the site and check in the static files.\n   If you wish to view your changes, you can run the site locally using these commands. The site will be available on the URL shown here:\ncd docs-source hugo server -b http://localhost:1313/weblogic-kubernetes-operator   When you are ready to submit your changes, push your branch to origin and submit a pull request. Remember to follow the guidelines in the CONTRIBUTING document.\n  "
},
{
	"uri": "/weblogic-kubernetes-operator/developerguide/backwards-compatibility/",
	"title": "Backward compatibility",
	"tags": [],
	"description": "",
	"content": "Starting with the 2.0.1 release, operator releases must be backward compatible with respect to the domain resource schema, operator Helm chart input values, configuration overrides template, Kubernetes resources created by the operator Helm chart, Kubernetes resources created by the operator, and the operator REST interface. We will maintain compatibility for three releases, except in the case of a clearly communicated deprecated feature, which will be maintained for one release after a replacement is available.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/model-in-image/overview/",
	"title": "Overview",
	"tags": [],
	"description": "Introduction to Model in Image, description of its runtime behavior, and references.",
	"content": " This feature is supported only in 3.0.0-rc1.\n Content  Introduction Runtime behavior overview Runtime updates overview Continuous integration and delivery (CI/CD) References  Introduction Model in Image is an alternative to the operator\u0026rsquo;s Domain in Image and Domain in PV domain types. See Choose a domain home source type for a comparison of operator domain types.\nUnlike Domain in PV and Domain in Image, Model in Image eliminates the need to pre-create your WebLogic domain home prior to deploying your domain resource.\nIt enables:\n Defining a WebLogic domain home configuration using WebLogic Deploy Tool (WDT) model files and application archives. Embedding model files and archives in a custom Docker image, and using the WebLogic Image Tool (WIT) to generate this image. Supplying additional model files using a Kubernetes ConfigMap. Supplying Kubernetes Secrets that resolve macro references within the models. For example, a secret can be used to supply a database credential. Updating WDT model files at runtime. For example, you can add a data source to a running domain. Note that all such updates currently cause the domain to \u0026lsquo;roll\u0026rsquo; in order to take effect.  This feature is supported for standard WLS domains, Restricted JRF domains, and JRF domains.\nWDT models are a convenient and simple alternative to WebLogic WLST configuration scripts and templates. They compactly define a WebLogic domain using YAML files and support including application archives in a ZIP file. The WDT model format is described in the open source, WebLogic Deploy Tool GitHub project.\nFor JRF domains, Model in Image provides additional support for initializing the infrastructure database for a domain, when a domain is started for the first time, supplying an database password, and obtaining an database wallet for re-use in subsequent restarts of the same domain. See Prerequisites for JRF domain types.\nRuntime behavior overview When you deploy a Model in Image domain resource:\n The operator will run a Kubernetes Job called the \u0026lsquo;introspector job\u0026rsquo; that:  Merges your WDT artifacts. Runs WDT tooling to generate a domain home. Packages the domain home and passes it to the operator.   After the introspector job completes:  The operator creates a ConfigMap named DOMAIN_UID-weblogic-domain-introspect-cm and puts the packaged domain home in it. The operator subsequently boots your domain\u0026rsquo;s WebLogic Server pods. The pods will obtain their domain home from the ConfigMap.    Runtime updates overview Model updates can be applied at runtime by changing the image, secrets, or WDT model ConfigMap after initial deployment. If the image name changes, or the domain resource restartVersion changes, then this will cause the introspector job to rerun and generate a new domain home, and subsequently the changed domain home will be propagated to the domain\u0026rsquo;s WebLogic pods using a rolling upgrade (each pod restarting one at a time). See Runtime updates.\nContinuous integration and delivery (CI/CD) To understand how Model in Image works with CI/CD, see CI/CD considerations.\nAlways use external state Regardless of the domain home source type, we recommend that you always keep state outside the Docker image. This includes JDBC stores for leasing tables, JMS and transaction stores, EJB timers, JMS queues, and so on. This ensures that data will not be lost when a container is destroyed.\nWe recommend that state be kept in a database to take advantage of built-in database server high availability features, and the fact that disaster recovery of sites across all but the shortest distances, almost always requires using a single database server to consolidate and replicate data (DataGuard).\nReferences  Model in Image sample WebLogic Deploy Tool (WDT) WebLogic Image Tool (WIT) Domain resource schema, documentation HTTP load balancers: Ingress documentation, sample CI/CD considerations  "
},
{
	"uri": "/weblogic-kubernetes-operator/faq/cannot-pull-image/",
	"title": "Cannot pull image",
	"tags": [],
	"description": "",
	"content": " My domain will not start and I see errors like ImagePullBackoff or Cannot pull image\n When you see these kinds of errors, it means that Kubernetes cannot find your Docker image. The most common causes are:\n The image value in your domain resource is set incorrectly, meaning Kubernetes will be trying to pull the wrong image. The image requires authentication or permission in order to pull it and you have not configured Kubernetes with the necessary credentials, for example in an imagePullSecret. You built the image on a machine that is not where your kubelet is running and Kubernetes cannot see the image, meaning you need to copy the image to the worker nodes or put it in a Docker registry that is accessible the to all of the worker nodes.  Let\u0026rsquo;s review what happens when Kubernetes starts a pod.\nThe definition of the pod contains a list of container specifications. Each container specification contains the name (and optionally, tag) of the image that should be used to run that container. In the example above, there is a container called c1 which is configured to use the Docker image some.registry.com/owner/domain1:1.0. This image name is in the format registry address / owner / name : tag, so in this case the registry is some.registry.com, the owner is owner, the image name is domain and the tag is 1.0. Tags are a lot like version numbers, but they are not required to be numbers or to be in any particular sequence or format. If you omit the tag, it is assumed to be latest.\nThe Docker tag latest is confusing - it does not actually mean the latest version of the image that was created or published in the registry; it just literally means whichever version the owner decided to call \u0026ldquo;latest\u0026rdquo;. Docker and Kubernetes make some assumptions about latest, and it is generally recommended to avoid using it and instead specify the actual version or tag that you really want.\n First, Kubernetes will check to see if the requested image is available in the local Docker image store on whichever worker node the pod was scheduled on. If it is there, then it will use that image to start the container. If it is not there, then Kubernetes will attempt to pull the image from a remote Docker registry.\nThere is another setting called imagePullPolicy that can be used to force Kubernetes to always pull the image, even if it is already present in the local Docker image store.\n If the image is available in the remote registry and it is public, that is it does not require authentication, then Kubernetes will pull the image to the local Docker image store and start the container.\nImages that require authentication If the remote Docker registry requires authentication, then you will need to provide the authentication details in a Kubernetes docker-registry secret and tell Kubernetes to use that secret when pulling the image.\nTo create a secret, you can use the following command:\nkubectl create secret docker-registry secret1 \\ --docker-server=some.registry.com \\ --docker-username=bob \\ --docker-password=bigSecret \\ --docker-email=bob@some.com \\ --namespace=default In this command, you would replace secret1 with the name of the secret; the docker-server is set to the registry name, without the https:// prefix; the docker-username, docker-password and docker-email are set to match the credentials you use to authenticate to the remote Docker registry; and the namespace must be set to the same namespace where you intend to use the image.\nSome registries may need a suffix making the docker-server something like some.registry.com/v2 for example. You will need to check with your registry provider\u0026rsquo;s documentation to determine if this is needed.\n After the secret is created, you need to tell Kubernetes to use it. This is done by adding an imagePullSecret to your Kubernetes YAML file. In the case of a WebLogic domain, you add the secret name to the imagePullSecret in the domain custom resource YAML file.\nHere is an example of part of a domain custom resource file with the imagePullSecret above specified:\napiVersion: \u0026quot;weblogic.oracle/v2\u0026quot; kind: Domain metadata: name: domain1 namespace: default labels: weblogic.resourceVersion: domain-v2 weblogic.domainUID: domain1 spec: domainHome: /u01/oracle/user_projects/domains/domain1 domainHomeSourceType: Image image: \u0026quot;some.registry.com/owner/domain1:1.0\u0026quot; imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; imagePullSecrets: - name: secret1 Alternatively, you can associate the secret with the service account that will be used to run the pod. If you do this, then you will not need to add the imagePullSecret to the domain resource. This is useful if you are running multiple domains in the same namespace.\nTo add the secret shown above to the default service account in the weblogic namespace, you would use a command like this:\nkubectl patch serviceaccount default \\ -n weblogic \\ -p '{\u0026quot;imagePullSecrets\u0026quot;: [{\u0026quot;name\u0026quot;: \u0026quot;secret1\u0026quot;}]}'  You can provide mutliple imagePullSecrets if you need to pull Docker images from multiple remote Docker registries or if your images require different authentication credentials. For more information, see Docker Image Protection.\n Manually copying the image to your worker nodes If you are not able to use a remote Docker registry, for example if your Kubernetes cluster is in a secure network with no external access, you can manually copy the Docker images to the cluster instead.\nOn the machine where you created the image, export it into a TAR file using this command:\ndocker save domain1:1.0 \u0026gt; domain1.tar Then copy that TAR file to each worker node in your Kubernetes cluster and run this command on each node:\ndocker load \u0026lt; domain1.tar Restart pods to clear the error After you have ensured that the images are accessible on all worker nodes, you may need to restart the pods so that Kubernetes will attempt to pull the images again. You can do this by deleting the pods themselves, or deleting the domain resource and then recreating it.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/model-in-image/usage/",
	"title": "Usage",
	"tags": [],
	"description": "Steps for creating and deploying Model in Image images and their associated domain resources.",
	"content": " This feature is supported only in 3.0.0-rc1.\n This document describes what\u0026rsquo;s needed to create and deploy a typical Model in Image domain.\nContents  WebLogic Server Kubernetes Operator WebLogic Server image Optional WDT model ConfigMap Required runtime encryption secret Secrets for model macros Domain resource attributes Prerequisites for JRF domain types  WebLogic Server Kubernetes Operator Deploy the operator and ensure that it is monitoring the desired namespace for your Model in Image domain. See Manage operators and Quick Start.\nWebLogic Server image Model in Image requires creating a Docker image that has WebLogic Server and WDT installed, plus optionally, your model and application files.\nYou can start with a WebLogic Server 12.2.1.3 or later pre-built base image obtained from Docker Hub or similar, manually build your own base image as per Preparing a Base Image, or build a base image using the WebLogic Image Tool. Note that any 12.2.1.3 image must also include patch 29135930 (the pre-built images already contain this patch). For an example of the first approach for both WLS and JRF domains, see the Model in Image sample.\nAfter you have a base image, Model in Image requires layering the following directory structure for its (optional) WDT model artifacts and (required) WDT binaries:\n   Directory Contents Extension     /u01/wdt/models Optional domain model YAML files .yaml   /u01/wdt/models Optional model variable files .properties   /u01/wdt/models Application archives .zip   /u01/wdt/weblogic-deploy Unzipped WebLogic deploy install      Note: Model YAML and variable files are optional in a Model in Image image /u01/wdt/models directory because Model in Image also supports supplying them dynamically using a ConfigMap referenced by the domain resource spec.model.configMap field. Application archives, if any, must be supplied in the Model in Image image. Application archives are not supported in a spec.model.configMap.\n There are two methods for layering Model in Image artifacts on top of a base image:\n  Manual Image Creation: Use Docker commands to layer the WDT artifacts from the above table on top of your base image into a new image.\n  WebLogic Image Tool: Use the WebLogic Image Tool. The WebLogic Image Tool (WIT) has built-in options for embedding WDT model files, WDT binaries, WebLogic Server binaries, and WebLogic Server patches in an image. The Model in Image sample uses the WIT approach. For an example, see the sample\u0026rsquo;s build_image_model.sh file in the operator source\u0026rsquo;s kubernetes/samples/scripts/create-weblogic-domain/model-in-image directory.\n  For more information about model file syntax, see Model files.\nOptional WDT model ConfigMap You can create a WDT model ConfigMap that defines additional model .yaml and .properties files beyond what you\u0026rsquo;ve already supplied in your image, and then reference this ConfigMap using your domain resource\u0026rsquo;s configuration.model.configMap attribute. This is optional if the supplied image already fully defines your model.\nWDT model ConfigMap files will be merged with the WDT files defined in your image at runtime before your domain home is created. The ConfigMap files can add to, remove from, or alter the model configuration that you supplied within your image.\nFor example, place additional .yaml and .properties files in a directory called /home/acmeuser/wdtoverride and run the following commands:\n$ kubectl -n MY-DOMAIN-NAMESPACE \\ create configmap MY-DOMAINUID-my-wdt-config-map \\ --from-file /home/acmeuser/wdtoverride $ kubectl -n MY-DOMAIN-NAMESPACE \\ label configmap MY-DOMAINUID-my-wdt-config-map \\ weblogic.domainUID=MY-DOMAINUID See Model files for a discussion of model file syntax and loading order, and see Runtime updates for a discussion of using WDT model ConfigMaps to update the model configuration of a running domain.\nRequired runtime encryption secret Model in Image requires a runtime encryption secret with a secure password key. This secret is used by the operator to encrypt model and domain home artifacts before it adds them to a runtime ConfigMap or log. You can safely change the password, at any time after you\u0026rsquo;ve fully shut down a domain, but it must remain the same for the life of a running domain. The runtime encryption secret that you create can be named anything, but note that it is a best practice to name and label secrets with their domain UID to help ensure that cleanup scripts can find and delete them.\nNOTE: Because the runtime encryption password does not need to be shared and needs to exist only for the life of a domain, you may want to use a password generator.\nExample:\n$ kubectl -n MY-DOMAIN-NAMESPACE \\ create secret generic MY-DOMAINUID-runtime-encrypt-secret \\ --from-literal=password=welcome1 $ kubectl -n MY-DOMAIN-NAMESPACE \\ label secret MY-DOMAINUID-runtime-encrypt-secret \\ weblogic.domainUID=MY-DOMAINUID Corresponding domain resource snippet:\nconfiguration: model: runtimeEncryptionSecret: MY-DOMAINUID-runtime-encrypt-secret Secrets for model macros Create additional secrets as needed by macros in your model files. For example, these can store database URLs and credentials that are accessed using @@SECRET macros in your model that reference the secrets. For a discussion of model macros, see Model files.\nDomain resource attributes The following domain resource attributes are specific to Model in Image domains.\n   Domain Resource Attribute Notes     domainHomeSourceType Required. Set to FromModel.   domainHome Must reference an empty or non-existent directory within your image. Do not include the mount path of any persistent volume. Note that Model in Image recreates the domain home for a WebLogic pod every time the pod restarts.   configuration.model.configMap Optional. Set if you have stored additional models in a ConfigMap as per Optional WDT model ConfigMap.   configuration.secrets Optional. Set this array if your image or ConfigMap models contain macros that reference custom Kubernetes Secrets. For example, if your macros depend on secrets my-secret and my-other-secret, then set to [my-secret, my-other-secret].   configuration.model.runtimeEncryptionSecret Required. All Model in Image domains must specify a runtime encryption secret. See Required runtime encryption secret.   configuration.model.domainType Set the type of domain. Valid values are WLS, JRF, and RestrictedJRF, where WLS is the default. See WDT Domain Types.    Notes:\n  There are additional attributes that are common to all domain home source types, such as the image field. See the Domain Resource schema and documentation for a full list of domain resource fields.\n  There are also additional fields that are specific to JRF domain types. For more information, see Prerequisites for JRF domain types.\n  Sample domain resource: For an example of a fully specified sample domain resource, see the the operator source\u0026rsquo;s kubernetes/samples/scripts/create-weblogic-domain/model-in-image/k8s-domain.yaml.template file for the Model in Image sample. Note that the @@ entries in this template are not processed by the operator; they need to replaced with actual values before the resource can be applied.\n  Prerequisites for JRF domain types This section applies only for a JRF domain type. Skip it if your domain type is WLS or RestrictedJRF.\n A JRF domain requires an infrastructure database, initializing this database using RCU, and configuring your domain to access this database. All of these steps must occur before you first deploy your domain. When you first deploy your domain, the introspector job will initialize it\u0026rsquo;s OPSS schema tables in the database - a process that can take several minutes.\nFurthermore, if you want to safely ensure that a restarted JRF domain can access updates to the infrastructure database that the domain made at an earlier time, the original domain\u0026rsquo;s wallet file must be safely saved as soon as practical, and the restarted domain must be supplied a wallet file that was obtained from a previous run of the domain.\nHere are the required settings for Model in Image JRF domains:\n  Set configuration.model.domainType to JRF.\n  Set configuration.opss.walletPasswordSecret to reference a secret that defines a walletPassword key. This is used to encrypt the domain\u0026rsquo;s OPSS wallet file. This is a required field for JRF domains.\n  Set configuration.opss.walletFileSecret to reference a secret that contains your domain\u0026rsquo;s OPSS wallet file in its walletFile key. This assumes you have an OPSS wallet file from a previous start of the same domain. It enables a restarted or migrated domain to access its database information. This is an optional field for JRF domains, but must always be set if you want a restarted or migrated domain to access its database information.\n  Set the configuration.introspectorJobActiveDeadlineSeconds introspection job timeout to at least 300 seconds. This is in an optional field but is needed because domain home creation takes a considerable amount of time the first time a JRF domain is created (due to initializing the domain\u0026rsquo;s database tables), and because Model in Image creates your domain home for you using the introspection job.\n  Define an RCUDbInfo stanza in your model. Access to an database requires defining a RCUDbInfo stanza in your model\u0026rsquo;s domainInfo stanza with the necessary information for accessing the domain\u0026rsquo;s schema within the database. Usually this information should be supplied using a secret that you deploy and reference in your domain resource\u0026rsquo;s configuration.secrets field. Here\u0026rsquo;s an example RCUDbInfo stanza:\ndomainInfo: RCUDbInfo: rcu_prefix: '@@SECRET:sample-domain1-rcu-access/rcu_prefix@@' rcu_schema_password: '@@SECRET:sample-domain1-rcu-access/rcu_schema_password@@' rcu_db_conn_string: '@@SECRET:sample-domain1-rcu-access/rcu_db_conn_string@@'   Important instructions when changing a database password:\n  Shut down all domains that access the database schema. For example, set their serverStartPolicy to NEVER.\n  Update the password in the database.\n  Update the Kubernetes Secret that contains your RCUDbInfo.rcu_schema_password for each domain.\n  Restart the domains. For example, change their serverStartPolicy from NEVER to IF_NEEDED.\n  Save your wallet files again, as changing your password generates a different wallet.\n  References:\nFor an example of using JRF in combination with Model in Image, see the Model in Image sample.\nSee also, Specifying RCU connection information in the model in the WDT documentation.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/faq/boot-identity-not-valid/",
	"title": "Boot identity not valid",
	"tags": [],
	"description": "",
	"content": " One or more WebLogic Server instances in my domain will not start and I see errors in the server log like this:\n\u0026lt;Feb 6, 2020 12:05:35,550 AM GMT\u0026gt; \u0026lt;Authentication denied: Boot identity not valid. The user name or password or both from the boot identity file (boot.properties) is not valid. The boot identity may have been changed since the boot identity file was created. Please edit and update the boot identity file with the proper values of username and password. The first time the updated boot identity file is used to start the server, these new values are encrypted.\u0026gt;\n When you see these kinds of errors, it means that the user name and password provided in the weblogicCredentialsSecret are incorrect. Prior to operator version 2.5.0, this error could have also indicated that the WebLogic domain directory\u0026rsquo;s security configuration files have changed in an incompatible way between when the operator scanned the domain directory, which occurs during the \u0026ldquo;introspection\u0026rdquo; phase, and when the server instance attempted to start. There is now a separate validation for that condition described in the Domain secret mismatch FAQ entry.\nCheck that the user name and password credentials stored in the Kubernetes Secret referenced by weblogicCredentialsSecret contain the expected values for an account with administrative privilege for the WebLogic domain. Then stop all WebLogic Server instances in the domain before restarting so that the operator will repeat its introspection and generate the corrected boot.properties files.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/faq/domain-secret-mismatch/",
	"title": "Domain secret mismatch",
	"tags": [],
	"description": "",
	"content": " One or more WebLogic Server instances in my domain will not start and the domain resource status or the pod log reports errors like this:\nDomain secret mismatch. The domain secret in DOMAIN_HOME/security/SerializedSystemIni.dat where DOMAIN_HOME=$DOMAIN_HOME does not match the domain secret found by the introspector job. WebLogic requires that all WebLogic Servers in the same domain share the same domain secret.\n When you see these kinds of errors, it means that the WebLogic domain directory\u0026rsquo;s security configuration files have changed in an incompatible way between when the operator scanned the domain directory, which occurs during the \u0026ldquo;introspection\u0026rdquo; phase, and when the server instance attempted to start.\nTo understand the \u0026ldquo;incompatible domain security configuration\u0026rdquo; type of failure, it\u0026rsquo;s important to review the contents of the WebLogic domain directory. Each WebLogic domain directory contains a security subdirectory that contains a file called SerializedSystemIni.dat. This file contains security data to bootstrap the WebLogic domain, including a domain-specific encryption key.\nDuring introspection, the operator generates a Kubernetes Job that runs a pod in the domain\u0026rsquo;s Kubernetes Namespace and with the same Kubernetes ServiceAccount that will be used later to run the Administration Server. This pod has access to the Kubernetes secret referenced by weblogicCredentialsSecret and encrypts these values with the domain-specific encryption key so that the secured value can be injected in to the boot.properties files when starting server instances.\nWhen the domain directory is changed such that the domain-specific encryption key is different, the boot.properties entries generated during introspection will now be invalid.\nThis can happen in a variety of ways, depending on the domain home source type.\nDomain in Image Rolling to an image containing new or unrelated domain directory The error occurs while rolling pods have containers based on a new Docker image that contains an entirely new or unrelated domain directory.\nThe problem is that WebLogic cannot support server instances being part of the same WebLogic domain if the server instances do not all share the same domain-specific encryption key. Additionally, operator introspection currently happens only when starting servers following a total shutdown. Therefore, the boot.properites files generated from introspecting the image containing the original domain directory will be invalid when used with a container started with the updated Docker image containing the new or unrelated domain directory.\nThe solution is to follow either the recommended CI/CD guidelines so that the original and new Docker images contain domain directories with consistent domain-specific encryption keys and bootstrapping security details, or to perform a total shutdown of the domain so that introspection reoccurs as servers are restarted.\nFull domain shutdown and restart The error occurs while starting servers after a full domain shutdown.\nIf your development model generates new Docker images with new and unrelated domain directories and then tags those images with the same tag, then different Kubernetes worker nodes may have different images under the same tag in their individual, local Docker repositories.\nThe simplest solution is to set imagePullPolicy to Always; however, the better solution would be to design your development pipeline to generate new Docker image tags on every build and to never reuse an existing tag.\nDomain in PV Completely replacing the domain directory The error occurs while starting servers when the domain directory change was made while other servers were still running.\nIf completely replacing the domain directory, then you must stop all running servers.\nBecause all servers will already be stopped, there is no requirement that the new contents of the domain directory be related to the previous contents of the domain directory. When starting servers again, the operator will perform its introspection of the domain directory. However, you may want to preserve the domain directory security configuration including the domain-specific encryption key and, in that case, you should follow a similar pattern as is described in the CI/CD guidelines for the domain in a Docker image model to preserve the original security-related domain directory files.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/model-in-image/model-files/",
	"title": "Model files",
	"tags": [],
	"description": "Model file requirements, macros, and loading order.",
	"content": " This feature is supported only in 3.0.0-rc1.\n Contents  Introduction Sample model file Important notes about Model in Image model files Model file naming and loading order Model file macros  Using secrets in model files Using environment variables in model files Combining secrets and environment variables in model files    Introduction This document describes basic Model in Image model file syntax, naming, and macros. For additional information, see the WebLogic Deploy Tool documentation.\nThe WDT Discover Domain Tool is particularly useful for generating model files from an existing domain home.\n Sample model file Here\u0026rsquo;s an example of a model YAML file that defines a WebLogic Server Administration Server and dynamic cluster.\ndomainInfo: AdminUserName: '@@SECRET:__weblogic-credentials__:username@@' AdminPassword: '@@SECRET:__weblogic-credentials__:password@@' ServerStartMode: 'prod' topology: Name: '@@ENV:DOMAIN_UID@@' AdminServerName: \u0026quot;admin-server\u0026quot; Cluster: \u0026quot;cluster-1\u0026quot;: DynamicServers: ServerTemplate: \u0026quot;cluster-1-template\u0026quot; ServerNamePrefix: \u0026quot;managed-server\u0026quot; DynamicClusterSize: 5 MaxDynamicClusterSize: 5 CalculatedListenPorts: false Server: \u0026quot;admin-server\u0026quot;: ListenPort: 7001 ServerTemplate: \u0026quot;cluster-1-template\u0026quot;: Cluster: \u0026quot;cluster-1\u0026quot; ListenPort: 8001 This sample model file:\n Includes a WebLogic credentials stanza that is required by Model in Image. Derives its domain name from the predefined environment variable DOMAIN_UID, but note that this is not required.  For a description of model file macro references to secrets and environment variables, see Model file macros.\nImportant notes about Model in Image model files   You can use model macros to reference arbitrary secrets from model files. This is recommended for handling mutable values such as database user names, passwords, and URLs. See Using secrets in model files.\n  All password fields in a model should use a secret macro. Passwords should not be directly included in property or model files because the files may appear in logs or debugging.\n  Model files encrypted with the WDT Encrypt Model Tool are not supported. Use secrets instead.\n    You can use model macros to reference arbitrary environment variables from model files. This is useful for handling plain text mutable values that you can define using an env stanza in your domain resource, and is also useful for accessing the built in DOMAIN_UID environment variable. See Using environment variables in model files.\n  For most models, it\u0026rsquo;s useful to minimize or eliminate the usage of model variable files (also known as property files) and use secrets or environment variables instead.\n  A model must contain a domainInfo stanza that references your WebLogic administrative credentials. You can use the @@SECRET macro with the reserved secret name __weblogic-credentials__ to reference your domain resource\u0026rsquo;s WebLogic credentials secret for this purpose. For example:\ndomainInfo: AdminUserName: '@@SECRET:__weblogic-credentials__:username@@' AdminPassword: '@@SECRET:__weblogic-credentials__:password@@'   You can control the order that WDT uses to load your model files, see Model file naming and loading order.\n  Model file naming and loading order Refer to this section if you need to control the order in which your model files are loaded. The order is important when two or more model files refer to the same configuration, because the last model that\u0026rsquo;s loaded has the highest precedence.\nDuring domain home creation, model, and property files are first loaded from the /u01/model_home/models directory within the image and are then loaded from the optional WDT ConfigMap, described in Optional WDT model ConfigMap.\nThe loading order within each of these locations is first determined using the convention filename.##.yaml and filename.##.properties, where ## are digits that specify the desired order when sorted numerically. Additional details:\n Embedding a .##. in a filename is optional and can appear anywhere in the file name before the properties or yaml extension.  The precedence of file names that include more than one .##. is undefined. The number can be any integer greater than or equal to zero.   File names that don\u0026rsquo;t include .##. sort before other files as if they implicitly have the lowest possible .##. If two files share the same number, the loading order is determined alphabetically as a tie-breaker.  If an image file and ConfigMap file both have the same name, then both files are loaded.\nFor example, if you have these files in the image directory /u01/model_home/models:\njdbc.20.yaml main-model.10.yaml my-model.10.yaml y.yaml And you have these files in the ConfigMap:\njdbc-dev-urlprops.10.yaml z.yaml Then the combined model files list is passed to the WebLogic Deploy Tool as:\ny.yaml,main-model.10.yaml,my-model.10.yaml,jdbc.20.yaml,z.yaml,jdbc-dev-urlprops.10.yaml\nProperty files (ending in .properties) use the same sorting algorithm, but they are appended together into a single file prior to passing them to the WebLogic Deploy Tool.\nModel file macros Using secrets in model files You can use WDT model @@SECRET macros to reference the WebLogic administrator username and password keys that are stored in a Kubernetes Secret and to optionally reference additional secrets. Here is the macro pattern for accessing these secrets:\n   Domain Resource Attribute Corresponding WDT Model @@SECRET Macro     webLogicCredentialsSecret @@SECRET:__weblogic-credentials__:username@@ and @@SECRET:__weblogic-credentials__:password@@   configuration.secrets @@SECRET:mysecret:mykey@@    For example, you can reference the WebLogic credential user name using @@SECRET:__weblogic-credentials__:username@@, and you can reference a custom secret mysecret with key mykey using @@SECRET:mysecret:mykey@@.\nAny secrets that are referenced by an @@SECRET macro must be deployed to the same namespace as your domain resource, and must be referenced in your domain resource using the weblogicCredentialsSecret and configuration.secrets fields.\nHere\u0026rsquo;s a sample snippet from a domain resource that sets a webLogicCredentialsSecret and two custom secrets my-custom-secret1 and my-custom-secret2.\n... spec: webLogicCredentialsSecret: name: my-weblogic-credentials-secret configuration: secrets: [ my-custom-secret1,my-custom-secret2 ] ... Using environment variables in model files You can reference operator environment variables in model files. This includes any that you define yourself in your domain resource, or the built-in DOMAIN_UID environment variable.\nFor example, the @@ENV:DOMAIN_UID@@ macro resolves to the current domain\u0026rsquo;s domain UID.\nCombining secrets and environment variables in model files You can embed an environment variable macro in a secret macro. This is useful for referencing secrets that you\u0026rsquo;ve named based on your domain\u0026rsquo;s domainUID.\nFor example, if your domainUID is domain1, then the macro @@SECRET:@@ENV:DOMAIN_UID@@-super-double-secret:mykey@@ resolves to the value stored in mykey for secret domain1-super-double-secret.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/faq/fan/",
	"title": "Disabling Fast Application Notifications",
	"tags": [],
	"description": "",
	"content": "To support Fast Application Notifications (FAN), Oracle databases configure GRID (Oracle Grid Infrastructure). GRID is typically associated with (and required by) Oracle RAC databases but can also be used in other configurations. Oracle Autonomous Database-Serverless (ATP-S) does not provide GRID.\nWhen connecting to a database that does not have GRID, the only type of WebLogic Server data source that is supported is the Generic Data Sources. Multi Data Sources and Active GridLink data sources cannot be used because they work with RAC.\nWebLogic Server 12.2.1.3.0 shipped with the 12.2.0.1 Oracle driver. When connecting with this driver to a database that does not have GRID, you will encounter the following exception (however, the 18.3 driver does not have this problem):\noracle.simplefan.impl.FanManager configure SEVERE: attempt to configure ONS in FanManager failed with oracle.ons.NoServersAvailable: Subscription time out To correct the problem, you must disable FAN, in one of two places:\n Through a system property at the domain, cluster, or server level.  To do this, edit the Domain Custom Resource to set the system property oracle.jdbc.fanEnabled to false as shown in the following example:\n serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026quot;-Dweblogic.StdoutDebugEnabled=false -Doracle.jdbc.fanEnabled=false\u0026quot; Configure the data source connection pool properties.  The following WLST script adds the oracle.jdbc.fanEnabled property, set to false, to an existing data source.\n fmwDb = 'jdbc:oracle:thin:@' + db print 'fmwDatabase ' + fmwDb cd('/JDBCSystemResource/LocalSvcTblDataSource/JdbcResource/LocalSvcTblDataSource') cd('JDBCDriverParams/NO_NAME_0') set('DriverName', 'oracle.jdbc.OracleDriver') set('URL', fmwDb) set('PasswordEncrypted', dbPassword) stbUser = dbPrefix + '_STB' cd('Properties/NO_NAME_0/Property/user') set('Value', stbUser) ls() cd('../..') ls() create('oracle.jdbc.fanEnabled','Property') ls() cd('Property/oracle.jdbc.fanEnabled') set('Value', 'false') ls() After changing the data source connection pool configuration, for the attribute to take effect, make sure to undeploy and redeploy the data source, or restart WebLogic Server.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/faq/oci-fss-pv/",
	"title": "Using OCI File Storage (FSS) for persistent volumes",
	"tags": [],
	"description": "",
	"content": "If you are running your Kubernetes cluster on Oracle Container Engine for Kubernetes (commonly known as OKE), and you use OCI File Storage (FSS) for persistent volumes to store the WebLogic domain home, then the file system handling demonstrated in the operator persistent volume sample will require an update to properly initialize the file ownership on the persistent volume when the domain is initially created.\nFile permission handling on persistent volumes can differ between cloud providers and even with the underlying storage handling on Linux based systems. These instructions provide one option to update file ownership used by the standard Oracle images where UID 1000 and GID 1000 typically represent the oracle or opc user. For more information on persistent volume handling, see Persistent storage.\n Failure during domain creation with persistent volume sample The existing sample for creation of a domain home on persistent volume uses a Kubernetes Job to create the domain. The sample uses an initContainers section to change the file ownership which will fail for OCI FSS created volumes used with an OKE cluster.\nThe OCI FSS volume contains some files that are not modifiable thus causing the Kubernetes Job to fail. The failure is seen in the description of the Kubernetes Job pod:\n$ kubectl describe -n domain1-ns pod domain1-create-weblogic-sample-domain-job-wdkvs : : Init Containers: fix-pvc-owner: Container ID: docker://7051b6abdc296c76e937246df03d157926f2f7477e63b6af3bf65f6ae1ceddee Image: container-registry.oracle.com/middleware/weblogic:12.2.1.3 Image ID: docker-pullable://container-registry.oracle.com/middleware/weblogic@sha256:47dfd4fdf6b56210a6c49021b57dc2a6f2b0d3b3cfcd253af7a75ff6e7421498 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Command: sh -c chown -R 1000:1000 /shared State: Terminated Reason: Error Exit Code: 1 Started: Wed, 12 Feb 2020 18:28:53 +0000 Finished: Wed, 12 Feb 2020 18:28:53 +0000 Ready: False Restart Count: 0 Environment: \u0026lt;none\u0026gt; : Updating the domain on persistent volume sample In the following snippet of the create-domain-job-template.yaml, you can see the updated command for the init container:\napiVersion: batch/v1 kind: Job metadata: name: %DOMAIN_UID%-create-weblogic-sample-domain-job namespace: %NAMESPACE% spec: template: metadata: : : spec: restartPolicy: Never initContainers: - name: fix-pvc-owner image: %WEBLOGIC_IMAGE% command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;chown 1000:1000 %DOMAIN_ROOT_DIR%/. \u0026amp;\u0026amp; find %DOMAIN_ROOT_DIR%/. -maxdepth 1 ! -name '.snapshot' ! -name '.' -print0 | xargs -r -0 chown -R 1000:1000\u0026quot;] volumeMounts: - name: weblogic-sample-domain-storage-volume mountPath: %DOMAIN_ROOT_DIR% securityContext: runAsUser: 0 runAsGroup: 0 containers: - name: create-weblogic-sample-domain-job image: %WEBLOGIC_IMAGE% : Use this new command in your copy of this template file. This will result in the ownership being updated for the expected files only, before the WebLogic domain is created on the persistent volume.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/model-in-image/runtime-updates/",
	"title": "Runtime updates",
	"tags": [],
	"description": "Updating a running Model in Image domain&#39;s images and model files.",
	"content": " This feature is supported only in 3.0.0-rc1.\n Contents  Overview Important notes Frequently asked questions Supported and unsupported updates Changing a domain resource restartVersion Using the WDT Discover Domain Tool Example of adding a data source  Overview If you want to make a configuration change to a running Model in Image domain, and you want the change to survive WebLogic pod restarts, then you can modify your existing model using one of the following approaches:\n  Changing secrets or environment variables that are referenced by macros in your model files.\n  Specifying a new or updated WDT ConfigMap that contains model files and use your domain resource configuration.model.configMap field to reference the map.\n  Supplying a new image with new or changed model files.\n  After the changes are in place, you can tell the operator to apply the changes and propagate them to a running domain by altering the domain resource\u0026rsquo;s image or restartVersion attribute.\nImportant notes   Check for Supported and unsupported updates.\n  If you specify multiple model files in your image or WDT ConfigMap, the order in which they\u0026rsquo;re loaded and merged is determined as described in Model file naming and loading order.\n  You can use the WDT Discover Domain Tool to help generate your model file updates. See Using the WDT Discover Domain Tool.\n  For simple ways to change restartVersion, see Changing a domain resource restartVersion.\n  For a sample of adding a data source to a running domain, see Example of adding a data source.\n  For a discussion of model file syntax, see the WebLogic Deploy Tool documentation and Model files.\n  If the introspector job reports a failure, see Debugging for debugging advice.\n  Frequently asked questions Why is it necessary to specify updates using model files?\nSimilar to Domain in Image, if you make a direct runtime WebLogic configuration update of a Model in Image domain using the WebLogic Server Administration Console or WLST scripts, then the update will be ephemeral. This is because the domain home is stored in an image directory which will not survive the restart of the pod.\nHow do Model in Image updates work during runtime?\nAfter you make a change to your domain resource restartVersion or image attribute, the operator will rerun the domain\u0026rsquo;s introspector job. This job will reload all of your secrets and environment variables, merge all of your model files, and generate a new domain home. If the job succeeds, then the operator will make the updated domain home available to pods using a ConfigMap named DOMAIN_UID-weblogic-domain-introspect-cm. Finally, the operator will subsequently roll (restart) each running WebLogic Server pod in the domain so that it can load the new configuration. A domain roll begins by restarting the domain\u0026rsquo;s Administration Server and then proceeds to restart each Managed Server in the domain.\nCan we use custom configuration overrides to do the updates instead?\nNo. Custom configuration overrides, which are WebLogic configuration overrides specified using a domain resource configuration.overridesConfigMap, as described in Configuration overrides, aren\u0026rsquo;t supported in combination with Model in Image. Model in Image will generate an error if custom overrides are specified. This should not be a concern because model file, secret, or model image updates are simpler and more flexible than custom configuration override updates. Unlike configuration overrides, the syntax for a model file update exactly matches the syntax for specifying your model file in the first place.\nSupported and unsupported updates   You can add new MBeans or resources simply by specifying their corresponding model file YAML snippet along with their parent bean hierarchy. See Example of adding a data source.\n  You can change or add secrets that your model references. For example, you can change a database password secret.\n  You can change or add environment variables that your model macros reference (macros that use the @@ENV:myenvvar@@ syntax).\n  You can remove a named MBean or resource by specifying a model file with an exclamation point (!) just before the bean or resource name. For example, if you have a data source named mynewdatasource defined in your model, it can be removed by specifying a small model file that loads after the model file that defines the data source, where the small model file looks like this:\nresources: JDBCSystemResource: !mynewdatasource: For more information, see Declaring Named MBeans to Delete in the WebLogic Deploying Tooling documentation.\n  You can add or alter an MBean attribute by specifying a YAML snippet along with its parent bean hierarchy that references an existing MBean and the attribute. For example, to add or alter the maximum capacity of a data source named mynewdatasource:\nresources: JDBCSystemResource: mynewdatasource: JdbcResource: JDBCConnectionPoolParams: MaxCapacity: 5 For more information, see Using Multiple Models in the WebLogic Deploy Tooling documentation.\n  There is no way to directly delete an attribute from an MBean that\u0026rsquo;s already been specified by a model file. The work-around is to do this using two model files: add a model file that deletes the named bean/resource that is a parent to the attribute you want to delete, and add another model file that will be loaded after the first one, which fully defines the named bean/resource but without the attribute you want to delete.\n  There is no way to directly change the MBean name of an attribute. Instead, you can remove a named MBean using the ! syntax as described above, and then add a new one as a replacement.\n  You cannot change the domain name at runtime.\n  The following types of runtime update configuration are not supported in this release of Model in Image. If you need to make these kinds of updates, shut down your domain entirely before making the change:\n Domain topology (cluster members) Network channel listen address, port, and enabled configuration Server and domain log locations Node Manager related configuration Changing any existing MBean name  Specifically, do not apply runtime updates for:\n Adding or removing:  Servers Clusters Network Access Points (custom channels)   Changing any of the following:  Dynamic cluster size Default, SSL, and Admin channel Enabled, listen address, and port Network Access Point (custom channel), listen address, or port Server and domain log locations \u0026ndash; use the logHome domain setting instead Node Manager access credentials    Note that it is permitted to override network access point public or external addresses and ports. External access to JMX (MBean) or online WLST requires that the network access point internal port and external port match (external T3 or HTTP tunneling access to JMS, RMI, or EJBs don\u0026rsquo;t require port matching).\n  Due to security considerations, we strongly recommend that T3 or any RMI protocol should not be exposed outside the cluster.\n Changing a domain resource restartVersion As was mentioned in the overview, one way to tell the operator to apply your configuration changes to a running domain is by altering the domain resource restartVersion. Here are some common ways to do this:\n  You can alter restartVersion interactively using kubectl edit -n MY_NAMESPACE domain MY_DOMAINUID.\n  If you have your domain\u0026rsquo;s resource file, then you can alter this file and call kubectl apply -f on the file.\n  You can use the Kubernetes get and patch commands. Here\u0026rsquo;s a sample automation script that takes a namespace as the first parameter (default sample-domain1-ns) and that takes a domainUID as the second parameter (default sample-domain1):\n#!/bin/bash NAMESPACE=${1:-sample-domain1-ns} DOMAINUID=${2:-sample-domain1} currentRV=$(kubectl -n ${NAMESPACE} get domain ${DOMAINUID} -o=jsonpath='{.spec.restartVersion}') if [ $? = 0 ]; then # we enter here only if the previous command succeeded nextRV=$((currentRV + 1)) echo \u0026quot;@@ Info: Rolling domain '${DOMAINUID}' in namespace '${NAMESPACE}' from restartVersion='${currentRV}' to restartVersion='${nextRV}'.\u0026quot; kubectl -n ${NAMESPACE} patch domain ${DOMAINUID} --type='json' \\ -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/restartVersion\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;'${nextRV}'\u0026quot; }]' fi   Using the WDT Discover Domain Tool The WebLogic Deploy Tooling Discover Domain Tool generates model files from an existing domain home. You can use this tool to help determine the model file contents you would need to supply to update an existing model.\nFor example, assuming you\u0026rsquo;ve installed WDT in /u01/wdt/weblogic-deploy and assuming your domain type is WLS:\n # (1) Run discover for your existing domain home. $ /u01/wdt/weblogic-deploy/bin/discoverDomain.sh \\ -oracle_home $ORACLE_HOME \\ -domain_home $DOMAIN_HOME \\ -domain_type WLS \\ -archive_file old.zip \\ -model_file old.yaml \\ -variable_file old.properties # (2) Now make some WebLogic config changes using the console or WLST. # (3) Run discover for your existing domain home. $ /u01/wdt/weblogic-deploy/bin/discoverDomain.sh \\ -oracle_home $ORACLE_HOME \\ -domain_home $DOMAIN_HOME \\ -domain_type WLS \\ -archive_file new.zip \\ -model_file new.yaml \\ -variable_file new.properties # (4) Compare your old and new yaml to see what changed. $ diff new.yaml old.yaml  Note: If your domain type isn\u0026rsquo;t WLS, remember to change the domain type to JRF or RestrictedJRF in the above commands.\n Example of adding a data source For details on how to add a data source, see the Update1 use case in the Model in Image sample.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/faq/oci-lb/",
	"title": "Using an OCI load balancer",
	"tags": [],
	"description": "",
	"content": "If you are running your Kubernetes cluster on Oracle Container Engine for Kubernetes (commonly known as OKE), you can have OCI automatically provision load balancers for you by creating a Service of type LoadBalancer instead of (or in addition to) installing an ingress controller like Traefik or Voyager.\nOKE Kubernetes worker nodes typically do not have public IP addresses. This means that the NodePort services created by the operator are not usable, because they would expose ports on the worker node\u0026rsquo;s private IP addresses only, which are not reachable from outside the cluster.\nInstead, you can use an OCI load balancer to provide access to services running in OKE.\nIt is also possible, if desirable, to have an OCI load balancer route traffic to an ingress controller running inside the Kubernetes cluster and have that ingress controller in turn route traffic to services in the cluster.\n Requesting an OCI load balancer When your domain is created by the operator, a number of Kubernetes services are created by the operator, including one for the WebLogic Server Administration Server and one for each Managed Server and cluster.\nIn the example below, there is a domain called bobs-bookstore in the bob namespace. This domain has a cluster called cluster-1 which exposes traffic on port 31111.\nThe Kubernetes YAML file below defines a new Service in the same namespace. The selector targets all of the pods in this namespace which are part of the cluster cluster-1, using the annotations that are placed on those pods by the operator. It also defines the port and protocol.\nYou can include the optional oci-load-balancer-shape annotation (as shown) if you want to specify the shape of the load balancer. Otherwise the default shape (100Mbps) will be used.\napiVersion: v1 kind: Service metadata: name: bobs-bookstore-oci-lb-service namespace: bob annotations: service.beta.kubernetes.io/oci-load-balancer-shape: 400Mbps spec: ports: - name: http port: 31111 protocol: TCP targetPort: 31111 selector: weblogic.clusterName: cluster-1 weblogic.domainUID: bobs-bookstore sessionAffinity: None type: LoadBalancer When you apply this YAML file to your cluster, you will see the new service is created but initially the external IP is shown as \u0026lt;pending\u0026gt;.\n$ kubectl -n bob get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE bobs-bookstore-admin-server ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,7001/TCP,30101/TCP 9d bobs-bookstore-admin-server-external NodePort 10.96.224.13 \u0026lt;none\u0026gt; 7001:32401/TCP 9d bobs-bookstore-cluster-cluster-1 ClusterIP 10.96.86.113 \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-oci-lb-service LoadBalancer 10.96.121.216 \u0026lt;pending\u0026gt; 31111:31671/TCP 9s After a short time (typically less than a minute), the OCI load balancer will be provisioned and the external IP address will be displayed:\n$ kubectl -n bob get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE bobs-bookstore-admin-server ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,7001/TCP,30101/TCP 9d bobs-bookstore-admin-server-external NodePort 10.96.224.13 \u0026lt;none\u0026gt; 7001:32401/TCP 9d bobs-bookstore-cluster-cluster-1 ClusterIP 10.96.86.113 \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-oci-lb-service LoadBalancer 10.96.121.216 132.145.235.215 31111:31671/TCP 55s You can now use the external IP address and port to access your pods. There are several options that can be used to configure more advanced load balancing behavior. For more information, including how to configure SSL support, supporting internal and external subnets, and so one, refer to the OCI documentation.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/userguide/managing-domains/model-in-image/debugging/",
	"title": "Debugging",
	"tags": [],
	"description": "Debugging a deployed Model in Image domain.",
	"content": " This feature is supported only in 3.0.0-rc1.\n Here are some suggestions for debugging problems with Model in Image after your domain resource is deployed.\nContents  Check the domain resource status Check the introspector job Check the WebLogic Server pods Check an operator log  Check the domain resource status To check the domain resource status: kubectl -n MY_NAMESPACE describe domain MY_DOMAINUID.\nCheck the introspector job If your introspector job failed, then examine the kubectl describe of the job and its pod, and also examine its log, if one exists.\nFor example, assuming your domain UID is sample-domain1 and your domain namespace is sample-domain1-ns:\n$ # here we see a failed introspector job pod among the domain's pods: $ kubectl -n sample-domain1-ns get pods -l weblogic.domainUID=sample-domain1 NAME READY STATUS RESTARTS AGE sample-domain1-admin-server 1/1 Running 0 19h sample-domain1-introspect-domain-job-v2l7k 0/1 Error 0 75m sample-domain1-managed-server1 1/1 Running 0 19h sample-domain1-managed-server2 1/1 Running 0 19h $ # let's look at the job's describe $ kubectl -n sample-domain1-ns describe job/sample-domain1-introspect-domain-job ... $ # now let's look at the job's pod describe, in particular look at its 'events' $ kubectl -n sample-domain1-ns describe pod/sample-domain1-introspect-domain-job-v2l7k ... $ # finally let's look at job's pod's log $ kubectl -n sample-domain1-ns logs job/sample-domain1-introspect-domain-job ... $ # alternative log command (will have same output as previous) # kubectl -n sample-domain1-ns logs pod/sample-domain1-introspect-domain-job-v2l7k A common reason for the introspector job to fail is because of an error in a model file. Here\u0026rsquo;s some sample log output from an introspector job that shows such a failure:\n... SEVERE Messages: 1. WLSDPLY-05007: Model file /u01/wdt/models/model1.yaml,/weblogic-operator/wdt-config-map/..2020_03_19_15_43_05.993607882/datasource.yaml contains an unrecognized section: TYPOresources. The recognized sections are domainInfo, topology, resources, appDeployments, kubernetes Check the WebLogic Server pods If your introspector job succeeded, then there will be no introspector job or pod, the operator will create a MY_DOMAIN_UID-weblogic-domain-introspect-cm ConfigMap for your domain, and the operator will then run the domain\u0026rsquo;s WebLogic pods.\nIf kubectl -n MY_NAMESPACE get pods reveals that your WebLogic pods have errors, then use kubectl -n MY_NAMESPACE describe pod POD_NAME and kubectl -n MY_NAMESPACE logs POD_NAME to debug.\nCheck an operator log Look for SEVERE and ERROR level messages in your operator logs. For example:\n$ # find your operator $ kubectl get deployment --all-namespaces=true -l weblogic.operatorName NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE sample-weblogic-operator-ns weblogic-operator 1 1 1 1 20h $ # grep operator log for SEVERE and WARNING level messages $ kubectl logs deployment/weblogic-operator -n sample-weblogic-operator-ns \\ | egrep -e \u0026quot;level...(SEVERE|WARNING)\u0026quot; {\u0026quot;timestamp\u0026quot;:\u0026quot;03-18-2020T20:42:21.702+0000\u0026quot;,\u0026quot;thread\u0026quot;:11,\u0026quot;fiber\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;domainUID\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;level\u0026quot;:\u0026quot;WARNING\u0026quot;,\u0026quot;class\u0026quot;:\u0026quot;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026quot;,\u0026quot;method\u0026quot;:\u0026quot;createAndValidateKubernetesVersion\u0026quot;,\u0026quot;timeInMillis\u0026quot;:1584564141702,\u0026quot;message\u0026quot;:\u0026quot;Kubernetes minimum version check failed. Supported versions are 1.13.5+,1.14.8+,1.15.7+, but found version v1.12.3\u0026quot;,\u0026quot;exception\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;code\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;headers\u0026quot;:{},\u0026quot;body\u0026quot;:\u0026quot;\u0026quot;} You can filter out operator log messages specific to your domainUID by piping the above logs command through grep \u0026quot;domainUID...MY_DOMAINUID\u0026quot;. For example, assuming your operator is running in namespace sample-weblogic-operator-ns and your domain UID is sample-domain1:\n$ kubectl logs deployment/weblogic-operator -n sample-weblogic-operator-ns \\ | egrep -e \u0026quot;level...(SEVERE|WARNING)\u0026quot; \\ | grep \u0026quot;domainUID...sample-domain1\u0026quot; "
},
{
	"uri": "/weblogic-kubernetes-operator/faq/volumes/",
	"title": "Providing access to a PersistentVolumeClaim",
	"tags": [],
	"description": "",
	"content": " I need to provide an instance with access to a PersistentVolumeClaim.\n Some applications need access to a file, either to read data or to provide additional logging beyond what is built into the operator. One common way of doing that within Kubernetes is to create a PersistentVolumeClaim (PVC) and map it to a file. The domain configuration can then be used to provide access to the claim across the domain, within a single cluster, or for a single server. In each case, the access is configured within the serverPod element of the configuration of the desired scope.\nFor example, here is a read-only PersistentVolumeClaim specification. Note that its name is myclaim.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: accessModes: - ReadOnlyMany volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: slow To provide access to this claim to all Managed Servers in the cluster-1 cluster, specify the following in your domain resource:\n clusters: - clusterName: cluster-1 serverPod: volumes: - name: my-volume-1 persistentVolumeClaim: claimName: myclaim volumeMounts: - name: my-volume-1 mountPath: /weblogic-operator/my/volume1 Note the use of the claim name in the claimName field of the volume entry. Both a volume and a volumeMount entry are required, and must have the same name. The volume entry associates that name with the claim, while the volumeMount entry defines the path to it that the application can use to access the file.\nNOTE: If the PVC is mapped either across the domain or to a cluster, its access mode must be either ReadOnlyMany or ReadWriteMany.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/faq/configmaps/",
	"title": "Providing access to a ConfigMap",
	"tags": [],
	"description": "",
	"content": " I need to provide an instance with access to a ConfigMap.\n Configuration files can be supplied to Kubernetes Pods and Jobs by a ConfigMap, which consists of a set of key-value pairs. Each entry may be accessed by one or more operator-managed nodes as a read-only text file. Access can be provided across the domain, within a single cluster, or for a single server. In each case, the access is configured within the serverPod element of the desired scope.\nFor example, given a ConfigMap named my-map with entries key-1 and key-2, you can provide access to both values as separate files in the same directory within the cluster-1 cluster with the following in your domain resource:\n clusters: - clusterName: cluster-1 serverPod: volumes: - name: my-volume-1 configMap: name: my-map items: - key: key-1 path: first - key: key-2 path: second volumeMounts: - name: my-volume-1 mountPath: /weblogic-operator/my This provides access to two files, found at paths /weblogic-operator/my/first and /weblogic-operator/my/second. Both a volume and a volumeMount entry are required, and must have the same name. The name of the ConfigMap is specified in the name field under the configMap entry. The items entry is an array, in which each entry maps a ConfigMap key to a file name under the directory specified as mountPath under a volumeMount.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/faq/external-clients/",
	"title": "External WebLogic clients",
	"tags": [],
	"description": "",
	"content": "Approaches There are two supported approaches for giving external WebLogic EJB or JMS clients access to a Kubernetes hosted WebLogic cluster: Load balancer tunneling and Kubernetes NodePorts.\nThis FAQ is for remote EJB and JMS clients - not JTA clients. The operator does not currently support external WebLogic JTA access to a WebLogic cluster, because external JTA access requires each server in the cluster to be individually addressable by the client, but this conflicts with the current operator requirement that a network channel in a cluster have the same port across all servers in the cluster.\n Load balancer tunneling The load balancer tunneling approach for giving external WebLogic EJB or JMS clients access to a Kubernetes hosted WebLogic cluster involves configuring a network channel on the desired WebLogic cluster that accepts T3 protocol traffic that\u0026rsquo;s tunneled over HTTP, deploying a load balancer that redirects external HTTP network traffic to the desired WebLogic network channel, and ensuring that EJB and JMS clients specify a URL that resolves the load balancer\u0026rsquo;s network address.\nHere are the steps:\n  Configure a custom channel for the T3 protocol in WebLogic that enables HTTP tunneling, and specifies an external address and port that correspond to the address and port remote clients will use to access the load balancer. See Adding a WebLogic custom channel for samples and details.\n  Set up a load balancer that redirects HTTP traffic to the custom channel. For more information on load balancers, see Ingress. If you\u0026rsquo;re also using OKE/OCI to host your Kubernetes cluster, also see Using an OCI Load Balancer.\n  Important: Ensure that the load balancer configures the HTTP flow to be \u0026lsquo;sticky\u0026rsquo; - for example, a Traefik load balancer has a sticky sessions option. This ensures that all of the packets of a tunneling client connection flow to the same pod, otherwise the connection will stall when its packets are load balanced to a different pod.\n  Remote clients can then access the custom channel using an http:// URL instead of a t3:// URL.\n  Review the Security notes.\n  Kubernetes NodePorts The Kubernetes NodePorts approach for giving external WebLogic EJB or JMS clients access to a Kubernetes hosted WebLogic cluster involves configuring a network channel on the desired WebLogic cluster that accepts T3 protocol traffic, and exposing a Kubernetes NodePort that redirects external network traffic on the Kubernetes Nodes to the network channel.\nThe NodePort approach is available only when worker nodes are accessible by the clients, for example, when they have public IP addresses. If private worker nodes are used and access to them is possible only through a load balancer or bastion, then the NodePort approach is not a valid option to provide access to external clients.\n Here are the steps:\n  Configure a custom channel for the T3 protocol in WebLogic that specifies an external address and port that are suitable for remote client use. See Adding a WebLogic custom channel.\n  Define a Kubernetes NodePort to publicly expose the WebLogic ports. See Setting up a NodePort.\n  Review the Security notes.\n  Adding a WebLogic custom channel When is a WebLogic custom channel needed? WebLogic implicitly creates a multi-protocol default channel that spans the Listen Address and Port fields specified on each server in the cluster, but this channel is usually unsuitable for external network traffic from EJB and JMS clients. Instead, you may need to configure an additional dedicated WebLogic custom channel to handle remote EJB or JMS client network traffic.\nA custom channel provides a way to configure an external listen address and port for use by external clients, unlike a default channel. External listen address or port configuration is needed when a channel\u0026rsquo;s configured listen address or port would not work if used to form a URL in the remote client. This is because remote EJB and JMS clients internally use their client\u0026rsquo;s channel\u0026rsquo;s configured network information to reconnect to WebLogic when needed. (The EJB and JMS clients do not always use the initial URL specified in the client\u0026rsquo;s JNDI context.)\nA custom channel can be locked down using two-way SSL as a way to prevent access by unauthorized external JMS and EJB clients, only accepts protocols that are explicitly enabled for the channel, and can be configured to be the only channel that accepts EJB/JMS clients that tunnel over HTTP. A default channel may often be deliberately unencrypted for convenient internal use, or, if used externally, is only used for web traffic (not tunneling traffic). In addition, a default channel supports several protocols but it\u0026rsquo;s a best practice to limit the protocols that can be accessed by external clients. Finally, external clients may require access using HTTP tunneling in order to make connections, but it\u0026rsquo;s often inadvisable to enable tunneling for an unsecured default channel that\u0026rsquo;s already servicing external HTTP traffic. This is because enabling HTTP tunneling would potentially allow unauthorized external JMS and EJB clients unsecured access to the WebLogic cluster through the same HTTP path.\nConfiguring a WebLogic custom channel The basic requirements for configuring a custom channel for remote EJB and JMS access are:\n  Configure a T3 protocol network access point (NAP) with the same name and port on each server (the operator will set the listen address for you).\n  Configure the external listen address and port on each NAP to match the address and port component of a URL your clients can use. For example, if you are providing access to remote clients using a load balancer, then these should match the address and port of the load balancer.\n  If you want WebLogic T3 clients to tunnel through HTTP, then enable HTTP tunneling on each NAP. This is often necessary for load balancers.\n  Do NOT set outbound-enabled to true on the network access point (the default is false), because this may cause internal network traffic to stall in an attempt to route through the network access point.\n  Ensure you haven\u0026rsquo;t enabled calculated-listen-ports for WebLogic dynamic cluster servers. The operator requires that a channel have the same port on each server in a cluster, but calculated-listen-ports causes the port to be different on each server.\n  For example, here is a snippet of a WebLogic domain config.xml for channel MyChannel defined for a WebLogic dynamic cluster named cluster-1:\n\u0026lt;server-template\u0026gt; \u0026lt;name\u0026gt;cluster-1-template\u0026lt;/name\u0026gt; \u0026lt;listen-port\u0026gt;8001\u0026lt;/listen-port\u0026gt; \u0026lt;cluster\u0026gt;cluster-1\u0026lt;/cluster\u0026gt; \u0026lt;network-access-point\u0026gt; \u0026lt;name\u0026gt;MyChannel\u0026lt;/name\u0026gt; \u0026lt;protocol\u0026gt;t3\u0026lt;/protocol\u0026gt; \u0026lt;public-address\u0026gt;some.public.address.com\u0026lt;/public-address\u0026gt; \u0026lt;listen-port\u0026gt;7999\u0026lt;/listen-port\u0026gt; \u0026lt;public-port\u0026gt;30999\u0026lt;/public-port\u0026gt; \u0026lt;http-enabled-for-this-protocol\u0026gt;true\u0026lt;/http-enabled-for-this-protocol\u0026gt; \u0026lt;tunneling-enabled\u0026gt;true\u0026lt;/tunneling-enabled\u0026gt; \u0026lt;outbound-enabled\u0026gt;false\u0026lt;/outbound-enabled\u0026gt; \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; \u0026lt;two-way-ssl-enabled\u0026gt;false\u0026lt;/two-way-ssl-enabled\u0026gt; \u0026lt;client-certificate-enforced\u0026gt;false\u0026lt;/client-certificate-enforced\u0026gt; \u0026lt;/network-access-point\u0026gt; \u0026lt;/server-template\u0026gt; \u0026lt;cluster\u0026gt; \u0026lt;name\u0026gt;cluster-1\u0026lt;/name\u0026gt; \u0026lt;cluster-messaging-mode\u0026gt;unicast\u0026lt;/cluster-messaging-mode\u0026gt; \u0026lt;dynamic-servers\u0026gt; \u0026lt;name\u0026gt;cluster-1\u0026lt;/name\u0026gt; \u0026lt;server-template\u0026gt;cluster-1-template\u0026lt;/server-template\u0026gt; \u0026lt;maximum-dynamic-server-count\u0026gt;5\u0026lt;/maximum-dynamic-server-count\u0026gt; \u0026lt;calculated-listen-ports\u0026gt;false\u0026lt;/calculated-listen-ports\u0026gt; \u0026lt;server-name-prefix\u0026gt;managed-server\u0026lt;/server-name-prefix\u0026gt; \u0026lt;dynamic-cluster-size\u0026gt;5\u0026lt;/dynamic-cluster-size\u0026gt; \u0026lt;max-dynamic-cluster-size\u0026gt;5\u0026lt;/max-dynamic-cluster-size\u0026gt; \u0026lt;/dynamic-servers\u0026gt; \u0026lt;/cluster\u0026gt; And, here is a snippet of offline WLST code that corresponds to the above config.xml snippet:\n templateName = \u0026quot;cluster-1-template\u0026quot; cd('/ServerTemplates/%s' % templateName) templateChannelName = \u0026quot;MyChannel\u0026quot; create(templateChannelName, 'NetworkAccessPoint') cd('NetworkAccessPoints/%s' % templateChannelName) set('Protocol', 't3') set('ListenPort', 7999) set('PublicPort', 30999) set('HttpEnabledForThisProtocol', true) set('TunnelingEnabled', true) set('OutboundEnabled', false) set('Enabled', true) set('TwoWaySslEnabled', false) set('ClientCertificateEnforced', false) In this example:\n  WebLogic binds the custom network channel to port 7999 and the default network channel to 8001.\n  The operator will automatically create a Kubernetes Service named DOMAIN_UID-cluster-cluster-1 for both the custom and default channel.\n  Internal clients running in the same Kubernetes cluster as the channel can access the cluster using t3://DOMAIN_UID-cluster-cluster-1:8001.\n  External clients would be expected to access the cluster using the custom channel using URLs like t3://some.public.address.com:30999 or, if using tunneling, http://some.public.address.com:30999.\n  WebLogic custom channel notes   Channel configuration for a configured cluster requires configuring the same network access point on each server. The operator currently doesn\u0026rsquo;t test or support network channels that have a different configuration on each server in the cluster.\n  Additional steps are required for external clients beyond configuring the custom channel - see Approaches.\n  Setting up a NodePort Getting started A Kubernetes NodePort exposes a port on each worker node in the Kubernetes cluster (they are not typically exposed on masters), where the port is accessible from outside of a Kubernetes cluster. This port redirects network traffic to pods within the Kubernetes cluster. Setting up a Kubernetes NodePort is one approach for giving external WebLogic clients access to JMS or EJBs.\nIf an EJB or JMS service is running on an Administration Server, then you can skip the rest of this section and use the spec.adminServer.adminService.channels domain resource attribute to have the operator create a NodePort for you. See Reference - Domain resource. Otherwise, if the EJB or JMS service is running in a WebLogic cluster or standalone WebLogic Server Managed Server, and you desire to provide access to the service using a NodePort, then the NodePort must be exposed \u0026lsquo;manually\u0026rsquo; - see the following sample and table.\nSetting up a NodePort usually also requires setting up a custom network channel. See Adding a WebLogic Custom Channel.\n Sample NodePort resource The following NodePort YAML file exposes an external node port of 30999 and internal port 7999 for a domain UID of DOMAIN_UID, a domain name of DOMAIN_NAME, and a cluster name of CLUSTER_NAME. It assumes that 7999 corresponds to a T3 protocol port of a channel that\u0026rsquo;s configured on your WebLogic cluster.\napiVersion: v1 kind: Service metadata: namespace: default name: DOMAIN_UID-cluster-CLUSTER_NAME-ext labels: weblogic.domainUID: DOMAIN_UID spec: type: NodePort externalTrafficPolicy: Cluster sessionAffinity: ClientIP selector: weblogic.domainUID: DOMAIN_UID weblogic.clusterName: CLUSTER_NAME ports: - name: myclustert3channel nodePort: 30999 port: 7999 protocol: TCP targetPort: 7999 Table of NodePort attributes    Attribute Description     metadata.name For this particular use case, the NodePort name can be arbitrary as long as it is DNS compatible. But, as a convention, it\u0026rsquo;s recommended to use DOMAIN_UID-cluster-CLUSTER_NAME-ext. To ensure the name is DNS compatible, use all lower case and convert any underscores (_) to dashes (-).   metadata.namespace Must match the namespace of your WebLogic cluster.   metadata.labels Optional. It\u0026rsquo;s helpful to set a weblogic.domainUid label so that cleanup scripts can locate all Kubernetes resources associated with a particular domain UID.   spec.type Must be NodePort.   spec.externalTrafficPolicy Set to Cluster for most use cases. This may lower performance, but ensures that a client that attaches to a node without any pods that match the spec.selector will be rerouted to a node with pods that do match. If set to Local, then connections to a particular node will route only to that node\u0026rsquo;s pods and will fail if the node doesn\u0026rsquo;t host any pods with the given spec.selector. It\u0026rsquo;s recommended for clients of a spec.externalTrafficPolicy: Local NodePort to use a URL that resolves to a list of all nodes, such as t3://mynode1,mynode2:30999, so that a client connect attempt will implicitly try mynode2 if mynode1 fails (alternatively, use a round-robin DNS address in place of mynode1,mynode2).   spec.sessionAffinity Set to ClientIP to ensure an HTTP tunneling connection always routes to the same pod, otherwise the connection may hang and fail.   spec.selector Specifies a weblogic.domainUID and weblogic.clusterName to associate the NodePort resource with your cluster\u0026rsquo;s pods. The operator automatically sets these labels on the WebLogic cluster pods that it deploys for you.   spec.ports.name This name is arbitrary.   spec.ports.nodePort The external port that clients will use. This must match the external port that\u0026rsquo;s configured on the WebLogic configured channels/network access points. By default, Kubernetes requires that this value range from 30000 to 32767.   spec.ports.port and spec.targetPort These must match the port that\u0026rsquo;s configured on the WebLogic configured channel/network access points.    Security notes   With some cloud providers, a load balancer or NodePort may implicitly expose a port to the public Internet.\n  If such a port supports a protocol suitable for WebLogic clients, note that WebLogic allows access to JNDI entries, EJB/RMI applications, and JMS by anonymous users by default.\n  You can configure a custom channel with a secure protocol and two-way SSL to help prevent external access by unwanted clients. See When is a WebLogic custom channel needed?.\n  Optional Reading   For sample JMS client code and JMS configuration, see Run Standalone WebLogic JMS Clients on Kubernetes.\n  For a detailed discussion of using T3 in combination with port mapping, see T3 RMI Communication for WebLogic Server Running on Kubernetes.\n  "
},
{
	"uri": "/weblogic-kubernetes-operator/faq/coherence-requirements/",
	"title": "Coherence Requirements",
	"tags": [],
	"description": "",
	"content": "If you are running Coherence on Kubernetes, either inside a WebLogic domain or standalone, then there are some additional requirements to make sure that Coherence can form clusters.\nNote that some Fusion Middleware products, like SOA Suite, use Coherence and so these requirements apply to them.\nUnicast and Well Known Address When the first Coherence process starts, it will form a cluster. The next Coherence process to start (for example, in a different pod), will use UDP to try to contact the senior member.\nIf you create a WebLogic domain which contains a Coherence cluster using the samples provided in this project, then that cluster will be configured correctly so that it is able to form; you do not need to do any additional manual configuration.\nIf you are running Coherence standalone (outside a WebLogic domain), then you should configure Coherence to use unicast and provide a \u0026ldquo;well known address (WKA)\u0026rdquo; so that all members can find the senior member. Most Kubernetes overlay network providers do not support multicast.\nThis is done by specifying Coherence well known addresses in a variable named coherence.wka as shown in the following example:\n-Dcoherence.wka=my-cluster-service In this example my-cluster-service should be the name of the Kubernetes service that is pointing to all of the members of that Coherence cluster.\nFor more information about running Coherence in Kubernetes outside of a WebLogic domain, refer to the Coherence operator documentation.\nOperating system library requirements In order for Coherence clusters to form correctly, the conntrack library must be installed. Most Kubernetes distributions will do this for you. If you have issues with clusters not forming, then you should check that conntrack is installed using this command (or equivalent):\n$ rpm -qa | grep conntrack libnetfilter_conntrack-1.0.6-1.el7_3.x86_64 conntrack-tools-1.4.4-4.el7.x86_64 You should see output similar to that shown above. If you do not, then you should install conntrack using your operating system tools.\nFirewall (iptables) requirements Some Kubernetes distributions create iptables rules that block some types of traffic that Coherence requires to form clusters. If you are not able to form clusters, then you can check for this issue using the following command:\n# iptables -t nat -v -L POST_public_allow -n Chain POST_public_allow (1 references) pkts bytes target prot opt in out source destination 164K 11M MASQUERADE all -- * !lo 0.0.0.0/0 0.0.0.0/0 0 0 MASQUERADE all -- * !lo 0.0.0.0/0 0.0.0.0/0 If you see output similar to the example above, for example, if you see any entries in this chain, then you need to remove them. You can remove the entries using this command:\n# iptables -t nat -v -D POST_public_allow 1 Note that you will need to run that command for each line. So in the example above, you would need to run it twice.\nAfter you are done, you can run the previous command again and verify that the output is now an empty list.\nAfter making this change, restart your domains and the Coherence cluster should now form correctly.\nMake iptables updates permanent across reboots The recommended way to make iptables updates permanent across reboots is to create a systemd service that applies the necessary updates during the startup process.\nHere is an example; you may need to adjust this to suit your own environment:\n  Create a systemd service:\necho \u0026#39;Set up systemd service to fix iptables nat chain at each reboot (so Coherence will work)...\u0026#39; mkdir -p /etc/systemd/system/ cat \u0026gt; /etc/systemd/system/fix-iptables.service \u0026lt;\u0026lt; EOF [Unit] Description=Fix iptables After=firewalld.service After=docker.service [Service] ExecStart=/sbin/fix-iptables.sh [Install] WantedBy=multi-user.target EOF   Create the script to update iptables:\ncat \u0026gt; /sbin/fix-iptables.sh \u0026lt;\u0026lt; EOF #!/bin/bash echo \u0026#39;Fixing iptables rules for Coherence issue...\u0026#39; TIMES=$((`iptables -t nat -v -L POST_public_allow -n --line-number | wc -l` - 2)) COUNTER=1 while [ $COUNTER -le $TIMES ]; do iptables -t nat -v -D POST_public_allow 1 ((COUNTER++)) done EOF   Start the service (or just reboot):\necho \u0026#39;Start the systemd service to fix iptables nat chain...\u0026#39; systemctl enable --now fix-iptables   "
},
{
	"uri": "/weblogic-kubernetes-operator/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Oracle WebLogic Server Kubernetes Operator Oracle is finding ways for organizations using WebLogic Server to run important workloads, to move those workloads into the cloud. By certifying on industry standards, such as Docker and Kubernetes, WebLogic now runs in a cloud neutral infrastructure. In addition, we\u0026rsquo;ve provided an open source Oracle WebLogic Server Kubernetes Operator (the “operator”) which has several key features to assist you with deploying and managing WebLogic domains in a Kubernetes environment. You can:\n Create WebLogic domains in a Kubernetes PersistentVolume. This PersistentVolume can reside in an NFS file system or other Kubernetes volume types. Create a WebLogic domain in a Docker image. Override certain aspects of the WebLogic domain configuration. Define WebLogic domains as a Kubernetes resource (using a Kubernetes custom resource definition). Start servers based on declarative startup parameters and desired states. Manage WebLogic configured or dynamic clusters. Expose the WebLogic Server Administration Console outside the Kubernetes cluster, if desired. Expose T3 channels outside the Kubernetes domain, if desired. Expose HTTP paths on a WebLogic domain outside the Kubernetes domain with load balancing and update the load balancer when Managed Servers in the WebLogic domain are started or stopped. Scale WebLogic domains by starting and stopping Managed Servers on demand, or by integrating with a REST API to initiate scaling based on WLDF, Prometheus, Grafana, or other rules. Publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana.  The fastest way to experience the operator is to follow the Quick Start guide, or you can peruse our documentation, read our blogs, or try out the samples.\nStep through the Tutorial using the operator to deploy and run a WebLogic domain container-packaged web application on an Oracle Cloud Infrastructure Container Engine for Kubernetes (OKE) cluster.\n  Current production release The current production release of the operator is 2.5.0. This release was published on February 26, 2020. See the operator prerequisites and supported environments here.\nPreview of next major release The current preview release of the operator is 3.0.0-rc1 (release candidate). This release candidate is suitable for use by early adopters who wish to test 3.0.0 features and provide feedback. This release candidate was published on May 8, 2020. There may be additional release candidates before the final 3.0.0 release.\nThis release candidate introduces non-backward compatible changes. This release candidate cannot be run in the same cluster as another release of the operator. You can upgrade from 2.5.0 to 3.0.0-rc1 without needing to restart or recreate any existing domains. However, please note that we do plan to support running the final 3.0.0 release in the same cluster with at least one 2.x release of the operator to allow for staged migration.\nThe feature changes in 3.0.0-rc1 are:\n Introduction of a new \u0026ldquo;Model In Image\u0026rdquo; feature which allows you to have a domain created at pod startup time from a WebLogic Deploy Tool model and archive. This supports user-requested use cases like creating multiple domains from the same model and automated updating of the domain based on model changes. The operator automates management of the domain encryption keys to ensure that they are not changed during domain updates. We provide a sample that demonstrates the key use cases for this feature. Support for running the operator on Kubernetes 1.16. Deprecation and removal of support for running the operator on Kubernetes 1.13 and earlier versions. Deprecation and removal of support for Helm 2.x. Helm 2.x uses the \u0026ldquo;tiller\u0026rdquo; pod which needs to run with elevated privileges (cluster-admin or very close to that) and which could be a vector for a privilege escalation attack. Helm 3.x removes Tiller and does not create the same exposure.   Recent changes and known issues See the Release Notes for recent changes to the operator and known issues.\nOperator earlier versions Documentation for prior releases of the operator is available here.\nBackward compatibility guidelines Starting from the 2.0.1 release, operator releases are backward compatible with respect to the domain resource schema, operator Helm chart input values, configuration overrides template, Kubernetes resources created by the operator Helm chart, Kubernetes resources created by the operator, and the operator REST interface. We intend to maintain compatibility for three releases, except in the case of a clearly communicated deprecated feature, which will be maintained for one release after a replacement is available.\nAbout this documentation This documentation includes sections targeted to different audiences. To help you find what you are looking for more easily, please consult this table of contents:\n The Quick Start guide explains how to quickly get the operator running, using the defaults, nothing special. The User guide contains detailed usage information, including how to install and configure the operator, and how to use it to create and manage WebLogic domains. The Samples provide detailed example code and instructions that show you how to perform various tasks related to the operator. The Developer guide provides details for people who want to understand how the operator is built, tested, and so on. Those who wish to contribute to the operator code will find useful information here. This section also includes API documentation (Javadoc) and Swagger/OpenAPI documentation for the REST APIs. The Contributing section provides information about contribution requirements.  User guide The User guide provides detailed information about all aspects of using the operator including:\n Installing and configuring the operator. Using the operator to create and manage WebLogic domains. Manually creating WebLogic domains to be managed by the operator. Scaling WebLogic clusters. Configuring Kubernetes load balancers. Configuring Elasticsearch and Kibana to access the operator\u0026rsquo;s log files. Shutting down domains. Removing/deleting domains. And much more!  Samples Please refer to our samples for information about the available sample code.\nDeveloper guide Developers interested in this project are encouraged to read the Developer guide to learn how to build the project, run tests, and so on. The Developer guide also provides details about the structure of the code, coding standards, and the Asynchronous Call facility used in the code to manage calls to the Kubernetes API.\nAPI documentation Documentation for APIs:\n  The operator provides a REST API that you can use to obtain configuration information and to initiate scaling actions. For details about how to use the REST APIs, see Use the operator\u0026rsquo;s REST services.\n  See the Swagger documentation for the operator\u0026rsquo;s REST interface.\n  See the Javadoc for the operator.\n  Need more help? Have a suggestion? Come and say, \u0026ldquo;Hello!\u0026rdquo; We have a public Slack channel where you can get in touch with us to ask questions about using the operator or give us feedback or suggestions about what features and improvements you would like to see. We would love to hear from you. To join our channel, please visit this site to get an invitation. The invitation email will include details of how to access our Slack workspace. After you are logged in, please come to #operator and say, \u0026ldquo;hello!\u0026rdquo;\nContributing to the operator Oracle welcomes contributions to this project from anyone. Contributions may be reporting an issue with the operator or submitting a pull request. Before embarking on significant development that may result in a large pull request, it is recommended that you create an issue and discuss the proposed changes with the existing developers first.\nIf you want to submit a pull request to fix a bug or enhance an existing feature, please first open an issue and link to that issue when you submit your pull request.\nIf you have any questions about a possible submission, feel free to open an issue too.\nContributing to the Oracle WebLogic Server Kubernetes Operator repository Pull requests can be made under The Oracle Contributor Agreement (OCA), which is available at https://www.oracle.com/technetwork/community/oca-486395.html.\nFor pull requests to be accepted, the bottom of the commit message must have the following line, using the contributor’s name and e-mail address as it appears in the OCA Signatories list.\nSigned-off-by: Your Name \u0026lt;you@example.org\u0026gt; This can be automatically added to pull requests by committing with:\ngit commit --signoff Only pull requests from committers that can be verified as having signed the OCA can be accepted.\nPull request process  Fork the repository. Create a branch in your fork to implement the changes. We recommend using the issue number as part of your branch name, for example, 1234-fixes. Ensure that any documentation is updated with the changes that are required by your fix. Ensure that any samples are updated if the base image has been changed. Submit the pull request. Do not leave the pull request blank. Explain exactly what your changes are meant to do and provide simple steps on how to validate your changes. Ensure that you reference the issue you created as well. We will assign the pull request to 2-3 people for review before it is merged.  Introducing a new dependency Please be aware that pull requests that seek to introduce a new dependency will be subject to additional review. In general, contributors should avoid dependencies with incompatible licenses, and should try to use recent versions of dependencies. Standard security vulnerability checklists will be consulted before accepting a new dependency. Dependencies on closed-source code, including WebLogic Server, will most likely be rejected.\n"
},
{
	"uri": "/weblogic-kubernetes-operator/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": "Recent changes    Date Version Introduces backward incompatibilities Change     May 8, 2020 v3.0.0-rc1 yes Adds Model in Image feature. Support for Kubernetes 1.16. Removal of support for Kubernetes 1.13 and earlier. Removal of support for Helm 2.x. This release candidate cannot be run in the same cluster as another release of the operator. You can upgrade from 2.5.0 to 3.0.0-rc1 without needing to restart or recreate any existing domains. However, please note that we do plan to support running the final 3.0.0 release in the same cluster with at least one 2.x release of the operator to allow for staged migration.   February 26, 2020 v2.5.0 no Support for Helm 3.x and OpenShift 4.3. Operator can be installed in a namespace-dedicated mode where operator requires no cluster-level Kubernetes privileges. This version is not supported on Kubernetes 1.16+, check the prerequisites.   November 15, 2019 v2.4.0 no Includes fixes for a variety of issues related to FMW infrastructure domains and pod variable substitution. Operator now uses WebLogic Deploy Tooling 1.6.0 and the latest version of the Kubernetes Java Client.   August 27, 2019 v2.3.0 no Added support for Coherence cluster rolling, pod templating and additional pod content, and experimental support for running under an Istio service mesh.   June 20, 2019 v2.2.1 no The operator now supports Kubernetes 1.14.0+. This release is primarily a bug fix release and resolves the following issues:Servers in domains, where the domain home is on a persistent volume, would sometimes fail to start. These failures would be during the introspection phase following a full domain shutdown. Now, the introspection script better handles the relevant error conditions.The domain resource provides an option to pre-create Kubernetes Services for WebLogic Servers that are not yet running so that the DNS addresses of these services are resolvable. These services are now created as non-headless so that they have an IP address.   June 6, 2019 v2.2.0 no Added support for FMW Infrastructure domains. WebLogic Server instances are now gracefully shut down by default and shutdown options are configurable. Operator is now built and runs on JDK 11.   April 4, 2019 v2.1 no Customers can add init and sidecar containers to generated pods.   March 4, 2019 v2.0.1 no OpenShift support is now certified. Many bug fixes, including fixes for configuration overrides, cluster services, and domain status processing.   January 24, 2019 v2.0 yes; not compatible with 1.x releases, but is compatible with 2.0-rc2. Final version numbers and documentation updates.   January 16, 2019 v2.0-rc2 yes Schema updates are completed, and various bugs fixed.   December 20, 2018 v2.0-rc1 yes Operator is now installed using Helm charts, replacing the earlier scripts. The operator now supports the domain home on a persistent volume or in Docker image use cases, which required a redesign of the domain schema. You can override the domain configuration using configuration override templates. Now load balancers and ingresses can be independently configured. You can direct WebLogic logs to a persistent volume or to the pod\u0026rsquo;s log. Added life cycle support for servers and significantly enhanced configurability for generated pods. The final v2.0 release will be the initial release where the operator team intends to provide backward compatibility as part of future releases.   September 11, 2018 v1.1 no Enhanced the documentation and fixed various bugs.   May 7, 2018 v1.0 no Added support for dynamic clusters, the Apache HTTP Server, the Voyager Ingress Controller, and for PV in NFS storage for multi-node environments.   April 4, 2018 0.2 yes Many Kubernetes artifact names and labels have changed. Also, the names of generated YAML files for creating a domain\u0026rsquo;s PV and PVC have changed. Because of these changes, customers must recreate their operators and domains.   March 20, 2018  yes Several files and input parameters have been renamed. This affects how operators and domains are created. It also changes generated Kubernetes artifacts, therefore customers must recreate their operators and domains.    Known issues    Issue Description     None currently     "
},
{
	"uri": "/weblogic-kubernetes-operator/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/weblogic-kubernetes-operator/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]