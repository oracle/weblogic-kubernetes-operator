[
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/elastic-stack/operator/",
	"title": "Operator",
	"tags": [],
	"description": "Sample for configuring the Elasticsearch and Kibana deployments and services for the operator&#39;s logs.",
	"content": "When you install the operator Helm chart, you can set elkIntegrationEnabled to true in your values.yaml file to direct the operator to send the contents of the operator\u0026rsquo;s logs to Elasticsearch.\nTypically, you would have already configured Elasticsearch and Kibana in the Kubernetes cluster, and also would have specified elasticSearchHost and elasticSearchPort in your values.yaml file to point to where Elasticsearch is already running.\nThis sample configures the Elasticsearch and Kibana deployments and services. It\u0026rsquo;s useful for trying out the operator in a Kubernetes cluster that doesn\u0026rsquo;t already have them configured.\nIt runs the Elastic Stack on the same host and port that the operator\u0026rsquo;s Helm chart defaults to, therefore, you only need to set elkIntegrationEnabled to true in your values.yaml file.\nTo control Elasticsearch memory parameters (Heap allocation and Enabling/Disabling swapping), open the file elasticsearch_and_kibana.yaml, search for env variables of the Elasticsearch container and change the values of the following:\n ES_JAVA_OPTS: value may contain, for example, -Xms512m -Xmx512m to lower the default memory usage (please be aware that this value is applicable for demonstration purposes only and it is not the one recommended by Elasticsearch). bootstrap.memory_lock: value may contain true (enables the usage of mlockall, to try to lock the process address space into RAM, preventing any Elasticsearch memory from being swapped out) or false (disables the usage of mlockall).  To install Elasticsearch and Kibana, use:\n$ kubectl apply -f kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml To remove them, use:\n$ kubectl delete -f kubernetes/samples/scripts/elasticsearch-and-kibana/elasticsearch_and_kibana.yaml "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/faq/newbie/",
	"title": "Answers for newcomers",
	"tags": [],
	"description": "Answers to commonly asked newcomer questions.",
	"content": "What is the WebLogic Kubernetes Operator, how can I get started with it, where is its documentation? It\u0026rsquo;s all here.\nHow much does it cost? The WebLogic Kubernetes Operator (the “operator”) is open source and free, licensed under the Universal Permissive license (UPL), Version 1.0.\nWebLogic Server is not open source. Licensing is required for each running WebLogic Server instance, just as with any deployment of WebLogic Server. Licensing is free for a single developer desktop development environment.\nHow can I get help? You are welcome to get in touch with us to ask questions, provide feedback, or give suggestions. To learn how, see Get help.\nWebLogic Server Certification Q: Which Java EE profiles are supported/certified on Kubernetes, only Web Profile or WLS Java EE full blown?\nA: We support the full Java EE Profile.\nWebLogic Server Configuration Q: How is the WebLogic Server domain configured in a container (for example, databases, JMS, and such) that is potentially shared by many domains?\nA: In a Kubernetes and container environment, the WebLogic domain home can be externalized to a persistent volume, or supplied in an image (by using a layer on top of a WebLogic Server image). For WebLogic domains that are supplied using an image, the domain logs and store locations optionally can be located on a persistent volume. See the samples in this project.\nWhen using the operator, each deployed domain is specified by a domain resource that you define which describes important aspects of the domain. These include the location of the WebLogic Server image you wish to use, a unique identifier for the domain called the domain-uid, any PVs or PVC the domain pods will need to mount, the WebLogic clusters and servers which you want to be running, and the location of its domain home.\nMultiple deployments of the same domain are supported by specifying a unique domain-uid string for each deployed domain and specifying a different domain resource. The domain-uid is in turn used by the operator as the name-prefix and/or label for the domain\u0026rsquo;s Kubernetes resources that the operator deploys for you. The WebLogic configuration of a domain\u0026rsquo;s deployments optionally can by customized by specifying configuration overrides in the domain resource \u0026ndash; which, for example, is useful for overriding the configuration of a data source URL, user name, or password.\nThe operator does not specify how a WebLogic domain home configuration is created. You can use WLST, REST, or a very convenient new tool called WebLogic Deploy Tooling (WDT). WDT allows you to compactly specify WebLogic configuration and deployments (including JMS, data sources, applications, authenticators, and such) using a YAML file and a ZIP file (which include the binaries). The operator samples show how to create domains using WLST and using WDT.\nQ: Is the Administration Server required? Node Manager?\nA: Certification of both WebLogic running in containers and WebLogic in Kubernetes consists of a WebLogic domain with the Administration Server. The operator configures and runs Node Managers for you within a domain\u0026rsquo;s pods - you don\u0026rsquo;t need to configure them yourself - so their presence is largely transparent.\n Communications Q: How is location transparency achieved and the communication between WLS instances handled?\nA: Inside the Kubernetes cluster, the operator generates a Kubernetes ClusterIP service for each WebLogic Server pod called DOMAINUID-WEBLOGICSERVERNAME and for each WebLogic cluster called DOMAINUID-cluster-CLUSTERNAME. The operator also overrides your WebLogic network listen address configuration to reflect the service names so that you don\u0026rsquo;t need to do this. The services act as DNS names, and allow the pods with WebLogic Servers to move to different nodes in the Kubernetes cluster and continue communicating with other servers.\nQ: How is communication from outside the Kubernetes cluster handled?\nA:\n  HTTP communication to your applications from locations outside the cluster: Typically, this is accomplished by deploying a load balancer that redirects traffic to your domain\u0026rsquo;s Kubernetes services (the operator automatically deploys these services for you); see Ingress. For an example, see the Quick Start, Install the operator and ingress controller.\n  JMS, EJB, and other types of RMI communication with locations outside of the Kubernetes cluster: This is typically accomplished by tunneling the RMI traffic over HTTP through a load balancer or, less commonly accomplished by using T3 or T3S directly with Kubernetes NodePorts; see External WebLogic clients.\n  Access the WebLogic Server Administration Console: This can be done through a load balancer; see the Model in Image sample. Or, this can be done through a Kubernetes NodePort service; run $ kubectl explain domain.spec.adminServer.adminService.channels.\n  Access the WebLogic Remote Console: This can be done using a load balancer or Kubernetes NodePort service; see Use the Remote Console.\n  Q: Are clusters supported on Kubernetes using both multicast and unicast?\nA: Only unicast is supported. Most Kubernetes network fabrics do not support multicast communication. Weave claims to support multicast but it is a commercial network fabric. We have certified on Flannel and Calico, which support unicast only.\n Q: For binding EJBs (presentation/business-tier), are unique and/or dynamic domain-names used?\nA: We do not enforce unique domain names. If you deploy two domains that must interoperate using RMI/EJB/JMS/JTA/and such, or that share RMI/EJB/JMS/JTA/and such clients, which concurrently communicate with both domains, then, as usual, the domain names must be configured to be different (even if they have different domain-uids).\n Load Balancers Q: Load balancing and failover inside a DataCenter (HTTPS and T3s)?\nA: We originally certified on Traefik with the Kubernetes cluster; this is a very basic load balancer. We have also certified other more sophisticated load balancers. See Ingress.\n Life Cycle and Scaling Q: How to deal with grow and shrink? Cluster and non-cluster mode.\nA: You can scale and shrink a configured WebLogic cluster (a set of preconfigured Managed Servers) or a dynamic WebLogic cluster (a cluster that uses templated Managed Servers) using different methods. See Scaling.\n Manually, using Kubernetes command-line interface, kubectl. WLDF rules and policies; when the rule is met the Administration Server sends a REST call to the operator which calls the Kubernetes API to start a new pod/container/server. We have developed and made open source the WebLogic Monitoring Exporter which exports WebLogic metrics to Prometheus and Grafana. In Prometheus, you can set rules similar to WLDF and when these rules are met, a REST call is made to the operator which invokes a Kubernetes API to start a new pod.  Q: Container life cycle: How to properly spin up and gracefully shut down WLS in a container?\nA: The operator manages container/pod/WebLogic Server life cycle automatically; it uses the Node Manager (internally) and additional approaches to do the following operations:\n Entrypoint - start WebLogic Server. Liveliness probe – check if the WebLogic Server is alive. Readiness probe – query if the WebLogic Server is ready to receive requests. Shutdown hook – gracefully shut down WebLogic Server.  These operations also can be done manually using the Kubernetes command-line interface, kubectl.\nFor more information, see the Domain life cycle documentation.\n Patching and Upgrades Q: Patching: rolling upgrades, handling of one-off-patches and overlays, CPUs, and such.\nA: For relevant information, see Patch WebLogic Server images, Rolling restarts, and CI/CD considerations.\n Diagnostics and Logging Q: Integration with ecosystems: logging, monitoring (OS, JVM and application level), and such.\nA: WebLogic Server stdout logging is echoed to their pod logs by default, and WebLogic Server file logs are optionally persisted to an external volume. We are working on a project to integrate WebLogic Server logs with the Elastic Stack. See Elastic Stack.\nWith regards to monitoring, all the tools that are traditionally used to monitor WebLogic Server can be used running in containers and Kubernetes. In addition, as mentioned previously, we have developed the WebLogic Monitoring Exporter, which exports WebLogic metrics in a format that can be read and displayed in dashboards like Prometheus and Grafana.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/cicd/layering/",
	"title": "Container image layering",
	"tags": [],
	"description": "Learn about container image layering and why it is important.",
	"content": "Container images are composed of layers, as shown in the diagram below. If you download the standard weblogic:12.2.1.4 image from the Oracle Container Registry, then you can see these layers using the command docker inspect container-registry.oracle.com/middleware/weblogic:12.2.1.4 (the domain layer will not be there). You are not required to use layers, but efficient use of layers is considered a best practice.\nWhy is it important to maintain the layering of images? Layering is an important technique in container images. Layers are important because they are shared between images. Let\u0026rsquo;s consider an example. In the diagram below, we have two domains that we have built using layers. The second domain has some additional patches that we needed on top of those provided in the standard WebLogic image. Those are installed in their own layer, and then the second domain is created in another layer on top of that.\nLet\u0026rsquo;s assume we have a three-node Kubernetes cluster and we are running both domains in this cluster. Sooner or later, we will end up with servers in each domain running on each node, so eventually all of the image layers are going to be needed on all of the nodes. Using the approach shown below (that is, standard image layering techniques) we are going to need to store all six of these layers on each node. If you add up the sizes, then you will see that it comes out to about 1.5GB per node.\nNow, let\u0026rsquo;s consider the alternative, where we do not use layers, but instead, build images for each domain and put everything in one big layer (this is often called \u0026ldquo;squashing\u0026rdquo; the layers). In this case, we have the same content, but if you add up the size of the images, you get 2.9GB per node. That’s almost twice the size!\nWith only two domains, you start to see the problem. In the layered approach, each new domain is adding only a relatively very small increment. In the non-layered approach, each new domain is essentially adding the entire stack over again. Imagine if we had ten domains, now the calculation looks like this:\n    With Layers Without Layers     Shared Layers 1.4GB 0GB   Dedicated/different layers 10 x 10MB = 100MB 10 x 1.5GB = 15GB   Total per node 1.5GB 15GB    You can see how the amount of storage for images really starts to add up, and it is not just a question of storage. When Kubernetes creates a container from an image, the size of the image has an impact on how long it takes to create and start the container.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/security/domain-security/image-protection/",
	"title": "Container image protection",
	"tags": [],
	"description": "WebLogic domain in image protection",
	"content": "WebLogic domain in image protection Oracle strongly recommends storing the container images that contain a WebLogic domain home as private in the container registry. In addition to any local registry, public container registries include GItHub Container Registry and the Oracle Cloud Infrastructure Registry (OCIR).\n The WebLogic domain home that is part of an image contains sensitive information about the domain including keys and credentials that are used to access external resources (for example, the data source password). In addition, the image may be used to create a running server that further exposes the WebLogic domain outside of the Kubernetes cluster.\nThere are two main options to pull images from a private registry:\n Specify the image pull secret on the WebLogic Domain resource. Set up the ServiceAccount in the domain namespace with an image pull secret.  1. Use imagePullSecrets with the Domain resource. In order to access an image that is protected by a private registry, the imagePullSecrets should be specified in the Kubernetes Domain resource definition:\napiVersion: \u0026#34;weblogic.oracle/v2\u0026#34; kind: Domain metadata: name: domain1 namespace: domain1-ns labels: weblogic.domainUID: domain1 spec: domainHomeSourceType: Image image: \u0026#34;my-domain-home-in-image\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; imagePullSecrets: - name: \u0026#34;my-registry-pull-secret\u0026#34; webLogicCredentialsSecret: name: \u0026#34;domain1-weblogic-credentials\u0026#34; To create the Kubernetes Secret, my-registry-pull-secret, in the namespace where the domain will be running, domain1-ns, the following command can be used:\n$ kubectl create secret docker-registry my-registry-pull-secret \\  -n domain1-ns \\  --docker-server=\u0026lt;registry-server\u0026gt; \\  --docker-username=\u0026lt;name\u0026gt; \\  --docker-password=\u0026lt;password\u0026gt; \\  --docker-email=\u0026lt;email\u0026gt; For more information about creating Kubernetes Secrets for accessing the registry, see the Kubernetes documentation about pulling an image from a private registry.\n2. Set up the Kubernetes ServiceAccount with imagePullSecrets. An additional option for accessing an image protected by a private registry is to set up the Kubernetes ServiceAccount in the namespace running the WebLogic domain with a set of image pull secrets thus avoiding the need to set imagePullSecrets for each Domain resource being created (because each resource instance represents a WebLogic domain that the operator is managing).\nThe Kubernetes Secret would be created in the same manner as shown above and then the ServiceAccount would be updated to include this image pull secret:\n$ kubectl patch serviceaccount default -n domain1-ns \\  -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;my-registry-pull-secret\u0026#34;}]}\u0026#39; For more information about updating a Kubernetes ServiceAccount for accessing the registry, see the Kubernetes documentation about configuring service accounts.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/security/certificates/",
	"title": "Certificates",
	"tags": [],
	"description": "Operator SSL/TLS certificate handling",
	"content": "Updating operator external certificates If the operator needs to update the external certificate and key currently being used or was installed without an external REST API SSL/TLS identity, the helm upgrade command is used to restart the operator with the new or updated Kubernetes tls secret that contains the desired certificates.\nThe operator requires a restart in order to begin using the new or updated external certificate. The Helm --recreate-pods flag is used to cause the existing Kubernetes Pod to be terminated and a new pod to be started with the updated configuration.\nFor example, if the operator was installed with the Helm release name weblogic-operator in the namespace weblogic-operator-ns and the Kubernetes tls secret is named weblogic-operator-cert, the following commands can be used to update the operator certificates and key:\n$ kubectl create secret tls weblogic-operator-cert -n weblogic-operator-ns \\  --cert=\u0026lt;path-to-certificate\u0026gt; --key=\u0026lt;path-to-private-key\u0026gt; $ helm get values weblogic-operator -n weblogic-operator-ns $ helm -n weblogic-operator-ns upgrade weblogic-operator kubernetes/charts/weblogic-operator \\  --wait --recreate-pods --reuse-values \\  --set externalRestEnabled=true \\  --set externalRestIdentitySecret=weblogic-operator-cert Additional reading  Configure the external REST interface SSL/TLS identity REST interface configuration settings Sample to create external certificate and key  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/credentials/",
	"title": "Credentials",
	"tags": [],
	"description": "Sample for creating a Kubernetes Secret that contains the Administration Server credentials. This Secret can be used in creating a WebLogic Domain YAML file.",
	"content": "Creating credentials for a WebLogic domain This sample demonstrates how to create a Kubernetes Secret containing the credentials for a WebLogic domain. The operator expects this secret to be named following the pattern domainUID-weblogic-credentials, where domainUID is the unique identifier of the domain. It must be in the same namespace that the domain will run in.\nTo use the sample, run the command:\n$ ./create-weblogic-credentials.sh -u \u0026lt;username\u0026gt; -p \u0026lt;password\u0026gt; -d domainUID -n namespace -s secretName The parameters are as follows:\n -u user name, must be specified. -p password, must be specified. -d domainUID, optional. The default value is domain1. If specified, the secret will be labeled with the domainUID unless the given value is an empty string. -n namespace, optional. Use the default namespace if not specified. -s secretName, optional. If not specified, the secret name will be determined based on the domainUID value. This creates a generic secret containing the user name and password as literal values.\nYou can check the secret with the kubectl get secret command. An example is shown below, including the output:\n$ kubectl -n domain-namespace-1 get secret domain1-weblogic-credentials -o yaml apiVersion: v1 data: password: d2VsY29tZTE= username: d2VibG9naWM= kind: Secret metadata: creationTimestamp: 2018-12-12T20:25:20Z labels: weblogic.domainName: domain1 weblogic.domainUID: domain1 name: domain1-weblogic-credentials namespace: domain-namespace-1 resourceVersion: \u0026quot;5680\u0026quot; selfLink: /api/v1/namespaces/domain-namespace-1/secrets/domain1-weblogic-credentials uid: 0c2b3510-fe4c-11e8-994d-00001700101d type: Opaque "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/domains/manually-create-domain/",
	"title": "Manually",
	"tags": [],
	"description": "Sample for creating the domain custom resource manually.",
	"content": "In some circumstances you may wish to manually create your domain custom resource. If you have created your own Container image containing your domain and the specific patches that it requires, then this approach will probably be the most suitable for your needs.\nTo create the domain custom resource, just make a copy of the sample domain.yaml file, and then edit it using the instructions provided in the comments in that file. When it is ready, you can create the domain in your Kubernetes cluster using the command:\n$ kubectl apply -f domain.yaml You can verify the domain custom resource was created using this command:\n$ kubectl -n YOUR_NAMESPACE get domains You can view the details of the domain using this command:\n$ kubectl -n YOUR_NAMESPACE describe domain YOUR_DOMAIN In both of these commands, replace YOUR_NAMESPACE with the namespace in which you created the domain, and replace YOUR_DOMAIN with the domainUID you chose.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/domains/model-in-image/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Prerequisites for all domain types   Choose the type of domain you\u0026rsquo;re going to use throughout the sample, WLS or JRF.\n The first time you try this sample, we recommend that you choose WLS even if you\u0026rsquo;re familiar with JRF. This is because WLS is simpler and will more easily familiarize you with Model in Image concepts. We recommend choosing JRF only if you are already familiar with JRF, you have already tried the WLS path through this sample, and you have a definite use case where you need to use JRF.    The JAVA_HOME environment variable must be set and must reference a valid JDK 8 or 11 installation.\n  Get the operator source and put it in /tmp/weblogic-kubernetes-operator.\nFor example:\n$ cd /tmp $ git clone --branch v3.3.2 https://github.com/oracle/weblogic-kubernetes-operator.git  Note: We will refer to the top directory of the operator source tree as /tmp/weblogic-kubernetes-operator; however, you can use a different location.\n For additional information about obtaining the operator source, see the Developer Guide Requirements.\n  Copy the sample to a new directory; for example, use directory /tmp/mii-sample.\n$ mkdir /tmp/mii-sample $ cp -r /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain/model-in-image/* /tmp/mii-sample  Note: We will refer to this working copy of the sample as /tmp/mii-sample; however, you can use a different location.     Make sure an operator is set up to manage namespace sample-domain1-ns. Also, make sure a Traefik ingress controller is managing the same namespace and listening on port 30305.\nFor example, follow the same steps as the Quick Start guide up through the Prepare for a domain step.\nMake sure you stop when you complete the \u0026ldquo;Prepare for a domain\u0026rdquo; step and then resume following these instructions.\n   Set up ingresses that will redirect HTTP from Traefik port 30305 to the clusters in this sample\u0026rsquo;s WebLogic domains.\n  Option 1: To create the ingresses, use the following YAML file to create a file called /tmp/mii-sample/ingresses/myingresses.yaml and then call kubectl apply -f /tmp/mii-sample/ingresses/myingresses.yaml:\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-ingress-sample-domain1-admin-server namespace: sample-domain1-ns labels: weblogic.domainUID: sample-domain1 annotations: kubernetes.io/ingress.class: traefik spec: routes: - kind: Rule match: PathPrefix(`/console`) services: - kind: Service name: sample-domain1-admin-server port: 7001 --- apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-ingress-sample-domain1-cluster-cluster-1 namespace: sample-domain1-ns labels: weblogic.domainUID: sample-domain1 annotations: kubernetes.io/ingress.class: traefik spec: routes: - kind: Rule match: Host(`sample-domain1-cluster-cluster-1.mii-sample.org`) services: - kind: Service name: sample-domain1-cluster-cluster-1 port: 8001 --- apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: traefik-ingress-sample-domain2-cluster-cluster-1 namespace: sample-domain1-ns labels: weblogic.domainUID: sample-domain2 annotations: kubernetes.io/ingress.class: traefik spec: routes: - kind: Rule match: Host(`sample-domain2-cluster-cluster-1.mii-sample.org`) services: - kind: Service name: sample-domain2-cluster-cluster-1 port: 8001   Option 2: Run kubectl apply -f on each of the ingress YAML files that are already included in the sample source /tmp/mii-sample/ingresses directory:\n$ cd /tmp/mii-sample/ingresses $ kubectl apply -f traefik-ingress-sample-domain1-admin-server.yaml $ kubectl apply -f traefik-ingress-sample-domain1-cluster-cluster-1.yaml $ kubectl apply -f traefik-ingress-sample-domain2-cluster-cluster-1.yaml    NOTE: We give each cluster ingress a different host name that is decorated using both its operator domain UID and its cluster name. This makes each cluster uniquely addressable even when cluster names are the same across different clusters. When using curl to access the WebLogic domain through the ingress, you will need to supply a host name header that matches the host names in the ingress.\n For more information on ingresses and load balancers, see Ingress.\n  Obtain the WebLogic 12.2.1.4 image that is required to create the sample\u0026rsquo;s model images.\na. Use a browser to access Oracle Container Registry.\nb. Choose an image location: for JRF domains, select Middleware, then fmw-infrastructure; for WLS domains, select Middleware, then weblogic.\nc. Select Sign In and accept the license agreement.\nd. Use your terminal to log in to the container registry: docker login container-registry.oracle.com.\ne. Later in this sample, when you run WebLogic Image Tool commands, the tool will use the image as a base image for creating model images. Specifically, the tool will implicitly call docker pull for one of the above licensed images as specified in the tool\u0026rsquo;s command line using the --fromImage parameter. For JRF, this sample specifies container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4, and for WLS, the sample specifies container-registry.oracle.com/middleware/weblogic:12.2.1.4.\nIf you prefer, you can create your own base image and then substitute this image name in the WebLogic Image Tool --fromImage parameter throughout this sample. For example, you may wish to start with a base image that has patches applied. See Preparing a Base Image.\n   Download the latest WebLogic Deploy Tooling (WDT) and WebLogic Image Tool (WIT) installer ZIP files to your /tmp/mii-sample/model-images directory. Both WDT and WIT are required to create your Model in Image container images.\n$ cd /tmp/mii-sample/model-images $ curl -m 120 -fL https://github.com/oracle/weblogic-deploy-tooling/releases/latest/download/weblogic-deploy.zip \\  -o /tmp/mii-sample/model-images/weblogic-deploy.zip $ curl -m 120 -fL https://github.com/oracle/weblogic-image-tool/releases/latest/download/imagetool.zip \\  -o /tmp/mii-sample/model-images/imagetool.zip   To set up the WebLogic Image Tool, run the following commands:\n$ cd /tmp/mii-sample/model-images $ unzip imagetool.zip $ ./imagetool/bin/imagetool.sh cache deleteEntry --key wdt_latest $ ./imagetool/bin/imagetool.sh cache addInstaller \\  --type wdt \\  --version latest \\  --path /tmp/mii-sample/model-images/weblogic-deploy.zip Note that the WebLogic Image Tool cache deleteEntry command does nothing if the wdt_latest key doesn\u0026rsquo;t have a corresponding cache entry. It is included because the WIT cache lookup information is stored in the $HOME/cache/.metadata file by default, and if the cache already has a version of WDT in its --type wdt --version latest location, then the cache addInstaller command would fail. For more information about the WIT cache, see the WIT Cache documentation.\nThese steps will install WIT to the /tmp/mii-sample/model-images/imagetool directory, plus put a wdt_latest entry in the tool\u0026rsquo;s cache which points to the WDT ZIP file installer. You will use WIT and its cached reference to the WDT installer later in the sample for creating model images.\n  Additional prerequisites for JRF domains  NOTE: If you\u0026rsquo;re using a WLS domain type, skip this section and continue here.\n JRF Prerequisites Contents  Introduction to JRF setups Set up and initialize an infrastructure database Increase introspection job timeout Important considerations for RCU model attributes, Domain fields, and secrets  Introduction to JRF setups  NOTE: The requirements in this section are in addition to Prerequisites for all domain types.\n A JRF domain requires an infrastructure database, initializing this database with RCU, and configuring your domain to access this database. You must perform all these steps before you create your domain.\nSet up and initialize an infrastructure database A JRF domain requires an infrastructure database and requires initializing this database with a schema and a set of tables for each different domain. The following example shows how to set up a database and use the RCU tool to create the infrastructure schemas for two JRF domains. The database is set up with the following attributes:\n   Attribute Value     database Kubernetes namespace default   database Kubernetes pod oracle-db   database image container-registry.oracle.com/database/enterprise:12.2.0.1-slim   database password Oradoc_db1   infrastructure schema prefixes FMW1 and FMW2 (for domain1 and domain2)   infrastructure schema password Oradoc_db1   database URL oracle-db.default.svc.cluster.local:1521/devpdb.k8s      Ensure that you have access to the database image, and then create a deployment using it:\n  Use a browser to log in to https://container-registry.oracle.com, select Database -\u0026gt; enterprise and accept the license agreement.\n  Get the database image:\n In the local shell, docker login container-registry.oracle.com. In the local shell, docker pull container-registry.oracle.com/database/enterprise:12.2.0.1-slim.    Use the sample script in /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oracle-db-service to create an Oracle database running in the pod, oracle-db.\n$ cd /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oracle-db-service $ start-db-service.sh This script will deploy a database in the default namespace with the connect string oracle-db.default.svc.cluster.local:1521/devpdb.k8s, and administration password Oradoc_db1.\nThis step is based on the steps documented in Run a Database.\nNOTE: If your Kubernetes cluster nodes do not all have access to the database image in a local cache, then deploy a Kubernetes docker secret to the default namespace with login credentials for container-registry.oracle.com, and pass the name of this secret as a parameter to start-db-service.sh using -s your-image-pull-secret. Alternatively, copy the database image to each local Docker cache in the cluster. For more information, see the Cannot pull image FAQ.\nWARNING: The Oracle Database images are supported only for non-production use. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1).\n    Use the sample script in /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-schema to create an RCU schema for each domain (schema prefixes FMW1 and FMW2).\nNote that this script assumes Oradoc_db1 is the DBA password, Oradoc_db1 is the schema password, and that the database URL is oracle-db.default.svc.cluster.local:1521/devpdb.k8s.\n$ cd /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-rcu-schema $ ./create-rcu-schema.sh -s FMW1 -i container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 $ ./create-rcu-schema.sh -s FMW2 -i container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 NOTE: If your Kubernetes cluster nodes do not all have access to the FMW infrastructure image in a local cache, then deploy a Kubernetes docker secret to the default namespace with login credentials for container-registry.oracle.com, and pass the name of this secret as a parameter to ./create-rcu-schema.sh using -p your-image-pull-secret. Alternatively, copy the FMW infrastructure image to each local Docker cache in the cluster. For more information, see the Cannot pull image FAQ.\nNOTE: If you need to drop the repository, use this command:\n$ drop-rcu-schema.sh -s FMW1   Increase introspection job timeout The JRF domain home creation can take more time than the introspection job\u0026rsquo;s default timeout. You should increase the timeout for the introspection job. Use the configuration.introspectorJobActiveDeadlineSeconds in your Domain to override the default with a value of at least 300 seconds (the default is 120 seconds). Note that the JRF versions of the Domain YAML files that are provided in /tmp/mii-sample/domain-resources already set this value.\nImportant considerations for RCU model attributes, Domain fields, and secrets To allow Model in Image to access the database and OPSS wallet, you must create an RCU access secret containing the database connect string, user name, and password that\u0026rsquo;s referenced from your model and an OPSS wallet password secret that\u0026rsquo;s referenced from your Domain before deploying your domain. It\u0026rsquo;s also necessary to define an RCUDbInfo stanza in your model.\nThe sample includes examples of JRF models and Domain YAML files in the /tmp/mii-sample/model-images and /tmp/mii-sample/domain-resources directories, and instructions in the following sections will describe setting up the RCU and OPSS secrets.\nWhen you follow the instructions in the samples, avoid instructions that are WLS only, and substitute JRF for WLS in the corresponding model image tags and Domain YAML file names.\nFor example, in this sample:\n  JRF Domain YAML files have an configuration.opss.walletPasswordSecret field that references a secret named sample-domain1-opss-wallet-password-secret, with password=welcome1.\n  JRF image models have a domainInfo -\u0026gt; RCUDbInfo stanza that reference a sample-domain1-rcu-access secret with appropriate values for attributes rcu_prefix, rcu_schema_password, and rcu_db_conn_string for accessing the Oracle database that you deployed to the default namespace as one of the prerequisite steps.\n  Important considerations for reusing or sharing OPSS tables We do not recommend that users share OPSS tables. Extreme caution is required when sharing OPSS tables between domains.\n When you successfully deploy your JRF Domain YAML file for the first time, the introspector job will initialize the OPSS tables for the domain using the domainInfo -\u0026gt; RCUDbInfo stanza in the WDT model plus the configuration.opss.walletPasswordSecret specified in the Domain YAML file. The job will also create a new domain home. Finally, the operator will also capture an OPSS wallet file from the new domain\u0026rsquo;s local directory and place this file in a new Kubernetes ConfigMap.\nThere are scenarios when the domain needs to be recreated between updates, such as when WebLogic credentials are changed, security roles defined in the WDT model have been changed, or you want to share the same infrastructure tables with different domains. In these scenarios, the operator needs the walletPasswordSecret as well as the OPSS wallet file, together with the exact information in domainInfo -\u0026gt; RCUDbInfo so that the domain can be recreated and access the same set of tables. Without the wallet file and wallet password, you will not be able to recreate a domain accessing the same set of tables, therefore we strongly recommend that you back up the wallet file.\nTo recover a domain\u0026rsquo;s OPSS tables between domain restarts or to share an OPSS schema between different domains, it is necessary to extract this wallet file from the domain\u0026rsquo;s automatically deployed introspector ConfigMap and save the OPSS wallet password secret that was used for the original domain. The wallet password and wallet file are needed again when you recreate the domain or share the database with other domains.\nTo save the wallet file, assuming that your namespace is sample-domain1-ns and your domain UID is sample-domain1:\n$ kubectl -n sample-domain1-ns \\  get configmap sample-domain1-weblogic-domain-introspect-cm \\  -o jsonpath=\u0026#39;{.data.ewallet\\.p12}\u0026#39; \\  \u0026gt; ./ewallet.p12 Alternatively, you can save the file using the sample\u0026rsquo;s wallet utility:\n$ /tmp/mii-sample/utils/opss-wallet.sh -n sample-domain1-ns -d sample-domain1 -wf ./ewallet.p12  # For help: /tmp/mii-sample/utils/opss-wallet.sh -? Important! Back up your wallet file to a safe location that can be retrieved later.\nTo reuse the wallet file in subsequent redeployments or to share the domain\u0026rsquo;s OPSS tables between different domains:\n Load the saved wallet file into a secret with a key named walletFile (again, assuming that your domain UID is sample-domain1 and your namespace is sample-domain1-ns):  $ kubectl -n sample-domain1-ns create secret generic sample-domain1-opss-walletfile-secret \\  --from-file=walletFile=./ewallet.p12 $ kubectl -n sample-domain1-ns label secret sample-domain1-opss-walletfile-secret \\  weblogic.domainUID=`sample-domain1` Alternatively, use the sample\u0026rsquo;s wallet utility:\n$ /tmp/mii-sample/utils/opss-wallet.sh -n sample-domain1-ns -d sample-domain1 -wf ./ewallet.p12 -ws sample-domain1-opss-walletfile-secret  # For help: /tmp/mii-sample/utils/opss-wallet.sh -? Modify your Domain JRF YAML files to provide the wallet file secret name, for example:  configuration: opss: # Name of secret with walletPassword for extracting the wallet walletPasswordSecret: sample-domain1-opss-wallet-password-secret # Name of secret with walletFile containing base64 encoded opss wallet walletFileSecret: sample-domain1-opss-walletfile-secret  Note: The sample JRF Domain YAML files included in /tmp/mii-sample/domain-resources already have the above YAML file stanza.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/storage/",
	"title": "Storage",
	"tags": [],
	"description": "Sample for creating a PV or PVC that can be used by a Domain YAML file as the persistent storage for the WebLogic domain home or log files.",
	"content": "Sample PersistentVolume and PersistentVolumeClaim The sample scripts demonstrate the creation of a Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC), which can then be used in a Domain YAML file as a persistent storage for the WebLogic domain home or log files.\nA PV and PVC can be shared by multiple WebLogic domains or dedicated to a particular domain.\nPrerequisites Before you begin, read this document, Persistent storage.\nUsing the scripts to create a PV and PVC Prior to running the create-pv-pvc.sh script, make a copy of the create-pv-pvc-inputs.yaml file, and uncomment and explicitly configure the weblogicDomainStoragePath property in the inputs file.\nRun the create script, pointing it at your inputs file and an output directory:\n$ ./create-pv-pvc.sh \\  -i create-pv-pvc-inputs.yaml \\  -o /path/to/output-directory The create-pv-pvc.sh script will create a subdirectory pv-pvcs under the given /path/to/output-directory directory. By default, the script generates two YAML files, namely weblogic-sample-pv.yaml and weblogic-sample-pvc.yaml, in the /path/to/output-directory/pv-pvcs. These two YAML files can be used to create the Kubernetes resources using the kubectl create -f command.\n$ kubectl create -f weblogic-sample-pv.yaml $ kubectl create -f weblogic-sample-pvc.yaml As a convenience, the script can optionally create the PV and PVC resources using the -e option.\nThe usage of the create script is as follows:\n$ sh create-pv-pvc.sh -h usage: create-pv-pvc.sh -i file -o dir [-e] [-h] -i Parameter inputs file, must be specified. -o Output directory for the generated yaml files, must be specified. -e Also create the Kubernetes objects using the generated yaml files -h Help If you copy the sample scripts to a different location, make sure that you copy everything in the \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts directory together into the target directory, maintaining the original directory hierarchy.\nConfiguration parameters The PV and PVC creation inputs can be customized by editing the create-pv-pvc-inputs.yaml file.\n   Parameter Definition Default     domainUID ID of the Domain to which the generated PV and PVC will be dedicated. Leave it empty if the PV and PVC are going to be shared by multiple domains. no default   namespace Kubernetes Namespace to create the PVC. default   baseName Base name of the PV and PVC. The generated PV and PVC will be \u0026lt;baseName\u0026gt;-pv and \u0026lt;baseName\u0026gt;-pvc respectively. weblogic-sample   weblogicDomainStoragePath Physical path of the storage for the PV. When weblogicDomainStorageType is set to HOST_PATH, this value should be set the to path to the domain storage on the Kubernetes host. When weblogicDomainStorageType is set to NFS, then weblogicDomainStorageNFSServer should be set to the IP address or name of the DNS server, and this value should be set to the exported path on that server. Note that the path where the domain is mounted in the WebLogic containers is not affected by this setting; that is determined when you create your domain. no default   weblogicDomainStorageReclaimPolicy Kubernetes PVC policy for the persistent storage. The valid values are: Retain, Delete, and Recycle. Retain   weblogicDomainStorageSize Total storage allocated for the PVC. 10Gi   weblogicDomainStorageType Type of storage. Legal values are NFS and HOST_PATH. If using NFS, weblogicDomainStorageNFSServer must be specified. HOST_PATH   weblogicDomainStorageNFSServer Name or IP address of the NFS server. This setting only applies if weblogicDomainStorateType is NFS. no default    Shared versus dedicated PVC By default, the domainUID is left empty in the inputs file, which means the generated PV and PVC will not be associated with a particular domain, but can be shared by multiple Domains in the same Kubernetes Namespaces as the PV and PVC. If the PV/PVC is being shared across domains, then, as a best practice, you should specify a unique baseName.\nFor the use cases where dedicated PV and PVC are desired for a particular domain, the domainUID needs to be set in the create-pv-pvc-inputs.yaml file. The presence of a non-empty domainUID in the inputs file will cause the generated PV and PVC to be associated with the specified domainUID. The association includes that the names of the generated YAML files and the Kubernetes PV and PVC objects are decorated with the domainUID, and the PV and PVC objects are also labeled with the domainUID.\nVerify the results The create script will verify that the PV and PVC were created, and will report a failure if there was any error. However, it may be desirable to manually verify the PV and PVC, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nGenerated YAML files with the default inputs The content of the generated weblogic-sample-pvc.yaml:\n# Copyright 2018, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: weblogic-sample-pvc namespace: default storageClassName: weblogic-sample-storage-class accessModes: - ReadWriteMany resources: requests: storage: 10Gi The content of the generated weblogic-sample-pv.yaml:\n# Copyright 2018, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. apiVersion: v1 kind: PersistentVolume metadata: name: weblogic-sample-pv # labels: # weblogic.domainUID: spec: storageClassName: weblogic-sample-storage-class capacity: storage: 10Gi accessModes: - ReadWriteMany # Valid values are Retain, Delete or Recycle persistentVolumeReclaimPolicy: Retain hostPath: # nfs: # server: %SAMPLE_STORAGE_NFS_SERVER% path: \u0026#34;/scratch/k8s_dir\u0026#34; Generated YAML files for dedicated PV and PVC The content of the generated domain1-weblogic-sample-pvc.yaml when domainUID is set to domain1:\n# Copyright 2018, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. kind: PersistentVolumeClaim apiVersion: v1 metadata: name: domain1-weblogic-sample-pvc namespace: default labels: weblogic.domainUID: domain1 spec: storageClassName: domain1-weblogic-sample-storage-class accessModes: - ReadWriteMany resources: requests: storage: 10Gi The content of the generated domain1-weblogic-sample-pv.yaml when domainUID is set to domain1:\n# Copyright 2018, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. apiVersion: v1 kind: PersistentVolume metadata: name: domain1-weblogic-sample-pv labels: weblogic.domainUID: domain1 spec: storageClassName: domain1-weblogic-sample-storage-class capacity: storage: 10Gi accessModes: - ReadWriteMany # Valid values are Retain, Delete or Recycle persistentVolumeReclaimPolicy: Retain hostPath: # nfs: # server: %SAMPLE_STORAGE_NFS_SERVER% path: \u0026#34;/scratch/k8s_dir\u0026#34; Verify the PV and PVC objects You can use this command to verify the PersistentVolume was created. Note that the Status field should have the value Bound, indicating the that PersistentVolume has been claimed:\n$ kubectl describe pv weblogic-sample-pv Name: weblogic-sample-pv Annotations: pv.kubernetes.io/bound-by-controller=yes StorageClass: weblogic-sample-storage-class Status: Bound Claim: default/weblogic-sample-pvc Reclaim Policy: Retain Access Modes: RWX Capacity: 10Gi Message: Source: Type: HostPath (bare host directory volume) Path: /scratch/k8s_dir HostPathType: Events: \u0026lt;none\u0026gt; You can use this command to verify the PersistentVolumeClaim was created:\n$ kubectl describe pvc weblogic-sample-pvc Name: weblogic-sample-pvc Namespace: default StorageClass: weblogic-sample-storage-class Status: Bound Volume: weblogic-sample-pv Annotations: pv.kubernetes.io/bind-completed=yes pv.kubernetes.io/bound-by-controller=yes Finalizers: [] Capacity: 10Gi Access Modes: RWX Events: \u0026lt;none\u0026gt; "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/developerguide/contributing/",
	"title": "Contribute to the operator",
	"tags": [],
	"description": "",
	"content": "Oracle welcomes contributions to this project from anyone. Contributions may be reporting an issue with the operator or submitting a pull request. Before embarking on significant development that may result in a large pull request, it is recommended that you create an issue and discuss the proposed changes with the existing developers first.\nIf you want to submit a pull request to fix a bug or enhance an existing feature, please first open an issue and link to that issue when you submit your pull request.\nIf you have any questions about a possible submission, feel free to open an issue too.\nContributing to the WebLogic Kubernetes Operator repository Pull requests can be made under The Oracle Contributor Agreement (OCA), which is available at https://www.oracle.com/technetwork/community/oca-486395.html.\nFor pull requests to be accepted, the bottom of the commit message must have the following line, using the contributor’s name and e-mail address as it appears in the OCA Signatories list.\nSigned-off-by: Your Name \u0026lt;you@example.org\u0026gt; This can be automatically added to pull requests by committing with:\n$ git commit --signoff Only pull requests from committers that can be verified as having signed the OCA can be accepted.\nPull request process  Fork the repository. Create a branch in your fork to implement the changes. We recommend using the issue number as part of your branch name, for example, 1234-fixes. Ensure that any documentation is updated with the changes that are required by your fix. Ensure that any samples are updated if the base image has been changed. Submit the pull request. Do not leave the pull request blank. Explain exactly what your changes are meant to do and provide simple steps on how to validate your changes. Ensure that you reference the issue you created as well. We will assign the pull request to 2-3 people for review before it is merged.  Introducing a new dependency Please be aware that pull requests that seek to introduce a new dependency will be subject to additional review. In general, contributors should avoid dependencies with incompatible licenses, and should try to use recent versions of dependencies. Standard security vulnerability checklists will be consulted before accepting a new dependency. Dependencies on closed-source code, including WebLogic Server, will most likely be rejected.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/developerguide/requirements/",
	"title": "Requirements",
	"tags": [],
	"description": "",
	"content": "In addition to the requirements listed here, the following software is also required to obtain and build the operator:\n Git (1.8 or later recommended) Java Developer Kit (11 required, 11.0.2 recommended) Apache Maven (3.5.3 min, 3.6 recommended)  The operator is written primarily in Java, BASH shell scripts, and WLST scripts.\nBecause the target runtime environment for the operator is Oracle Linux, no particular effort has been made to ensure the build or tests run on any other operating system. Please be aware that Oracle will not provide support, or accept pull requests to add support for other operating systems.\nObtaining the operator source code The operator source code is published on GitHub at https://github.com/oracle/weblogic-kubernetes-operator. Developers may clone this repository to a local machine or, if desired, create a fork in their personal namespace and clone the fork. Developers who are planning to submit a pull request are advised to create a fork.\nTo clone the repository from GitHub, issue this command:\n$ git clone https://github.com/oracle/weblogic-kubernetes-operator.git "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/domain-lifecycle/startup/",
	"title": "Startup and shutdown",
	"tags": [],
	"description": "There are fields on the Domain that specify which WebLogic Server instances should be running, started, or restarted. To start, stop, or restart servers, modify these fields on the Domain.",
	"content": "Contents  Introduction Starting and stopping servers  serverStartPolicy rules Available serverStartPolicy values Administration Server start and stop rules Standalone Managed Server start and stop rules Clustered Managed Server start and stop rules   Server start state Common starting and stopping scenarios  Normal running state Shut down all the servers Only start the Administration Server Shut down a cluster Shut down a specific standalone server Force a specific clustered Managed Server to start   Shutdown options  Shutdown environment variables shutdown rules   Restarting servers  Fields that cause servers to be restarted   Rolling restarts Draining a node and PodDisruptionBudget Common restarting scenarios  Using restartVersion to force the operator to restart servers Restart all the servers in the domain Restart all the servers in the cluster Restart the Administration Server Restart a standalone or clustered Managed Server Full domain restarts   Domain lifecycle sample scripts  Introduction There are fields on the Domain that specify which servers should be running, which servers should be restarted, and the desired initial state. To start, stop, or restart servers, modify these fields on the Domain (for example, by using kubectl or the Kubernetes REST API). The operator will detect the changes and apply them. Beginning with operator version 2.2.0, there are now fields to control server shutdown handling, such as whether the shutdown will be graceful, the timeout, and if in-flight sessions are given the opportunity to complete.\nStarting and stopping servers The serverStartPolicy and replicas fields of the Domain controls which servers should be running. The operator monitors these fields and creates or deletes the corresponding WebLogic Server instance Pods.\nDo not use the WebLogic Server Administration Console to start or stop servers.\n serverStartPolicy rules You can specify the serverStartPolicy property at the domain, cluster, and server levels. Each level supports a different set of values.\nAvailable serverStartPolicy values    Level Default Value Supported Values     Domain IF_NEEDED IF_NEEDED, ADMIN_ONLY, NEVER   Cluster IF_NEEDED IF_NEEDED, NEVER   Server IF_NEEDED IF_NEEDED, ALWAYS, NEVER    Administration Server start and stop rules    Domain Admin Server Started / Stopped     NEVER any value Stopped   ADMIN_ONLY, IF_NEEDED NEVER Stopped   ADMIN_ONLY, IF_NEEDED IF_NEEDED, ALWAYS Started    Standalone Managed Server start and stop rules    Domain Standalone Server Started / Stopped     ADMIN_ONLY, NEVER any value Stopped   IF_NEEDED NEVER Stopped   IF_NEEDED IF_NEEDED, ALWAYS Started    Clustered Managed Server start and stop rules    Domain Cluster Clustered Server Started / Stopped     ADMIN_ONLY, NEVER any value any value Stopped   IF_NEEDED NEVER any value Stopped   IF_NEEDED IF_NEEDED NEVER Stopped   IF_NEEDED IF_NEEDED ALWAYS Started   IF_NEEDED IF_NEEDED IF_NEEDED Started if needed to get to the cluster\u0026rsquo;s replicas count    Servers configured as ALWAYS count toward the cluster\u0026rsquo;s replicas count.\n If more servers are configured as ALWAYS than the cluster\u0026rsquo;s replicas count, they will all be started and the replicas count will be exceeded.\n Server start state For some use cases, such as an externally managed zero downtime patching (ZDP), it may be necessary to start WebLogic Server instances so that at the end of its startup process, the server is in an administrative state. This can be achieved using the serverStartState field, which is available at domain, cluster, and server levels. When serverStartState is set to ADMIN, then servers will progress only to the administrative state. Then you could use the WebLogic Server Administration Console, REST API, or a WLST script to make any necessary updates before advancing the server to the running state.\nChanges to the serverStartState property do not affect already started servers.\nCommon starting and stopping scenarios Normal running state Normally, the Administration Server, all of the standalone Managed Servers, and enough Managed Servers members in each cluster to satisfy its replicas count, should be started. In this case, the Domain does not need to specify serverStartPolicy, or list any clusters or servers, but it does need to specify a replicas count.\nFor example:\n kind: Domain metadata: name: domain1 spec: image: ... replicas: 3 Shut down all the servers Sometimes you need to completely shut down the domain (for example, take it out of service).\n kind: Domain metadata: name: domain1 spec: serverStartPolicy: \u0026quot;NEVER\u0026quot; ... Only start the Administration Server Sometimes you want to start the Administration Server only, that is, take the Managed Servers out of service but leave the Administration Server running so that you can administer the domain.\n kind: Domain metadata: name: domain1 spec: serverStartPolicy: \u0026quot;ADMIN_ONLY\u0026quot; ... Shut down a cluster To shut down a cluster (for example, take it out of service), add it to the Domain and set its serverStartPolicy to NEVER.\n kind: Domain metadata: name: domain1 spec: clusters: - clusterName: \u0026quot;cluster1\u0026quot; serverStartPolicy: \u0026quot;NEVER\u0026quot; ... Shut down a specific standalone server To shut down a specific standalone server, add it to the Domain and set its serverStartPolicy to NEVER.\n kind: Domain metadata: name: domain1 spec: managedServers: - serverName: \u0026quot;server1\u0026quot; serverStartPolicy: \u0026quot;NEVER\u0026quot; ...  The Administration Server can be shut down by setting the serverStartPolicy of the adminServer to NEVER. Care should be taken when shutting down the Administration Server. If a Managed Server cannot connect to the Administration Server during startup, it will try to start up in Managed Server Independence (MSI) mode but this could fail due to reasons such as no accessible Authentication Provider from the Managed Server pod.\n Force a specific clustered Managed Server to start Normally, all of the Managed Servers members in a cluster are identical and it doesn\u0026rsquo;t matter which ones are running as long as the operator starts enough of them to get to the cluster\u0026rsquo;s replicas count. However, sometimes some of the Managed Servers are different (for example, support some extra services that the other servers in the cluster use) and need to always be started.\nThis is done by adding the server to the Domain and setting its serverStartPolicy to ALWAYS.\n kind: Domain metadata: name: domain1 spec: managedServers: - serverName: \u0026quot;cluster1_server1\u0026quot; serverStartPolicy: \u0026quot;ALWAYS\u0026quot; ...  The server will count toward the cluster\u0026rsquo;s replicas count. Also, if you configure more than the replicas servers count to ALWAYS, they will all be started, even though the replicas count will be exceeded.\n Shutdown options The Domain YAML file includes the field serverPod that is available under spec, adminServer, and each entry of clusters and managedServers. The serverPod field controls many details of how Pods are generated for WebLogic Server instances.\nThe shutdown field of serverPod controls how servers will be shut down and has three fields: shutdownType, timeoutSeconds, and ignoreSessions. The shutdownType field can be set to either Graceful, the default, or Forced specifying the type of shutdown. The timeoutSeconds property configures how long the server is given to complete shutdown before the server is killed. The ignoreSessions property, which is only applicable for graceful shutdown, when false, the default, allows the shutdown process to take longer to give time for any active sessions to complete up to the configured timeout. The operator runtime monitors this property but will not restart any server pods solely to adjust the shutdown options. Instead, server pods created or restarted because of another property change will be configured to shutdown, at the appropriate time, using the shutdown options set when the WebLogic Server instance Pod is created.\nShutdown environment variables The operator configures shutdown behavior with the use of the following environment variables. Users may instead simply configure these environment variables directly. When a user-configured environment variable is present, the operator will not override the environment variable based on the shutdown configuration.\n   Environment Variables Default Value Supported Values     SHUTDOWN_TYPE Graceful Graceful or Forced   SHUTDOWN_TIMEOUT 30 Whole number in seconds where 0 means no timeout   SHUTDOWN_IGNORE_SESSIONS false Boolean indicating if active sessions should be ignored; only applicable if shutdown is graceful    shutdown rules You can specify the serverPod field, including the shutdown field, at the domain, cluster, and server levels. If shutdown is specified at multiple levels, such as for a cluster and for a member server that is part of that cluster, then the shutdown configuration for a specific server is the combination of all of the relevant values with each field having the value from the shutdown field at the most specific scope.\nFor instance, given the following Domain YAML file:\n kind: Domain metadata: name: domain1 spec: serverPod: shutdown: shutdownType: Graceful timeoutSeconds: 45 clusters: - clusterName: \u0026quot;cluster1\u0026quot; serverPod: shutdown: ignoreSessions: true managedServers: - serverName: \u0026quot;cluster1_server1\u0026quot; serverPod: shutdown: timeoutSeconds: 60 ignoreSessions: false ... Graceful shutdown is used for all servers in the domain because this is specified at the domain level and is not overridden at any cluster or server level. The \u0026ldquo;cluster1\u0026rdquo; cluster defaults to ignoring sessions; however, the \u0026ldquo;cluster1_server1\u0026rdquo; server instance will not ignore sessions and will have a longer timeout.\nRestarting servers The operator automatically recreates (restarts) WebLogic Server instance Pods when fields on the Domain that affect Pod generation change (such as image, volumes, and env). The restartVersion field on the Domain lets you force the operator to restart a set of WebLogic Server instance Pods.\nThe operator does rolling restarts of clustered servers so that service is maintained.\nFields that cause servers to be restarted The operator will restart servers when any of the follow fields on the Domain that affect the WebLogic Server instance Pod generation are changed:\n auxiliaryImages auxiliaryImageVolumes containerSecurityContext domainHome domainHomeInImage domainHomeSourceType env image imagePullPolicy imagePullSecrets includeServerOutInPodLog logHomeEnabled logHome livenessProbe nodeSelector podSecurityContext readinessProbe resources restartVersion volumes volumeMounts  For Model in Image, a change to the introspectVersion field, which causes the operator to initiate a new introspection, will result in the restarting of servers if the introspection results in the generation of a modified WebLogic domain home. See the documentation on Model in Image runtime updates for a description of changes to the model or associated resources, such as Secrets, that will cause the generation of a modified WebLogic domain home.\nIf the only change detected is the addition or modification of a customer-specified label or annotation, the operator will patch the Pod rather than restarting it. Removing a label or annotation from the Domain will cause neither a restart nor a patch. It is possible to force a restart to remove such a label or annotation by modifying the restartVersion.\n Prior to version 2.2.0, the operator incorrectly restarted servers when the serverStartState field was changed. Now, this property has no affect on already running servers.\n Rolling restarts Clustered servers that need to be restarted are gradually restarted (for example, \u0026ldquo;rolling restarted\u0026rdquo;) so that the cluster is not taken out of service and in-flight work can be migrated to other servers in the cluster.\nThe maxUnavailable field on the Domain determines how many of the cluster\u0026rsquo;s servers may be taken out of service at a time when doing a rolling restart. It can be specified at the domain and cluster levels and defaults to 1 (that is, by default, clustered servers are restarted one at a time).\nWhen using in-memory session replication, Oracle WebLogic Server employs a primary-secondary session replication model to provide high availability of application session state (that is, HTTP and EJB sessions). The primary server creates a primary session state on the server to which the client first connects, and a secondary replica on another WebLogic Server instance in the cluster. Specifying a maxUnavailable property value of 1 protects against inadvertent session state loss which could occur if both the primary and secondary servers are shut down at the same time during the rolling restart process.\nIf you are supplying updated models or secrets for a running Model in Image domain, and you want the configuration updates to take effect using a rolling restart, consult Modifying WebLogic Configuration and Runtime updates before consulting this chapter.\n Draining a node and PodDisruptionBudget A Kubernetes cluster administrator can drain a Node for repair, upgrade, or scaling down the Kubernetes cluster.\nBeginning in version 3.2, the operator takes advantage of the PodDisruptionBudget feature offered by Kubernetes for high availability during a Node drain operation. The operator creates a PodDisruptionBudget (PDB) for each WebLogic cluster in the Domain namespace to limit the number of WebLogic Server pods simultaneously evicted when draining a node. The maximum number of WebLogic cluster\u0026rsquo;s server pods evicted simultaneously is determined by the maxUnavailable field on the Domain resource. The .spec.minAvailable field of the PDB for a cluster is calculated from the difference of the current replicas count and maxUnavailable value configured for the cluster. For example, if you have a WebLogic cluster with three replicas and a maxUnavailable of 1, the .spec.minAvailable for PDB is set to 2. In this case, Kubernetes ensures that at least two pods for the WebLogic cluster\u0026rsquo;s Managed Servers are available at any given time, and it only evicts a pod when all three pods are ready. For details about safely draining a node and the PodDisruptionBudget concept, see Safely Drain a Node and PodDisruptionBudget.\nCommon restarting scenarios Using restartVersion to force the operator to restart servers The restartVersion property lets you force the operator to restart servers.\nEach time you want to restart some servers, you need to set restartVersion to a different value. The specific value does not matter so most customers use whole number values.\nThe operator will detect the new value and restart the affected servers (using the same mechanisms as when other fields that affect the WebLogic Server instance Pod generation are changed, including doing rolling restarts of clustered servers).\nThe restartVersion property can be specified at the domain, cluster, and server levels. A server will be restarted if any of these three values change.\nThe servers will also be restarted if restartVersion is removed from the Domain (for example, if you had previously specified a value to cause a restart, then you remove that value after the previous restart has completed).\n Restart all the servers in the domain Set restartVersion at the domain level to a new value.\n kind: Domain metadata: name: domain1 spec: restartVersion: \u0026quot;5\u0026quot; ... Restart all the servers in the cluster Set restartVersion at the cluster level to a new value.\n kind: Domain metadata: name: domain1 spec: clusters: - clusterName : \u0026quot;cluster1\u0026quot; restartVersion: \u0026quot;5\u0026quot; maxUnavailable: 2 ... Restart the Administration Server Set restartVersion at the adminServer level to a new value.\n kind: Domain metadata: name: domain1 spec: adminServer: restartVersion: \u0026quot;5\u0026quot; ... Restart a standalone or clustered Managed Server Set restartVersion at the managedServer level to a new value.\n kind: Domain metadata: name: domain1 spec: managedServers: - serverName: \u0026quot;standalone_server1\u0026quot; restartVersion: \u0026quot;1\u0026quot; - serverName: \u0026quot;cluster1_server1\u0026quot; restartVersion: \u0026quot;2\u0026quot; ... Full domain restarts To do a full domain restart, first shut down all servers (Administration Server and Managed Servers), taking the domain out of service, then restart them. Unlike rolling restarts, the operator cannot detect and initiate a full domain restart; you must always manually initiate it.\nTo manually initiate a full domain restart:\n Change the domain-level serverStartPolicy on the Domain to NEVER.   kind: Domain metadata: name: domain1 spec: serverStartPolicy: \u0026quot;NEVER\u0026quot; ...  Wait for the operator to stop ALL the servers for that domain.\n  To restart the domain, set the domain level serverStartPolicy back to IF_NEEDED. Alternatively, you do not have to specify the serverStartPolicy as the default value is IF_NEEDED.\n   kind: Domain metadata: name: domain1 spec: serverStartPolicy: \u0026quot;IF_NEEDED\u0026quot; ... The operator will restart all the servers in the domain.  Domain lifecycle sample scripts Beginning in version 3.1.0, the operator provides sample scripts to start up or shut down a specific Managed Server or cluster in a deployed domain, or the entire deployed domain.\nNote: Prior to running these scripts, you must have previously created and deployed the domain.\nThe scripts are located in the kubernetes/samples/scripts/domain-lifecycle directory. They are helpful when scripting the life cycle of a WebLogic Server domain. For more information, see the README.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-operators/installation/",
	"title": "Install the operator",
	"tags": [],
	"description": "",
	"content": "The operator uses Helm to create the necessary resources and then deploy the operator in a Kubernetes cluster. This document describes how to install, upgrade, and uninstall the operator.\nContents  Operator image Install the operator Helm chart Alternatively, install the operator Helm chart from the GitHub chart repository Upgrade the operator Uninstall the operator  Operator image You can find the operator image in GitHub Container Registry.\nInstall the operator Helm chart Use the helm install command to install the operator Helm chart. As part of this, you must specify a \u0026ldquo;release\u0026rdquo; name for the operator.\nYou can override default configuration values in the chart by doing one of the following:\n Creating a custom YAML file containing the values to be overridden, and specifying the --value option on the Helm command line. Overriding individual values directly on the Helm command line, using the --set option.  You supply the –-namespace argument from the helm install command line to specify the namespace in which the operator will be installed. If not specified, then it defaults to default. If the namespace does not already exist, then Helm will automatically create it (and create a default service account in the new namespace), but will not remove it when the release is uninstalled. If the namespace already exists, then Helm will use it. These are standard Helm behaviors.\nSimilarly, you may override the default serviceAccount configuration value to specify a service account in the operator\u0026rsquo;s namespace, the operator will use. If not specified, then it defaults to default (for example, the namespace\u0026rsquo;s default service account). If you want to use a different service account, then you must create the operator\u0026rsquo;s namespace and the service account before installing the operator Helm chart.\nFor example, using Helm 3.x:\n$ kubectl create namespace weblogic-operator-namespace $ helm install weblogic-operator kubernetes/charts/weblogic-operator \\  --namespace weblogic-operator-namespace \\  --values custom-values.yaml --wait Or:\n$ helm install weblogic-operator kubernetes/charts/weblogic-operator \\  --namespace weblogic-operator-namespace \\  --set \u0026#34;javaLoggingLevel=FINE\u0026#34; --wait This creates a Helm release, named weblogic-operator in the weblogic-operator-namespace namespace, and configures a deployment and supporting resources for the operator.\nYou can verify the operator installation by examining the output from the helm install command.\nFor more information on specifying the registry credentials when the operator image is stored in a private registry, see Operator image pull secret.\n Alternatively, install the operator Helm chart from the GitHub chart repository Add this repository to the Helm installation:\n$ helm repo add weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts --force-update Verify that the repository was added correctly:\n$ helm repo list NAME URL weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts Install the operator from the repository:\n$ helm install weblogic-operator weblogic-operator/weblogic-operator Upgrade the operator Because operator 3.0.0 introduces non-backward compatible changes, you cannot use helm upgrade to upgrade a 2.6.0 operator to a 3.x operator. Instead, you must delete the 2.6.0 operator and then install the 3.x operator.\n The deletion of the 2.6.0 operator will not affect the Domain CustomResourceDefinition (CRD) and will not stop any WebLogic Server instances already running.\nWhen the 3.x operator is installed, it will automatically roll any running WebLogic Server instances created by the 2.6.0 operator. This rolling restart will preserve WebLogic cluster availability guarantees (for clustered members only) similarly to any other rolling restart.\nTo delete the 2.6.0 operator:\n$ helm delete weblogic-operator -n weblogic-operator-namespace Then install the 3.x operator using the installation instructions above.\nThe following instructions will be applicable to upgrade operators within the 3.x release family as additional versions are released.\nTo upgrade the operator, use the helm upgrade command. Make sure that the weblogic-kubernetes-operator repository on your local machine is at the operator release to which you are upgrading. When upgrading the operator, the helm upgrade command requires that you supply a new Helm chart and image. For example:\n$ helm upgrade \\  --reuse-values \\  --set image=ghcr.io/oracle/weblogic-kubernetes-operator:3.3.2 \\  --namespace weblogic-operator-namespace \\  --wait \\  weblogic-operator \\  kubernetes/charts/weblogic-operator Uninstall the operator The helm uninstall command is used to remove an operator release and its associated resources from the Kubernetes cluster. The release name used with the helm uninstall command is the same release name used with the helm install command (see Install the Helm chart). For example:\n$ helm uninstall weblogic-operator -n weblogic-operator-namespace  If the operator\u0026rsquo;s namespace did not exist before the Helm chart was installed, then Helm will create it, however, helm uninstall will not remove it.\n After removing the operator deployment, you should also remove the Domain custom resource definition (CRD):\n$ kubectl delete customresourcedefinition domains.weblogic.oracle Note that the Domain custom resource definition is shared. Do not delete the CRD if there are other operators in the same cluster.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/choosing-a-model/",
	"title": "Choose a domain home source type",
	"tags": [],
	"description": "",
	"content": "When using the operator to start WebLogic Server instances from a domain, you have the choice of the following WebLogic domain home source types:\n  Domain in PV:\n Supply a WebLogic installation in an image and supply WebLogic configuration as a domain home in a persistent volume. Supply WebLogic applications in the persistent volume. Mutate WebLogic configuration using WLST, the WebLogic Server Administration Console, or configuration overrides supplied in a Kubernetes ConfigMap.    Domain in Image:\n Supply a WebLogic installation in an image and supply WebLogic configuration as a domain home layered on this image. Supply WebLogic applications layered on the installation image. Mutate WebLogic configuration by supplying a new image and rolling, or by configuration overrides supplied in a Kubernetes ConfigMap.    Model in Image:\n Supply a WebLogic installation in an image and supply WebLogic configuration in one of three ways:  As WebLogic Deployment Tool (WDT) model YAML file layered on the WebLogic installation image. As WDT model YAML file supplied in separate auxiliary images. As WDT model YAML file in a Kubernetes ConfigMap.   Supply WebLogic applications in one of two ways:  Layered on the installation image. In auxiliary images.   Mutate WebLogic configuration by supplying a new image and rolling, or model updates supplied in a Kubernetes ConfigMap.    Note that you can use different domain home types for different domains; there\u0026rsquo;s no restriction on having domains with different domain home types in the same Kubernetes cluster or namespace.\nThere are advantages for each domain home source type where Model in Image is the most popular choice, but sometimes there are also technical limitations of various cloud providers that may make one type better suited to your needs. The following table compares the types:\n   Domain in PV Domain in Image Model in Image     Lets you use the same standard WebLogic Server image for every server in every domain. Requires a different image for each domain, but all servers in that domain use the same image. Different domains can use the same image, but require different domainUID and may have different configuration.   No state is kept in images making the containers created from these images completely throw away (cattle not pets). Runtime state should not be kept in the images, but applications and configuration are. Runtime state should not be kept in the images. Application and configuration may be.   You can deploy new applications using the Administration Console or WLST. If you want to deploy application updates, then you must create a new image. If you want to deploy application updates, then you must create a new image, which optionally can be an auxiliary image that doesn\u0026rsquo;t include a WebLogic installation.   You can use configuration overrides to mutate the domain configuration before it is deployed, but there are limitations. Same as Domain in PV. You can deploy model files to a ConfigMap to mutate the domain before it is deployed. The model file syntax is far simpler and less error prone than the configuration override syntax, and, unlike configuration overrides, allows you to directly add data sources and JMS modules.   You can change WebLogic domain configuration at runtime using the Administration Console or WLST. You can also change configuration overrides and distribute the new overrides to running servers; however, non-dynamic configuration attributes can be changed only when servers are starting and some changes may require a full domain restart. You also can change configuration overrides and distribute the new overrides to running servers; however, non-dynamic configuration attributes can be changed only when servers are starting and some changes may require a full domain restart. You should not use the Administration Console or WLST for these domains as changes are ephemeral and will be lost when servers restart. You can change configuration at runtime using model YAML file snippets supplied in runtime updates (which are substantially easier to specify than configuration overrides); however, non-dynamic configuration attributes will change only when servers are restarted (rolled) and some changes may require a full domain restart. You should not use the Administration Console or WLST for these domains as changes are ephemeral and will be lost when servers restart.   Logs are automatically placed on persistent storage and sent to the pod\u0026rsquo;s stdout. Logs are kept in the containers and sent to the pod\u0026rsquo;s log (stdout) by default. To change the log location, you can set the Domain logHomeEnabled to true and configure the desired directory using logHome. Same as Domain in Image.   Patches can be applied by simply changing the image and rolling the domain. To apply patches, you must update the domain-specific image and then restart or roll the domain depending on the nature of the patch. Same as Domain in PV when using dedicated auxiliary images to supply model artifacts; same as Domain in Image otherwise.   Many cloud providers do not provide persistent volumes that are shared across availability zones, so you may not be able to use a single persistent volume. You may need to use some kind of volume replication technology or a clustered file system. Provided you do not store and state in containers, you do not have to worry about volume replication across availability zones because each pod has its own copy of the domain. WebLogic replication will handle propagation of any online configuration changes. Same as Domain in Image.   CI/CD pipelines may be more complicated because you would need to run WLST against the live domain directory to effect changes. CI/CD pipelines are simpler because you can create the whole domain in the image and don\u0026rsquo;t have to worry about a persistent copy of the domain. CI/CD pipelines are even simpler because you don\u0026rsquo;t need to generate a domain home. The operator will create a domain home for you based on the model that you supply.   There are fewer images to manage and store, which could provide significant storage and network savings. There are more images to manage and store in this approach. Same as Domain in Image unless you use the auxiliary images approach. With auxiliary images, you can use a single image to distribute your WebLogic installation (similar to Domain on PV), plus one or more specific dedicated images that contain your WebLogic configuration and applications.   You may be able to use standard Oracle-provided images or, at least, a very small number of self-built images, for example, with patches installed. You may need to do more work to set up processes to build and maintain your images. Same as Domain in Image.    "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/prepare/",
	"title": "Prepare to run a domain",
	"tags": [],
	"description": "",
	"content": "Perform these steps to prepare your Kubernetes cluster to run a WebLogic domain:\n  Create the domain namespace or namespaces. One or more domains can share a namespace. A single instance of the operator can manage multiple namespaces.\n$ kubectl create namespace domain-namespace-1 Replace domain-namespace-1 with name you want to use. The name must follow standard Kubernetes naming conventions, that is, lowercase, numbers, and hyphens.\n  Create a Kubernetes Secret containing the Administration Server boot credentials. You can do this manually or by using the provided sample. To create the secret manually, use this command:\n$ kubectl -n domain-namespace-1 \\  create secret generic domain1-weblogic-credentials \\  --from-literal=username=username \\  --from-literal=password=password  Replace domain-namespace-1 with the namespace that the domain will be in. Replace domain1-weblogic-credentials with the name of the secret. It is a recommended best practice to name the secret using the domain\u0026rsquo;s domainUID followed by the literal string -weblogic-credentials where domainUID is a unique identifier for the domain. Many of the samples follow this practice and use a domainUID of domain1 or sample-domain1. Replace the string username in the third line with the user name for the administrative user. Replace the string password in the fourth line with the password.    Optionally, create a PV \u0026amp; PersistentVolumeClaim (PVC) which can hold the domain home, logs, and application binaries. Even if you put your domain in an image, you may want to put the logs on a persistent volume so that they are available after the pods terminate. This may be instead of, or as well as, other approaches like streaming logs into Elasticsearch.\n  Optionally, configure load balancer to manage access to any WebLogic clusters.\n  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/introduction/terms/",
	"title": "Important terms",
	"tags": [],
	"description": "Define important terms used throughout this documentation.",
	"content": "This documentation uses several important terms which are intended to have a specific meaning.\n   Term Definition     Cluster Because this term is ambiguous, it will be prefixed to indicate which type of cluster is meant. A WebLogic cluster is a group of WebLogic Managed Servers that together host some application or component and which are able to share load and state between them; a single WebLogic domain can define multiple WebLogic clusters. A Kubernetes cluster is a group of machines (“Nodes”) that all host Kubernetes resources, like Pods and Services, and which appear to the external user as a single entity. If the term “cluster” is not prefixed, then it should be assumed to mean a Kubernetes cluster.   Domain Because this term is ambiguous, it will be prefixed or suffixed to indicate which type of cluster is meant. A WebLogic domain is a group of related applications and configuration information necessary to run them with or without Kubernetes. A domain resource is a Kubernetes resource that is of custom resource type Domain. Domain resources are monitored by the WebLogic Kubernetes operator, and a domain resource references a WebLogic domain\u0026rsquo;s WebLogic install, WebLogic domain configuration, and potentially additional Kubernetes resources that are necessary for running the WebLogic domain. If the term “domain” is not prefixed or suffixed, then it should be assumed to mean a Kubernetes domain resource.   Domain UID A Domain UID identifies a particular Domain resource and is a unique string that is minimally unique within the scope of a Kubernetes namespace. If the Domain UID is not explicitly set in a Domain resource\u0026rsquo;s spec.domainUID field, then the Domain UID defaults to being the metadata.Name of the resource. As a convention, any resource that is associated with a particular Domain UID is typically assigned a Kubernetes label weblogic.domainUID that is assigned to that name.   Ingress A Kubernetes Ingress provides access to applications and services in a Kubernetes environment to external clients. An Ingress may also provide additional features like load balancing.   Labels A Kubernetes Label is a name and value pair that is associated with any Kubernetes resource. A resource can have multiple arbitrary labels and labels are often used to allow resources to reference each other. For example, a Service may define a selector that matches the service to Pods with a given label.   Namespace A Kubernetes Namespace is a named scope within a Kubernetes cluster that can be used to group together related Kubernetes resources, for example, Pods and Services. Same named resources of different types can reside in the same Kubernetes namespace, but same named resources of the same type must be targeted to different namespaces. Note that some Kubernetes resources are global in scope and cannot targeted to a specific namespace (a Kubernetes ClusterRole for example).   Operator A Kubernetes operator is software that performs management of complex applications. The WebLogic Kubernetes operator is an operator that manages Domain resources. If the term “operator” is not prefixed or suffixed, then it should be assumed to mean a WebLogic Kubernetes operator.   Pod A Kubernetes Pod contains one or more containers and is the object that provides the execution environment for an instance of an application component, such as a web server or database.   Job A Kubernetes Job is a type of controller that creates one or more Pods that run to completion to complete a specific task.   Secret A Kubernetes Secret is a named object that can store secret information like user names, passwords, X.509 certificates, or any other arbitrary data.   Service A Kubernetes Service exposes application network endpoints inside a Pod to other Pods, or outside the Kubernetes cluster. A Service may also provide additional features like load balancing. The name of service is used to generate a DNS name that applications can use to access the service. Kubernetes requires that the names of some resource types, including services, follow the DNS label standard as defined in DNS Label Names and RFC 1123. Therefore, the operator enforces that the names of the Kubernetes resources do not exceed Kubernetes limits (see Meet Kubernetes resource name restrictions); for example, when the operator generates a service for you, it generates service names that are lowercase and that have their underscores _ converted to hyphens -.    "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "Gain an overall understanding of the operator and learn where you can get help.",
	"content": "Gain an overall understanding of the operator and learn where you can get help.\n Important terms  Define important terms used throughout this documentation.\n Design philosophy  Define the expected roles of an administrator, the operator, and domain resources.\n Architecture  An architectural overview of the operator runtime and related resources.\n Get help  Where to get help, submit suggestions, or submit issues.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/quickstart/",
	"title": "Quick Start",
	"tags": [],
	"description": "",
	"content": "The Quick Start guide provides a simple tutorial to help you get the operator up and running quickly. Use this Quick Start guide to create a WebLogic Server deployment in a Kubernetes cluster with the WebLogic Kubernetes Operator. Please note that this walk-through is for demonstration purposes only, not for use in production. These instructions assume that you are already familiar with Kubernetes. If you need more detailed instructions, please refer to the User guide.\nAll Kubernetes distributions and managed services have small differences. In particular, the way that persistent storage and load balancers are managed varies significantly.\nYou may need to adjust the instructions in this guide to suit your particular flavor of Kubernetes.\n For this exercise, you’ll need a Kubernetes cluster. If you need help setting one up, check out our cheat sheet. This guide assumes a single node cluster.\nThe operator uses Helm to create and deploy the necessary resources and then run the operator in a Kubernetes cluster. For Helm installation and usage information, see Install Helm.\nYou should clone this repository to your local machine so that you have access to the various sample files mentioned throughout this guide:\n$ git clone --branch v3.2.3 https://github.com/oracle/weblogic-kubernetes-operator "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/azure-kubernetes-service/domain-on-pv/",
	"title": "Domain home on a PV",
	"tags": [],
	"description": "Sample for creating a WebLogic domain home on an existing PV or PVC on the Azure Kubernetes Service.",
	"content": "This sample demonstrates how to use the WebLogic Kubernetes Operator (hereafter \u0026ldquo;the operator\u0026rdquo;) to set up a WebLogic Server (WLS) cluster on the Azure Kubernetes Service (AKS) using the model in persistence volume approach. After going through the steps, your WLS domain runs on an AKS cluster instance and you can manage your WLS domain by accessing the WebLogic Server Administration Console.\nContents  Prerequisites Create an AKS cluster Install WebLogic Kubernetes Operator Create WebLogic domain Automation Deploy sample application Access WebLogic Server logs Clean up resources Troubleshooting Useful links  Prerequisites This sample assumes the following prerequisite environment.\n Operating System: GNU/Linux, macOS or WSL2 for Windows 10. Git; use git --version to test if git works. This document was tested with version 2.17.1. Azure CLI; use az --version to test if az works. This document was tested with version 2.9.1. Docker for Desktop. This document was tested with Docker version 20.10.2, build 2291f61 kubectl; use kubectl version to test if kubectl works. This document was tested with version v1.16.3. Helm, version 3.1 and later; use helm version to check the helm version. This document was tested with version v3.2.5.  Create a Service Principal for AKS An AKS cluster requires either an Azure Active Directory (AD) service principal or a managed identity to interact with Azure resources.\nWe will use a service principal to create an AKS cluster. Follow the commands below to create a new service principal.\nPlease run az login first. Do set the subscription you want to work with. You can get a list of your subscriptions by running az account list.\n# Login $ az login # Set your working subscription $ export SUBSCRIPTION_ID=\u0026lt;your-subscription-id\u0026gt; $ az account set -s $SUBSCRIPTION_ID Create the new service principal with the following commands:\n# Create Service Principal $ export SP_NAME=myAKSClusterServicePrincipal $ az ad sp create-for-rbac --skip-assignment --name $SP_NAME # Copy the output to a file, we will use it later. If you see an error similar to the following:\nFound an existing application instance of \u0026#34;5pn2s201-nq4q-43n1-z942-p9r9571qr3rp\u0026#34;. We will patch it Insufficient privileges to complete the operation. The problem may be a pre-existing service principal with the same name. Either delete the other service principal or pick a different name.\nSuccessful output will look like the following:\n{ \u0026#34;appId\u0026#34;: \u0026#34;r3qnq743-61s9-4758-8163-4qpo87s72s54\u0026#34;, \u0026#34;displayName\u0026#34;: \u0026#34;myAKSClusterServicePrincipal\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;http://myAKSClusterServicePrincipal\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;TfhR~uOJ1C1ftD5NS_LzJJj6UOjS2OwXfz\u0026#34;, \u0026#34;tenant\u0026#34;: \u0026#34;82sr215n-0ns5-404e-9161-206r0oqyq999\u0026#34; } Grant your service principal with a contributor role to create AKS resources.\n# Use the \u0026lt;appId\u0026gt; from the output of the last command $ export SP_APP_ID=r3qnq743-61s9-4758-8163-4qpo87s72s54 $ az role assignment create --assignee $SP_APP_ID --role Contributor Successful output will look like the following:\n{ \u0026#34;canDelegate\u0026#34;: null, \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/p7844r91-o11q-4n7s-np6s-996308sopqo9/providers/Microsoft.Authorization/roleAssignments/4oq396os-rs95-4n6s-n3qo-sqqpnpo91035\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;4oq396os-rs95-4n6s-n3qo-sqqpnpo91035\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;952551r8-n129-4on3-oqo9-231n0s6011n3\u0026#34;, \u0026#34;principalType\u0026#34;: \u0026#34;ServicePrincipal\u0026#34;, \u0026#34;roleDefinitionId\u0026#34;: \u0026#34;/subscriptions/p7844r91-o11q-4n7s-np6s-996308sopqo9/providers/Microsoft.Authorization/roleDefinitions/o24988np-6180-42n0-no88-20s7382qq24p\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;/subscriptions/p7844r91-o11q-4n7s-np6s-996308sopqo9\u0026#34;, } Oracle Container Registry You will need an Oracle account. The following steps will direct you to accept the license agreement for WebLogic Server. Make note of your Oracle Account password and email. This sample pertains to 12.2.1.4, but other versions may work as well.\n In a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them. The Oracle Container Registry provides a WebLogic Server 12.2.1.4.0 Docker image, which already has the necessary patches applied, and the Oracle WebLogic Server 12.2.1.4.0 and 14.1.1.0.0 images, which do not require any patches. Ensure that Docker desktop is running. Find and then pull the WebLogic 12.2.1.4 install image: $ docker pull container-registry.oracle.com/middleware/weblogic:12.2.1.4   If you have problems accessing the Oracle Container Registry, you can build your own Docker images from the Oracle GitHub repository.\nClone WebLogic Kubernetes Operator repository Clone the WebLogic Kubernetes Operator repository to your machine. We will use several scripts in this repository to create a WebLogic domain. This sample was tested with v3.1.1, but should work with the latest release.\n$ git clone --branch v3.3.2 https://github.com/oracle/weblogic-kubernetes-operator.git  The following sections of the sample instructions will guide you, step-by-step, through the process of setting up a WebLogic cluster on AKS - remaining as close as possible to a native Kubernetes experience. This lets you understand and customize each step. If you wish to have a more automated experience that abstracts some lower level details, you can skip to the Automation section.\n Create the AKS cluster This sample requires that you disable the AKS addon http_application_routing by default. If you want to enable http_application_routing, then follow HTTP application routing.\nRun the following commands to create the AKS cluster instance.\n# Change these parameters as needed for your own environment # Specify a prefix to name resources, only allow lowercase letters and numbers, between 1 and 7 characters $ export NAME_PREFIX=wls # Used to generate resource names. $ export TIMESTAMP=`date +%s` $ export AKS_CLUSTER_NAME=\u0026#34;${NAME_PREFIX}aks${TIMESTAMP}\u0026#34; $ export AKS_PERS_RESOURCE_GROUP=\u0026#34;${NAME_PREFIX}resourcegroup${TIMESTAMP}\u0026#34; $ export AKS_PERS_LOCATION=eastus $ export SP_APP_ID=\u0026lt;appId from the az ad sp create-for-rbac command\u0026gt; $ export SP_CLIENT_SECRET=\u0026lt;password from the az ad sp create-for-rbac command\u0026gt; $ az group create --name $AKS_PERS_RESOURCE_GROUP --location $AKS_PERS_LOCATION $ az aks create \\  --resource-group $AKS_PERS_RESOURCE_GROUP \\  --name $AKS_CLUSTER_NAME \\  --node-count 2 \\  --generate-ssh-keys \\  --nodepool-name nodepool1 \\  --node-vm-size Standard_DS2_v2 \\  --location $AKS_PERS_LOCATION \\  --service-principal $SP_APP_ID \\  --client-secret $SP_CLIENT_SECRET Successful output will be a JSON object with the entry \u0026quot;type\u0026quot;: \u0026quot;Microsoft.ContainerService/ManagedClusters\u0026quot;.\nAfter the deployment finishes, run the following command to connect to the AKS cluster. This command updates your local ~/.kube/config so that subsequent kubectl commands interact with the named AKS cluster.\n$ az aks get-credentials --resource-group $AKS_PERS_RESOURCE_GROUP --name $AKS_CLUSTER_NAME Successful output will look similar to:\nMerged \u0026#34;wlsaks1596087429\u0026#34; as current context in /home/username/.kube/config After your Kubernetes cluster is up and running, run the following commands to make sure kubectl can access the Kubernetes cluster:\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME aks-pool1haiche-33688868-vmss000000 Ready agent 4m25s v1.17.13 10.240.0.4 \u0026lt;none\u0026gt; Ubuntu 16.04.7 LTS 4.15.0-1098-azure docker://19.3.12 aks-pool1haiche-33688868-vmss000001 Ready agent 4m12s v1.17.13 10.240.0.5 \u0026lt;none\u0026gt; Ubuntu 16.04.7 LTS 4.15.0-1098-azure docker://19.3.12  Note: If you run into VM size failure, see Troubleshooting - Virtual Machine size is not supported.\nCreate storage and set up file share Our usage pattern for the operator involves creating Kubernetes \u0026ldquo;persistent volumes\u0026rdquo; to allow the WebLogic Server to persist its configuration and data separately from the Kubernetes Pods that run WebLogic Server workloads.\nWe will create an external data volume to access and persist data. There are several options for data sharing as described in Storage options for applications in Azure Kubernetes Service (AKS).\nWe will use Azure Files as a Kubernetes volume. For details about this full featured cloud storage solution, see the Azure Files Documentation.\nCreate an Azure Storage account Create a storage account using the Azure CLI. Note that the storage account name can contain only lowercase letters and numbers, and must be between 3 and 24 characters in length:\n# Change the value as needed for your own environment $ export AKS_PERS_STORAGE_ACCOUNT_NAME=\u0026#34;${NAME_PREFIX}storage${TIMESTAMP}\u0026#34; $ az storage account create \\  -n $AKS_PERS_STORAGE_ACCOUNT_NAME \\  -g $AKS_PERS_RESOURCE_GROUP \\  -l $AKS_PERS_LOCATION \\  --sku Standard_LRS Successful output will be a JSON object with the entry \u0026quot;type\u0026quot;: \u0026quot;Microsoft.Storage/storageAccounts\u0026quot;.\nNow we need to create a file share. To create the file share, you need a storage connection string. Run the show-connection-string command to get connection string, then create the share with az storage share create, as shown here.\n# Change value as needed for your own environment $ export AKS_PERS_SHARE_NAME=\u0026#34;${NAME_PREFIX}-weblogic-${TIMESTAMP}\u0026#34; # Get connection string $ export AZURE_STORAGE_CONNECTION_STRING=$(az storage account show-connection-string -n $AKS_PERS_STORAGE_ACCOUNT_NAME -g $AKS_PERS_RESOURCE_GROUP -o tsv) # Create file share $ az storage share create -n $AKS_PERS_SHARE_NAME --connection-string $AZURE_STORAGE_CONNECTION_STRING Successful output will be exactly the following:\n{ \u0026#34;created\u0026#34;: true } The operator uses Kubernetes Secrets. We need a storage key for the secret. These commands query the storage account to obtain the key, and then stores the storage account key as a Kubernetes secret.\n$ export STORAGE_KEY=$(az storage account keys list --resource-group $AKS_PERS_RESOURCE_GROUP --account-name $AKS_PERS_STORAGE_ACCOUNT_NAME --query \u0026#34;[0].value\u0026#34; -o tsv) Verify the successful output by examining the STORAGE_KEY environment variable. It must not be empty. It must be a long ASCII string.\nWe will use the kubernetes/samples/scripts/create-kubernetes-secrets/create-azure-storage-credentials-secret.sh script to create the storage account key as a Kubernetes secret, naming the secret with value ${NAME_PREFIX}azure-secret. Please run:\n# Please change persistentVolumeClaimNameSuffix if you changed pre-defined value \u0026#34;regcred\u0026#34; before generating the configuration files. $ export SECRET_NAME_AZURE_FILE=\u0026#34;${NAME_PREFIX}azure-secret\u0026#34; #cd kubernetes/samples/scripts/create-kubernetes-secrets $ ./create-azure-storage-credentials-secret.sh -s $SECRET_NAME_AZURE_FILE -a $AKS_PERS_STORAGE_ACCOUNT_NAME -k $STORAGE_KEY You will see the following output:\nsecret/wlsazure-secret created The secret wlsazure-secret has been successfully created in the default namespace. Create PV and PVC This sample uses Kubernetes Persistent Volume Claims (PVC) as storage resource. These features are passed to Kubernetes using YAML files. The script kubernetes/samples/scripts/create-weblogic-domain-on-azure-kubernetes-service/create-domain-on-aks.sh generates the required configuration files automatically, given an input file containing the parameters. A parameters file is provided at kubernetes/samples/scripts/create-weblogic-domain-on-azure-kubernetes-service/create-domain-on-aks-inputs.yaml. Copy and customize this file for your needs.\nTo generate YAML files to create PV and PVC in the AKS cluster, the following values must be substituted in your copy of the input file.\n   Name in YAML file Example value Notes     azureServicePrincipalAppId nr086o75-pn59-4782-no5n-nq2op0rsr1q6 Application ID of your service principal.   azureServicePrincipalClientSecret 8693089o-q190-45ps-9319-or36252s3s90 A client secret of your service principal.   azureServicePrincipalTenantId 72s988os-86s1-cafe-babe-2q7pq011qo47 Tenant (Directory ) ID of your service principal.   dockerEmail yourDockerEmail Oracle Single Sign-On (SSO) account email, used to pull the WebLogic Server Docker image.   dockerPassword yourDockerPassword Password for Oracle SSO account, used to pull the WebLogic Server Docker image, in clear text.   dockerUserName yourDockerId The same value as dockerEmail.   namePrefix wls Alphanumeric value used as a disambiguation prefix for several Kubernetes resources. Make sure the value matches the value of ${NAME_PREFIX} to keep names in step-by-step commands the same with those in configuration files.    Use the following command to generate configuration files, assuming the output directory is ~/azure. The script will overwrite any files generated by a previous invocation.\n#cd kubernetes/samples/scripts/create-weblogic-domain-on-azure-kubernetes-service $ cp create-domain-on-aks-inputs.yaml my-create-domain-on-aks-inputs.yaml $ ./create-domain-on-aks.sh -i my-create-domain-on-aks-inputs.yaml -o ~/azure -u ${TIMESTAMP} After running the command, all needed configuration files are generated and output to ~/azure/weblogic-on-aks:\nThe following files were generated: /home/username/azure/weblogic-on-aks/pv.yaml /home/username/azure/weblogic-on-aks/pvc.yaml /home/username/azure/weblogic-on-aks/admin-lb.yaml /home/username/azure/weblogic-on-aks/cluster-lb.yaml /home/username/azure/weblogic-on-aks/domain1.yaml /home/username/azure/weblogic-on-aks/cluster-admin-role.yaml Completed Note: Beyond the required and default configurations generated by the command, you can modify the generated YAML files to further customize your deployment. For further information about customizing your deployment, consult the operator documentation, AKS documentation, and Kubernetes references.\nApply generated configuration files In order to mount the file share as a persistent volume, we have provided a configuration file pv.yaml. You can find it in your output directory. The following content is an example that uses the value wls-weblogic as \u0026ldquo;shareName\u0026rdquo;, wlsazure-secret as \u0026ldquo;secretName\u0026rdquo;, and the persistent volume name is wls-azurefile.\nWe will use the storage class azurefile. If you want to create a new class, follow this document Create a storage class. For more information, see the page Storage options for applications in Azure Kubernetes Service (AKS).\napiVersion: v1 kind: PersistentVolume metadata: name: wls-azurefile spec: capacity: storage: 5Gi accessModes: - ReadWriteMany storageClassName: azurefile azureFile: secretName: wlsazure-secret shareName: wls-weblogic-1597391432 readOnly: false mountOptions: - dir_mode=0777 - file_mode=0777 - uid=1000 - gid=1000 - mfsymlinks - nobrl Note: This sample includes nobrl in the mountOptions to disable byte range file locking on the azurefile storage class. Currently, this is necessary because the azurefile storage class does not support advisory byte range locking. This approach is documented in the Azure Kubernetes Service FAQ.\n Additional important file locking information Some action must be taken to deal with the presence or absence of advisory byte range locking when running WebLogic on Kubernetes. Failure to address this issue will cause WebLogic file store locking issues during domain lifecycle management. When this happens, WebLogic will not be able to start.\nIf it is not possible to use a different file system that fully supports advisory byte range locking, such as NFS file shares in Azure Files, be aware that disabling locking risks data corruption and additional steps are required to mitigate this risk, as shown in the next section.\nHere are several different approaches to disable file locking.\n  When using the azurefile storage class, you can universally disable locking on the entire file system by enabling the nobrl mount option, as shown previously.\n Note that this affects all software using the same file system. Steps to reduce the consequent corruption risk will vary based on software, and will differ from the steps used for WebLogic.    You can disable file locking in the WebLogic configuration for each default file store, custom file store, and JMS paging store by following the steps outlined in Handling NFS Locking Errors.\n When using the operator, you can provide this configuration without needing to modify your original configuration using configuration overrides for Domain on PV or Domain in Image, or runtime updates for Model in Image Note that this can be a substantial amount of work and error prone as it requires configuration updates for each individual default store, custom file store, and JMS paging store.    You can disable all file store locks on a particular WebLogic server JVM by both applying patch 32471832 and setting -Dweblogic.store.file.LockEnabled=false. When using the operator, you can set command-line values using the JAVA_OPTIONS env var in spec.serverPod.env domain resource attribute. This will work for any operator domain home source type.\n  Mitigating corruption risk when locking is disabled It is important to mitigate the risk of WebLogic data corruption when locking is disabled.\n  Do not configure service migration for WebLogic JTA default file stores or custom file stores.\n  File store service migration is not supported when file locking is disabled because it relies on file locks for safe behavior.\n  Service migration is a WebLogic high availability option that is typically configured to enable data recovery on surviving WebLogic Servers in a cluster upon an unexpected WebLogic server failure. It is also used to enable JMS and JTA data recovery from WebLogic Servers that are shutdown due to a cluster shrink.\n    If at all possible, do not store important data in WebLogic default or custom file stores.\n  Configure WebLogic JMS, JTA, and EJB Timers to use database storage instead of file storage.\n  For example, use the \u0026lsquo;TLOG-in-DB\u0026rsquo; feature for JTA and custom database stores instead of file stores for JMS.\n  This will require a change to your original WebLogic configuration. It is not practical to use operator configuration overrides or runtime updates for this purpose because the changes are too extensive.\n    Note that it is fine to configure service migration for database stores even when file locking is disabled.\n  Take additional steps in your CI/CD processes to guard against the user errors that file locks normally help prevent. In particular, ensure you have procedures in place to prevent administrators or testers from mistakenly starting a duplicate WebLogic domain in the same shared file system.\n   We have provided another configuration file pvc.yaml for the PersistentVolumeClaim. Both pv.yaml and pvc.yaml have exactly the same content for storageClassName attributes. This is required. We set the same value to the metadata property in both files. The following content is an example that uses the persistent volume claim name wls-azurefile.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: wls-azurefile spec: accessModes: - ReadWriteMany storageClassName: azurefile resources: requests: storage: 5Gi Use the kubectl command to create the persistent volume and persistent volume claim to the default namespace.\n$ kubectl apply -f ~/azure/weblogic-on-aks/pv.yaml persistentvolume/wls-azurefile created $ kubectl apply -f ~/azure/weblogic-on-aks/pvc.yaml persistentvolumeclaim/wls-azurefile created Use the following command to verify:\n$ kubectl get pv,pvc Example output:\n$ kubectl get pv,pvc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/wls-azurefile 5Gi RWX Retain Bound default/wls-azurefile azurefile 16m NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/wls-azurefile Bound wls-azurefile 5Gi RWX azurefile 16m Note: Carefully inspect the output and verify it matches the above. ACCESS MODES, CLAIM, and STORAGECLASS are vital.\nInstall WebLogic Kubernetes Operator into the AKS cluster The WebLogic Kubernetes Operator is an adapter to integrate WebLogic Server and Kubernetes, allowing Kubernetes to serve as a container infrastructure hosting WLS instances. The operator runs as a Kubernetes Pod and stands ready to perform actions related to running WLS on Kubernetes.\nKubernetes Operators use Helm to manage Kubernetes applications. The operator’s Helm chart is located in the kubernetes/charts/weblogic-operator directory. Please install the operator by running the corresponding command.\n$ helm repo add weblogic-operator https://oracle.github.io/weblogic-kubernetes-operator/charts --force-update $ helm install weblogic-operator weblogic-operator/weblogic-operator --version \u0026#34;3.1.1\u0026#34; The output will show something similar to the following:\n$ helm install weblogic-operator weblogic-operator/weblogic-operator --version \u0026#34;3.1.1\u0026#34; NAME: weblogic-operator LAST DEPLOYED: Wed Jul 1 23:47:44 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None Verify the operator with the following command; the STATUS must be Running. The READY must be 1/1.\n$ kubectl get pods -w NAME READY STATUS RESTARTS AGE weblogic-operator-56654bcdb7-qww7f 1/1 Running 0 25m  You will have to press Ctrl-C to exit this command due to the -w flag.\n Create WebLogic domain  Create secrets Create WebLogic Domain  Now that we have created the AKS cluster, installed the operator, and verified that the operator is ready to go, we can have the operator create a WLS domain.\nCreate secrets We will use the kubernetes/samples/scripts/create-weblogic-domain-credentials/create-weblogic-credentials.sh script to create the domain credentials as a Kubernetes secret. Please run:\n# cd kubernetes/samples/scripts/create-weblogic-domain-credentials $ ./create-weblogic-credentials.sh -u weblogic -p welcome1 -d domain1 secret/domain1-weblogic-credentials created secret/domain1-weblogic-credentials labeled The secret domain1-weblogic-credentials has been successfully created in the default namespace. We will use the kubernetes/samples/scripts/create-kubernetes-secrets/create-docker-credentials-secret.sh script to create the Docker credentials as a Kubernetes secret. Please run:\n# Please change imagePullSecretNameSuffix if you change pre-defined value \u0026#34;regcred\u0026#34; before generating the configuration files. $ export SECRET_NAME_DOCKER=\u0026#34;${NAME_PREFIX}regcred\u0026#34; # cd kubernetes/samples/scripts/create-kubernetes-secrets $ ./create-docker-credentials-secret.sh -s ${SECRET_NAME_DOCKER} -e oracleSsoEmail@bar.com -p oracleSsoPassword -u oracleSsoEmail@bar.com secret/regcred created The secret regcred has been successfully created in the default namespace. Verify secrets with the following command:\n$ kubectl get secret NAME TYPE DATA AGE wlsazure-secret Opaque 2 17m regcred kubernetes.io/dockerconfigjson 1 2m25s default-token-csdvd kubernetes.io/service-account-token 3 25m domain1-weblogic-credentials Opaque 2 3m42s sh.helm.release.v1.weblogic-operator.v1 helm.sh/release.v1 1 5m41s weblogic-operator-secrets Opaque 1 5m41s Note: If the NAME column in your output is missing any of the values shown above, please reexamine your execution of the preceding steps in this sample to ensure that you correctly followed all of them. The default-token-mwdj8 shown above will have a different ending in your output.\nCreate WebLogic Domain We will use the kubernetes/samples/scripts/create-weblogic-domain/domain-home-on-pv/create-domain.sh script to create the WLS domain in the persistent volume we created previously.\nThe create-domain.sh script and its inputs file are for demonstration purposes only; its contents and the domain resource file that it generates for you might change without notice. In production, we strongly recommend that you use the WebLogic Image Tool and WebLogic Deploy Tooling (when applicable), and directly work with domain resource files instead.\n We need to set up the domain configuration for the WebLogic domain.\n  Check if resources are ready.\nIf you used the automation script to create the AKS cluster, skip this step and go to step 2.\nIf you created Azure resources, step-by-step, according to the previous steps, then validate that all the resources above were created by using the script kubernetes/samples/scripts/create-weblogic-domain-on-azure-kubernetes-service/validate.sh.\nUse the following commands to check if the resources are ready:\n# cd kubernetes/samples/scripts/create-weblogic-domain-on-azure-kubernetes-service $ ./validate.sh -g ${AKS_PERS_RESOURCE_GROUP} \\  --aks-name ${AKS_CLUSTER_NAME} \\  --file-share ${AKS_PERS_SHARE_NAME} \\  --storage-account ${AKS_PERS_STORAGE_ACCOUNT_NAME} \\  --domain-uid domain1 \\  --pv-name ${NAME_PREFIX}-azurefile-${TIMESTAMP} \\  --pvc-name ${NAME_PREFIX}-azurefile-${TIMESTAMP} \\  --secret-docker ${SECRET_NAME_DOCKER} \\  --secret-storage ${SECRET_NAME_AZURE_FILE} You will see output with PASS if all the resources are ready. The following is an example of output:\nPASS You can create your domain with the following resources ready: Azure resource group: wlsresourcegroup1612795811 Azure Kubenetes Service instacne: wlsaks1612795811 Azure storage account: wlsstorage1612795811 Azure file share: wls-weblogic-1612795811 Kubenetes secret for Azure storage: wlsazure-secret Kubenetes secret for Docker Account: regcred Kubenetes secret for Weblogic domain: domain1-weblogic-credentials Persistent Volume: wls-azurefile-1612795811 Persistent Volume Claim: wls-azurefile-1612795811   Now let\u0026rsquo;s ask the operator to create a WebLogic Server domain within the AKS cluster.\nFor complete details on domain creation, see Domain home on a PV - Use the script to create a domain. If you do not want the complete details and just want to continue with the domain creation for AKS, invoke the create-domain.sh script as shown next.\n# cd kubernetes/samples/scripts/create-weblogic-domain/domain-home-on-pv $ ./create-domain.sh -i ~/azure/weblogic-on-aks/domain1.yaml -o ~/azure -e -v You may observe error-related output during the creation of the domain. This is due to timing issues during domain creation. The script accounts for this with a series of retries. The error output looks similar to the following:\nWaiting for the job to complete... Error from server (BadRequest): container \u0026#34;create-weblogic-sample-domain-job\u0026#34; in pod \u0026#34;domain1-create-weblogic-sample-domain-job-4l767\u0026#34; is waiting to start: PodInitializing status on iteration 1 of 20 pod domain1-create-weblogic-sample-domain-job-4l767 status is Init:0/1 status on iteration 2 of 20 pod domain1-create-weblogic-sample-domain-job-4l767 status is Running If you see error messages that include the status ImagePullBackOff along with output similar to the following, it is likely your credentials for the Oracle Container Registry have not been successfully conveyed to the AKS cluster.\nFailed to pull image \u0026quot;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026quot;: rpc error: code = Unknown desc = Error response from daemon: Get https://container-registry-phx.oracle.com/v2/middleware/weblogic/manifests/12.2.1.4: unauthorized: authentication required Ensure that the arguments you passed to the script create-docker-credentials-secret.sh are correct with respect to your Oracle SSO credentials.\nThe following example output shows the WebLogic domain was created successfully.\n  Click here to view the example output.   $ ./create-domain.sh -i ~/azure/weblogic-on-aks/my-create-domain-inputs.yaml -o ~/azure -e -v Input parameters being used export version=\u0026quot;create-weblogic-sample-domain-inputs-v1\u0026quot; export adminPort=\u0026quot;7001\u0026quot; export adminServerName=\u0026quot;admin-server\u0026quot; export domainUID=\u0026quot;domain1\u0026quot; export domainHome=\u0026quot;/shared/domains/domain1\u0026quot; export serverStartPolicy=\u0026quot;IF_NEEDED\u0026quot; export clusterName=\u0026quot;cluster-1\u0026quot; export configuredManagedServerCount=\u0026quot;5\u0026quot; export initialManagedServerReplicas=\u0026quot;2\u0026quot; export managedServerNameBase=\u0026quot;managed-server\u0026quot; export managedServerPort=\u0026quot;8001\u0026quot; export image=\u0026quot;store/oracle/weblogic:12.2.1.4\u0026quot; export imagePullPolicy=\u0026quot;IfNotPresent\u0026quot; export imagePullSecretName=\u0026quot;regcred\u0026quot; export productionModeEnabled=\u0026quot;true\u0026quot; export weblogicCredentialsSecretName=\u0026quot;domain1-weblogic-credentials\u0026quot; export includeServerOutInPodLog=\u0026quot;true\u0026quot; export logHome=\u0026quot;/shared/logs/domain1\u0026quot; export httpAccessLogInLogHome=\u0026quot;true\u0026quot; export t3ChannelPort=\u0026quot;30012\u0026quot; export exposeAdminT3Channel=\u0026quot;false\u0026quot; export adminNodePort=\u0026quot;30701\u0026quot; export exposeAdminNodePort=\u0026quot;true\u0026quot; export namespace=\u0026quot;default\u0026quot; javaOptions=-Dweblogic.StdoutDebugEnabled=false export persistentVolumeClaimName=\u0026quot;wls-azurefile\u0026quot; export domainPVMountPath=\u0026quot;/shared\u0026quot; export createDomainScriptsMountPath=\u0026quot;/u01/weblogic\u0026quot; export createDomainScriptName=\u0026quot;create-domain-job.sh\u0026quot; export createDomainFilesDir=\u0026quot;wlst\u0026quot; export serverPodMemoryRequest=\u0026quot;768Mi\u0026quot; export serverPodCpuRequest=\u0026quot;250m\u0026quot; export istioEnabled=\u0026quot;false\u0026quot; export istioReadinessPort=\u0026quot;8888\u0026quot; Generating /home/username/azure/weblogic-domains/domain1/create-domain-job.yaml Generating /home/username/azure/weblogic-domains/domain1/delete-domain-job.yaml Generating /home/username/azure/weblogic-domains/domain1/domain.yaml Checking to see if the secret domain1-weblogic-credentials exists in namespace default Checking if the persistent volume claim wls-azurefile in NameSpace default exists The persistent volume claim wls-azurefile already exists in NameSpace default Wwls 07:15:52.866794 53745 helpers.go:535] --dry-run is deprecated and can be replaced with --dry-run=client. configmap/domain1-create-weblogic-sample-domain-job-cm created Checking the configmap domain1-create-weblogic-sample-domain-job-cm was created configmap/domain1-create-weblogic-sample-domain-job-cm labeled Checking if object type job with name domain1-create-weblogic-sample-domain-job exists No resources found in default namespace. Creating the domain by creating the job /home/weblogic/azure/weblogic-domains/domain1/create-domain-job.yaml job.batch/domain1-create-weblogic-sample-domain-job created Waiting for the job to complete... Error from server (BadRequest): container \u0026quot;create-weblogic-sample-domain-job\u0026quot; in pod \u0026quot;domain1-create-weblogic-sample-domain-job-4l767\u0026quot; is waiting to start: PodInitializing status on iteration 1 of 20 pod domain1-create-weblogic-sample-domain-job-4l767 status is Init:0/1 status on iteration 2 of 20 pod domain1-create-weblogic-sample-domain-job-4l767 status is Running status on iteration 3 of 20 pod domain1-create-weblogic-sample-domain-job-4l767 status is Completed domain.weblogic.oracle/domain1 created Domain domain1 was created and will be started by the WebLogic Kubernetes Operator Administration console access is available at http://wlswls1596-wlsresourcegrou-685ba0-7434b4f5.hcp.eastus.azmk8s.io:30701/console The following files were generated: /home/username/azure/weblogic-domains/domain1/create-domain-inputs.yaml /home/username/azure/weblogic-domains/domain1/create-domain-job.yaml /home/username/azure/weblogic-domains/domain1/domain.yaml Completed    If your output does not show a successful completion, you must troubleshoot the reason and resolve it before proceeding to the next step.\nThis sample creates WebLogic Server pods with reasonable values for memory, CPU, and JVM heap size (as a percentage of memory). You can supply different values. Edit ~/azure/weblogic-on-aks/domain1.yaml and set the desired values for serverPodMemoryRequest, serverPodMemoryLimit, serverPodCpuRequest, serverPodCpuLimit and javaOptions before running ./create-domain.sh -i ~/azure/weblogic-on-aks/domain1.yaml -o ~/azure -e -v.\n Here is an excerpt showing reasonable values:\nserverPodMemoryRequest: \u0026#34;1.5Gi\u0026#34; serverPodCpuRequest: \u0026#34;250m\u0026#34; serverPodMemoryLimit: \u0026#34;1.5Gi\u0026#34; serverPodCpuLimit: \u0026#34;250m\u0026#34; javaOptions: -Dweblogic.StdoutDebugEnabled=false -XX:MinRAMPercentage=25.0 -XX:MaxRAMPercentage=50.0 Notice that the Limit and Request values are the same for each of serverPodMemory and serverPodCpu. This is intentional. To learn why, see Create a Pod that gets assigned a QoS class of Guaranteed. You must have allocated sufficient CPU and memory resources so that the pod can be scheduled for running by Kubernetes. This is an example of capacity planning, a very important Kubernetes success factor. For more details on capacity planning with AKS, see Azure Kubernetes Service Cluster Capacity Planning . For more details about Java and capacity planning, see Java heap size and memory resource considerations.\nThe complete set of values that can be configured in this way is described in configuration parameters. If you want further advanced domain configuration, then run ./create-domain.sh -i ~/azure/weblogic-on-aks/domain1.yaml -o ~/azure, which will output a Kubernetes domain resource YAML file in ~/azure/weblogic-domains/domain.yaml. Edit the domain.yaml file and use kubectl create -f ~/azure/weblogic-domains/domain.yaml to create domain resources.\n  You must create LoadBalancer services for the Administration Server and the WLS cluster. This enables WLS to service requests from outside the AKS cluster.\nUse the sample configuration file kubernetes/samples/scripts/create-weblogic-domain-on-azure-kubernetes-service/domain-on-pv/admin-lb.yaml to create a load balancer service for the Administration Server. If you are choosing not to use the predefined YAML file and instead created new one with customized values, then substitute the following content with your domain values.\napiVersion: v1 kind: Service metadata: name: domain1-admin-server-external-lb namespace: default spec: ports: - name: default port: 7001 protocol: TCP targetPort: 7001 selector: weblogic.domainUID: domain1 weblogic.serverName: admin-server sessionAffinity: None type: LoadBalancer Use the sample configuration file kubernetes/samples/scripts/create-weblogic-domain-on-azure-kubernetes-service/domain-on-pv/cluster-lb.yaml to create a load balancer service for the Managed Servers. If you are choosing not to use the predefined YAML file and instead created new one with customized values, then substitute the following content with you domain values.\napiVersion: v1 kind: Service metadata: name: domain1-cluster-1-lb namespace: default spec: ports: - name: default port: 8001 protocol: TCP targetPort: 8001 selector: weblogic.domainUID: domain1 weblogic.clusterName: cluster-1 sessionAffinity: None type: LoadBalancer Create the load balancer services using the following commands:\n$ kubectl apply -f ~/azure/weblogic-on-aks/admin-lb.yaml service/domain1-admin-server-external-lb created $ kubectl apply -f ~/azure/weblogic-on-aks/cluster-lb.yaml service/domain1-cluster-1-external-lb created After a short time, you will see the Administration Server and Managed Servers running.\nUse the following command to check server pod status:\n$ kubectl get pods --watch It may take you up to 20 minutes to deploy all pods, please wait and make sure everything is ready.\nYou can tail the logs of the Administration Server with this command:\nkubectl logs -f domain1-admin-server The final example of pod output is as following:\n$ kubectl get pods --watch NAME READY STATUS RESTARTS AGE domain1-admin-server 1/1 Running 0 11m domain1-create-weblogic-sample-domain-job-4l767 0/1 Completed 0 13m domain1-managed-server1 1/1 Running 0 3m56s domain1-managed-server2 1/1 Running 0 3m56s weblogic-operator-56654bcdb7-qww7f 1/1 Running 0 25m  If Kubernetes advertises the WebLogic pod as Running you can be assured the WebLogic Server actually is running because the operator ensures that the Kubernetes health checks are actually polling the WebLogic health check mechanism.\n Get the addresses of the Administration Server and Managed Servers (please wait for the external IP addresses to be assigned):\n$ kubectl get svc --watch The final example of service output is as following:\n$ kubectl get svc --watch NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE domain1-admin-server ClusterIP None \u0026lt;none\u0026gt; 30012/TCP,7001/TCP 2d20h domain1-admin-server-ext NodePort 10.0.182.50 \u0026lt;none\u0026gt; 7001:30701/TCP 2d20h domain1-admin-server-external-lb LoadBalancer 10.0.67.79 52.188.176.103 7001:32227/TCP 2d20h domain1-cluster-1-lb LoadBalancer 10.0.112.43 104.45.176.215 8001:30874/TCP 2d17h domain1-cluster-cluster-1 ClusterIP 10.0.162.19 \u0026lt;none\u0026gt; 8001/TCP 2d20h domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 2d20h domain1-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 2d20h internal-weblogic-operator-svc ClusterIP 10.0.192.13 \u0026lt;none\u0026gt; 8082/TCP 2d22h kubernetes ClusterIP 10.0.0.1 \u0026lt;none\u0026gt; 443/TCP 2d22h In the example, the URL to access the Administration Server is: http://52.188.176.103:7001/console. The default user name for the Administration Console is weblogic and the default password is welcome1. Please change this for production deployments.\nIf the WLS Administration Console is still not available, use kubectl describe domain to check domain status.\n$ kubectl describe domain domain1 Make sure the status of cluster-1 is ServersReady and Available.   Click here to view the example status.   Status: Clusters: Cluster Name: cluster-1 Maximum Replicas: 5 Minimum Replicas: 1 Ready Replicas: 2 Replicas: 2 Replicas Goal: 2 Conditions: Last Transition Time: 2020-07-06T05:39:32.539Z Reason: ServersReady Status: True Type: Available Replicas: 2 Servers: Desired State: RUNNING Node Name: aks-nodepool1-11471722-vmss000001 Server Name: admin-server State: RUNNING Cluster Name: cluster-1 Desired State: RUNNING Node Name: aks-nodepool1-11471722-vmss000001 Server Name: managed-server1 State: RUNNING Cluster Name: cluster-1 Desired State: RUNNING Node Name: aks-nodepool1-11471722-vmss000001 Server Name: managed-server2 State: RUNNING Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server3 Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server4 Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server5   \n  To deploy a sample application on WLS, you may skip to the section Deploy sample application. The next section includes a script that automates all of the preceding steps.\nAutomation If you want to automate the above steps of creating AKS cluster and WLS domain, you can use the script kubernetes/samples/scripts/create-weblogic-domain-on-azure-kubernetes-service/create-domain-on-aks.sh.\nThe sample script will create a WLS domain home on the AKS cluster, including:\n Creating a new Azure resource group, with a new Azure Storage Account and Azure File Share to allow WebLogic to persist its configuration and data separately from the Kubernetes pods that run WLS workloads. Creating WLS domain home. Generating the domain resource YAML files, which can be used to restart the Kubernetes artifacts of the corresponding domain.  For input values, you can edit kubernetes/samples/scripts/create-weblogic-domain-on-azure-kubernetes-service/create-domain-on-aks-inputs.yaml directly, or copy the file and edit your copy. The following values must be specified:\n   Name in YAML file Example value Notes     azureServicePrincipalAppId nr086o75-pn59-4782-no5n-nq2op0rsr1q6 Application ID of your service principal; refer to the application ID in the Create Service Principal section.   azureServicePrincipalClientSecret 8693089o-q190-45ps-9319-or36252s3s90 A client secret of your service principal; refer to the client secret in the Create Service Principal section.   azureServicePrincipalTenantId 72s988os-86s1-cafe-babe-2q7pq011qo47 Tenant (Directory ) ID of your service principal; refer to the client secret in the Create Service Principal section.   dockerEmail yourDockerEmail Oracle Single Sign-On (SSO) account email, used to pull the WebLogic Server Docker image.   dockerPassword yourDockerPassword Password for Oracle SSO account, used to pull the WebLogic Server Docker image, in clear text.   dockerUserName yourDockerId The same value as dockerEmail.   namePrefix 0730 Alphanumeric value used as a disambiguation prefix for several Kubernetes resources.    If you don\u0026rsquo;t want to change the other parameters, you can use the default values. Please make sure no extra whitespaces are added!\n# Use ~/azure as output directory, please change it according to your requirement. # cd kubernetes/samples/scripts/create-weblogic-domain-on-azure-kubernetes-service $ cp create-domain-on-aks-inputs.yaml my-create-domain-on-aks-inputs.yaml $ ./create-domain-on-aks.sh -i my-create-domain-on-aks-inputs.yaml -o ~/azure -e The script will print the Administration Server address after a successful deployment. The default user name for the Administration Console is weblogic and the default password is welcome1. Please change this for production deployments. To interact with the cluster using kubectl, use az aks get-credentials as shown in the script output.\nYou now have created an AKS cluster with PersistentVolumeClaim and PersistentVolume to contain the WLS domain configuration files. Using those artifacts, you have used the operator to create a WLS domain.\n Deploy sample application Now that you have WLS running in AKS, you can test the cluster by deploying the simple sample application included in the repository:\n Go to the WebLogic Server Administration Console, Select \u0026ldquo;Lock \u0026amp; Edit\u0026rdquo;. Select Deployments. Select Install. Select Upload your file(s). For the Deployment Archive, Select \u0026ldquo;Choose File\u0026rdquo;. Select the file kubernetes/samples/charts/application/testwebapp.war. Select Next. Choose \u0026lsquo;Install this deployment as an application\u0026rsquo;. Select Next. Select cluster-1 and All servers in the cluster. Select Next. Accept the defaults in the next screen and select Next Select Finish. Select Activate Changes.    Click here to view the application deployment screenshot.     Next you will need to start the application:\n Go to Deployments. Select Control. Select the check box next to testwebapp. Select Start. Select Servicing all requests. Select Yes.  After the successful deployment, go to the application through the domain1-cluster-1-lb external IP.\n$ kubectl get svc domain1-cluster-1-external-lb NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE domain1-cluster-1-external-lb LoadBalancer 10.0.108.249 52.224.248.40 8001:32695/TCP 30m In the example, the application address is: http://52.224.248.40:8001/testwebapp.\nThe test application will list the server host and server IP on the page.\nAccess WebLogic Server logs The logs are stored in the Azure file share. Follow these steps to access the log:\n Go to the Azure Portal. Go to your resource group. Open the storage account. In the \u0026ldquo;File service\u0026rdquo; section of the left panel, select File shares. Select the file share name (e.g. weblogic in this example). Select logs. Select domain1. WebLogic Server logs are listed in the folder.    Click here to view the WebLogic Server logs screenshot.     Clean up resources The output from the create-domain-on-aks.sh script includes a statement about the Azure resources created by the script. To delete the cluster and free all related resources, simply delete the resource groups. The output will list the resource groups, such as.\nThe following Azure resources have been created: Resource groups: ejb8191resourcegroup1597641911, MC_ejb8191resourcegroup1597641911_ejb8191akscluster1597641911_eastus Given the above output, the following Azure CLI commands will delete the resource groups.\n$ az group delete --yes --no-wait --name ejb8191resourcegroup1597641911 $ az group delete --yes --no-wait --name MC_ejb8191resourcegroup1597641911_ejb8191akscluster1597641911_eastus  If you created the AKS cluster step by step, run the following commands to clean up resources.\n$ az group delete --yes --no-wait --name $AKS_PERS_RESOURCE_GROUP $ az group delete --yes --no-wait --name \u0026#34;MC_$AKS_PERS_RESOURCE_GROUP\u0026#34;_\u0026#34;$AKS_CLUSTER_NAME\u0026#34;_\u0026#34;$AKS_PERS_LOCATION\u0026#34; $ az ad sp delete --id $SP_APP_ID  Troubleshooting For troubleshooting advice, see Troubleshooting.\nUseful links  Domain on a PV sample  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/elastic-stack/weblogic-domain/",
	"title": "WebLogic domain",
	"tags": [],
	"description": "Sample for using Fluentd for WebLogic domain and operator&#39;s logs.",
	"content": "Overview This document describes to how to configure a WebLogic domain to use Fluentd to send log information to Elasticsearch.\nHere\u0026rsquo;s the general mechanism for how this works:\n fluentd runs as a separate container in the Administration Server and Managed Server pods. The log files reside on a volume that is shared between the weblogic-server and fluentd containers. fluentd tails the domain logs files and exports them to Elasticsearch. A ConfigMap contains the filter and format rules for exporting log records.  Sample code The samples in this document assume an existing domain is being edited. However, all changes to the domain YAML file can be performed before the domain is created.\nFor sample purposes, this document will assume a domain with the following attributes is being configured:\n Domain name is bobs-bookstore Kubernetes Namespace is bob Kubernetes Secret is bobs-bookstore-weblogic-credentials  The sample Elasticsearch configuration is:\nelasticsearchhost: elasticsearch.bobs-books.sample.com elasticsearchport: 443 elasticsearchuser: bob elasticsearchpassword: changeme Configure log files to use a volume The domain log files must be written to a volume that can be shared between the weblogic-server and fluentd containers. The following elements are required to accomplish this:\n logHome must be a path that can be shared between containers. logHomeEnabled must be set to true so that the logs will be written outside the pod and persist across pod restarts. A volume must be defined on which the log files will reside. In the example, emptyDir is a volume that gets created empty when a pod is created. It will persist across pod restarts but deleting the pod would delete the emptyDir content. The volumeMounts mounts the named volume created with emptyDir and establishes the base path for accessing the volume.  NOTE: For brevity, only the paths to the relevant configuration being added is shown. A complete example of a domain definition is at the end of this document.\nExample: kubectl edit domain bobs-bookstore -n bob and make the following edits:\nspec: logHome: /scratch/logs/bobs-bookstore logHomeEnabled: true serverPod: volumes: - emptyDir: {} name: weblogic-domain-storage-volume volumeMounts: - mountPath: /scratch name: weblogic-domain-storage-volume Add Elasticsearch secrets to WebLogic domain credentials The fluentd container will be configured to look for Elasticsearch parameters in the domain credentials. Edit the domain credentials and add the parameters shown in the example below.\nExample: kubectl edit secret bobs-bookstore-weblogic-credentials -n bob and add the base64 encoded values of each Elasticsearch parameter:\nelasticsearchhost: ZWxhc3RpY3NlYXJjaC5ib2JzLWJvb2tzLnNhbXBsZS5jb20= elasticsearchport: NDQz elasticsearchuser: Ym9i elasticsearchpassword: d2VsY29tZTE= Create Fluentd configuration Create a ConfigMap named fluentd-config in the namespace of the domain. The ConfigMap contains the parsing rules and Elasticsearch configuration.\nHere\u0026rsquo;s an explanation of some elements defined in the ConfigMap:\n The @type tail indicates that tail will be used to obtain updates to the log file. The path of the log file is obtained from the LOG_PATH environment variable that is defined in the fluentd container. The tag value of log records is obtained from the DOMAIN_UID environment variable that is defined in the fluentd container. The \u0026lt;parse\u0026gt; section defines how to interpret and tag each element of a log record. The \u0026lt;match **\u0026gt; section contains the configuration information for connecting to Elasticsearch and defines the index name of each record to be the domainUID.  The following is an example of how to create the ConfigMap:\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: labels: weblogic.domainUID: bobs-bookstore name: fluentd-config namespace: bob data: fluentd.conf: | \u0026lt;match fluent.**\u0026gt; @type null \u0026lt;/match\u0026gt; \u0026lt;source\u0026gt; @type tail path \u0026#34;#{ENV[\u0026#39;LOG_PATH\u0026#39;]}\u0026#34; pos_file /tmp/server.log.pos read_from_head true tag \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}\u0026#34; # multiline_flush_interval 20s \u0026lt;parse\u0026gt; @type multiline format_firstline /^####/ format1 /^####\u0026lt;(?\u0026lt;timestamp\u0026gt;(.*?))\u0026gt;/ format2 / \u0026lt;(?\u0026lt;level\u0026gt;(.*?))\u0026gt;/ format3 / \u0026lt;(?\u0026lt;subSystem\u0026gt;(.*?))\u0026gt;/ format4 / \u0026lt;(?\u0026lt;serverName\u0026gt;(.*?))\u0026gt;/ format5 / \u0026lt;(?\u0026lt;serverName2\u0026gt;(.*?))\u0026gt;/ format6 / \u0026lt;(?\u0026lt;threadName\u0026gt;(.*?))\u0026gt;/ format7 / \u0026lt;(?\u0026lt;info1\u0026gt;(.*?))\u0026gt;/ format8 / \u0026lt;(?\u0026lt;info2\u0026gt;(.*?))\u0026gt;/ format9 / \u0026lt;(?\u0026lt;info3\u0026gt;(.*?))\u0026gt;/ format10 / \u0026lt;(?\u0026lt;sequenceNumber\u0026gt;(.*?))\u0026gt;/ format11 / \u0026lt;(?\u0026lt;severity\u0026gt;(.*?))\u0026gt;/ format12 / \u0026lt;(?\u0026lt;messageID\u0026gt;(.*?))\u0026gt;/ format13 / \u0026lt;(?\u0026lt;message\u0026gt;(.*?))\u0026gt;/ # use the timestamp field in the message as the timestamp # instead of the time the message was actually read time_key timestamp keep_time_key true \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;match **\u0026gt; @type elasticsearch host \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_HOST\u0026#39;]}\u0026#34; port \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PORT\u0026#39;]}\u0026#34; user \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_USER\u0026#39;]}\u0026#34; password \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PASSWORD\u0026#39;]}\u0026#34; index_name \u0026#34;#{ENV[\u0026#39;DOMAIN_UID\u0026#39;]}\u0026#34; scheme https ssl_version TLSv1_2 key_name timestamp types timestamp:time # inject the @timestamp special field (as type time) into the record # so you will be able to do time based queries. # not to be confused with timestamp which is of type string!!! include_timestamp true \u0026lt;/match\u0026gt; EOF Mount the ConfigMap as a volume in the weblogic-server container Edit the domain definition and configure a volume for the ConfigMap containing the fluentd configuration.\nNOTE: For brevity, only the paths to the relevant configuration being added is shown. A complete example of a domain definition is at the end of this document.\nExample: kubectl edit domain bobs-bookstore -n bob and add the following portions to the domain definition.\nspec: serverPod: volumes: - configMap: defaultMode: 420 name: fluentd-config name: fluentd-config-volume Add fluentd container Add a container to the domain that will run fluentd in the Administration Server and Managed Server pods.\nNotice the container definition:\n Defines a LOG_PATH environment variable that points to the log location of bobbys-front-end. Defines ELASTICSEARCH_HOST, ELASTICSEARCH_PORT, ELASTICSEARCH_USER, and ELASTICSEARCH_PASSWORD environment variables that are all retrieving their values from the secret bobs-bookstore-weblogic-credentials. Has volume mounts for the fluentd-config ConfigMap and the volume containing the domain logs.  NOTE: For brevity, only the paths to the relevant configuration being added is shown. A complete example of a domain definition is at the end of this document.\nExample: kubectl edit domain bobs-bookstore -n bob and add the following container definition.\nspec: serverPod: containers: - args: - -c - /etc/fluent.conf env: - name: DOMAIN_UID valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;weblogic.domainUID\u0026#39;] - name: SERVER_NAME valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;weblogic.serverName\u0026#39;] - name: LOG_PATH value: /scratch/logs/bobs-bookstore/$(SERVER_NAME).log - name: FLUENTD_CONF value: fluentd.conf - name: FLUENT_ELASTICSEARCH_SED_DISABLE value: \u0026#34;true\u0026#34; - name: ELASTICSEARCH_HOST valueFrom: secretKeyRef: key: elasticsearchhost name: bobs-bookstore-weblogic-credentials - name: ELASTICSEARCH_PORT valueFrom: secretKeyRef: key: elasticsearchport name: bobs-bookstore-weblogic-credentials - name: ELASTICSEARCH_USER valueFrom: secretKeyRef: key: elasticsearchuser name: bobs-bookstore-weblogic-credentials optional: true - name: ELASTICSEARCH_PASSWORD valueFrom: secretKeyRef: key: elasticsearchpassword name: bobs-bookstore-weblogic-credentials optional: true image: fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3 imagePullPolicy: IfNotPresent name: fluentd resources: {} volumeMounts: - mountPath: /fluentd/etc/fluentd.conf name: fluentd-config-volume subPath: fluentd.conf - mountPath: /scratch name: weblogic-domain-storage-volume Verify logs are exported to Elasticsearch After the Administration Server and Managed Server pods have started with all the changes described above, the logs should now be sent to Elasticsearch.\nYou can check if the fluentd container is successfully tailing the log by executing a command like kubectl logs -f bobs-bookstore-admin-server -n bob fluentd. The log output should look similar to this:\n2019-10-01 16:23:44 +0000 [info]: #0 starting fluentd worker pid=13 ppid=9 worker=0 2019-10-01 16:23:44 +0000 [warn]: #0 /scratch/logs/bobs-bookstore/managed-server1.log not found. Continuing without tailing it. 2019-10-01 16:23:44 +0000 [info]: #0 fluentd worker is now running worker=0 2019-10-01 16:24:01 +0000 [info]: #0 following tail of /scratch/logs/bobs-bookstore/managed-server1.log When you connect to Kibana, you will see an index created for the domainUID.\nExample Kibana log output:\ntimestamp:Oct 1, 2019 4:18:07,111 PM GMT level:Info subSystem:Management serverName:bobs-bookstore-admin-server serverName2: threadName:Thread-8 info1: info2: info3: sequenceNumber:1569946687111 severity:[severity-value: 64] [partition-id: 0] [partition-name: DOMAIN] messageID:BEA-141107 message:Version: WebLogic Server 12.2.1.3.0 Thu Aug 17 13:39:49 PDT 2017 1882952 _id:OQIeiG0BGd1zHsxmUrEJ _type:fluentd _index:bobs-bookstore _score:1 Domain example The following is a complete example of a domain custom resource with a fluentd container configured.\napiVersion: weblogic.oracle/v8 kind: Domain metadata: labels: weblogic.domainUID: bobs-bookstore name: bobs-bookstore namespace: bob spec: adminServer: adminService: channels: - channelName: default nodePort: 32401 - channelName: T3Channel nodePort: 32402 clusters: - clusterName: cluster-1 serverPod: domainHome: /u01/oracle/user_projects/domains/bobs-bookstore domainHomeSourceType: Image domainUID: bobs-bookstore experimental: istio: enabled: true readinessPort: 8888 image: phx.ocir.io/bobs-bookstore imagePullPolicy: IfNotPresent imagePullSecrets: - name: ocir includeServerOutInPodLog: true logHome: /scratch/logs/bobs-bookstore logHomeEnabled: true replicas: 2 serverPod: containers: - args: - -c - /etc/fluent.conf env: - name: DOMAIN_UID valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;weblogic.domainUID\u0026#39;] - name: SERVER_NAME valueFrom: fieldRef: fieldPath: metadata.labels[\u0026#39;weblogic.serverName\u0026#39;] - name: LOG_PATH value: /scratch/logs/bobs-bookstore/$(SERVER_NAME).log - name: FLUENTD_CONF value: fluentd.conf - name: FLUENT_ELASTICSEARCH_SED_DISABLE value: \u0026#34;true\u0026#34; - name: ELASTICSEARCH_HOST valueFrom: secretKeyRef: key: elasticsearchhost name: bobs-bookstore-weblogic-credentials - name: ELASTICSEARCH_PORT valueFrom: secretKeyRef: key: elasticsearchport name: bobs-bookstore-weblogic-credentials - name: ELASTICSEARCH_USER valueFrom: secretKeyRef: key: elasticsearchuser name: bobs-bookstore-weblogic-credentials optional: true - name: ELASTICSEARCH_PASSWORD valueFrom: secretKeyRef: key: elasticsearchpassword name: bobs-bookstore-weblogic-credentials optional: true image: fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3 imagePullPolicy: IfNotPresent name: fluentd resources: {} volumeMounts: - mountPath: /fluentd/etc/fluentd.conf name: fluentd-config-volume subPath: fluentd.conf - mountPath: /scratch name: weblogic-domain-storage-volume env: - name: JAVA_OPTIONS value: -Dweblogic.StdoutDebugEnabled=false - name: USER_MEM_ARGS value: \u0026#39;-Djava.security.egd=file:/dev/./urandom -Xms64m -Xmx256m \u0026#39; - name: WL_HOME value: /u01/oracle/wlserver - name: MW_HOME value: /u01/oracle volumeMounts: - mountPath: /scratch name: weblogic-domain-storage-volume volumes: - emptyDir: {} name: weblogic-domain-storage-volume - configMap: defaultMode: 420 name: fluentd-config name: fluentd-config-volume serverStartPolicy: IF_NEEDED webLogicCredentialsSecret: name: bobs-bookstore-weblogic-credentials "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/faq/namespace-management/",
	"title": "Managing domain namespaces",
	"tags": [],
	"description": "Considerations for managing namespaces while the operator is running.",
	"content": "Each operator deployment manages a number of Kubernetes namespaces. For more information, see Operator Helm configuration values. A number of Kubernetes resources must be present in a namespace before any WebLogic Server instances can be successfully started. Those Kubernetes resources are created either as part of the installation of a release of the operator\u0026rsquo;s Helm chart, or created by the operator.\nThis FAQ describes some considerations to be aware of when you manage the namespaces while the operator is running. For example:\n Check the namespaces that the operator manages Add a namespace for the operator to manage Delete a namespace from the operator\u0026rsquo;s domain namespace list Delete and recreate a Kubernetes Namespace that the operator manages  For others, see Common Mistakes and Solutions.\nThere can be multiple operators in a Kubernetes cluster, and in that case, you must ensure that the namespaces managed by these operators do not overlap.\n Check the namespaces that the operator manages Prior to version 3.1.0, the operator supported specifying the namespaces that it would manage only through a list. Now, the operator supports a list of namespaces, a label selector, or a regular expression matching namespace names.\nFor operators that specify namespaces by a list, you can find the list of the namespaces using the helm get values command. For example, the following command shows all the values of the operator release weblogic-operator; the domainNamespaces list contains default and ns1:\n$ helm get values weblogic-operator domainNamespaces: - default - ns1 elasticSearchHost: elasticsearch.default.svc.cluster.local elasticSearchPort: 9200 elkIntegrationEnabled: false externalDebugHttpPort: 30999 externalRestEnabled: false externalRestHttpsPort: 31001 image: ghcr.io/oracle/weblogic-kubernetes-operator:3.3.2 imagePullPolicy: IfNotPresent internalDebugHttpPort: 30999 javaLoggingLevel: INFO logStashImage: logstash:6.6.0 remoteDebugNodePortEnabled: false serviceAccount: default suspendOnDebugStartup: false For operators that select namespaces with a selector, simply list namespaces using that selector:\n$ kubectl get ns --selector=\u0026#34;weblogic-operator=enabled\u0026#34; For operators that select namespaces with a regular expression matching the name, you can use a combination of kubectl and any command-line tool that can process the regular expression, such as grep:\n$ kubectl get ns -o go-template=\u0026#39;{{range .items}}{{.metadata.name}}{{\u0026#34;\\n\u0026#34;}}{{end}}\u0026#39; | grep \u0026#34;^weblogic\u0026#34; If you don\u0026rsquo;t know the release name of the operator, you can use helm list to list all the releases for a specified namespace or all namespaces:\n$ helm list --namespace \u0026lt;namespace\u0026gt; $ helm list --all-namespaces Add a Kubernetes namespace to the operator When the operator is configured to manage a list of namespaces and you want the operator to manage an additional namespace, you need to add the namespace to the operator\u0026rsquo;s domainNamespaces list. Note that this namespace has to already exist, for example, using the kubectl create command.\nAdding a namespace to the domainNamespaces list tells the operator to initialize the necessary Kubernetes resources so that the operator is ready to manage WebLogic Server instances in that namespace.\nWhen the operator is managing the default namespace, the following example Helm command adds the namespace ns1 to the domainNamespaces list, where weblogic-operator is the release name of the operator, and kubernetes/charts/weblogic-operator is the location of the operator\u0026rsquo;s Helm charts:\n$ helm upgrade \\  weblogic-operator \\  kubernetes/charts/weblogic-operator \\  --reuse-values \\  --set \u0026#34;domainNamespaces={default,ns1}\u0026#34; \\  --wait You can verify that the operator has initialized a namespace by confirming the existence of the required configmap resource.\n$ kubetctl get cm -n \u0026lt;namespace\u0026gt; For example, the following example shows that the domain configmap resource exists in the namespace ns1.\n$ kubectl get cm -n ns1 NAME DATA AGE weblogic-scripts-cm 14 12m For operators configured to select managed namespaces through the use of a label selector or regular expression, you simply need to create a namespace with the appropriate labels or with a name that matches the expression, respectively.\nIf you did not choose to enable the value, enableClusterRoleBinding, then the operator will not have the necessary permissions to manage the namespace. You can do this by performing a helm upgrade with the values used when installing the Helm release:\n$ helm upgrade \\  weblogic-operator \\  kubernetes/charts/weblogic-operator \\  --reuse-values Delete a Kubernetes namespace from the operator When the operator is configured to manage a list of namespaces and you no longer want a namespace to be managed by the operator, you need to remove it from the operator\u0026rsquo;s domainNamespaces list, so that the resources that are associated with the namespace can be cleaned up.\nWhile the operator is running and managing the default and ns1 namespaces, the following example Helm command removes the namespace ns1 from the domainNamespaces list, where weblogic-operator is the release name of the operator, and kubernetes/charts/weblogic-operator is the location of the operator Helm charts:\n$ helm upgrade \\  --reuse-values \\  --set \u0026#34;domainNamespaces={default}\u0026#34; \\  --wait \\  --force \\  weblogic-operator \\  kubernetes/charts/weblogic-operator For operators configured to select managed namespaces through the use of a label selector or regular expression, you simply need to delete the namespace. For the label selector option, you can also adjust the labels on the namespace so that the namespace no longer matches the selector.\nRecreate a previously deleted Kubernetes namespace When the operator is configured to manage a list of namespaces and if you need to delete a namespace (and the resources in it) and then recreate it, remember to remove the namespace from the operator\u0026rsquo;s domainNamespaces list after you delete the namespace, and add it back to the domainNamespaces list after you recreate the namespace using the helm upgrade commands that were illustrated previously.\nIf a domain custom resource is created before the namespace is ready, you might see that the introspector job pod fails to start, with a warning like the following, when you review the description of the introspector pod. Note that domain1 is the name of the domain in the following example output.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned domain1-introspector-bz6rw to slc16ffk Normal SuccessfulMountVolume 1m kubelet, slc16ffk MountVolume.SetUp succeeded for volume \u0026quot;weblogic-credentials-volume\u0026quot; Normal SuccessfulMountVolume 1m kubelet, slc16ffk MountVolume.SetUp succeeded for volume \u0026quot;default-token-jzblm\u0026quot; Warning FailedMount 27s (x8 over 1m) kubelet, slc16ffk MountVolume.SetUp failed for volume \u0026quot;weblogic-scripts-cm-volume\u0026quot; : configmaps \u0026quot;weblogic-scripts-cm\u0026quot; not found If you still run into problems after you perform the helm upgrade to re-initialize a namespace that is deleted and recreated, you can restart the operator pod as shown in the following examples, where the operator itself is running in the namespace weblogic-operator-namespace with the release name, weblogic-operator.\n Kill the operator pod, and let Kubernetes restart it.  $ kubectl delete pod/weblogic-operator-65b95bc5b5-jw4hh -n weblogic-operator-namespace  Scale the operator deployment to 0 and then back to 1 by changing the value of the replicas.  $ kubectl scale deployment.apps/weblogic-operator -n weblogic-operator-namespace --replicas=0 $ kubectl scale deployment.apps/weblogic-operator -n weblogic-operator-namespace --replicas=1 Note that restarting the operator pod makes the operator temporarily unavailable for managing its namespaces. For example, a domain that is created while the operator is restarting will not be started until the operator pod is fully up again.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/cicd/why-layering-matters/",
	"title": "Why layering matters",
	"tags": [],
	"description": "Learn why container image layering affects CI/CD processes.",
	"content": "How does layering affect our CI/CD process? Now that we know more about layering, let’s talk about why it is important to our CI/CD process. Let\u0026rsquo;s consider the kinds of updates we might want to make to our domain:\nYou might want to update the domain by:\n Installing a patch on the operating system or a library. Updating the version of the JDK you are using. Picking up a new version of WebLogic Server. Installing patches on WebLogic Server. Updating the domain configuration, for example:  Adding or changing a resource like a data source or queue. Installing or updating applications. Changing various settings in the domain configuration.    If we just want to update the domain configuration itself, that is the top layer, then it is pretty easy. We can make the necessary changes and save a new version of that layer, and then roll the domain. We could also choose to just build another layer on top of the existing top layer that contains our delta. If the change is small, then we will just end up with another small layer, and as we have seen, the small layers are no problem.\nBut consider a more complicated scenario - let\u0026rsquo;s take updating the JDK as an example to understand the impact of layers. Say we want to update from JDK 8u201 to 8u202 as shown in the example above. If we took the \u0026ldquo;your first domain\u0026rdquo; image and updated the JDK, then we would end up with a new layer on top containing JDK 8u202. That other layer with JDK 8u201 is still there; even if we \u0026ldquo;delete\u0026rdquo; the directory, we don\u0026rsquo;t get that space back. So now our 1.5GB \u0026ldquo;image\u0026rdquo; has grown to 1.75GB. This is not ideal, and the more often we try to change lower layers, the worse it gets.\nYou might be asking, \u0026ldquo;Can\u0026rsquo;t we just swap out the JDK layer for a new one?\u0026rdquo; That is an excellent question, but the unfortunate reality today is that there is no reliable way to do that. There are various attempts to create a \u0026ldquo;rebasing\u0026rdquo; capability for Docker that would enable such an action, but some research will show you that they are mostly abandoned due to limited documentation of how the layering works at the level of detail needed to implement something like this.\nNext you might think, \u0026ldquo;Oh, that’s ok, we can just rebuild the layers above the JDK on top of this new layer.\u0026rdquo; That is very true, we can. But there is a big caveat here for Domain in Image domains. When you create a WebLogic domain, a domain encryption key is created. This key is stored in the security/SerializedSystemIni.dat file in your domain and it is used to encrypt several other things in your domain configuration, like passwords, for example. Today (in WebLogic Server 12.2.1.4.0) there is no way to conveniently \u0026ldquo;extract\u0026rdquo; or \u0026ldquo;reuse\u0026rdquo; this encryption key. So what does this mean in practice?\nIf you recreate a Domain in Image domain in your CI/CD process, even though you may end up with a domain that is for all intents and purposes identical to the previous domain, it will have a different encryption key.\n This means that technically, it is a \u0026ldquo;different\u0026rdquo; domain for Domain in Image type domains. Does this matter? Maybe, maybe not. It depends. If you want to do a rolling restart of your domain, then yes, it matters. First of all, the \u0026ldquo;new\u0026rdquo; servers will fail to start because the operator will be trying to inject credentials to start the server which were encrypted with the \u0026ldquo;old\u0026rdquo; domain encryption key.\nBut even if this did not prevent Domain in Image pods from starting, there would still be a problem. You cannot have members of a domain with different encryption keys. If WebLogic saw a new member trying to join the domain with a different key, it would consider it to be an intruder and refuse to accept it into the domain. Client HTTP sessions would not work across the two different sets of servers, so clients could see errors and need to retry. Worse, if these two different sets of servers tried to access the same resources this could lead to data corruption.\nSo what can we do? Well, we could not roll the domain, but instead completely shut down the old version first, and then start up the new one. This way we avoid any issues with incompatibilities, but we do introduce a brief outage. This may be acceptable, or it may not.\nAnother option is to find a way to keep the \u0026ldquo;same\u0026rdquo; domain, that is, the same domain encryption key, so that we can still roll the domain and there will be no conflicts.\nMutating Domain in Image domain home configuration without losing encryption keys If we want to make a change in a lower layer in Domain in Image domains without losing our domain encryption keys, then we need to find a way to \u0026ldquo;save\u0026rdquo; the domain and then put it back into a new layer, later, on top of the other new (lower) layers, as depicted in the image below:\nThe process looks like this:\n From our existing image (left), we extract the domain into some kind of archive. Then we start with the new JDK image which was built on top of the same base image (or we build it ourselves, if needed). We build a new WebLogic layer (or grab the one that Oracle built for us) on top of this new JDK. Then we need to “restore” our domain from the archive into a new layer.  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/faq/cannot-pull-image/",
	"title": "Cannot pull image",
	"tags": [],
	"description": "My domain will not start and I see errors like `ImagePullBackoff` or `Cannot pull image`.",
	"content": " My domain will not start and I see errors like ImagePullBackoff or Cannot pull image\n When you see these kinds of errors, it means that Kubernetes cannot find your container image. The most common causes are:\n The image value in your Domain is set incorrectly, meaning Kubernetes will be trying to pull the wrong image. The image requires authentication or permission in order to pull it and you have not configured Kubernetes with the necessary credentials, for example in an imagePullSecret. You built the image on a machine that is not where your kubelet is running and Kubernetes cannot see the image, meaning you need to copy the image to the worker nodes or put it in a container registry that is accessible the to all of the worker nodes.  Let\u0026rsquo;s review what happens when Kubernetes starts a pod.\nThe definition of the pod contains a list of container specifications. Each container specification contains the name (and optionally, tag) of the image that should be used to run that container. In the example above, there is a container called c1 which is configured to use the container image some.registry.com/owner/domain1:1.0. This image name is in the format registry address / owner / name : tag, so in this case the registry is some.registry.com, the owner is owner, the image name is domain and the tag is 1.0. Tags are a lot like version numbers, but they are not required to be numbers or to be in any particular sequence or format. If you omit the tag, it is assumed to be latest.\nThe tag latest is confusing - it does not actually mean the latest version of the image that was created or published in the registry; it just literally means whichever version the owner decided to call \u0026ldquo;latest\u0026rdquo;. Docker and Kubernetes make some assumptions about latest, and it is generally recommended to avoid using it and instead specify the actual version or tag that you really want.\n First, Kubernetes will check to see if the requested image is available in the local container image store on whichever worker node the pod was scheduled on. If it is there, then it will use that image to start the container. If it is not there, then Kubernetes will attempt to pull the image from a remote container registry.\nThere is another setting called imagePullPolicy that can be used to force Kubernetes to always pull the image, even if it is already present in the local container image store.\n If the image is available in the remote registry and it is public, that is it does not require authentication, then Kubernetes will pull the image to the local container image store and start the container.\nImages that require authentication If the remote container registry requires authentication, then you will need to provide the authentication details in a Kubernetes docker-registry secret and tell Kubernetes to use that secret when pulling the image.\nTo create a secret, you can use the following command:\n$ kubectl create secret docker-registry secret1 \\  --docker-server=some.registry.com \\  --docker-username=bob \\  --docker-password=bigSecret \\  --docker-email=bob@some.com \\  --namespace=default In this command, you would replace secret1 with the name of the secret; the docker-server is set to the registry name, without the https:// prefix; the docker-username, docker-password and docker-email are set to match the credentials you use to authenticate to the remote container registry; and the namespace must be set to the same namespace where you intend to use the image.\nSome registries may need a suffix making the docker-server something like some.registry.com/v2 for example. You will need to check with your registry provider\u0026rsquo;s documentation to determine if this is needed.\n After the secret is created, you need to tell Kubernetes to use it. This is done by adding an imagePullSecret to your Kubernetes YAML file. In the case of a WebLogic domain, you add the secret name to the imagePullSecret in the domain custom resource YAML file.\nHere is an example of part of a domain custom resource file with the imagePullSecret above specified:\napiVersion: \u0026#34;weblogic.oracle/v8\u0026#34; kind: Domain metadata: name: domain1 namespace: default labels: weblogic.domainUID: domain1 spec: domainHome: /u01/oracle/user_projects/domains/domain1 domainHomeSourceType: Image image: \u0026#34;some.registry.com/owner/domain1:1.0\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; imagePullSecrets: - name: secret1 Alternatively, you can associate the secret with the service account that will be used to run the pod. If you do this, then you will not need to add the imagePullSecret to the domain resource. This is useful if you are running multiple domains in the same namespace.\nTo add the secret shown above to the default service account in the weblogic namespace, you would use a command like this:\n$ kubectl patch serviceaccount default \\  -n weblogic \\  -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;secret1\u0026#34;}]}\u0026#39;  You can provide multiple imagePullSecrets if you need to pull container images from multiple remote container registries or if your images require different authentication credentials. For more information, see Container Image Protection.\n Pushing the image to a repository If you have an image in your local repository that you would like to copy to a remote repository, then the Docker steps are:\n Use docker login to log in to the target repository\u0026rsquo;s registry. For example:  $ docker login some.registry.com -u username -p password  Use docker tag to mark the image with the target registry, owner, repository name, and tag. For example:  $ docker tag domain1:1.0 some.registry.com/owner/domain1:1.0  Use docker push to push the image to the repository. For example:  $ docker push some.registry.com/owner/domain1:1.0 Manually copying the image to your worker nodes If you are not able to use a remote container registry, for example if your Kubernetes cluster is in a secure network with no external access, then you can manually copy the container images to the cluster instead.\nOn the machine where you created the image, export it into a TAR file using this command:\n$ docker save domain1:1.0 \u0026gt; domain1.tar Then copy that TAR file to each worker node in your Kubernetes cluster and run this command on each node:\n$ docker load \u0026lt; domain1.tar Restart pods to clear the error After you have ensured that the images are accessible on all worker nodes, you may need to restart the pods so that Kubernetes will attempt to pull the images again. You can do this by deleting the pods themselves, or deleting the Domain and then recreating it.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/security/domain-security/",
	"title": "Domain security",
	"tags": [],
	"description": "WebLogic domain security and the operator",
	"content": "  Container image protection  WebLogic domain in image protection\n External network access security  Remote access security\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/security/domain-security/weblogic-channels/",
	"title": "External network access security",
	"tags": [],
	"description": "Remote access security",
	"content": "WebLogic T3 and administrative channels Oracle recommends not exposing any administrative, RMI, or T3 channels outside the Kubernetes cluster unless absolutely necessary.\n If exposing an administrative, RMI, EJB, JMS, or T3 capable channel using a load balancer, port forwarding, NodePorts, or similar, then limit access by using a custom dedicated WebLogic Server port that you have configured with the T3 or administration protocol (a network access point) instead of relaying the traffic to a default port, leverage two-way SSL, use controls like security lists, and/or set up a Bastion to provide access. A custom channel is preferred over a default channel because a default port supports multiple protocols.\nWhen accessing T3 or RMI based channels for administrative purposes, such as running WLST, the preferred approach is to kubectl exec into the Kubernetes Pod and then run wlst.sh, or set up Bastion access and then run java weblogic.WLST or $ORACLE_HOME/oracle_common/common/bin/wlst.sh from the Bastion host to connect to the Kubernetes cluster (some cloud environments use the term Jump Host or Jump Server instead of Bastion).\nAlso, if you need to use cross-domain T3 access between clouds, data centers, and such, consider a private VPN.\nWebLogic HTTP channels When providing remote access to HTTP using a load balancer, port forwarding, NodePorts, or similar, Oracle recommends relaying the traffic to a dedicated WebLogic Server port that you have configured using a custom HTTP channel (network access point) instead of relaying the traffic to a default port. This helps ensure that external traffic is limited to the HTTP protocol. A custom HTTP channel is preferred over a default port because a default port supports multiple protocols.\nDo not enable tunneling on an HTTP channel that is exposed for remote access unless you specifically intend to allow it to handle T3 traffic (tunneling allows T3 to tunnel through the channel using HTTP) and you perform the additional steps that may be necessary to further secure access, as described in WebLogic T3 and administrative channels.\nLimit use of Kubernetes NodePorts Although Kubernetes NodePorts are good for use in demos and getting-started guides, they are typically not suited for production systems for multiple reasons, including:\n With some cloud providers, a NodePort may implicitly expose a port to the public Internet. They bypass almost all network security in Kubernetes. They allow all protocols (load balancers can limit to the HTTP protocol). They cannot expose standard, low-numbered ports like 80 and 443 (or even 8080 and 8443). Some Kubernetes cloud environments cannot expose usable NodePorts because their Kubernetes clusters run on a private network that cannot be reached by external clients.  General advice   Set up administration ports: Configure an administration port on WebLogic, or an administrative channel, to prevent all other channels from accepting administration-privileged traffic (this includes preventing administration-privileged traffic from a WebLogic console over HTTP).\n  Be aware of anonymous defaults: If an externally available port supports a protocol suitable for WebLogic JNDI, EJB/RMI, or JMS clients, then note that by default:\n WebLogic enables anonymous users to access such a port. JNDI entries, EJB/RMI applications, and JMS are open to anonymous users.    Configure SSL: You can configure two-way SSL to help prevent external access by unwanted applications (often SSL is setup between the caller and the load balancer, and plain-text traffic flows internally from the load balancer to WebLogic).\n  See also  External WebLogic clients Remote Console, Administration Console, WLST, and Port Forwarding access  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/domain-lifecycle/restarting/",
	"title": "Restarting",
	"tags": [],
	"description": "This document describes when WebLogic Server instances should and will be restarted in the Kubernetes environment.",
	"content": "This document describes when WebLogic Server instances should and will be restarted in the Kubernetes environment.\nOverview There are many situations where changes to the WebLogic or Kubernetes environment configuration require that all the servers in a domain or cluster be restarted, for example, when applying a WebLogic Server patch or when upgrading an application.\nOne of the operator\u0026rsquo;s most important jobs is to start and stop WebLogic Server instances by creating and deleting their corresponding Kubernetes pods. Sometimes, you need to make changes that make the pods obsolete, therefore the pods need to be deleted and recreated. Depending on the change, often the pods can be gradually recreated, without taking the domain or cluster out of service (for example, \u0026ldquo;rolling restarts\u0026rdquo;) and sometimes all the pods need to be deleted and then recreated as part of a downtime (for example, \u0026ldquo;full restarts\u0026rdquo;).\nThe following types of server restarts are supported by the operator:\n  Rolling restarts - a coordinated and controlled shut down of all of the servers in a domain or cluster while ensuring that service to the end user is not interrupted.\n  Operator initiated - where the WebLogic Kubernetes Operator can detect some types of changes and will automatically initiate rolling restarts of pods in a domain or cluster.\n  Manually initiated - required when certain changes in the Oracle WebLogic Server in Kubernetes environment cannot be detected by the operator, so a rolling restart must be manually initiated.\n    Full domain restarts - the Administration Server and all the Managed Servers in a domain are shutdown, impacting service availability to the end user, and then restarted. Unlike a rolling restart, the operator cannot detect and initiate a full domain restart; it must always be manually initiated.\n  For detailed information on how to restart servers using the operator, see Starting, stopping, and restarting servers.\nCommon restart scenarios This document describes what actions you need to take to properly restart your servers for a number of common scenarios:\n Modifying the WebLogic domain configuration Changing the domain configuration overrides (also called situational configuration) for Domain in PV and Domain in Image domains Changing the model files for Model in Image domains Changing the WebLogic Server credentials (the user name and password) Changing fields on the Domain that affect WebLogic Server instance Pod generation (such as image, volumes, and env) Applying WebLogic Server patches Updating deployed applications for Domain in Image or Model in Image  Use cases Modifying the WebLogic domain configuration Changes to the WebLogic domain configuration may require either a rolling or full domain restart depending on the domain home location and the type of configuration change.\nDomain in Image For Domain in Image, you may only perform a rolling restart if both the WebLogic configuration changes between the present image and a new image are dynamic and you have followed the CI/CD guidelines to create an image with compatible encryption keys.\nOtherwise, use of a new image that does not have compatible encryption keys or any non-dynamic configuration changes require a full domain restart.\n If you create a new image with a new name, then you must avoid a rolling restart, which can cause unexpected behavior for the running domain due to configuration inconsistencies as seen by the various servers, by following the steps in Avoiding a rolling restart when changing image field on a Domain. If you create a new image with the same name, then you must manually initiate a full domain restart. See Full domain restarts.  Model in Image   Any image that supplies configuration changes that are incompatible with the current running domain require a full shut down before changing the Domain image field, instead of a rolling restart. For changes that support a rolling restart, see Supported and unsupported updates.\n  If you create a new image with a new name, and you want to avoid a rolling restart, see Avoiding a rolling restart when changing the image field on a Domain.\n  If you create a new image with the same name, then you must manually initiate either a full domain restart or rolling restart for pods to run with the new image. To initiate a full restart, see Full domain restarts. To initiate a rolling restart, change the value of your Domain restartVersion field. See Restarting servers and Rolling restarts.\n  If you are supplying updated models or Secrets for a running domain, then see Runtime updates.\n  Domain in PV For Domain in PV, the type of restart needed depends on the nature of the WebLogic domain configuration change:\n Domain configuration changes that add new clusters (either configured or dynamic), member servers for these new clusters, or non-clustered servers can now be performed dynamically. This support requires that the new clusters or servers are added to the domain configuration and then that you initiate the operator\u0026rsquo;s introspection of that new configuration. Other changes to parts of the domain configuration that the operator introspects, require a full shutdown and restart, even if the changes are dynamic for WebLogic Server, such as:  Adding or removing a network access point Adding a server to an existing cluster Changing a cluster, server, dynamic server, or network access point name Enabling or disabling the listen port, SSL port, or admin port Changing any port numbers Changing a network access point\u0026rsquo;s public address   Other dynamic WebLogic configuration changes do not require a restart. For example, a change to a server\u0026rsquo;s connection timeout property is dynamic and does not require a restart. Other non-dynamic domain configuration changes require either a manually initiated rolling restart or a full domain shut down and restart, depending on the nature of the change.  For example, a rolling restart is applicable when changing a WebLogic Server stuck thread timer interval property. See Restart all the servers in the domain.    The preceding description of the operator\u0026rsquo;s life cycle of responding to WebLogic domain configuration changes applies to version 3.0.0 and later. Prior to operator version 3.0.0, while you could make changes to WebLogic domain configuration using the Administration Console or WLST, the operator would only detect and respond to those changes following a full domain shut down and restart.\n Changing the domain configuration overrides Beginning with operator version 3.0.0, many changes to domain configuration overrides can be applied dynamically or as part of a rolling restart. Previously, any changes to the configuration overrides required a full domain shutdown and restart. Changes to configuration overrides include:\n Changing the Domain YAML file\u0026rsquo;s configuration.overridesConfigMap to point to a different ConfigMap Changing the Domain YAML file\u0026rsquo;s configuration.secrets to point to a different list of Secrets Changing the contents of the ConfigMap referenced by configuration.overridesConfigMap Changing the contents to any of the Secrets referenced by configuration.secrets  The changes to the above fields or contents of related resources are not processed automatically. Instead, these fields are processed only when you initiate operator introspection. The operator then will apply the new configuration overrides dynamically or only apply the overrides when WebLogic Server instances restart, depending on the strategy that you select.\nChanges to configuration overrides distributed to running WebLogic Server instances can only take effect if the corresponding WebLogic configuration MBean attribute is \u0026ldquo;dynamic\u0026rdquo;. For instance, the Data Source \u0026ldquo;passwordEncrypted\u0026rdquo; attribute is dynamic while the \u0026ldquo;Url\u0026rdquo; attribute is non-dynamic.\n Changing the WebLogic Server credentials A change to the WebLogic Server credentials (the user name and password), contained in the Kubernetes Secret for the domain, requires a full domain restart. The Kubernetes Secret can be updated directly or a new Secret can be created and then referenced by the webLogicCredentialsSecret field in the Domain YAML file.\nChanging fields on the Domain that affect WebLogic Server instance Pods The operator will initiate a rolling restart of the domain when you modify any of the Domain YAML file fields that affect the WebLogic Server instance Pod generation, such as image, volumes, and env. For a complete list, see Fields that cause servers to be restarted.\nYou can modify these fields using the kubectl command-line tool\u0026rsquo;s edit and patch commands or through the Kubernetes REST API.\nFor example, to edit the Domain YAML file directly using the kubectl command-line tool:\n$ kubectl edit domain \u0026lt;domain name\u0026gt; -n \u0026lt;domain namespace\u0026gt; The edit command opens a text editor which lets you edit the Domain in place.\nTypically, it\u0026rsquo;s better to edit the Domain YAML file directly; otherwise, if you scaled the domain, and you edit only the original domain.yaml file and reapply it, you could go back to your old replicas count.\n Applying WebLogic Server patches Oracle provides different types of patches for WebLogic Server, such as Patch Set Updates, Security Patch Updates, and One-Off patches. Information on whether a patch is rolling-compatible or requires a manual full domain restart usually can be found in the patch\u0026rsquo;s documentation, such as the README file.\nWebLogic Server patches can be applied to either a domain home in image or a domain home on PV.\nWith rolling-compatible patches:\n If you update the image property with a new image name, then the operator will initiate a rolling restart. If you keep the same image name, then you must manually initiate a rolling restart. See Restart all the servers in the domain.  With patches that are not rolling-compatible:\n If you keep the same image name, then you must manually initiate a full domain restart. See Full domain restarts. If you update the image property with a new image name, then you must avoid the rolling restart by following the steps in Avoiding a rolling restart when changing the image field on a Domain.  Updating deployed applications Frequent updates of deployed applications using a continuous integration/continuous delivery (CI/CD) process is a very common use case. The process for applying an updated application is different for domain home in image and model in image than it is for domain home on PV. A rolling-compatible application update is where some servers are running the old version and some are running the new version of the application during the rolling restart process. On the other hand, an application update that is not rolling-compatible requires that all the servers in the domain be shut down and restarted.\nIf the application update is rolling-compatible:\n If you update the image property with a new image name, then the operator will initiate a rolling restart. If you keep the same image name, then you must manually initiate a rolling restart. See Restart all the servers in the domain.  If the application update is not rolling-compatible:\n If you keep the same image name, then you must manually initiate a full domain restart. See Full domain restarts. If you update the image property with a new image name, then you must avoid the rolling restart by following the steps in Avoiding a rolling restart when changing the image field on a Domain.  Rolling out an updated domain home in image or model in image Follow these steps to create new rolling-compatible image if you only need to patch your WebLogic Server domain or update application deployment files:\na. Select a different name for the new image.\nb. For Domain in Image, it is important to keep your original domain home in your new image.\nUsing the same domain home-in-image image as a base, create a new image by copying (COPY command in a Dockerfile) the updated application deployment files or WebLogic Server patches into the image during the image build.\nThe key here is to make sure that you do not re-run WLST or WDT to create a new domain home even though it will have the same configuration. Creating a new domain will change the domain encryption secret and you won\u0026rsquo;t be able to do a rolling restart.\n c. Deploy the new image to your container registry with the new name.\nd. Update the image field of the Domain YAML file, specifying the new image name.\nFor example:\n ```yaml domain: spec: image: ghcr.io/oracle/weblogic-updated:3.3.2 ```  e. The operator will now initiate a rolling restart, which will apply the updated image, for all the servers in the domain.\nAvoiding a rolling restart when changing the image field on a Domain If you\u0026rsquo;ve created a new image that is not rolling-compatible, and you\u0026rsquo;ve changed the image name, then:\n  Bring the domain down (stopping all the server pods) by setting the serverStartPolicy to NEVER. See Shut down all the servers.\n  Update the image property with a new image name.\n  Start up the domain (starting all the server pods) by setting the serverStartPolicy to IF_NEEDED.\n  Other considerations for restarting a domain   Consider the order of changes:\nIf you need to make multiple changes to your domain at the same time, you\u0026rsquo;ll want to be careful about the order in which you do your changes, so that servers aren\u0026rsquo;t restarted prematurely or restarted needlessly. For example, if you want to change the readiness probe\u0026rsquo;s tuning parameters and the Java options (both of which are rolling-compatible), then you should update the Domain YAML file once, changing both values, so that the operator rolling restarts the servers once. Or, if you want to change the readiness probe\u0026rsquo;s tuning parameters (which is rolling-compatible) and change the domain customizations (which require a full restart), then you should do a full shutdown first, then make the changes, and then restart the servers.\nAlternatively, if you know that your set of changes are not rolling-compatible, then you must avoiding a rolling restart by:\n  Bringing the domain down (stopping all the server pods) by setting the serverStartPolicy to NEVER. See Shut down all the servers.\n  Make all your changes to the Oracle WebLogic Server in Kubernetes environment.\n  Starting up the domain (starting all the server pods) by setting the serverStartPolicy to IF_NEEDED.\n    Changes that require domain knowledge.\nSometimes you need to make changes that require server restarts, yet the changes are not to the domain configuration, the image, or the Kubernetes resources that register your domain with the operator. For example, your servers are caching information from an external database and you\u0026rsquo;ve modified the contents of the database.\nIn these cases, you must manually initiate a restart.\n  Managed Coherence Servers safe shut down.\nIf the domain is configured to use a Coherence cluster, then you will need to increase the Kubernetes graceful timeout value. When a server is shut down, Coherence needs time to recover partitions and rebalance the cluster before it is safe to shut down a second server. Using the Kubernetes graceful termination feature, the operator will automatically wait until the Coherence HAStatus MBean attribute indicates that it is safe to shut down the server. However, after the graceful termination timeout expires, the pod will be deleted regardless. Therefore, it is important to set the domain YAML timeoutSeconds to a large enough value to prevent the server from shutting down before Coherence is safe. Furthermore, if the operator is not able to access the Coherence MBean, then the server will not be shut down until the domain timeoutSeconds expires. To minimize any possibility of cache data loss, you should increase the timeoutSeconds value to a large number, for example, 15 minutes.\n  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/introduction/design/",
	"title": "Design philosophy",
	"tags": [],
	"description": "Define the expected roles of an administrator, the operator, and domain resources.",
	"content": "The WebLogic Kubernetes Operator (the “operator”) is designed to fulfill a similar role to that which a human operator would fill in a traditional data center deployment. It contains a set of useful built-in knowledge about how to perform various life cycle operations on a domain correctly.\nHuman operators are typically responsible for starting and stopping environments, performing scaling operations, performing manual tasks associated with disaster recovery and high availability needs and coordinating actions with other operators in other data centers. It is envisaged that the operator will have similar responsibilities in a Kubernetes environment.\nIt is important to note the distinction between an operator and an administrator. A WebLogic Server administrator typically has different responsibilities centered around managing the detailed configuration of the WebLogic domains. The operator has only limited interest in the domain configuration, with its main concern being the high-level topology of the domain; for example, how many clusters and servers, and information about network access points, such as channels.\nHuman operators may manage more than one domain, and the operator is also designed to be able to manage more than one domain. Like its human counterpart, the operator will only take actions against domains that it is told to manage, and will ignore any other domains that may be present in the same environment.\nLike a human operator, the operator is designed to be event-based. It waits for a significant event to occur, or for a scheduled time to perform some action, and then takes the appropriate action. Examples of significant events include being made aware of a new domain that needs to be managed, receiving a request to scale up a WebLogic cluster, or applying a WebLogic Server patch or an application while preserving cluster availability.\nThere are some operator tasks, such as initiating backups, that are presently not implemented by the WebLogic Kubernetes Operator. We welcome any feedback or requirements as this helps us to properly create our roadmap.\nThe operator is designed with security in mind from the outset. Some examples of the specific security practices we follow are:\n During the deployment of the operator, Kubernetes Roles are defined and assigned to the operator. These roles are designed to give the operator the minimum amount of privileges that it requires to perform its tasks. The code base is regularly scanned with security auditing tools and any issues that are identified are promptly resolved. All HTTP communications – between the operator and an external client, between the operator and WebLogic Server Administration Servers, and so on – are configured to require SSL and TLS 1.2. Unused code is pruned from the code base regularly. Dependencies are kept as up-to-date as possible and are regularly reviewed for security vulnerabilities.  The operator is designed to avoid imposing any arbitrary restriction on how WebLogic Server may be configured or used in Kubernetes. Where there are restrictions, these are based on the availability of some specific feature in Kubernetes; for example, multicast support.\nThe operator learns of WebLogic domains through instances of a domain Kubernetes resource. When the operator is installed, it creates a Kubernetes Custom Resource Definition. This custom resource definition defines the Domain type. After this type is defined, you can manage Domains using kubectl just like any other resource type. For instance, kubectl get domain or kubectl edit domain domain1.\nThe schema for the Domain type is designed to be as sparse as possible. It includes the connection details for the Administration Server, but all of the other content is operational details about which servers should be started, environment variables, and details about what should be exposed outside the Kubernetes cluster. This way, the WebLogic domain\u0026rsquo;s configuration remains the normative configuration.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/accessing-the-domain/admin-console/",
	"title": "Use the Remote Console",
	"tags": [],
	"description": "Use the WebLogic Remote Console to manage a domain running in Kubernetes.",
	"content": "The WebLogic Remote Console is a lightweight, open source console that does not need to be collocated with a WebLogic Server domain. It is an alternative to the WebLogic Server Administration Console. You can install and run the Remote Console anywhere. For an introduction, read the blog, \u0026ldquo;The NEW WebLogic Remote Console\u0026rdquo;. For detailed documentation, see the WebLogic Remote Console GitHub project.\nA major benefit of using the Remote Console is that it runs in your browser or a desktop application, and can be used to connect to different WebLogic Server instances. You can use the Remote Console with WebLogic Server slim installers, available on the OTN or OSDC. Slim installers reduce the size of WebLogic Server downloads, installations, container images, and Kubernetes pods. For example, a WebLogic Server 12.2.1.4 slim installer download is approximately 180 MB smaller.\nThe Remote Console is deployed as a standalone Java program, which can connect to multiple WebLogic Server Administration Servers using REST APIs. You connect to the Remote Console and, when prompted, supply the WebLogic Server login credentials along with the URL of the WebLogic Server Administration Server\u0026rsquo;s administration port to which you want to connect.\nNote: An Administration Server administration port typically is the same as its default port unless either an SSL port or an administration port is configured and enabled.\nExternally exposing administrative, RMI, or T3 capable WebLogic channels using a Kubernetes NodePort, load balancer, port forwarding, or a similar method can create an insecure configuration. For more information, see External network access security.\n Setup To set up access to WebLogic Server domains running in Kubernetes using the Remote Console:\n  Install, configure, and start the Remote Console according to these instructions.\nNOTE: These instructions assume that you are installing and running the Remote Console Java program externally to your Kubernetes cluster.\n  When you first connect your browser to the Remote Console, which is at http://localhost:8012 by default, the console will prompt you with a login dialog for a WebLogic Server Administration Server URL. To give the Remote Console access to an Administration Server running in Kubernetes, you can:\n  Use an Administration Server NodePort.\n  Deploy a load balancer with ingress path routing rules.\n  Use a kubectl port-forward connection.\n  Note: If you want to customize the Remote Console listen address, then see Specify a Listen Address for the Remote Console Host. This is useful if you want to run the Remote Console on a different machine than your browser, or if you want the Remote Console to use SSL.\n  Use an Administration Server NodePort For the Remote Console to connect to the Kubernetes WebLogic Server Administration Server’s NodePort, use the following URL after you have connected to the Remote Console with your browser and it prompts for the location of your WebLogic Server Administration Server:\nhttp://hostname:adminserver-NodePort/ The adminserver-NodePort is the port number of the Administration Server outside the Kubernetes cluster. For information about the NodePort Service on an Administration Server, see the Domain resource document. For an example of setting up the NodePort on an Administration Server, see Use a NodePort for WLST.\nConfigure ingress path routing rules   Configure an ingress path routing rule. For information about ingresses, see the Ingress documentation.\nFor an example, see the following path-routing YAML file for a Traefik load balancer:\napiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: annotations: kubernetes.io/ingress.class: traefik name: traefik-pathrouting-1 namespace: weblogic-domain spec: routes: - kind: Rule match: PathPrefix(`/`) services: - kind: Service name: domain1-adminserver namespace: weblogic-domain port: 7001   After you have connected to the Remote Console with your browser, it will prompt for the location of your WebLogic Server Administration Server. For the Remote Console to connect to the Kubernetes WebLogic Server Administration Server, supply a URL that resolves to the load balancer host and ingress that you supplied in the previous step. For example:\nhttp://${HOSTNAME}:${LB_PORT}/ Where:\n  ${HOSTNAME} is where the ingress load balancer is running.\n  To determine the ${LB_PORT} when using a Traefik load balancer:\n$ export LB_PORT=$(kubectl -n traefik get service traefik-operator -o jsonpath='{.spec.ports[?(@.name==\u0026quot;web\u0026quot;)].nodePort}')\n    Use a kubectl port-forward connection   Forward a local port (that is external to Kubernetes) to the administration port of the Administration Server Pod according to these instructions.\nNOTE: If you plan to run the Remote Console Java program on a different machine than the port forwarding command, then the port forwarding command needs to specify a --address parameter with the IP address of the machine that is hosting the command.\n  After you have connected to the Remote Console with your browser, it will prompt you for the location of your WebLogic Server Administration Server. Supply a URL using the local hostname or IP address from the port-forward command in the first step, plus the local port from this same command. For example:\nhttp://${LOCAL_HOSTNAME}:${LOCAL_PORT}/ Where:\n  ${LOCAL_HOSTNAME} is the hostname or the defined IP address of the machine where the kubectl port-forward command is running. This is customizable on the port-forward command and is localhost or 127.0.0.1, by default.\n  ${LOCAL_PORT} is the local port where the kubectl port-forward command is running. This is specified on the port-forward command.\n    Test To verify that your WebLogic Server Administration Server URL is correct, and to verify that that your load balancer, NodePort, or kubectl port-forward are working as expected, run the following curl commands at the same location as your browser:\n$ curl --user username:password \\ http://${HOSTNAME}:${LB_PORT}/management/weblogic/latest/domainRuntime?fields=name\\\u0026amp;links=none ; echo $ curl --user username:password \\ http://${HOSTNAME}:${LB_PORT}/management/weblogic/latest/serverRuntime?fields=name\\\u0026amp;links=none ; echo These commands access the REST interface of the WebLogic Server Administration Server in a way that is similar to the Remote Console\u0026rsquo;s use of REST. If successful, then the output from the two commands will be {\u0026quot;name\u0026quot;: \u0026quot;your-weblogic-domain-name\u0026quot;} and {\u0026quot;name\u0026quot;: \u0026quot;your-weblogic-admin-server-name\u0026quot;}, respectively.\nIf you want to see the full content of the domainRuntime and serverRuntime beans, then rerun the commands but remove ?fields=name\\\u0026amp;links=none, which is appended at the end of each URL.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/domains/domain-home-on-pv/",
	"title": "Domain home on a PV",
	"tags": [],
	"description": "Sample for creating a WebLogic domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated WebLogic domain.",
	"content": "The sample scripts demonstrate the creation of a WebLogic domain home on an existing Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC). The scripts also generate the domain resource YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain. Optionally, the scripts start up the domain, and WebLogic Server pods and services.\nPrerequisites Before you begin, read this document, Domain resource.\nThe following prerequisites must be met prior to running the create domain script:\n Make sure the WebLogic Kubernetes Operator is running. The operator requires an image with Oracle WebLogic Server 12.2.1.3.0 with patch 29135930 applied, or Oracle WebLogic Server 12.2.1.4.0, or Oracle WebLogic Server 14.1.1.0.0. The existing WebLogic Server image, container-registry.oracle.com/middleware/weblogic:12.2.1.3, has all the necessary patches applied. For details on how to obtain or create the image, see WebLogic Server images. Create a Kubernetes Namespace for the domain unless you intend to use the default namespace. In the same Kubernetes Namespace, create the Kubernetes PersistentVolume (PV) where the domain home will be hosted, and the Kubernetes PersistentVolumeClaim (PVC) for the domain. For samples to create a PV and PVC, see Create sample PV and PVC. By default, the create-domain.sh script creates a domain with the domainUID set to domain1 and expects the PVC domain1-weblogic-sample-pvc to be present. You can create domain1-weblogic-sample-pvc using create-pv-pvc.sh with an inputs file that has the domainUID set to domain1. Create the Kubernetes Secrets username and password of the administrative account in the same Kubernetes Namespace as the domain.  Please note the following important considerations about using persistent storage.\n There are a number of different Kubernetes storage providers that can be used to create persistent volumes. Depending on which variant and version of Kubernetes you are using, there will be different steps required to create persistent volumes. You must use a storage provider that supports the ReadWriteMany option.\nMany storage providers will create a file system on the persistent volume which is owned by the root user. In those cases, you will need to update the file permissions or ownership so that the oracle user (uid 1000) that is used in the standard WebLogic Server images can write to the file system in the persistent volume.\nThis sample will automatically set the owner of all files on the persistent volume to uid 1000. If you want to change that behavior, please edit kubernetes/samples/scripts/create-weblogic-domain/domain-home-on-pv/create-domain-job-template.yaml and edit or remove the initContainer section.\nIn some variants of Kubernetes (for example OpenShift), you may also need to configure your pods to run with user 1000 to make sure that the WebLogic processes can access the file system on the persistent volume.\nUse the script to create a domain Make a copy of the create-domain-inputs.yaml file, and run the create script, pointing it at your inputs file and an output directory:\n$ ./create-domain.sh \\  -i create-domain-inputs.yaml \\  -o /\u0026lt;path to output-directory\u0026gt;  The create-domain.sh script and its inputs file are for demonstration purposes only; its contents and the domain resource file that it generates for you might change without notice. In production, we strongly recommend that you use the WebLogic Image Tool and WebLogic Deploy Tooling (when applicable), and directly work with domain resource files instead.\n The script will perform the following steps:\n Create a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The pathname is /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, then its contents must be removed before using this script. Create a Kubernetes Job that will start up a utility WebLogic Server container and run offline WLST scripts, or WebLogic Deploy Tool (WDT) scripts, to create the domain on the shared storage. Run and wait for the job to finish. Create a Kubernetes domain resource YAML file, domain.yaml, in the directory that is created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command:  $ kubectl apply -f /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml  Create a convenient utility script, delete-domain-job.yaml, to clean up the domain home created by the create script.  As a convenience, using the -e option, the script can optionally create the domain object, which in turn results in the creation of the corresponding WebLogic Server pods and services as well.\nThe usage of the create script is as follows:\n$ sh create-domain.sh -h usage: create-domain.sh -o dir -i file [-e] [-v] [-h] -i Parameter inputs file, must be specified. -o Output directory for the generated YAML files, must be specified. -e Also create the resources in the generated YAML files, optional. -v Validate the existence of persistentVolumeClaim, optional. -h Help If you copy the sample scripts to a different location, make sure that you copy everything in the \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts directory together into the target directory, maintaining the original directory hierarchy.\nThe default domain created by the script has the following characteristics:\n An Administration Server named admin-server listening on port 7001. A dynamic cluster named cluster-1 of size 5. Two Managed Servers, named managed-server1 and managed-server2, listening on port 8001. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;. No applications deployed. No data sources or JMS resources. A T3 channel.  The domain creation inputs can be customized by editing create-domain-inputs.yaml.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Definition Default     adminPort Port number of the Administration Server inside the Kubernetes cluster. 7001   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. admin-server   clusterName Name of the WebLogic cluster instance to generate for the domain. cluster-1   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. It can also be set to the relative path wdt, and then the built-in WDT scripts will be used instead. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes ConfigMap, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes Pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes Job to run the script (specified in the createDomainScriptName property) in a Kubernetes Pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes Pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes Job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the WebLogic domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /shared/domains/domain1   domainPVMountPath Mount path of the domain persistent volume. /shared   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Domain. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes Service name. domain1   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   httpAccessLogInLogHome Boolean indicating if server HTTP access log files should be written to the same directory as logHome. Otherwise, server HTTP access log files will be written to the directory specified in the WebLogic domain home configuration. true   image WebLogic Server image. The operator requires either Oracle WebLogic Server 12.2.1.3.0 with patch 29135930 applied, or Oracle WebLogic Server 12.2.1.4.0, or Oracle WebLogic Server 14.1.1.0.0. The existing WebLogic Server image, container-registry.oracle.com/middleware/weblogic:12.2.1.3, has all the necessary patches applied. For details on how to obtain or create the image, see WebLogic Server images. container-registry.oracle.com/middleware/weblogic:12.2.1.3   imagePullPolicy WebLogic Server image pull policy. Legal values are IfNotPresent, Always, or Never IfNotPresent   imagePullSecretName Name of the Kubernetes Secret to access the container registry to pull the WebLogic Server image. The presence of the secret will be validated when this parameter is specified    includeServerOutInPodLog Boolean indicating whether to include the server .out in the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to start initially for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for domain log, server logs, server out, introspector out, Node Manager log, and server HTTP access log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /shared/logs/domain1   managedServerNameBase Base string used to generate Managed Server names. managed-server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes Namespace in which to create the domain. default   persistentVolumeClaimName Name of the persistent volume claim. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc domain1-weblogic-sample-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Servers will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the network access point. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only, in a single server (all-in-one) Kubernetes Deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster.   weblogicCredentialsSecretName Name of the Kubernetes Secret for the Administration Server user name and password. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. domain1-weblogic-credentials   serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName, and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes Service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create a WebLogic domain home and associated Kubernetes resources for a domain that only has one cluster. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain resource YAML file could also be modified to cover more use cases.\nVerify the results The create script will verify that the domain was created, and will report failure if there was any error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nNote that the example results below use the default Kubernetes Namespace. If you are using a different namespace, you need to replace NAMESPACE in the example kubectl commands with the actual Kubernetes Namespace.\nGenerated YAML files with the default inputs The content of the generated domain.yaml:\n# Copyright (c) 2017, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026#34;weblogic.oracle/v8\u0026#34; kind: Domain metadata: name: domain1 namespace: default labels: weblogic.domainUID: domain1 spec: # The WebLogic Domain Home domainHome: /shared/domains/domain1 # Set domain home type to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: PersistentVolume # The WebLogic Server image that the operator uses to start the domain image: \u0026#34;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#34; # imagePullPolicy defaults to \u0026#34;Always\u0026#34; if image version is :latest imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: domain1-weblogic-credentials # Whether to include the server out file into the pod\u0026#39;s stdout, default is true includeServerOutInPodLog: true # Whether to enable log home logHomeEnabled: true # The in-pod name location for domain log, server logs, server out, introspector out, and Node Manager log files logHome: /shared/logs/domain1 # serverStartPolicy legal values are \u0026#34;NEVER\u0026#34;, \u0026#34;IF_NEEDED\u0026#34;, or \u0026#34;ADMIN_ONLY\u0026#34; # This determines which WebLogic Servers the operator will start up when it discovers this Domain # - \u0026#34;NEVER\u0026#34; will not start any server in the domain # - \u0026#34;ADMIN_ONLY\u0026#34; will start up only the administration server (no managed servers will be started) # - \u0026#34;IF_NEEDED\u0026#34; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026#34;IF_NEEDED\u0026#34; serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false\u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-Djava.security.egd=file:/dev/./urandom \u0026#34; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: domain1-weblogic-sample-pvc volumeMounts: - mountPath: /shared name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026#34;RUNNING\u0026#34; or \u0026#34;ADMIN\u0026#34; # \u0026#34;RUNNING\u0026#34; means the listed server will be started up to \u0026#34;RUNNING\u0026#34; mode # \u0026#34;ADMIN\u0026#34; means the listed server will be start up to \u0026#34;ADMIN\u0026#34; mode serverStartState: \u0026#34;RUNNING\u0026#34; # adminService: # channels: # The Admin Server\u0026#39;s NodePort # - channelName: default # nodePort: 30701 # Uncomment to export the T3Channel as a service # - channelName: T3Channel # clusters is used to configure the desired behavior for starting member servers of a cluster. # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: cluster-1 serverStartState: \u0026#34;RUNNING\u0026#34; replicas: 2 # The number of managed servers to start for unlisted clusters # replicas: 1 Verify the domain To confirm that the domain was created, use this command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\nHere is an example of the output of this command:\n$ kubectl describe domain domain1 Name: domain1 Namespace: default Labels: weblogic.domainUID=domain1 Annotations: kubectl.kubernetes.io/last-applied-configuration={\u0026quot;apiVersion\u0026quot;:\u0026quot;weblogic.oracle/v2\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Domain\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;labels\u0026quot;:{\u0026quot;weblogic.domainUID\u0026quot;:\u0026quot;domain1\u0026quot;,... API Version: weblogic.oracle/v2 Kind: Domain Metadata: Cluster Name: Creation Timestamp: 2019-01-10T14:50:52Z Generation: 1 Resource Version: 3700284 Self Link: /apis/weblogic.oracle/v2/namespaces/default/domains/domain1 UID: 2023ae0a-14e7-11e9-b751-fa163e855ac8 Spec: Admin Server: Server Pod: Annotations: Container Security Context: Env: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Clusters: Cluster Name: cluster-1 Cluster Service: Annotations: Labels: Replicas: 2 Server Pod: Annotations: Container Security Context: Env: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Domain Home: /shared/domains/domain1 Domain Home In Image: false Image: container-registry.oracle.com/middleware/weblogic:12.2.1.4 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Log Home: /shared/logs/domain1 Log Home Enabled: true Managed Servers: Server Pod: Annotations: Container Security Context: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Xms64m -Xmx256m Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Mount Path: /shared Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: domain1-weblogic-sample-pvc Server Service: Annotations: Labels: Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: domain1-weblogic-credentials Status: Conditions: Last Transition Time: 2019-01-10T14:52:33.878Z Reason: ServersReady Status: True Type: Available Servers: Health: Activation Time: 2019-01-10T14:52:07.351Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: admin-server State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-01-10T14:53:30.352Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: managed-server1 State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-01-10T14:53:26.503Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: managed-server2 State: RUNNING Start Time: 2019-01-10T14:50:52.104Z Events: \u0026lt;none\u0026gt; In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued very soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. The operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Use the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE domain1-admin-server 1/1 Running 0 1m domain1-managed-server1 1/1 Running 0 8m domain1-managed-server2 1/1 Running 0 8m Verify the services Use the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command:\n$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE domain1-admin-server ClusterIP 10.96.206.134 \u0026lt;none\u0026gt; 7001/TCP 23m domain1-admin-server-ext NodePort 10.107.164.241 \u0026lt;none\u0026gt; 30012:30012/TCP 22m domain1-cluster-cluster-1 ClusterIP 10.109.133.168 \u0026lt;none\u0026gt; 8001/TCP 22m domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 22m domain1-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 22m Delete the generated domain home Sometimes in production, but most likely in testing environments, you might want to remove the domain home that is generated using the create-domain.sh script. Do this by running the generated delete domain job script in the /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt; directory.\n$ kubectl create -f delete-domain-job.yaml Troubleshooting Message: status on iteration 20 of 20 pod domain1-create-weblogic-sample-domain-job-4qwt2 status is Pending The create domain job is not showing status completed after waiting 300 seconds.\nThe most likely cause is related to the value of persistentVolumeClaimName, defined in domain-home-on-pv/create-domain-inputs.yaml.\nTo determine if this is the problem:\n* Execute `kubectl get all --all-namespaces` to find the name of the `create-weblogic-sample-domain-job`. * Execute `kubectl describe pod \u0026lt;name-of-create-weblogic-sample-domain-job\u0026gt;` to see if there is an event that has text similar to `persistentvolumeclaim \u0026quot;domain1-weblogic-sample-pvc\u0026quot; not found`. * Find the name of the PVC that was created by executing [create-pv-pvc.sh](https://github.com/oracle/weblogic-kubernetes-operator/blob/main/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc/create-pv-pvc.sh), using `kubectl describe pvc`. It is likely to be `weblogic-sample-pvc`. * Change the value of `persistentVolumeClaimName` to match the name created when you executed [create-pv-pvc.sh](https://github.com/oracle/weblogic-kubernetes-operator/blob/main/kubernetes/samples/scripts/create-weblogic-domain-pv-pvc/create-pv-pvc.sh). * Rerun the `create-domain.sh` script with the same arguments as you did before. * Verify that the operator is deployed. Use the command:  $ kubectl get all --all-namespaces Look for lines similar to:\nweblogic-operator1 pod/weblogic-operator- If you do not find something similar in the output, the WebLogic Kubernetes Operator might not have been installed completely. Review the operator installation instructions.\nMessage: ERROR: Unable to create folder /shared/domains\nThe most common cause is a poor choice of value for weblogicDomainStoragePath in the input file used when you executed:\n$ create-pv-pvc.sh You should delete the resources for your sample domain, correct the value in that file, and rerun the commands to create the PV/PVC and the credential before you attempt to rerun:\n$ create-domain.sh A correct value for weblogicDomainStoragePath will meet the following requirements:\n Must be the name of a directory. The directory must be world writable.  Optionally, follow these steps to tighten permissions on the named directory after you run the sample the first time:\n Become the root user. ls -nd $value-of-weblogicDomainStoragePath  Note the values of the third and fourth field of the output.   chown $third-field:$fourth-field $value-of-weblogicDomainStoragePath chmod 755 $value-of-weblogicDomainStoragePath Return to your normal user ID.  Message: ERROR: The create domain job will not overwrite an existing domain. The domain folder /shared/domains/domain1 already exists\nYou will see this message if the directory domains/domain1 exists in the directory named as the value of weblogicDomainStoragePath in create-pv-pvc-inputs.yaml. For example, if the value of weblogicDomainStoragePath is /tmp/wls-op-4-k8s, you would need to remove (or move) /tmp/wls-op-4-k8s/domains/domain1.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/domains/model-in-image/initial/",
	"title": "Initial use case",
	"tags": [],
	"description": "",
	"content": "Contents  Overview Image creation  Image creation - Introduction Understanding your first archive Staging a ZIP file of the archive Staging model files Creating the image with WIT   Deploy resources  Deploy resources - Introduction Secrets Domain resource    Overview In this use case, you set up an initial WebLogic domain. This involves:\n A WDT archive ZIP file that contains your applications. A WDT model that describes your WebLogic configuration. A container image that contains your WDT model files and archive. Creating secrets for the domain. Creating a Domain YAML for the domain that references your Secrets and image.  After the Domain is deployed, the operator starts an \u0026lsquo;introspector job\u0026rsquo; that converts your models into a WebLogic configuration, and then passes this configuration to each WebLogic Server in the domain.\nPerform the steps in Prerequisites for all domain types before performing the steps in this use case.\nIf you are taking the JRF path through the sample, then substitute JRF for WLS in your image names and directory paths. Also note that the JRF-v1 model YAML file differs from the WLS-v1 YAML file (it contains an additional domainInfo -\u0026gt; RCUDbInfo stanza).\n Image creation - Introduction The goal of the initial use case \u0026lsquo;image creation\u0026rsquo; is to demonstrate using the WebLogic Image Tool to create an image named model-in-image:WLS-v1 from files that you will stage to /tmp/mii-sample/model-images/model-in-image:WLS-v1/. The staged files will contain a web application in a WDT archive, and WDT model configuration for a WebLogic Administration Server called admin-server and a WebLogic cluster called cluster-1.\nOverall, a Model in Image image must contain a WebLogic installation and a WebLogic Deploy Tooling installation in its /u01/wdt/weblogic-deploy directory. In addition, if you have WDT model archive files, then the image must also contain these files in its /u01/wdt/models directory. Finally, an image optionally may also contain your WDT model YAML file and properties files in the same /u01/wdt/models directory. If you do not specify a WDT model YAML file in your /u01/wdt/models directory, then the model YAML file must be supplied dynamically using a Kubernetes ConfigMap that is referenced by your Domain spec.model.configMap field. We provide an example of using a model ConfigMap later in this sample.\nHere are the steps for creating the image model-in-image:WLS-v1:\n Understanding your first archive Staging a ZIP file of the archive Staging model files Creating the image with WIT  Understanding your first archive The sample includes a predefined archive directory in /tmp/mii-sample/archives/archive-v1 that you will use to create an archive ZIP file for the image.\nThe archive top directory, named wlsdeploy, contains a directory named applications, which includes an \u0026lsquo;exploded\u0026rsquo; sample JSP web application in the directory, myapp-v1. Three useful aspects to remember about WDT archives are:\n A model image can contain multiple WDT archives. WDT archives can contain multiple applications, libraries, and other components. WDT archives have a well defined directory structure, which always has wlsdeploy as the top directory.    If you are interested in the web application source, click here to see the JSP code.   \u0026lt;%-- Copyright (c) 2019, 2021, Oracle and/or its affiliates. --%\u0026gt; \u0026lt;%-- Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. --%\u0026gt; \u0026lt;%@ page import=\u0026#34;javax.naming.InitialContext\u0026#34; %\u0026gt; \u0026lt;%@ page import=\u0026#34;javax.management.*\u0026#34; %\u0026gt; \u0026lt;%@ page import=\u0026#34;java.io.*\u0026#34; %\u0026gt; \u0026lt;% InitialContext ic = null; try { ic = new InitialContext(); String srName=System.getProperty(\u0026#34;weblogic.Name\u0026#34;); String domainUID=System.getenv(\u0026#34;DOMAIN_UID\u0026#34;); String domainName=System.getenv(\u0026#34;CUSTOM_DOMAIN_NAME\u0026#34;); out.println(\u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt;\u0026#34;); out.println(\u0026#34;*****************************************************************\u0026#34;); out.println(); out.println(\u0026#34;Hello World! This is version \u0026#39;v1\u0026#39; of the mii-sample JSP web-app.\u0026#34;); out.println(); out.println(\u0026#34;Welcome to WebLogic Server \u0026#39;\u0026#34; + srName + \u0026#34;\u0026#39;!\u0026#34;); out.println(); out.println(\u0026#34; domain UID = \u0026#39;\u0026#34; + domainUID +\u0026#34;\u0026#39;\u0026#34;); out.println(\u0026#34; domain name = \u0026#39;\u0026#34; + domainName +\u0026#34;\u0026#39;\u0026#34;); out.println(); MBeanServer mbs = (MBeanServer)ic.lookup(\u0026#34;java:comp/env/jmx/runtime\u0026#34;); // display the current server\u0026#39;s cluster name  Set\u0026lt;ObjectInstance\u0026gt; clusterRuntimes = mbs.queryMBeans(new ObjectName(\u0026#34;*:Type=ClusterRuntime,*\u0026#34;), null); out.println(\u0026#34;Found \u0026#34; + clusterRuntimes.size() + \u0026#34; local cluster runtime\u0026#34; + (String)((clusterRuntimes.size()!=1)?\u0026#34;s\u0026#34;:\u0026#34;\u0026#34;) + \u0026#34;:\u0026#34;); for (ObjectInstance clusterRuntime : clusterRuntimes) { String cName = (String)mbs.getAttribute(clusterRuntime.getObjectName(), \u0026#34;Name\u0026#34;); out.println(\u0026#34; Cluster \u0026#39;\u0026#34; + cName + \u0026#34;\u0026#39;\u0026#34;); } out.println(); // display the Work Manager configuration created by the sample  Set\u0026lt;ObjectInstance\u0026gt; minTCRuntimes = mbs.queryMBeans(new ObjectName(\u0026#34;*:Type=MinThreadsConstraintRuntime,Name=SampleMinThreads,*\u0026#34;), null); for (ObjectInstance minTCRuntime : minTCRuntimes) { String cName = (String)mbs.getAttribute(minTCRuntime.getObjectName(), \u0026#34;Name\u0026#34;); int count = (int)mbs.getAttribute(minTCRuntime.getObjectName(), \u0026#34;ConfiguredCount\u0026#34;); out.println(\u0026#34;Found min threads constraint runtime named \u0026#39;\u0026#34; + cName + \u0026#34;\u0026#39; with configured count: \u0026#34; + count); } out.println(); Set\u0026lt;ObjectInstance\u0026gt; maxTCRuntimes = mbs.queryMBeans(new ObjectName(\u0026#34;*:Type=MaxThreadsConstraintRuntime,Name=SampleMaxThreads,*\u0026#34;), null); for (ObjectInstance maxTCRuntime : maxTCRuntimes) { String cName = (String)mbs.getAttribute(maxTCRuntime.getObjectName(), \u0026#34;Name\u0026#34;); int count = (int)mbs.getAttribute(maxTCRuntime.getObjectName(), \u0026#34;ConfiguredCount\u0026#34;); out.println(\u0026#34;Found max threads constraint runtime named \u0026#39;\u0026#34; + cName + \u0026#34;\u0026#39; with configured count: \u0026#34; + count); } out.println(); // display local data sources  // - note that data source tests are expected to fail until the sample Update 4 use case updates the data source\u0026#39;s secret  ObjectName jdbcRuntime = new ObjectName(\u0026#34;com.bea:ServerRuntime=\u0026#34; + srName + \u0026#34;,Name=\u0026#34; + srName + \u0026#34;,Type=JDBCServiceRuntime\u0026#34;); ObjectName[] dataSources = (ObjectName[])mbs.getAttribute(jdbcRuntime, \u0026#34;JDBCDataSourceRuntimeMBeans\u0026#34;); out.println(\u0026#34;Found \u0026#34; + dataSources.length + \u0026#34; local data source\u0026#34; + (String)((dataSources.length!=1)?\u0026#34;s\u0026#34;:\u0026#34;\u0026#34;) + \u0026#34;:\u0026#34;); for (ObjectName dataSource : dataSources) { String dsName = (String)mbs.getAttribute(dataSource, \u0026#34;Name\u0026#34;); String dsState = (String)mbs.getAttribute(dataSource, \u0026#34;State\u0026#34;); String dsTest = (String)mbs.invoke(dataSource, \u0026#34;testPool\u0026#34;, new Object[] {}, new String[] {}); out.println( \u0026#34; Datasource \u0026#39;\u0026#34; + dsName + \u0026#34;\u0026#39;: \u0026#34; + \u0026#34; State=\u0026#39;\u0026#34; + dsState + \u0026#34;\u0026#39;,\u0026#34; + \u0026#34; testPool=\u0026#39;\u0026#34; + (String)(dsTest==null ? \u0026#34;Passed\u0026#34; : \u0026#34;Failed\u0026#34;) + \u0026#34;\u0026#39;\u0026#34; ); if (dsTest != null) { out.println( \u0026#34; ---TestPool Failure Reason---\\n\u0026#34; + \u0026#34; NOTE: Ignore \u0026#39;mynewdatasource\u0026#39; failures until the MII sample\u0026#39;s Update 4 use case.\\n\u0026#34; + \u0026#34; ---\\n\u0026#34; + \u0026#34; \u0026#34; + dsTest.replaceAll(\u0026#34;\\n\u0026#34;,\u0026#34;\\n \u0026#34;).replaceAll(\u0026#34;\\n *\\n\u0026#34;,\u0026#34;\\n\u0026#34;) + \u0026#34;\\n\u0026#34; + \u0026#34; -----------------------------\u0026#34;); } } out.println(); out.println(\u0026#34;*****************************************************************\u0026#34;); } catch (Throwable t) { t.printStackTrace(new PrintStream(response.getOutputStream())); } finally { out.println(\u0026#34;\u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34;); if (ic != null) ic.close(); } %\u0026gt;    The application displays important details about the WebLogic Server instance that it\u0026rsquo;s running on: namely its domain name, cluster name, and server name, as well as the names of any data sources that are targeted to the server. Also, you can see that application output reports that it\u0026rsquo;s at version v1; you will update this to v2 in a later use case that demonstrates upgrading the application.\nStaging a ZIP file of the archive When you create the image, you will use the files in the staging directory, /tmp/mii-sample/model-images/model-in-image__WLS-v1. In preparation, you need it to contain a ZIP file of the WDT application archive.\nRun the following commands to create your application archive ZIP file and put it in the expected directory:\n# Delete existing archive.zip in case we have an old leftover version $ rm -f /tmp/mii-sample/model-images/model-in-image__WLS-v1/archive.zip # Move to the directory which contains the source files for our archive $ cd /tmp/mii-sample/archives/archive-v1 # Zip the archive to the location will later use when we run the WebLogic Image Tool $ zip -r /tmp/mii-sample/model-images/model-in-image__WLS-v1/archive.zip wlsdeploy Staging model files In this step, you explore the staged WDT model YAML file and properties in the /tmp/mii-sample/model-images/model-in-image__WLS-v1 directory. The model in this directory references the web application in your archive, configures a WebLogic Server Administration Server, and configures a WebLogic cluster. It consists of only two files, model.10.properties, a file with a single property, and, model.10.yaml, a YAML file with your WebLogic configuration model.10.yaml.\nCLUSTER_SIZE=5 Here is the WLS model.10.yaml:\ndomainInfo: AdminUserName: \u0026#39;@@SECRET:__weblogic-credentials__:username@@\u0026#39; AdminPassword: \u0026#39;@@SECRET:__weblogic-credentials__:password@@\u0026#39; ServerStartMode: \u0026#39;prod\u0026#39; topology: Name: \u0026#39;@@ENV:CUSTOM_DOMAIN_NAME@@\u0026#39; AdminServerName: \u0026#39;admin-server\u0026#39; Cluster: \u0026#39;cluster-1\u0026#39;: DynamicServers: ServerTemplate: \u0026#39;cluster-1-template\u0026#39; ServerNamePrefix: \u0026#39;managed-server\u0026#39; DynamicClusterSize: \u0026#39;@@PROP:CLUSTER_SIZE@@\u0026#39; MaxDynamicClusterSize: \u0026#39;@@PROP:CLUSTER_SIZE@@\u0026#39; MinDynamicClusterSize: \u0026#39;0\u0026#39; CalculatedListenPorts: false Server: \u0026#39;admin-server\u0026#39;: ListenPort: 7001 ServerTemplate: \u0026#39;cluster-1-template\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 appDeployments: Application: myapp: SourcePath: \u0026#39;wlsdeploy/applications/myapp-v1\u0026#39; ModuleType: ear Target: \u0026#39;cluster-1\u0026#39; resources: SelfTuning: MinThreadsConstraint: SampleMinThreads: Target: \u0026#39;cluster-1\u0026#39; Count: 1 MaxThreadsConstraint: SampleMaxThreads: Target: \u0026#39;cluster-1\u0026#39; Count: 10 WorkManager: SampleWM: Target: \u0026#39;cluster-1\u0026#39; MinThreadsConstraint: \u0026#39;SampleMinThreads\u0026#39; MaxThreadsConstraint: \u0026#39;SampleMaxThreads\u0026#39;    Click here to view the JRF `model.10.yaml`, and note the `RCUDbInfo` stanza and its references to a `DOMAIN_UID-rcu-access` secret.   domainInfo: AdminUserName: \u0026#39;@@SECRET:__weblogic-credentials__:username@@\u0026#39; AdminPassword: \u0026#39;@@SECRET:__weblogic-credentials__:password@@\u0026#39; ServerStartMode: \u0026#39;prod\u0026#39; RCUDbInfo: rcu_prefix: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_prefix@@\u0026#39; rcu_schema_password: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_schema_password@@\u0026#39; rcu_db_conn_string: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-rcu-access:rcu_db_conn_string@@\u0026#39; topology: AdminServerName: \u0026#39;admin-server\u0026#39; Name: \u0026#39;@@ENV:CUSTOM_DOMAIN_NAME@@\u0026#39; Cluster: \u0026#39;cluster-1\u0026#39;: Server: \u0026#39;admin-server\u0026#39;: ListenPort: 7001 \u0026#39;managed-server1-c1-\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 \u0026#39;managed-server2-c1-\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 \u0026#39;managed-server3-c1-\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 \u0026#39;managed-server4-c1-\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 appDeployments: Application: myapp: SourcePath: \u0026#39;wlsdeploy/applications/myapp-v1\u0026#39; ModuleType: ear Target: \u0026#39;cluster-1\u0026#39; resources: SelfTuning: MinThreadsConstraint: SampleMinThreads: Target: \u0026#39;cluster-1\u0026#39; Count: 1 MaxThreadsConstraint: SampleMaxThreads: Target: \u0026#39;cluster-1\u0026#39; Count: 10 WorkManager: SampleWM: Target: \u0026#39;cluster-1\u0026#39; MinThreadsConstraint: \u0026#39;SampleMinThreads\u0026#39; MaxThreadsConstraint: \u0026#39;SampleMaxThreads\u0026#39;    The model files:\n  Define a WebLogic domain with:\n Cluster cluster-1 Administration Server admin-server A cluster-1 targeted ear application that\u0026rsquo;s located in the WDT archive ZIP file at wlsdeploy/applications/myapp-v1 A Work Manager SampleWM configured with minimum threads constraint SampleMinThreads and maximum threads constraint SampleMaxThreads    Leverage macros to inject external values:\n The property file CLUSTER_SIZE property is referenced in the model YAML file DynamicClusterSize and MaxDynamicClusterSize fields using a PROP macro. The model file domain name is injected using a custom environment variable named CUSTOM_DOMAIN_NAME using an ENV macro.  You set this environment variable later in this sample using an env field in its Domain. This conveniently provides a simple way to deploy multiple differently named domains using the same model image.   The model file administrator user name and password are set using a weblogic-credentials secret macro reference to the WebLogic credential secret.  This secret is in turn referenced using the webLogicCredentialsSecret field in the Domain. The weblogic-credentials is a reserved name that always dereferences to the owning Domain actual WebLogic credentials secret name.      A Model in Image image can contain multiple properties files, archive ZIP files, and YAML files but in this sample you use just one of each. For a complete discussion of Model in Images model file naming conventions, file loading order, and macro syntax, see Model files in the Model in Image user documentation.\nCreating the image with WIT  Note: If you are using JRF in this sample, substitute JRF for each occurrence of WLS in the imagetool command line below, plus substitute container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 for the --fromImage value.\n At this point, you have staged all of the files needed for image model-in-image:WLS-v1; they include:\n /tmp/mii-sample/model-images/weblogic-deploy.zip /tmp/mii-sample/model-images/model-in-image__WLS-v1/model.10.yaml /tmp/mii-sample/model-images/model-in-image__WLS-v1/model.10.properties /tmp/mii-sample/model-images/model-in-image__WLS-v1/archive.zip  If you don\u0026rsquo;t see the weblogic-deploy.zip file, then you missed a step in the prerequisites.\nNow, you use the Image Tool to create an image named model-in-image:WLS-v1 that\u0026rsquo;s layered on a base WebLogic image. You\u0026rsquo;ve already set up this tool during the prerequisite steps.\nRun the following commands to create the model image and verify that it worked:\n$ cd /tmp/mii-sample/model-images $ ./imagetool/bin/imagetool.sh update \\  --tag model-in-image:WLS-v1 \\  --fromImage container-registry.oracle.com/middleware/weblogic:12.2.1.4 \\  --wdtModel ./model-in-image__WLS-v1/model.10.yaml \\  --wdtVariables ./model-in-image__WLS-v1/model.10.properties \\  --wdtArchive ./model-in-image__WLS-v1/archive.zip \\  --wdtModelOnly \\  --wdtDomainType WLS \\  --chown oracle:root If you don\u0026rsquo;t see the imagetool directory, then you missed a step in the prerequisites.\nThis command runs the WebLogic Image Tool in its Model in Image mode, and does the following:\n Builds the final container image as a layer on the container-registry.oracle.com/middleware/weblogic:12.2.1.4 base image. Copies the WDT ZIP file that\u0026rsquo;s referenced in the WIT cache into the image.  Note that you cached WDT in WIT using the keyword latest when you set up the cache during the sample prerequisites steps. This lets WIT implicitly assume it\u0026rsquo;s the desired WDT version and removes the need to pass a -wdtVersion flag.   Copies the specified WDT model, properties, and application archives to image location /u01/wdt/models.  When the command succeeds, it should end with output like the following:\n[INFO ] Build successful. Build time=36s. Image tag=model-in-image:WLS-v1 Also, if you run the docker images command, then you will see an image named model-in-image:WLS-v1.\n Note: If you have Kubernetes cluster worker nodes that are remote to your local machine, then you need to put the image in a location that these nodes can access. See Ensuring your Kubernetes cluster can access images.\n Deploy resources - Introduction In this section, you will deploy the new image to namespace sample-domain1-ns, including the following steps:\n Create a Secret containing your WebLogic administrator user name and password. Create a Secret containing your Model in Image runtime encryption password:  All Model in Image domains must supply a runtime encryption Secret with a password value. It is used to encrypt configuration that is passed around internally by the operator. The value must be kept private but can be arbitrary; you can optionally supply a different secret value every time you restart the domain.   If your domain type is JRF, create secrets containing your RCU access URL, credentials, and prefix. Deploy a Domain YAML file that references the new image. Wait for the domain\u0026rsquo;s Pods to start and reach their ready state.  Secrets First, create the secrets needed by both WLS and JRF type model domains. In this case, you have two secrets.\nRun the following kubectl commands to deploy the required secrets:\n$ kubectl -n sample-domain1-ns create secret generic \\  sample-domain1-weblogic-credentials \\  --from-literal=username=weblogic --from-literal=password=welcome1 $ kubectl -n sample-domain1-ns label secret \\  sample-domain1-weblogic-credentials \\  weblogic.domainUID=sample-domain1 $ kubectl -n sample-domain1-ns create secret generic \\  sample-domain1-runtime-encryption-secret \\  --from-literal=password=my_runtime_password $ kubectl -n sample-domain1-ns label secret \\  sample-domain1-runtime-encryption-secret \\  weblogic.domainUID=sample-domain1 Some important details about these secrets:\n  The WebLogic credentials secret:\n It is required and must contain username and password fields. It must be referenced by the spec.webLogicCredentialsSecret field in your Domain. It also must be referenced by macros in the domainInfo.AdminUserName and domainInfo.AdminPassWord fields in your model YAML file.    The Model WDT runtime secret:\n This is a special secret required by Model in Image. It must contain a password field. It must be referenced using the spec.model.runtimeEncryptionSecret field in its Domain. It must remain the same for as long as the domain is deployed to Kubernetes but can be changed between deployments. It is used to encrypt data as it\u0026rsquo;s internally passed using log files from the domain\u0026rsquo;s introspector job and on to its WebLogic Server pods.    Deleting and recreating the secrets:\n You delete a secret before creating it, otherwise the create command will fail if the secret already exists. This allows you to change the secret when using the kubectl create secret command.    You name and label secrets using their associated domain UID for two reasons:\n To make it obvious which secrets belong to which domains. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all resources associated with a domain.    If you\u0026rsquo;re following the JRF path through the sample, then you also need to deploy the additional secret referenced by macros in the JRF model RCUDbInfo clause, plus an OPSS wallet password secret. For details about the uses of these secrets, see the Model in Image user documentation.\n  Click here for the commands for deploying additional secrets for JRF.   $ kubectl -n sample-domain1-ns create secret generic \\  sample-domain1-rcu-access \\  --from-literal=rcu_prefix=FMW1 \\  --from-literal=rcu_schema_password=Oradoc_db1 \\  --from-literal=rcu_db_conn_string=oracle-db.default.svc.cluster.local:1521/devpdb.k8s $ kubectl -n sample-domain1-ns label secret \\  sample-domain1-rcu-access \\  weblogic.domainUID=sample-domain1 $ kubectl -n sample-domain1-ns create secret generic \\  sample-domain1-opss-wallet-password-secret \\  --from-literal=walletPassword=welcome1 $ kubectl -n sample-domain1-ns label secret \\  sample-domain1-opss-wallet-password-secret \\  weblogic.domainUID=sample-domain1    Domain resource Now, you create a Domain YAML file. A Domain is the key resource that tells the operator how to deploy a WebLogic domain.\nCopy the following to a file called /tmp/mii-sample/mii-initial.yaml or similar, or use the file /tmp/mii-sample/domain-resources/WLS/mii-initial-d1-WLS-v1.yaml that is included in the sample source.\n  Click here to view the WLS Domain YAML file.   # # This is an example of how to define a Domain resource. # apiVersion: \u0026#34;weblogic.oracle/v8\u0026#34; kind: Domain metadata: name: sample-domain1 namespace: sample-domain1-ns labels: weblogic.domainUID: sample-domain1 spec: # Set to \u0026#39;FromModel\u0026#39; to indicate \u0026#39;Model in Image\u0026#39;. domainHomeSourceType: FromModel # The WebLogic Domain Home, this must be a location within # the image for \u0026#39;Model in Image\u0026#39; domains. domainHome: /u01/domains/sample-domain1 # The WebLogic Server image that the Operator uses to start the domain image: \u0026#34;model-in-image:WLS-v1\u0026#34; # Defaults to \u0026#34;Always\u0026#34; if image tag (version) is \u0026#39;:latest\u0026#39; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: regsecret # Identify which Secret contains the WebLogic Admin credentials, # the secret must contain \u0026#39;username\u0026#39; and \u0026#39;password\u0026#39; fields. webLogicCredentialsSecret: name: sample-domain1-weblogic-credentials # Whether to include the WebLogic Server stdout in the pod\u0026#39;s stdout, default is true includeServerOutInPodLog: true # Whether to enable overriding your log file location, see also \u0026#39;logHome\u0026#39; #logHomeEnabled: false # The location for domain log, server logs, server out, introspector out, and Node Manager log files # see also \u0026#39;logHomeEnabled\u0026#39;, \u0026#39;volumes\u0026#39;, and \u0026#39;volumeMounts\u0026#39;. #logHome: /shared/logs/sample-domain1 # Set which WebLogic Servers the Operator will start # - \u0026#34;NEVER\u0026#34; will not start any server in the domain # - \u0026#34;ADMIN_ONLY\u0026#34; will start up only the administration server (no managed servers will be started) # - \u0026#34;IF_NEEDED\u0026#34; will start all non-clustered servers, including the administration server, and clustered servers up to their replica count. serverStartPolicy: \u0026#34;IF_NEEDED\u0026#34; # Settings for all server pods in the domain including the introspector job pod serverPod: # Optional new or overridden environment variables for the domain\u0026#39;s pods # - This sample uses CUSTOM_DOMAIN_NAME in its image model file # to set the WebLogic domain name env: - name: CUSTOM_DOMAIN_NAME value: \u0026#34;domain1\u0026#34; - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false\u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-XX:+UseContainerSupport -Djava.security.egd=file:/dev/./urandom \u0026#34; # Optional volumes and mounts for the domain\u0026#39;s pods. See also \u0026#39;logHome\u0026#39;. #volumes: #- name: weblogic-domain-storage-volume # persistentVolumeClaim: # claimName: sample-domain1-weblogic-sample-pvc #volumeMounts: #- mountPath: /shared # name: weblogic-domain-storage-volume # The desired behavior for starting the domain\u0026#39;s administration server. adminServer: # The serverStartState legal values are \u0026#34;RUNNING\u0026#34; or \u0026#34;ADMIN\u0026#34; # \u0026#34;RUNNING\u0026#34; means the listed server will be started up to \u0026#34;RUNNING\u0026#34; mode # \u0026#34;ADMIN\u0026#34; means the listed server will be start up to \u0026#34;ADMIN\u0026#34; mode serverStartState: \u0026#34;RUNNING\u0026#34; # Setup a Kubernetes node port for the administration server default channel #adminService: # channels: # - channelName: default # nodePort: 30701 # The number of managed servers to start for unlisted clusters replicas: 1 # The desired behavior for starting a specific cluster\u0026#39;s member servers clusters: - clusterName: cluster-1 serverStartState: \u0026#34;RUNNING\u0026#34; replicas: 2 # Change the `restartVersion` to force the introspector job to rerun # and apply any new model configuration, to also force a subsequent # roll of your domain\u0026#39;s WebLogic Server pods. restartVersion: \u0026#39;1\u0026#39; configuration: # Settings for domainHomeSourceType \u0026#39;FromModel\u0026#39; model: # Valid model domain types are \u0026#39;WLS\u0026#39;, \u0026#39;JRF\u0026#39;, and \u0026#39;RestrictedJRF\u0026#39;, default is \u0026#39;WLS\u0026#39; domainType: \u0026#34;WLS\u0026#34; # Optional configmap for additional models and variable files #configMap: sample-domain1-wdt-config-map # All \u0026#39;FromModel\u0026#39; domains require a runtimeEncryptionSecret with a \u0026#39;password\u0026#39; field runtimeEncryptionSecret: sample-domain1-runtime-encryption-secret # Secrets that are referenced by model yaml macros # (the model yaml in the optional configMap or in the image) #secrets: #- sample-domain1-datasource-secret      Click here to view the JRF Domain YAML file.   # Copyright (c) 2020, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026#34;weblogic.oracle/v8\u0026#34; kind: Domain metadata: name: sample-domain1 namespace: sample-domain1-ns labels: weblogic.domainUID: sample-domain1 spec: # Set to \u0026#39;FromModel\u0026#39; to indicate \u0026#39;Model in Image\u0026#39;. domainHomeSourceType: FromModel # The WebLogic Domain Home, this must be a location within # the image for \u0026#39;Model in Image\u0026#39; domains. domainHome: /u01/domains/sample-domain1 # The WebLogic Server image that the Operator uses to start the domain image: \u0026#34;model-in-image:JRF-v1\u0026#34; # Defaults to \u0026#34;Always\u0026#34; if image tag (version) is \u0026#39;:latest\u0026#39; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: regsecret # Identify which Secret contains the WebLogic Admin credentials, # the secret must contain \u0026#39;username\u0026#39; and \u0026#39;password\u0026#39; fields. webLogicCredentialsSecret: name: sample-domain1-weblogic-credentials # Whether to include the WebLogic Server stdout in the pod\u0026#39;s stdout, default is true includeServerOutInPodLog: true # Whether to enable overriding your log file location, see also \u0026#39;logHome\u0026#39; #logHomeEnabled: false # The location for domain log, server logs, server out, introspector out, and Node Manager log files # see also \u0026#39;logHomeEnabled\u0026#39;, \u0026#39;volumes\u0026#39;, and \u0026#39;volumeMounts\u0026#39;. #logHome: /shared/logs/sample-domain1 # Set which WebLogic Servers the Operator will start # - \u0026#34;NEVER\u0026#34; will not start any server in the domain # - \u0026#34;ADMIN_ONLY\u0026#34; will start up only the administration server (no managed servers will be started) # - \u0026#34;IF_NEEDED\u0026#34; will start all non-clustered servers, including the administration server, and clustered servers up to their replica count. serverStartPolicy: \u0026#34;IF_NEEDED\u0026#34; # Settings for all server pods in the domain including the introspector job pod serverPod: # Optional new or overridden environment variables for the domain\u0026#39;s pods # - This sample uses CUSTOM_DOMAIN_NAME in its image model file # to set the WebLogic domain name env: - name: CUSTOM_DOMAIN_NAME value: \u0026#34;domain1\u0026#34; - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false\u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-XX:+UseContainerSupport -Djava.security.egd=file:/dev/./urandom \u0026#34; # Optional volumes and mounts for the domain\u0026#39;s pods. See also \u0026#39;logHome\u0026#39;. #volumes: #- name: weblogic-domain-storage-volume # persistentVolumeClaim: # claimName: sample-domain1-weblogic-sample-pvc #volumeMounts: #- mountPath: /shared # name: weblogic-domain-storage-volume # The desired behavior for starting the domain\u0026#39;s administration server. adminServer: # The serverStartState legal values are \u0026#34;RUNNING\u0026#34; or \u0026#34;ADMIN\u0026#34; # \u0026#34;RUNNING\u0026#34; means the listed server will be started up to \u0026#34;RUNNING\u0026#34; mode # \u0026#34;ADMIN\u0026#34; means the listed server will be start up to \u0026#34;ADMIN\u0026#34; mode serverStartState: \u0026#34;RUNNING\u0026#34; # Setup a Kubernetes node port for the administration server default channel #adminService: # channels: # - channelName: default # nodePort: 30701 # The number of managed servers to start for unlisted clusters replicas: 1 # The desired behavior for starting a specific cluster\u0026#39;s member servers clusters: - clusterName: cluster-1 serverStartState: \u0026#34;RUNNING\u0026#34; replicas: 2 # Change the restartVersion to force the introspector job to rerun # and apply any new model configuration, to also force a subsequent # roll of your domain\u0026#39;s WebLogic Server pods. restartVersion: \u0026#39;1\u0026#39; configuration: # Settings for domainHomeSourceType \u0026#39;FromModel\u0026#39; model: # Valid model domain types are \u0026#39;WLS\u0026#39;, \u0026#39;JRF\u0026#39;, and \u0026#39;RestrictedJRF\u0026#39;, default is \u0026#39;WLS\u0026#39; domainType: \u0026#34;JRF\u0026#34; # Optional configmap for additional models and variable files #configMap: sample-domain1-wdt-config-map # All \u0026#39;FromModel\u0026#39; domains require a runtimeEncryptionSecret with a \u0026#39;password\u0026#39; field runtimeEncryptionSecret: sample-domain1-runtime-encryption-secret # Secrets that are referenced by model yaml macros # (the model yaml in the optional configMap or in the image) secrets: #- sample-domain1-datasource-secret - sample-domain1-rcu-access # Increase the introspector job active timeout value for JRF use cases introspectorJobActiveDeadlineSeconds: 300 opss: # Name of secret with walletPassword for extracting the wallet, used for JRF domains walletPasswordSecret: sample-domain1-opss-wallet-password-secret # Name of secret with walletFile containing base64 encoded opss wallet, used for JRF domains #walletFileSecret: sample-domain1-opss-walletfile-secret     Note: Before you deploy the domain custom resource, determine if you have Kubernetes cluster worker nodes that are remote to your local machine. If so, you need to put the Domain\u0026rsquo;s image in a location that these nodes can access and you may also need to modify your Domain YAML file to reference the new location. See Ensuring your Kubernetes cluster can access images.\n Run the following command to create the domain custom resource:\n$ kubectl apply -f /tmp/mii-sample/domain-resources/WLS/mii-initial-d1-WLS-v1.yaml  Note: If you are choosing not to use the predefined Domain YAML file and instead created your own Domain YAML file earlier, then substitute your custom file name in the above command. Previously, we suggested naming it /tmp/mii-sample/mii-initial.yaml.\n If you run kubectl get pods -n sample-domain1-ns --watch, then you will see the introspector job run and your WebLogic Server pods start. The output will look something like this:\n  Click here to expand.   $ kubectl get pods -n sample-domain1-ns --watch ``` ``` NAME READY STATUS RESTARTS AGE sample-domain1-introspector-lqqj9 0/1 Pending 0 0s sample-domain1-introspector-lqqj9 0/1 ContainerCreating 0 0s sample-domain1-introspector-lqqj9 1/1 Running 0 1s sample-domain1-introspector-lqqj9 0/1 Completed 0 65s sample-domain1-introspector-lqqj9 0/1 Terminating 0 65s sample-domain1-admin-server 0/1 Pending 0 0s sample-domain1-admin-server 0/1 ContainerCreating 0 0s sample-domain1-admin-server 0/1 Running 0 1s sample-domain1-admin-server 1/1 Running 0 32s sample-domain1-managed-server1 0/1 Pending 0 0s sample-domain1-managed-server2 0/1 Pending 0 0s sample-domain1-managed-server1 0/1 ContainerCreating 0 0s sample-domain1-managed-server2 0/1 ContainerCreating 0 0s sample-domain1-managed-server1 0/1 Running 0 2s sample-domain1-managed-server2 0/1 Running 0 2s sample-domain1-managed-server1 1/1 Running 0 43s sample-domain1-managed-server2 1/1 Running 0 42s    Alternatively, you can run /tmp/mii-sample/utils/wl-pod-wait.sh -p 3. This is a utility script that provides useful information about a domain\u0026rsquo;s pods and waits for them to reach a ready state, reach their target restartVersion, and reach their target image before exiting.\n  Click here to display the `wl-pod-wait.sh` usage.   $ ./wl-pod-wait.sh -? ``` ``` Usage: wl-pod-wait.sh [-n mynamespace] [-d mydomainuid] \\  [-p expected_pod_count] \\  [-t timeout_secs] \\  [-q] Exits non-zero if \u0026#39;timeout_secs\u0026#39; is reached before \u0026#39;pod_count\u0026#39; is reached. Parameters: -d \u0026lt;domain_uid\u0026gt; : Defaults to \u0026#39;sample-domain1\u0026#39;. -n \u0026lt;namespace\u0026gt; : Defaults to \u0026#39;sample-domain1-ns\u0026#39;. -p 0 : Wait until there are no running WebLogic Server pods for a domain. The default. -p \u0026lt;pod_count\u0026gt; : Wait until all of the following are true for exactly \u0026#39;pod_count\u0026#39; WebLogic Server pods in the domain: - ready - same \u0026#39;weblogic.domainRestartVersion\u0026#39; label value as the domain resource\u0026#39;s \u0026#39;spec.restartVersion\u0026#39; - same \u0026#39;weblogic.introspectVersion\u0026#39; label value as the domain resource\u0026#39;s \u0026#39;spec.introspectVersion\u0026#39; - same image as the the domain resource\u0026#39;s image -t \u0026lt;timeout\u0026gt; : Timeout in seconds. Defaults to \u0026#39;1000\u0026#39;. -q : Quiet mode. Show only a count of wl pods that have reached the desired criteria. -? : This help.      Click here to view sample output from `wl-pod-wait.sh`.   @@ [2020-04-30T13:50:42][seconds=0] Info: Waiting up to 1000 seconds for exactly '3' WebLogic Server pods to reach the following criteria: @@ [2020-04-30T13:50:42][seconds=0] Info: ready='true' @@ [2020-04-30T13:50:42][seconds=0] Info: image='model-in-image:WLS-v1' @@ [2020-04-30T13:50:42][seconds=0] Info: domainRestartVersion='1' @@ [2020-04-30T13:50:42][seconds=0] Info: namespace='sample-domain1-ns' @@ [2020-04-30T13:50:42][seconds=0] Info: domainUID='sample-domain1' @@ [2020-04-30T13:50:42][seconds=0] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:50:42][seconds=0] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------------------- ------- ----- ----- --------- 'sample-domain1-introspector-rkdkg' '' '' '' 'Pending' @@ [2020-04-30T13:50:45][seconds=3] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:50:45][seconds=3] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------------------- ------- ----- ----- --------- 'sample-domain1-introspector-rkdkg' '' '' '' 'Running' @@ [2020-04-30T13:51:50][seconds=68] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:51:50][seconds=68] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE ---- ------- ----- ----- ----- @@ [2020-04-30T13:51:59][seconds=77] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:51:59][seconds=77] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE ----------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'false' 'Pending' @@ [2020-04-30T13:52:02][seconds=80] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:52:02][seconds=80] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE ----------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'false' 'Running' @@ [2020-04-30T13:52:32][seconds=110] Info: '1' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:52:32][seconds=110] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'false' 'Pending' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'false' 'Pending' @@ [2020-04-30T13:52:34][seconds=112] Info: '1' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:52:34][seconds=112] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'false' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'false' 'Running' @@ [2020-04-30T13:53:14][seconds=152] Info: '3' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:53:14][seconds=152] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:53:14][seconds=152] Info: Success!    If you see an error, then consult Debugging in the Model in Image user guide.\nInvoke the web application Now that all the initial use case resources have been deployed, you can invoke the sample web application through the Traefik ingress controller\u0026rsquo;s NodePort.\nNote: The web application will display a list of any data sources it finds, but at this point, we don\u0026rsquo;t expect it to find any because the model doesn\u0026rsquo;t contain any.\nSend a web application request to the load balancer:\n$ curl -s -S -m 10 -H \u0026#39;host: sample-domain1-cluster-cluster-1.mii-sample.org\u0026#39; \\  http://localhost:30305/myapp_war/index.jsp Or, if Traefik is unavailable and your Administration Server pod is running, you can use kubectl exec:\n$ kubectl exec -n sample-domain1-ns sample-domain1-admin-server -- bash -c \\  \u0026#34;curl -s -S -m 10 http://sample-domain1-cluster-cluster-1:8001/myapp_war/index.jsp\u0026#34; You will see output like the following:\n\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v1\u0026#39; of the mii-sample JSP web-app. Welcome to WebLogic Server \u0026#39;managed-server2\u0026#39;! domain UID = \u0026#39;sample-domain1\u0026#39; domain name = \u0026#39;domain1\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found min threads constraint runtime named \u0026#39;SampleMinThreads\u0026#39; with configured count: 1 Found max threads constraint runtime named \u0026#39;SampleMaxThreads\u0026#39; with configured count: 10 Found 0 local data sources: ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; Note: If you\u0026rsquo;re running your curl commands on a remote machine, then substitute localhost with an external address suitable for contacting your Kubernetes cluster. A Kubernetes cluster address that often works can be obtained by using the address just after https:// in the KubeDNS line of the output from the kubectl cluster-info command.\nIf you want to continue to the Update 1 use case, then leave your domain running.\nTo remove the resources you have created in this sample, see Cleanup.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-operators/using-helm/",
	"title": "Configure the operator using Helm",
	"tags": [],
	"description": "",
	"content": "Contents  Useful Helm operations Operator Helm configuration values  Overall operator information Creating the operator pod WebLogic domain management Elastic Stack integration REST interface configuration Debugging options   Common mistakes and solutions  Note that the operator Helm chart is available from the GitHub chart repository. For more details, see Alternatively, install the operator Helm chart from the GitHub chart repository.\nUseful Helm operations Show the available operator configuration values and their defaults:\n$ helm inspect values kubernetes/charts/weblogic-operator Show the custom values you configured for the operator Helm release:\n$ helm get values weblogic-operator Show all of the values your operator Helm release is using:\n$ helm get values --all weblogic-operator List the Helm releases for a specified namespace or all namespaces:\n$ helm list --namespace \u0026lt;namespace\u0026gt; $ helm list --all-namespaces Get the status of the operator Helm release:\n$ helm status weblogic-operator --namespace \u0026lt;namespace\u0026gt; Show the history of the operator Helm release:\n$ helm history weblogic-operator --namespace \u0026lt;namespace\u0026gt; Roll back to a previous version of this operator Helm release, in this case, the first version:\n$ helm rollback weblogic-operator 1 --namespace \u0026lt;namespace\u0026gt; Change one or more values in the operator Helm release. In this example, the --reuse-values flag indicates that previous overrides of other values should be retained:\n$ helm upgrade \\  --reuse-values \\  --set \u0026#34;domainNamespaces={sample-domains-ns1}\u0026#34; \\  --set \u0026#34;javaLoggingLevel=FINE\u0026#34; \\  --wait \\  weblogic-operator \\  kubernetes/charts/weblogic-operator Operator Helm configuration values This section describes the details of the operator Helm chart\u0026rsquo;s available configuration values.\nOverall operator information serviceAccount Specifies the name of the service account in the operator\u0026rsquo;s namespace that the operator will use to make requests to the Kubernetes API server. You are responsible for creating the service account. The helm install or helm upgrade command with a non-existing service account results in a Helm chart validation error.\nDefaults to default.\nExample:\nserviceAccount: \u0026#34;weblogic-operator\u0026#34; javaLoggingLevel Specifies the level of Java logging that should be enabled in the operator. Valid values are: SEVERE, WARNING, INFO, CONFIG, FINE, FINER, and FINEST.\nDefaults to INFO.\nExample:\njavaLoggingLevel: \u0026#34;FINE\u0026#34; kubernetesPlatform Specify the Kubernetes platform on which the operator is running. This setting has no default, the only valid value is OpenShift, and the setting should be left unset for other platforms. When set to OpenShift, the operator:\n Sets the domain home file permissions in each WebLogic Server pod to work correctly in OpenShift for Model in Image, and Domain home in Image domains. Specifically, it sets file group permissions so that they match file user permissions. Sets the weblogic.SecureMode.WarnOnInsecureFileSystem Java system property to false on the command line of each WebLogic Server. This flag suppresses insecure file system warnings reported in the WebLogic Server console when the WebLogic Server is in production mode. These warnings result from setting the file permissions necessary to work with restricted security context constraints on OpenShift.  For more information about the security requirements for running WebLogic in OpenShift, see the OpenShift chapter in the Security section.\nExample:\nkubernetesPlatform: OpenShift Creating the operator pod image Specifies the container image containing the operator code.\nDefaults to ghcr.io/oracle/weblogic-kubernetes-operator:3.3.2.\nExample:\nimage: \u0026#34;ghcr.io/oracle/weblogic-kubernetes-operator:some-tag\u0026#34; imagePullPolicy Specifies the image pull policy for the operator container image.\nDefaults to IfNotPresent.\nExample:\nimage: \u0026#34;Always\u0026#34; imagePullSecrets Contains an optional list of Kubernetes Secrets, in the operator\u0026rsquo;s namespace, that are needed to access the registry containing the operator image. You are responsible for creating the secret. If no secrets are required, then omit this property.\nExample:\nimagePullSecrets: - name: \u0026#34;my-image-pull-secret\u0026#34; annotations Specifies a set of key-value annotations that will be added to each pod running the operator. If no customer defined annotations are required, then omit this property.\nExample:\nannotations: stage: production You may also specify annotations using the --set parameter to the Helm install command, as follows:\n--set annotations.stage=production labels Specifies a set of key-value labels that will be added to each pod running the operator. The Helm chart will automatically add any required labels, so the customer is not required to define those here. If no customer defined labels are required, then omit this property.\nExample:\nlabels: sidecar.istio.io/inject: \u0026#34;false\u0026#34; You may also specify labels using the --set parameter to the Helm install command, as follows:\n--set labels.\u0026quot;sidecar\\.istio\\.io/inject\u0026quot;=false nodeSelector Allows you to run the operator Pod on a Node whose labels match the specified nodeSelector labels. You can use this optional feature if you want the operator Pod to run on a Node with particular labels. For more details, see Assign Pods to Nodes in the Kubernetes documentation for more details. This is not required if the operator Pod can run on any Node.\nExample:\nnodeSelector: disktype: ssd affinity Allows you to constrain the operator Pod to be scheduled on a Node with certain labels; it is conceptually similar to nodeSelector. affinity provides advanced capabilities to limit Pod placement on specific Nodes. For more details, see Assign Pods to Nodes in the Kubernetes documentation for more details. This is optional and not required if the operator Pod can run on any Node or when using nodeSelector.\nExample:\naffinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: nodeType operator: In values: - dev - test preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: another-node-label-key operator: In values: - another-node-label-value enableClusterRoleBinding Specifies whether the roles necessary for the operator to manage domains will be granted using a ClusterRoleBinding rather than using RoleBindings in each managed namespace.\nDefaults to false.\nThis option greatly simplifies managing namespaces when the selection is done using label selectors or regular expressions as the operator will already have privilege in any namespace.\nCustomers who deploy the operator in Kubernetes clusters that run unrelated workloads will likely not want to use this option.\nIf enableClusterRoleBinding is false and you select namespaces that the operator will manage using label selectors or a regular expression, then the Helm release will only include RoleBindings in each namespace that match at the time the Helm release is created. If you later create namespaces that the operator should manage, the new namespaces will not yet have the necessary RoleBinding.\nYou can correct this by upgrading the Helm release and reusing values:\n$ helm upgrade \\  --reuse-values \\  weblogic-operator \\  kubernetes/charts/weblogic-operator WebLogic domain management domainNamespaceSelectionStrategy Specifies how the operator will select the set of namespaces that it will manage. Legal values are: List, LabelSelector, RegExp, and Dedicated.\nDefaults to List.\nIf set to List, then the operator will manage the set of namespaces listed by the domainNamespaces value. If set to LabelSelector, then the operator will manage the set of namespaces discovered by a list of namespaces using the value specified by domainNamespaceLabelSelector as a label selector. If set to RegExp, then the operator will manage the set of namespaces discovered by a list of namespaces using the value specified by domainNamespaceRegExp as a regular expression matched against the namespace names. Finally, if set to Dedicated, then operator will manage WebLogic Domains only in the same namespace which the operator itself is deployed, which is the namespace of the Helm release.\ndomainNamespaces Specifies a list of namespaces that the operator manages. The names must be lowercase. You are responsible for creating these namespaces. The operator will only manage Domains found in these namespaces. This value is required if domainNamespaceSelectionStrategy is List and ignored otherwise.\nExample 1: In the configuration below, the operator will manage the default Kubernetes Namespace:\ndomainNamespaces: - \u0026#34;default\u0026#34; Example 2: In the configuration below, the operator will manage namespace1 and namespace2:\ndomainNamespaces: [ \u0026#34;namespace1\u0026#34;, \u0026#34;namespace2\u0026#34; ]  These examples show two valid YAML syntax options for arrays.\n You must include the default namespace in the list if you want the operator to monitor both the default namespace and some other namespaces.\n This value is ignored if dedicated is set to true. Then, the operator will manage only domains in its own namespace.\n For more details about managing domainNamespaces, see Managing domain namespaces.\ndomainNamespaceLabelSelector Specifies a label selector that will be used when searching for namespaces that the operator will manage. The operator will only manage Domains found in namespaces matching this selector. This value is required if domainNamespaceSelectionStrategy is LabelSelector and ignored otherwise.\nIf enableClusterRoleBinding is false, the Helm chart will create RoleBindings in each namespace that matches the selector. These RoleBindings give the operator\u0026rsquo;s service account the necessary privileges in the namespace. The Helm chart will only create these RoleBindings in namespaces that match the label selector at the time the chart is installed. If you later create namespaces that match the selector or label existing namespaces that make them now match the selector, then the operator will not have privilege in these namespaces until you upgrade the Helm release.\nExample 1: In the configuration below, the operator will manage namespaces that have the label \u0026ldquo;weblogic-operator\u0026rdquo; regardless of the value of that label:\ndomainNamespaceLabelSelector: weblogic-operator Example 2: In the configuration below, the operator will manage all namespaces that have the label \u0026ldquo;environment\u0026rdquo;, but where the value of that label is not \u0026ldquo;production\u0026rdquo; or \u0026ldquo;systemtest\u0026rdquo;:\ndomainNamespaceLabelSelector: environment notin (production,systemtest)  To specify the above sample on the Helm command line, escape spaces and commas as follows:\n--set \u0026quot;domainNamespaceLabelSelector=environment\\\\ notin\\\\ (production\\\\,systemtest)\u0026quot;  domainNamespaceRegExp Specifies a regular expression that will be used when searching for namespaces that the operator will manage. The operator will only manage Domains found in namespaces matching this regular expression. This value is required if domainNamespaceSelectionStrategy is RegExp and ignored otherwise.\nIf enableClusterRoleBinding is false, the Helm chart will create RoleBindings in each namespace that matches the regular expression. These RoleBindings give the operator\u0026rsquo;s service account the necessary privileges in the namespace. The Helm chart will only create these RoleBindings in namespaces that match the regular expression at the time the chart is installed. If you later create namespaces that match the selector or label existing namespaces that make them now match the selector, the operator will not have privilege in these namespaces until you upgrade the Helm release.\nThe regular expression functionality included with Helm is restricted to linear time constructs and, in particular, does not support lookarounds. The operator, written in Java, supports these complicated expressions. If you need to use a complex regular expression, then either set enableClusterRoleBinding to true or create the necessary RoleBindings outside of Helm.\n dedicated (Deprecated) Specifies if this operator will manage WebLogic domains only in the same namespace in which the operator itself is deployed. If set to true, then the domainNamespaces value is ignored.\nThis field is deprecated. Use domainNamespaceSelectionStrategy: Dedicated instead.\nDefaults to false.\nExample:\ndedicated: false In the dedicated mode, the operator does not require permissions to access the cluster-scoped Kubernetes resources, such as CustomResourceDefinitions, PersistentVolumes, and Namespaces. In those situations, the operator may skip some of its operations, such as verifying the WebLogic domain CustomResoruceDefinition domains.weblogic.oracle (and creating it when it is absent), watching namespace events, and cleaning up PersistentVolumes as part of deleting a domain.\nIt is the responsibility of the administrator to make sure that the required CustomResourceDefinition (CRD) domains.weblogic.oracle is deployed in the Kubernetes cluster before the operator is installed. The creation of the CRD requires the Kubernetes cluster-admin privileges. A YAML file for creating the CRD can be found at domain-crd.yaml.\n domainPresenceFailureRetryMaxCount and domainPresenceFailureRetrySeconds Specify the number of introspector job retries for a Domain and the interval in seconds between these retries.\nDefaults to 5 retries and 10 seconds between each retry.\nExample:\ndomainPresenceFailureRetryMaxCount: 10 domainPresenceFailureRetrySeconds: 30 introspectorJobNameSuffix and externalServiceNameSuffix Specify the suffixes that the operator uses to form the name of the Kubernetes job for the domain introspector, and the name of the external service for the WebLogic Administration Server, if the external service is enabled.\nDefaults to -introspector and -ext respectively. The values cannot be more than 25 and 10 characters respectively.\nPrior to the operator 3.1.0 release, the suffixes are hard-coded to -introspect-domain-job and -external. The defaults are shortened in newer releases to support longer names in the domain resource and WebLogic domain configurations, such as the domainUID, and WebLogic cluster and server names.\n In order to work with Kubernetes limits to resource names, the resultant names for the domain introspector job and the external service should not be more than 63 characters. For more details, see Meet Kubernetes resource name restrictions.\n clusterSizePaddingValidationEnabled Specifies if the operator needs to reserve additional padding when validating the server service names to account for longer Managed Server names as a result of expanding a cluster\u0026rsquo;s size in WebLogic domain configurations.\nDefaults to true.\nIf clusterSizePaddingValidationEnabed is set to true, two additional characters will be reserved if the configured cluster\u0026rsquo;s size is between one and nine, and one additional character will be reserved if the configured cluster\u0026rsquo;s size is between 10 and 99. No additional character is reserved if the configured cluster\u0026rsquo;s size is greater than 99.\nElastic Stack integration elkIntegrationEnabled Specifies whether or not Elastic Stack integration is enabled.\nDefaults to false.\nExample:\nelkIntegrationEnabled: true logStashImage Specifies the container image containing Logstash. This parameter is ignored if elkIntegrationEnabled is false.\nDefaults to logstash:6.6.0.\nExample:\nlogStashImage: \u0026#34;logstash:6.2\u0026#34; elasticSearchHost Specifies the hostname where Elasticsearch is running. This parameter is ignored if elkIntegrationEnabled is false.\nDefaults to elasticsearch.default.svc.cluster.local.\nExample:\nelasticSearchHost: \u0026#34;elasticsearch2.default.svc.cluster.local\u0026#34; elasticSearchPort Specifies the port number where Elasticsearch is running. This parameter is ignored if elkIntegrationEnabled is false.\nDefaults to 9200.\nExample:\nelasticSearchPort: 9201 REST interface configuration externalRestEnabled Determines whether the operator\u0026rsquo;s REST interface will be exposed outside the Kubernetes cluster.\nDefaults to false.\nIf set to true, you must provide the externalRestIdentitySecret property that contains the name of the Kubernetes Secret which contains the SSL certificate and private key for the operator\u0026rsquo;s external REST interface.\nExample:\nexternalRestEnabled: true externalRestHttpsPort Specifies the node port that should be allocated for the external operator REST HTTPS interface.\nOnly used when externalRestEnabled is true, otherwise ignored.\nDefaults to 31001.\nExample:\nexternalRestHttpsPort: 32009 externalRestIdentitySecret Specifies the user supplied secret that contains the SSL/TLS certificate and private key for the external operator REST HTTPS interface. The value must be the name of the Kubernetes tls secret previously created in the namespace where the operator is deployed. This parameter is required if externalRestEnabled is true, otherwise, it is ignored. In order to create the Kubernetes tls secret you can use the following command:\n$ kubectl create secret tls \u0026lt;secret-name\u0026gt; \\  -n \u0026lt;operator-namespace\u0026gt; \\  --cert=\u0026lt;path-to-certificate\u0026gt; \\  --key=\u0026lt;path-to-private-key\u0026gt; There is no default value.\nThe Helm installation will produce an error, similar to the following, if externalRestIdentitySecret is not specified (left blank) and externalRestEnabled is true:\nError: render error in \u0026quot;weblogic-operator/templates/main.yaml\u0026quot;: template: weblogic-operator/templates/main.yaml:9:3: executing \u0026quot;weblogic-operator/templates/main.yaml\u0026quot; at \u0026lt;include \u0026quot;operator.va...\u0026gt;: error calling include: template: weblogic-operator/templates/_validate-inputs.tpl:42:14: executing \u0026quot;operator.validateInputs\u0026quot; at \u0026lt;include \u0026quot;utils.endVa...\u0026gt;: error calling include: template: weblogic-operator/templates/_utils.tpl:22:6: executing \u0026quot;utils.endValidation\u0026quot; at \u0026lt;fail $scope.validati...\u0026gt;: error calling fail: string externalRestIdentitySecret must be specified Example:\nexternalRestIdentitySecret: weblogic-operator-external-rest-identity externalOperatorCert (Deprecated) Use externalRestIdentitySecret instead\n Specifies the user supplied certificate to use for the external operator REST HTTPS interface. The value must be a string containing a Base64 encoded PEM certificate. This parameter is required if externalRestEnabled is true, otherwise, it is ignored.\nThere is no default value.\nThe Helm installation will produce an error, similar to the following, if externalOperatorCert is not specified (left blank) and externalRestEnabled is true:\nError: render error in \u0026quot;weblogic-operator/templates/main.yaml\u0026quot;: template: weblogic-operator/templates/main.yaml:4:3: executing \u0026quot;weblogic-operator/templates/main.yaml\u0026quot; at \u0026lt;include \u0026quot;operator.va...\u0026gt;: error calling include: template: weblogic-operator/templates/_validate-inputs.tpl:53:4: executing \u0026quot;operator.validateInputs\u0026quot; at \u0026lt;include \u0026quot;operator.re...\u0026gt;: error calling include: template: weblogic-operator/templates/_utils.tpl:137:6: executing \u0026quot;operator.reportValidationErrors\u0026quot; at \u0026lt;fail .validationErro...\u0026gt;: error calling fail: The string property externalOperatorCert must be specified. Example:\nexternalOperatorCert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQwakNDQXJxZ0F3S ... externalOperatorKey (Deprecated) Use externalRestIdentitySecret instead\n Specifies user supplied private key to use for the external operator REST HTTPS interface. The value must be a string containing a Base64 encoded PEM key. This parameter is required if externalRestEnabled is true, otherwise, it is ignored.\nThere is no default value.\nThe Helm installation will produce an error, similar to the following, if externalOperatorKey is not specified (left blank) and externalRestEnabled is true:\nError: render error in \u0026quot;weblogic-operator/templates/main.yaml\u0026quot;: template: weblogic-operator/templates/main.yaml:4:3: executing \u0026quot;weblogic-operator/templates/main.yaml\u0026quot; at \u0026lt;include \u0026quot;operator.va...\u0026gt;: error calling include: template: weblogic-operator/templates/_validate-inputs.tpl:53:4: executing \u0026quot;operator.validateInputs\u0026quot; at \u0026lt;include \u0026quot;operator.re...\u0026gt;: error calling include: template: weblogic-operator/templates/_utils.tpl:137:6: executing \u0026quot;operator.reportValidationErrors\u0026quot; at \u0026lt;fail .validationErro...\u0026gt;: error calling fail: The string property externalOperatorKey must be specified. Example:\nexternalOperatorKey: QmFnIEF0dHJpYnV0ZXMKICAgIGZyaWVuZGx5TmFtZTogd2VibG9naWMtb3B ... tokenReviewAuthentication If set to true, tokenReviewAuthentication specifies whether the the operator\u0026rsquo;s REST API should use:\n Kubernetes token review API for authenticating users and Kubernetes subject access review API for authorizing a user\u0026rsquo;s operation (get, list, patch, and such) on a resource. Update the Domain resource using the operator\u0026rsquo;s privileges.  If set to false, the operator\u0026rsquo;s REST API will use the caller\u0026rsquo;s bearer token for any update to the Domain resource so that it is done using the caller\u0026rsquo;s privileges.\nDefaults to false.\nExample:\ntokenReviewAuthentication: true Debugging options remoteDebugNodePortEnabled Specifies whether or not the operator will start a Java remote debug server on the provided port and suspend execution until a remote debugger has attached.\nDefaults to false.\nExample:\nremoteDebugNodePortEnabled: true internalDebugHttpPort Specifies the port number inside the Kubernetes cluster for the operator\u0026rsquo;s Java remote debug server.\nThis parameter is required if remoteDebugNodePortEnabled is true. Otherwise, it is ignored.\nDefaults to 30999.\nExample:\ninternalDebugHttpPort: 30888 externalDebugHttpPort Specifies the node port that should be allocated for the Kubernetes cluster for the operator\u0026rsquo;s Java remote debug server.\nThis parameter is required if remoteDebugNodePortEnabled is true. Otherwise, it is ignored.\nDefaults to 30999.\nExample:\nexternalDebugHttpPort: 30777 Common mistakes and solutions Installing the operator a second time into the same namespace A new FAILED Helm release is created.\n$ helm install --no-hooks --name op2 --namespace myuser-op-ns --values custom-values.yaml kubernetes/charts/weblogic-operator Error: release op2 failed: secrets \u0026quot;weblogic-operator-secrets\u0026quot; already exists Both the previous and new release own the resources created by the previous operator.\n You can\u0026rsquo;t modify it to change the namespace (because helm upgrade does not let you change the namespace). You can\u0026rsquo;t fix it by deleting this release because it removes your previous operator\u0026rsquo;s resources. You can\u0026rsquo;t fix it by rolling back this release because it is not in the DEPLOYED state. You can\u0026rsquo;t fix it by deleting the previous release because it removes the operator\u0026rsquo;s resources too. All you can do is delete both operator releases and reinstall the original operator. See https://github.com/helm/helm/issues/2349  Installing an operator and having it manage a domain namespace that another operator is already managing A new FAILED Helm release is created.\n$ helm install --no-hooks --name op2 --namespace myuser-op2-ns --values custom-values.yaml kubernetes/charts/weblogic-operator Error: release op2 failed: rolebindings.rbac.authorization.k8s.io \u0026quot;weblogic-operator-rolebinding-namespace\u0026quot; already exists To recover:\n helm delete --purge the failed release.  NOTE: This deletes the role binding in the domain namespace that was created by the first operator release, to give the operator access to the domain namespace.   helm upgrade \u0026lt;old op release\u0026gt; kubernetes/charts/weblogic-operator --values \u0026lt;old op custom-values.yaml\u0026gt;  This recreates the role binding. There might be intermittent failures in the operator for the period of time when the role binding was deleted.    Upgrading an operator and having it manage a domain namespace that another operator is already managing The helm upgrade succeeds, and silently adopts the resources the first operator\u0026rsquo;s Helm chart created in the domain namespace (for example, rolebinding), and, if you also instructed it to stop managing another domain namespace, it abandons the role binding it created in that namespace.\nFor example, if you delete this release, then the first operator will end up without the role binding it needs. The problem is that you don\u0026rsquo;t get a warning, so you don\u0026rsquo;t know that there\u0026rsquo;s a problem to fix.\n This can be fixed by just upgrading the Helm release. This may also be fixed by rolling back the Helm release.  Installing an operator and assigning it the same external REST port number as another operator A new FAILED Helm release is created.\n$ helm install --no-hooks --name op2 --namespace myuser-op2-ns --values o.yaml kubernetes/charts/weblogic-operator Error: release op2 failed: Service \u0026quot;external-weblogic-operator-svc\u0026quot; is invalid: spec.ports[0].nodePort: Invalid value: 31023: provided port is already allocated To recover:\n $ helm delete --purge the failed release. Change the port number and helm install the second operator again.  Upgrading an operator and assigning it the same external REST port number as another operator The helm upgrade fails and moves the release to the FAILED state.\n$ helm upgrade --no-hooks --values o23.yaml op2 kubernetes/charts/weblogic-operator --wait Error: UPGRADE FAILED: Service \u0026quot;external-weblogic-operator-svc\u0026quot; is invalid: spec.ports[0].nodePort: Invalid value: 31023: provided port is already allocated  You can fix this by upgrading the Helm release (to fix the port number). You can also fix this by rolling back the Helm release.  Installing an operator and assigning it a service account that doesn\u0026rsquo;t exist The following helm install command fails because it tries to install an operator release with a non-existing service account op2-sa.\n$ helm install op2 kubernetes/charts/weblogic-operator --namespace myuser-op2-ns --set serviceAccount=op2-sa --wait --no-hooks The output contains the following error message.\nServiceAccount op2-sa not found in namespace myuser-op2-ns To recover:\n Create the service account. helm install again.  Upgrading an operator and assigning it a service account that doesn\u0026rsquo;t exist The helm upgrade with a non-existing service account fails with the same error message as mentioned in the previous section, and the existing operator deployment stays unchanged.\nTo recover:\n Create the service account. helm upgrade again.  Installing an operator and having it manage a domain namespace that doesn\u0026rsquo;t exist A new FAILED Helm release is created.\n$ helm install --no-hooks --name op2 --namespace myuser-op2-ns --values o.yaml kubernetes/charts/weblogic-operator Error: release op2 failed: namespaces \u0026quot;myuser-d2-ns\u0026quot; not found To recover:\n helm delete --purge the failed release. Create the domain namespace. helm install again.  Upgrading an operator and having it manage a domain namespace that doesn\u0026rsquo;t exist The helm upgrade fails and moves the release to the FAILED state.\n$ helm upgrade myuser-op kubernetes/charts/weblogic-operator --values o.yaml --no-hooks Error: UPGRADE FAILED: failed to create resource: namespaces \u0026quot;myuser-d2-ns\u0026quot; not found To recover:\n helm rollback Create the domain namespace. helm upgrade again.  Deleting and recreating a namespace that an operator manages without informing the operator If you create a new domain in a namespace that is deleted and recreated, the domain does not start up until you notify the operator. For more details about the problem and solutions, see Managing domain namespaces.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/domain-resource/",
	"title": "Domain resource",
	"tags": [],
	"description": "",
	"content": "Contents  Overview Prerequisites Deploying domain resource YAML files Domain resource custom resource definition (CRD) Domain resource attribute references Using kubectl explain Domain spec elements JVM memory and Java option environment variables Pod generation  Overview Use this document to create your own Domain resource, which can be used to configure the operation of your WebLogic Server domain. The Domain resource does not replace the traditional domain configuration files, but instead cooperates with those files to describe the Kubernetes artifacts of the corresponding domain. For instance, the domain configuration will still specify deployed applications, data sources, and most other details about the domain while the Domain resource will specify the number of cluster members currently running or the persistent volumes that will be mounted into the containers running WebLogic Server instances.\nMany of the samples accompanying the operator project include scripts to generate an initial Domain resource from a set of simplified inputs; however, the Domain resource is the actual source of truth for how the operator will manage each WebLogic Server domain. You are encouraged to either start with the Domain resource YAML files generated by the various samples or create Domain resources manually or by using other tools based on the schema referenced here or this documentation.\n Prerequisites The following prerequisites must be fulfilled before proceeding with the creation of a domain resource:\n Create a Kubernetes Namespace for the Domain unless the intention is to use the default namespace. Make sure the WebLogic Kubernetes Operator is running and is configured to monitor the namespace. Make sure any resources that the domain resource references are deployed to the same namespace. For example, all domain resources have a spec.webLogicCredentialsSecret field that references a Kubernetes Secret containing the username and password of the WebLogic server administrative account. Make sure a domain resource configuration and its corresponding WebLogic configuration meet Kubernetes resource name restrictions.  For example, see the Quick Start.\nDeploying domain resource YAML files Domains are defined using YAML files. For each WebLogic Server domain you want to run, you should create one Domain resource YAML file and apply it. In the example referenced below, the sample scripts generate a Domain resource YAML file that you can use as a basis. Copy the file and override the default settings so that it matches all the WebLogic Server domain parameters that define your domain.\nSee the WebLogic Server samples, Domain home on a PV, Domain home in Image, and Model in Image.\nAfter you have written your YAML files, you use them to create your domain artifacts using the kubectl apply -f command.\n$ kubectl apply -f domain-resource.yaml To confirm that the Domain was created, use this command:\n$ kubectl get domains -n [namespace] To view all of the attributes of a running domain, including the domain\u0026rsquo;s status, use this command:\n$ kubectl describe domain [domain name] -n [namespace] Or this command:\n$ kubectl get domain [domain name] -n [namespace] -o yaml Domain resource custom resource definition (CRD) The Domain type is defined by a Kubernetes CustomResourceDefinition (CRD) and, like all Kubernetes objects, is described by three sections: metadata, spec, and status.\nThe operator installs the CRD for the Domain type when the operator first starts. Customers may also choose to install the CRD in advance by using one of the provided YAML files. Installing the CRD in advance allows you to run the operator without giving it privilege (through Kubernetes roles and bindings) to access or update the CRD or other cluster-scoped resources. This may be necessary in environments where the operator cannot have cluster-scoped privileges, such as OpenShift Dedicated. The operator\u0026rsquo;s role based access control (RBAC) requirements are documented here.\n$ kubectl create -f kubernetes/crd/domain-crd.yaml After the CustomResourceDefinition is installed, either by the operator or using one of the create commands above, you can verify that the CRD is installed correctly using:\n$ kubectl get crd domains.weblogic.oracle Domain resource attribute references The domain resource metadata section names the Domain and its namespace. The name of the Domain is the default value for the domainUID which is used by the operator to distinguish domains running in the Kubernetes cluster that may have the same domain name. The Domain name must be unique in the namespace and the domainUID should be unique across the cluster. The domainUID, Domain resource name, and domain name (from the WebLogic domain configuration) may all be different.\nThe domain resource spec section describes the intended running state of the domain, including intended runtime state of WebLogic Server instances, number of cluster members started, and details about Kubernetes Pod or Service generation, such as resource constraints, scheduling requirements, or volume mounts.\nThe operator automatically updates the status section of a deploy domain resource to describe the actual running state of the domain, including WebLogic Server instance runtime states and current health.\nHere are some references you can use for the fields in these sections:\n See Domain spec elements, Pod Generation, and JVM memory and Java option environment variables in this doc. See Domain resource. Swagger documentation is available here. Use kubectl explain from the command line.  Using kubectl explain If you are using Kubernetes 1.16 or later, you can access the description of any field of the Domain using kubectl explain. For instance, the following command displays the description of the domainUID field:\n$ kubectl explain domain.spec.domainUID KIND: Domain VERSION: weblogic.oracle/v8 FIELD: domainUID \u0026lt;string\u0026gt; DESCRIPTION: Domain unique identifier. It is recommended that this value be unique to assist in future work to identify related domains in active-passive scenarios across data centers; however, it is only required that this value be unique within the namespace, similarly to the names of Kubernetes resources. This value is distinct and need not match the domain name from the WebLogic domain configuration. Defaults to the value of `metadata.name`. Domain spec elements The Domain spec section contains elements for configuring the domain operation and sub-sections specific to the Administration Server, specific clusters, or specific Managed Servers.\n Note: This section details elements that are unique to the operator. For more general Kubernetes pod elements such as environment variables, node affinity, and volumes, see Domain resource attribute references.\n Elements related to domain identification, container image, and domain home:\n domainUID: Domain unique identifier. This identifier is required to be no more than 45 characters, and practically, should be shorter in order to help ensure Kubernetes restrictions are met (for more details, see Meet Kubernetes resource name restrictions). It is recommended that this value be unique to assist in future work to identify related domains in active-passive scenarios across data centers; however, it is only required that this value be unique within the namespace, similarly to the names of Kubernetes resources. This value is distinct and need not match the domain name from the WebLogic domain configuration. Defaults to the value of metadata.name. image: The WebLogic container image; required when domainHomeSourceType is Image or FromModel; otherwise, defaults to container-registry.oracle.com/middleware/weblogic:12.2.1.4. imagePullPolicy: The image pull policy for the WebLogic container image. Legal values are Always, Never, and IfNotPresent. Defaults to Always if image ends in :latest; IfNotPresent, otherwise. imagePullSecrets: A list of image pull Secrets for the WebLogic container image. domainHome: The directory containing the WebLogic domain configuration inside the container. Defaults to /shared/domains/domains/if domainHomeSourceType is PersistentVolume. Defaults to /u01/oracle/user_projects/domains/ if domainHomeSourceType is Image. Defaults to /u01/domains/if domainHomeSourceType is FromModel. domainHomeSourceType: Domain home file system source type: Legal values: Image, PersistentVolume, FromModel. Image indicates that the domain home file system is present in the container image specified by the image field. PersistentVolume indicates that the domain home file system is located on a persistent volume. FromModel indicates that the domain home file system will be created and managed by the operator based on a WDT domain model. If this field is specified, it overrides the value of domainHomeInImage. If both fields are unspecified, then domainHomeSourceType defaults to Image. dataHome: An optional directory in a server\u0026rsquo;s container for data storage of default and custom file stores. If dataHome is not specified or its value is either not set or empty, then the data storage directories are determined from the WebLogic domain configuration.  Elements related to logging:\n includeServerOutInPodLog: Specifies whether the server .out file will be included in the Pod\u0026rsquo;s log. Defaults to true. logHome: The directory in a server\u0026rsquo;s container in which to store the domain, Node Manager, server logs, server *.out, introspector .out, and optionally HTTP access log files if httpAccessLogInLogHome is true. Ignored if logHomeEnabled is false. logHomeEnabled: Specifies whether the log home folder is enabled. Defaults to true if domainHomeSourceType is PersistentVolume; false, otherwise. httpAccessLogInLogHome: Specifies whether the server HTTP access log files will be written to the same directory specified in logHome. Otherwise, server HTTP access log files will be written to the directory configured in the WebLogic domain configuration. Defaults to true.  Elements related to security:\n webLogicCredentialsSecret: Reference to a Kubernetes Secret that contains the user name and password needed to boot a WebLogic Server under the username and password fields. See also elements under configuration below.  Elements related to domain startup and shutdown:\n serverStartPolicy: The strategy for deciding whether to start a WebLogic Server instance. Legal values are ADMIN_ONLY, NEVER, or IF_NEEDED. Defaults to IF_NEEDED. serverStartState: The WebLogic runtime state in which the server is to be started. Use ADMIN if the server should start in the admin state. Defaults to RUNNING. restartVersion: Changes to this field cause the operator to restart WebLogic Server instances. replicas: The default number of cluster member Managed Server instances to start for each WebLogic cluster in the domain configuration, unless replicas is specified for that cluster under the clusters field. For each cluster, the operator will sort cluster member Managed Server names from the WebLogic domain configuration by normalizing any numbers in the Managed Server name and then sorting alphabetically. This is done so that server names such as \u0026ldquo;managed-server10\u0026rdquo; come after \u0026ldquo;managed-server9\u0026rdquo;. The operator will then start Managed Servers from the sorted list, up to the replicas count, unless specific Managed Servers are specified as starting in their entry under the managedServers field. In that case, the specified Managed Servers will be started and then additional cluster members will be started, up to the replicas count, by finding further cluster members in the sorted list that are not already started. If cluster members are started because of their entries under managedServers, then a cluster may have more cluster members running than its replicas count. Defaults to 0. maxClusterConcurrentStartup: The maximum number of cluster member Managed Server instances that the operator will start in parallel for a given cluster, if maxConcurrentStartup is not specified for a specific cluster under the clusters field. A value of 0 means there is no configured limit. Defaults to 0. allowReplicasBelowMinDynClusterSize: Whether to allow the number of running cluster member Managed Server instances to drop below the minimum dynamic cluster size configured in the WebLogic domain configuration, if this is not specified for a specific cluster under the clusters field. Defaults to true. introspectVersion: Changes to this field cause the operator to repeat its introspection of the WebLogic domain configuration (see Initiating introspection). Repeating introspection is required for the operator to recognize changes to the domain configuration, such as adding a new WebLogic cluster or Managed Server instance, to regenerate configuration overrides, or to regenerate the WebLogic domain home when the domainHomeSourceType is FromModel. Introspection occurs automatically, without requiring change to this field, when servers are first started or restarted after a full domain shut down. For the FromModel domainHomeSourceType, introspection also occurs when a running server must be restarted because of changes to any of the fields listed here. See also overrideDistributionStrategy.  Elements related to specifying and overriding WebLogic domain configuration:\n  These elements are under configuration.\n overridesConfigMap: The name of the ConfigMap for WebLogic configuration overrides. If this field is specified, then the value of spec.configOverrides is ignored. overrideDistributionStrategy: Determines how updated configuration overrides are distributed to already running WebLogic Server instances following introspection when the domainHomeSourceType is PersistentVolume or Image. Configuration overrides are generated during introspection from Secrets, the overridesConfigMap field, and WebLogic domain topology. Legal values are DYNAMIC, which means that the operator will distribute updated configuration overrides dynamically to running servers, and ON_RESTART, which means that servers will use updated configuration overrides only after the server\u0026rsquo;s next restart. The selection of ON_RESTART will not cause servers to restart when there are updated configuration overrides available. See also introspectVersion. Defaults to DYNAMIC. secrets: A list of names of the Secrets for WebLogic configuration overrides or model. If this field is specified, then the value of spec.configOverrideSecrets is ignored. introspectorJobActiveDeadlineSeconds: The introspector job timeout value in seconds. If this field is specified, then the operator\u0026rsquo;s ConfigMap data.introspectorJobActiveDeadlineSeconds value is ignored. Defaults to 120 seconds.    These elements are under configuration.model, only apply if the domainHomeSourceType is FromModel, and are discussed in Model in Image.\n configMap: (Optional) Name of a ConfigMap containing WebLogic Deploy Tooling model YAML files or .properties files. domainType: WebLogic Deploy Tooling domain type. Legal values: WLS, RestrictedJRF, JRF. Defaults to WLS. runtimeEncryptionSecret: The name of the Secret containing the runtime encryption password, which must be in a field named password. Required when domainHomeSourceType is set to FromModel. modelHome: Location of the WebLogic Deploy Tooling model home directory, which can include model YAML files, .properties variable files, and application .zip archives. Defaults to /u01/wdt/models. onlineUpdate.*: Settings related to the online update option for Model In Image dynamic updates.  onlineUpdate.enabled: Enable online update for model changes to a running domain. Default is false. onlineUpdate.onNonDynamicChanges: Controls behavior when non-dynamic WebLogic configuration changes are detected during an online update. Non-dynamic changes are changes that require a domain restart to take effect. Valid values are CommitUpdateOnly (default) and CommitUpdateAndRoll. For more information, see Online update handling of non-dynamic WebLogic configuration changes in the Runtime Updates section of the Model in Image user guide. onlineUpdate.wdtTimeouts.*: Rarely needed timeout settings for online update calls to the WebLogic domain from the WebLogic Deploy Tool within the introspector job. All timeouts are specified in milliseconds and default to two or three minutes. For a full list of timeouts, call kubectl explain domain.spec.configuration.model.onlineUpdate.wdtTimeouts.      These elements are under configuration.opss, and only apply if the domainHomeSourceType is FromModel and the domainType is JRF.\n walletPasswordSecret: Name of a Secret containing the OPSS key passphrase, which must be in a field named walletPassword. Used to encrypt and decrypt the wallet that is used for accessing the domain\u0026rsquo;s entries in its RCU database. walletFileSecret: Name of a Secret containing the OPSS key wallet file, which must be in a field named walletFile. Use this to allow a JRF domain to reuse its entries in the RCU database. This allows you to specify a wallet file that was obtained from the domain home after the domain was booted for the first time.    These elements are under configuration.istio.\n enabled: True, if this domain is deployed under an Istio service mesh. Defaults to true when the istio field is specified. readinessPort: The operator will create a WebLogic network access point with this port that will then be exposed from the container running the WebLogic Server instance. The readiness probe will use this network access point to verify that the server instance is ready for application traffic. Defaults to 8888.    Elements related to Kubernetes Pod and Service generation:\n serverPod: Customization affecting the generation of Pods for WebLogic Server instances. serverService: Customization affecting the generation of ClusterIP Services for WebLogic Server instances.  Sub-sections related to the Administration Server, specific clusters, or specific Managed Servers:\n adminServer: Lifecycle options for the Administration Server, including Java options, environment variables, additional Pod content, and which channels or network access points should be exposed using a NodePort Service. clusters: Lifecycle options for all of the Managed Server members of a WebLogic cluster, including Java options, environment variables, additional Pod content, and the ability to explicitly start, stop, or restart cluster members. The clusterName field of each entry must match a cluster that already exists in the WebLogic domain configuration. managedServers: Lifecycle options for individual Managed Servers, including Java options, environment variables, additional Pod content, and the ability to explicitly start, stop, or restart a named server instance. The serverName field of each entry must match a Managed Server that already exists in the WebLogic domain configuration or that matches a dynamic cluster member based on the server template.  The elements serverStartPolicy, serverStartState, serverPod and serverService are repeated under adminServer and under each entry of clusters or managedServers. The values directly under spec, set the defaults for the entire domain. The values under a specific entry under clusters, set the defaults for cluster members of that cluster. The values under adminServer or an entry under managedServers, set the values for that specific server. Values from the domain scope and values from the cluster (for cluster members) are merged with or overridden by the setting for the specific server depending on the element. See Startup and shutdown for details about serverStartPolicy combinations.\nElements related to the customization of liveness and readiness probes:\n See Liveness probe customization for details about the elements related to liveness probe customization and Readiness probe customization for details about the elements related to readiness probe customization.  For additional domain resource attribute reference material, see Domain resource attribute references.\n JVM memory and Java option environment variables You can use the following environment variables to specify JVM memory and JVM option arguments to WebLogic Server Managed Server and Node Manager instances:\n JAVA_OPTIONS: Java options for starting WebLogic Server. USER_MEM_ARGS: JVM memory arguments for starting WebLogic Server. NODEMGR_JAVA_OPTIONS: Java options for starting a Node Manager instance. NODEMGR_MEM_ARGS: JVM memory arguments for starting a Node Manager instance. WLST_PROPERTIES: System properties for WLST commands in introspector jobs or WebLogic Server instance containers. WLST_EXTRA_PROPERTIES: System properties appended to WLST_PROPERTIES for WLST commands in introspector jobs or WebLogic Server instance containers. WLSDEPLOY_PROPERTIES: System properties for WebLogic Deploy Tool commands during Model in Image introspector jobs or WebLogic Server instance containers. PRE_CLASSPATH: Path(s) that are prepended to the WebLogic Server system classpath; delimit multiple paths with a colon :. CLASSPATH: Path(s) that are appended to the WebLogic Server system classpath; delimit multiple paths with a colon :.  Notes:\n The following behavior occurs depending on whether or not NODEMGR_JAVA_OPTIONS and NODEMGR_MEM_ARGS are defined:  If NODEMGR_JAVA_OPTIONS is not defined and JAVA_OPTIONS is defined, then the JAVA_OPTIONS value will be applied to the Node Manager instance. If NODEMGR_MEM_ARGS is not defined, then default memory and Java security property values (-Xms64m -Xmx100m -Djava.security.egd=file:/dev/./urandom) will be applied to the Node Manager instance. It can be explicitly set to another value in your Domain YAML file using the env attribute under the serverPod configuration.   The USER_MEM_ARGS and WLST_EXTRA_PROPERTIES environment variables both default to -Djava.security.egd=file:/dev/./urandom in all WebLogic Server pods and the WebLogic introspection job. They can be explicitly set to another value in your Domain YAML file using the env attribute under the serverPod configuration. Notice that the NODEMGR_MEM_ARGS, USER_MEM_ARGS, and WLST_EXTRA_PROPERTIES environment variables all include -Djava.security.egd=file:/dev/./urandom by default. This helps to speed up the Node Manager and WebLogic Server startup on systems with low entropy, plus similarly helps to speed up introspection job usage of the WLST encrypt command. For a detailed discussion of Java and pod memory tuning see the Pod memory and CPU resources FAQ. You can use JAVA_OPTIONS and WLSDEPLOY_PROPERTIES to disable Fast Application Notifications (FAN); see the Disable Fast Application Notifications FAQ for details.  This example snippet illustrates how to add some of the above environment variables using the env attribute under the serverPod configuration in your Domain YAML file.\n# Copyright (c) 2017, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # apiVersion: \u0026#34;weblogic.oracle/v8\u0026#34; kind: Domain metadata: name: domain1 namespace: domains23 labels: weblogic.domainUID: domain1 spec: serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false \u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-Djava.security.egd=file:/dev/./urandom \u0026#34; - name: NODEMGR_JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false \u0026#34; - name: NODEMGR_MEM_ARGS value: \u0026#34;-Xms64m -Xmx100m -Djava.security.egd=file:/dev/./urandom \u0026#34; - name: PRE_CLASSPATH value: \u0026#34;/sample/path/prepended/to/weblogic/system/classpath:/another/prepended/path\u0026#34; - name: CLASSPATH value: \u0026#34;/sample/path/appended/to/weblogic/system/classpath:/another/appended/path\u0026#34; Pod generation The operator creates a Pod for each running WebLogic Server instance. This Pod will have a container, named weblogic-server, based on the container image specified by the image field. Additional Pod or container content can be specified using the elements under serverPod. This includes Kubernetes sidecar and init containers, labels, annotations, volumes, volume mounts, scheduling constraints, including anti-affinity, resource requirements, or security context.\nCustomer provided labels and annotations may not begin with \u0026ldquo;weblogic\u0026rdquo; and the operator will generate the following labels:\n weblogic.createdByOperator: \u0026quot;true\u0026quot; weblogic.domainName: \u0026lt;domain-name\u0026gt;, where \u0026lt;domain-name\u0026gt; is the name of the WebLogic domain weblogic.domainUID: \u0026lt;uid\u0026gt;, where \u0026lt;uid\u0026gt; is the domain UID from the Domain resource weblogic.serverName: \u0026lt;server-name\u0026gt;, where \u0026lt;server-name\u0026gt; is the name of the WebLogic Server instance weblogic.clusterName: \u0026lt;cluster-name\u0026gt;, where \u0026lt;cluster-name\u0026gt; is the name of the cluster of which this instance is a member, if any weblogic.domainRestartVersion: \u0026lt;restart-version\u0026gt;, matches domain.spec.restartVersion after the pod is up to date with this version weblogic.introspectVersion: \u0026lt;introspect-version\u0026gt;, matches domain.spec.introspectVersion after the pod is up to date with this version  Prior to creating a Pod, the operator replaces variable references allowing the Pod content to be templates. The format of these variable references is $(VARIABLE_NAME) where VARIABLE_NAME is one of the variable names available in the container for the WebLogic Server instance. The default set of environment variables includes:\n DOMAIN_NAME: The WebLogic Server domain name. DOMAIN_UID: The domain unique identifier. DOMAIN_HOME: The domain home location as a file system path within the container. SERVER_NAME: The WebLogic Server instance name. CLUSTER_NAME: The WebLogic cluster name, if this is a cluster member. LOG_HOME: If the `domain.spec.logHomeEnabled' attribute is set to true, then this contains the WebLogic log location as a file system path within the container  This example domain YAML file specifies that Pods for WebLogic Server instances in the cluster-1 cluster will have a per-Managed Server volume and volume mount (similar to a Kubernetes StatefulSet), an init container to initialize some files in that volume, and anti-affinity scheduling so that the server instances are scheduled, as much as possible, on different Nodes:\n# Copyright (c) 2019, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # apiVersion: \u0026#34;weblogic.oracle/v8\u0026#34; kind: Domain metadata: name: domain1 namespace: domains23 labels: weblogic.domainUID: domain1 spec: domainHome: /u01/oracle/user_projects/domains/domain1 domainHomeSourceType: Image image: \u0026#34;phx.ocir.io/weblogick8s/my-domain-home-in-image:12.2.1.4\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; imagePullSecrets: - name: ocirsecret webLogicCredentialsSecret: name: domain1-weblogic-credentials includeServerOutInPodLog: true serverStartPolicy: \u0026#34;IF_NEEDED\u0026#34; serverPod: env: - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false\u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-Djava.security.egd=file:/dev/./urandom \u0026#34; adminServer: serverStartState: \u0026#34;RUNNING\u0026#34; clusters: - clusterName: cluster-1 serverStartState: \u0026#34;RUNNING\u0026#34; serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026#34;weblogic.clusterName\u0026#34; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; volumes: - name: $(SERVER_NAME)-volume emptyDir: {} volumeMounts: - mountPath: /server-volume name: $(SERVER_NAME)-volume initContainers: - name: volumeinit image: \u0026#34;oraclelinux:7-slim\u0026#34; imagePullPolicy: IfNotPresent command: [\u0026#34;/usr/bin/sh\u0026#34;] args: [\u0026#34;echo\u0026#34;, \u0026#34;Replace with command to initialize files in /init-volume\u0026#34;] volumeMounts: - mountPath: /init-volume name: $(SERVER_NAME)-volume replicas: 2 The operator uses an \u0026ldquo;introspection\u0026rdquo; job to discover details about the WebLogic domain configuration, such as the list of clusters and network access points. The Job Pod for the introspector is generated using the serverPod entries for the Administration Server. Because the Administration Server name is not known until the introspection step is complete, the value of the $(SERVER_NAME) variable for the introspection job will be \u0026ldquo;introspector\u0026rdquo;.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/database/",
	"title": "Run a database",
	"tags": [],
	"description": "Run an ephemeral database in Kubernetes that is suitable for sample or basic testing purposes.",
	"content": "Contents  Overview Oracle database in Kubernetes MySQL database in Kubernetes  Overview This section describes how to run an ephemeral Oracle database or MySQL database in your Kubernetes cluster using approaches suitable for sample or basic testing purposes.\nNotes:\n  The databases are configured with ephemeral storage, which means all information will be lost on any shutdown or pod failure.\n  The Oracle Database images are supported for non-production use only. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1).\n  Oracle database in Kubernetes The following example shows how to set up an ephemeral Oracle database with the following attributes:\n   Attribute Value     Kubernetes namespace default   Kubernetes pod oracle-db   Kubernetes service name oracle-db   Kubernetes service port 1521   Kubernetes node port 30011   Image container-registry.oracle.com/database/enterprise:12.2.0.1-slim   DBA user (with full privileges) sys as sysdba   DBA password Oradoc_db1   Database URL inside Kubernetes cluster (from any namespace) oracle-db.default.svc.cluster.local:1521/devpdb.k8s   Database URL outside Kubernetes cluster dns-name-that-resolves-to-node-location:30011/devpdb.k8s      Get the operator source and put it in /tmp/weblogic-kubernetes-operator.\nFor example:\n$ cd /tmp $ git clone --branch v3.2.3 https://github.com/oracle/weblogic-kubernetes-operator.git  Note: We will refer to the top directory of the operator source tree as /tmp/weblogic-kubernetes-operator; however, you can use a different location.\n For additional information about obtaining the operator source, see the Developer Guide Requirements.\n  Ensure that you have access to the database image:\n  Use a browser to log in to https://container-registry.oracle.com, select Database -\u0026gt; enterprise, and accept the license agreement.\n  Get the database image:\n In the local shell, docker login container-registry.oracle.com. In the local shell, docker pull container-registry.oracle.com/database/enterprise:12.2.0.1-slim.    If your Kubernetes cluster nodes do not all have access to the database image in a local cache, then:\n Deploy a Kubernetes docker secret to the default namespace with login credentials for container-registry.oracle.com: kubectl create secret docker-registry docker-secret \\  --docker-server=container-registry.oracle.com \\  --docker-username=your.email@some.com \\  --docker-password=your-password \\  --docker-email=your.email@some.com \\  -n default Pass the name of this secret as a parameter to the start-db-service.sh in the following step using -s your-image-pull-secret.\n Alternatively, copy the database image to each local Docker cache in the cluster. For more information, see the Cannot pull image FAQ.  WARNING: The Oracle Database images are supported only for non-production use. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1).\n    Create a deployment using the database image:\nUse the sample script in /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oracle-db-service to create an Oracle database running in the pod, oracle-db.\n$ cd /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oracle-db-service $ start-db-service.sh   MySQL database in Kubernetes The following example shows how to set up an ephemeral MySQL database with the following attributes:\n   Attribute Value     Kubernetes namespace default   Kubernetes pod mysql-db   Kubernetes service name mysql-db   Kubernetes service port 3306   Image mysql:5.6   Root user (with full privileges) root   Root password password   Database URL inside Kubernetes cluster (from any namespace) jdbc:mysql://mysql-db.default.svc.cluster.local:3306/mysql    Copy the following YAML into a file named mysql.yaml:\napiVersion: v1 kind: Pod metadata: name: mysql-db namespace: default labels: app: mysql-db spec: terminationGracePeriodSeconds: 5 containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: root-password ports: - containerPort: 3306 name: mysql --- apiVersion: v1 kind: Service metadata: name: mysql-db namespace: default spec: ports: - port: 3306 selector: app: mysql-db clusterIP: None --- apiVersion: v1 kind: Secret metadata: name: mysql-secret namespace: default data: # echo -n \u0026#34;root\u0026#34; | base64 root-user: cm9vdA== # echo -n \u0026#34;password\u0026#34; | base64 root-password: cGFzc3dvcmQ= Deploy MySQL using the command kubectl create -f mysql.yaml.\nTo shut down and clean up the resources, use kubectl delete -f mysql.yaml.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/prerequisites/introduction/",
	"title": "Operator prerequisites",
	"tags": [],
	"description": "Review the prerequisites for the current release of the operator.",
	"content": "For the current production release 3.3.2:\n Kubernetes 1.16.15+, 1.17.13+, 1.18.10+, 1.19.7+, and 1.20.6+ (check with kubectl version). Flannel networking v0.9.1-amd64 or later (check with docker images | grep flannel), Calico networking v3.16.1 or later, or OpenShift SDN on OpenShift 4.3 systems. Docker 18.9.1 or 19.03.1+ (check with docker version) or CRI-O 1.20.2+ (check with crictl version | grep RuntimeVersion). Helm 3.3.4+ (check with helm version --client --short). For domain home source type Model in Image, WebLogic Deploy Tooling 1.9.11. Either Oracle WebLogic Server 12.2.1.3.0 with patch 29135930, Oracle WebLogic Server 12.2.1.4.0, or Oracle WebLogic Server 14.1.1.0.0.  The existing WebLogic Server image, container-registry.oracle.com/middleware/weblogic:12.2.1.3, has all the necessary patches applied. Check the WLS version with docker run container-registry.oracle.com/middleware/weblogic:12.2.1.3 sh -c 'source $ORACLE_HOME/wlserver/server/bin/setWLSEnv.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; java weblogic.version'. Check the WLS patches with docker run container-registry.oracle.com/middleware/weblogic:12.2.1.3 sh -c '$ORACLE_HOME/OPatch/opatch lspatches'.   Container images based on Oracle Linux 8 are now supported. The Oracle Container Registry hosts container images based on both Oracle Linux 7 and 8, including Oracle WebLogic Server 14.1.1.0.0 images based on Java 8 and 11. You must have the cluster-admin role to install the operator. The operator does not need the cluster-admin role at runtime. For more information, see the role-based access control, RBAC, documentation. We do not currently support running WebLogic in non-Linux containers.  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/reference/swagger/",
	"title": "Swagger",
	"tags": [],
	"description": "Swagger REST API documentation.",
	"content": "View the Swagger REST API documentation here.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/",
	"title": "User Guide",
	"tags": [],
	"description": "",
	"content": "The User Guide provides detailed information about all aspects of using the operator.\n Introduction  Gain an overall understanding of the operator and learn where you can get help.\n Operator prerequisites  Review the prerequisites for the current release of the operator.\n Setup checklist  Follow these steps to set up your environment.\n Supported platforms  See the operator supported environments.\n Manage operators  Helm is used to create and deploy necessary operator resources and to run the operator in a Kubernetes cluster. Use the operator\u0026#39;s Helm chart to install and manage the operator.\n WebLogic Server images  Create or obtain WebLogic Server images.\n Manage WebLogic domains  Important considerations for WebLogic domains in Kubernetes.\n Manage FMW Infrastructure domains  FMW Infrastructure domains contain the Java Required Files (JRF) feature and are the prerequisite for upper stack products like Oracle SOA Suite.\n Azure Kubernetes Service (AKS)  Deploying WebLogic Server on Azure Kubernetes Service.\n CI/CD considerations  Learn about managing domain images with continuous integration and continuous delivery (CI/CD).\n Istio support  Lets you run the operator, and WebLogic domains managed by the operator, with Istio sidecar injection enabled. You can use Istio gateways and virtual services to access applications deployed in these domains.\n Set up Kubernetes  Get help for setting up a Kubernetes environment\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/azure-kubernetes-service/model-in-image/",
	"title": "Model in Image",
	"tags": [],
	"description": "Sample for creating a WebLogic cluster on the Azure Kubernetes Service with model in image approach.",
	"content": "This sample demonstrates how to use the WebLogic Kubernetes Operator (hereafter \u0026ldquo;the operator\u0026rdquo;) to set up a WebLogic Server (WLS) cluster on the Azure Kubernetes Service (AKS) using the model in image approach. After going through the steps, your WLS domain runs on an AKS cluster instance and you can manage your WLS domain by interacting with the operator.\nContents  Prerequisites Create an AKS cluster Install WebLogic Kubernetes Operator Create Docker image Create WebLogic domain Invoke the web application Rolling updates Clean up resource Troubleshooting Useful links  Prerequisites This sample assumes the following prerequisite environment.\n Operating System: GNU/Linux, macOS or WSL2 for Windows 10. Git; use git --version to test if git works. This document was tested with version 2.17.1. Azure CLI; use az --version to test if az works. This document was tested with version 2.9.1. Docker for Desktop. This document was tested with Docker version 20.10.2, build 2291f61 kubectl; use kubectl version to test if kubectl works. This document was tested with version v1.16.3. Helm, version 3.1 and later; use helm version to check the helm version. This document was tested with version v3.2.5. A Java JDK, Version 8 or 11. Azure recommends Azul Zulu for Azure. Ensure that your JAVA_HOME environment variable is set correctly in the shells in which you run the commands. Ensure that you have the zip/unzip utility installed; use zip/unzip -v to test if zip/unzip works.  Create a Service Principal for AKS An AKS cluster requires either an Azure Active Directory (AD) service principal or a managed identity to interact with Azure resources.\nWe will use a service principal to create an AKS cluster. Follow the commands below to create a new service principal.\nPlease run az login first. Do set the subscription you want to work with. You can get a list of your subscriptions by running az account list.\n# Login $ az login # Set your working subscription $ export SUBSCRIPTION_ID=\u0026lt;your-subscription-id\u0026gt; $ az account set -s $SUBSCRIPTION_ID Create the new service principal with the following commands:\n# Create Service Principal $ export SP_NAME=myAKSClusterServicePrincipal $ az ad sp create-for-rbac --skip-assignment --name $SP_NAME # Copy the output to a file, we will use it later. If you see an error similar to the following:\nFound an existing application instance of \u0026#34;5pn2s201-nq4q-43n1-z942-p9r9571qr3rp\u0026#34;. We will patch it Insufficient privileges to complete the operation. The problem may be a pre-existing service principal with the same name. Either delete the other service principal or pick a different name.\nSuccessful output will look like the following:\n{ \u0026#34;appId\u0026#34;: \u0026#34;r3qnq743-61s9-4758-8163-4qpo87s72s54\u0026#34;, \u0026#34;displayName\u0026#34;: \u0026#34;myAKSClusterServicePrincipal\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;http://myAKSClusterServicePrincipal\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;TfhR~uOJ1C1ftD5NS_LzJJj6UOjS2OwXfz\u0026#34;, \u0026#34;tenant\u0026#34;: \u0026#34;82sr215n-0ns5-404e-9161-206r0oqyq999\u0026#34; } Grant your service principal with a contributor role to create AKS resources.\n# Use the \u0026lt;appId\u0026gt; from the output of the last command $ export SP_APP_ID=r3qnq743-61s9-4758-8163-4qpo87s72s54 $ az role assignment create --assignee $SP_APP_ID --role Contributor Successful output will look like the following:\n{ \u0026#34;canDelegate\u0026#34;: null, \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/p7844r91-o11q-4n7s-np6s-996308sopqo9/providers/Microsoft.Authorization/roleAssignments/4oq396os-rs95-4n6s-n3qo-sqqpnpo91035\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;4oq396os-rs95-4n6s-n3qo-sqqpnpo91035\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;952551r8-n129-4on3-oqo9-231n0s6011n3\u0026#34;, \u0026#34;principalType\u0026#34;: \u0026#34;ServicePrincipal\u0026#34;, \u0026#34;roleDefinitionId\u0026#34;: \u0026#34;/subscriptions/p7844r91-o11q-4n7s-np6s-996308sopqo9/providers/Microsoft.Authorization/roleDefinitions/o24988np-6180-42n0-no88-20s7382qq24p\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;/subscriptions/p7844r91-o11q-4n7s-np6s-996308sopqo9\u0026#34;, } Oracle Container Registry You will need an Oracle account. The following steps will direct you to accept the license agreement for WebLogic Server. Make note of your Oracle Account password and email. This sample pertains to 12.2.1.4, but other versions may work as well.\n In a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them. The Oracle Container Registry provides a WebLogic Server 12.2.1.4.0 Docker image, which already has the necessary patches applied, and the Oracle WebLogic Server 12.2.1.4.0 and 14.1.1.0.0 images, which do not require any patches. Ensure that Docker desktop is running. Find and then pull the WebLogic 12.2.1.4 install image: $ docker pull container-registry.oracle.com/middleware/weblogic:12.2.1.4   If you have problems accessing the Oracle Container Registry, you can build your own Docker images from the Oracle GitHub repository.\nClone WebLogic Kubernetes Operator repository Clone the WebLogic Kubernetes Operator repository to your machine. We will use several scripts in this repository to create a WebLogic domain. This sample was tested with v3.1.1, but should work with the latest release.\n$ git clone --branch v3.3.2 https://github.com/oracle/weblogic-kubernetes-operator.git $ cd weblogic-kubernetes-operator  Create the AKS cluster This sample requires that you disable the AKS addon http_application_routing by default. If you want to enable http_application_routing, then follow HTTP application routing.\nRun the following commands to create the AKS cluster instance.\n# Change these parameters as needed for your own environment # Specify a prefix to name resources, only allow lowercase letters and numbers, between 1 and 7 characters $ export NAME_PREFIX=wls # Used to generate resource names. $ export TIMESTAMP=`date +%s` $ export AKS_CLUSTER_NAME=\u0026#34;${NAME_PREFIX}aks${TIMESTAMP}\u0026#34; $ export AKS_PERS_RESOURCE_GROUP=\u0026#34;${NAME_PREFIX}resourcegroup${TIMESTAMP}\u0026#34; $ export AKS_PERS_LOCATION=eastus $ export SP_APP_ID=\u0026lt;appId from the az ad sp create-for-rbac command\u0026gt; $ export SP_CLIENT_SECRET=\u0026lt;password from the az ad sp create-for-rbac command\u0026gt; $ az group create --name $AKS_PERS_RESOURCE_GROUP --location $AKS_PERS_LOCATION $ az aks create \\  --resource-group $AKS_PERS_RESOURCE_GROUP \\  --name $AKS_CLUSTER_NAME \\  --node-count 2 \\  --generate-ssh-keys \\  --nodepool-name nodepool1 \\  --node-vm-size Standard_DS2_v2 \\  --location $AKS_PERS_LOCATION \\  --service-principal $SP_APP_ID \\  --client-secret $SP_CLIENT_SECRET Successful output will be a JSON object with the entry \u0026quot;type\u0026quot;: \u0026quot;Microsoft.ContainerService/ManagedClusters\u0026quot;.\nAfter the deployment finishes, run the following command to connect to the AKS cluster. This command updates your local ~/.kube/config so that subsequent kubectl commands interact with the named AKS cluster.\n$ az aks get-credentials --resource-group $AKS_PERS_RESOURCE_GROUP --name $AKS_CLUSTER_NAME Successful output will look similar to:\nMerged \u0026#34;wlsaks1596087429\u0026#34; as current context in /home/username/.kube/config After your Kubernetes cluster is up and running, run the following commands to make sure kubectl can access the Kubernetes cluster:\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME aks-pool1haiche-33688868-vmss000000 Ready agent 4m25s v1.17.13 10.240.0.4 \u0026lt;none\u0026gt; Ubuntu 16.04.7 LTS 4.15.0-1098-azure docker://19.3.12 aks-pool1haiche-33688868-vmss000001 Ready agent 4m12s v1.17.13 10.240.0.5 \u0026lt;none\u0026gt; Ubuntu 16.04.7 LTS 4.15.0-1098-azure docker://19.3.12  Note: If you run into VM size failure, see Troubleshooting - Virtual Machine size is not supported.\nInstall WebLogic Kubernetes Operator The WebLogic Kubernetes Operator is an adapter to integrate WebLogic Server and Kubernetes, allowing Kubernetes to serve as a container infrastructure hosting WLS instances. The operator runs as a Kubernetes Pod and stands ready to perform actions related to running WLS on Kubernetes.\nCreate a namespace and service account for the operator.\n$ kubectl create namespace sample-weblogic-operator-ns namespace/sample-weblogic-operator-ns created $ kubectl create serviceaccount -n sample-weblogic-operator-ns sample-weblogic-operator-sa serviceaccount/sample-weblogic-operator-sa created Validate the service account was created with this command.\n$ kubectl -n sample-weblogic-operator-ns get serviceaccount NAME SECRETS AGE default 1 9m24s sample-weblogic-operator-sa 1 9m5s Install the operator. Ensure your current directory is weblogic-kubernetes-operator. It may take you several minutes to install the operator.\n# cd weblogic-kubernetes-operator $ helm install weblogic-operator kubernetes/charts/weblogic-operator \\ --namespace sample-weblogic-operator-ns \\ --set image=ghcr.io/oracle/weblogic-kubernetes-operator:3.1.1 \\ --set serviceAccount=sample-weblogic-operator-sa \\ --set \u0026quot;enableClusterRoleBinding=true\u0026quot; \\ --set \u0026quot;domainNamespaceSelectionStrategy=LabelSelector\u0026quot; \\ --set \u0026quot;domainNamespaceLabelSelector=weblogic-operator\\=enabled\u0026quot; \\ --wait NAME: weblogic-operator LAST DEPLOYED: Tue Nov 17 09:33:58 2020 NAMESPACE: sample-weblogic-operator-ns STATUS: deployed REVISION: 1 TEST SUITE: None  If you wish to use a more recent version of the operator, replace the 3.1.1 in the preceding command with the other version number. To see the list of version numbers, visit the GitHub releases page.\n Verify the operator with the following commands; the status will be Running.\n$ helm list -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION sample-weblogic-operator sample-weblogic-operator-ns 1 2020-11-17 09:33:58.584239273 -0700 PDT deployed weblogic-operator-3.1 $ kubectl get pods -n sample-weblogic-operator-ns NAME READY STATUS RESTARTS AGE weblogic-operator-775b668c8f-nwwnn 1/1 Running 0 32s  You can sepcify the operator image by changing value of --set image. If you run into failures, see Troubleshooting - WebLogic Kubernetes Operator installation failure.\n If you have a Docker image built with domain models following Model in Image, you can go to Create WebLogic domain directly.\n Create Docker image  Image creation prerequisites Image creation - Introduction Understanding your first archive Staging a ZIP file of the archive Staging model files Creating the image with WIT Pushing the image to Azure Container Registry  Image creation prerequisites   The JAVA_HOME environment variable must be set and must reference a valid JDK 8 or 11 installation.\n  Copy the sample to a new directory; for example, use the directory /tmp/mii-sample.\n$ mkdir /tmp/mii-sample $ cd kubernetes/samples/scripts/create-weblogic-domain/model-in-image $ cp -r * /tmp/mii-sample Note: We will refer to this working copy of the sample as /tmp/mii-sample, (mii is short for model in image); however, you can use a different location.\n  Download the latest WebLogic Deploying Tooling (WDT) and WebLogic Image Tool (WIT) installer ZIP files to your /tmp/mii-sample/model-images directory. Both WDT and WIT are required to create your Model in Image Docker images.\n$ cd /tmp/mii-sample/model-images $ curl -m 120 -fL https://github.com/oracle/weblogic-deploy-tooling/releases/latest/download/weblogic-deploy.zip \\  -o /tmp/mii-sample/model-images/weblogic-deploy.zip $ curl -m 120 -fL https://github.com/oracle/weblogic-image-tool/releases/latest/download/imagetool.zip \\  -o /tmp/mii-sample/model-images/imagetool.zip To set up the WebLogic Image Tool, run the following commands:\n$ cd /tmp/mii-sample/model-images $ unzip imagetool.zip $ ./imagetool/bin/imagetool.sh cache addInstaller \\  --type wdt \\  --version latest \\  --path /tmp/mii-sample/model-images/weblogic-deploy.zip These steps will install WIT to the /tmp/mii-sample/model-images/imagetool directory, plus put a wdt_latest entry in the tool’s cache which points to the WDT ZIP file installer. You will use WIT later in the sample for creating model images.\n  Image creation - Introduction The goal of image creation is to demonstrate using the WebLogic Image Tool to create an image named model-in-image:WLS-v1 from files that you will stage to /tmp/mii-sample/model-images/model-in-image:WLS-v1/. The staged files will contain a web application in a WDT archive, and WDT model configuration for a WebLogic Administration Server called admin-server and a WebLogic cluster called cluster-1.\nA \u0026ldquo;Model in Image\u0026rdquo; image contains the following elements:\n A WebLogic Server installation and a WebLogic Deploy Tooling installation in its /u01/wdt/weblogic-deploy directory. If you have WDT model archive files, then the image must also contain these files in its /u01/wdt/models directory. If you have WDT model YAML file and properties files, then they go in in the same /u01/wdt/models directory. If you do not specify a WDT model YAML file in your /u01/wdt/models directory, then the model YAML file must be supplied dynamically using a Kubernetes ConfigMap that is referenced by your Domain spec.model.configMap field.  We provide an example of using a model ConfigMap later in this sample.\nThe following sections contain the steps for creating the image model-in-image:WLS-v1.\nUnderstanding your first archive The sample includes a predefined archive directory in /tmp/mii-sample/archives/archive-v1 that you will use to create an archive ZIP file for the image.\nThe archive top directory, named wlsdeploy, contains a directory named applications, which includes an ‘exploded’ sample JSP web application in the directory, myapp-v1. Three useful aspects to remember about WDT archives are:\n A model image can contain multiple WDT archives. WDT archives can contain multiple applications, libraries, and other components. WDT archives have a well defined directory structure, which always has wlsdeploy as the top directory.  The application displays important details about the WebLogic Server instance that it’s running on: namely its domain name, cluster name, and server name, as well as the names of any data sources that are targeted to the server.\nStaging a ZIP file of the archive When you create the image, you will use the files in the staging directory, /tmp/mii-sample/model-in-image__WLS-v1. In preparation, you need it to contain a ZIP file of the WDT application archive.\nRun the following commands to create your application archive ZIP file and put it in the expected directory:\n# Delete existing archive.zip in case we have an old leftover version $ rm -f /tmp/mii-sample/model-images/model-in-image__WLS-v1/archive.zip # Move to the directory which contains the source files for our archive $ cd /tmp/mii-sample/archives/archive-v1 # Zip the archive to the location will later use when we run the WebLogic Image Tool $ zip -r /tmp/mii-sample/model-images/model-in-image__WLS-v1/archive.zip wlsdeploy Staging model files In this step, you explore the staged WDT model YAML file and properties in the /tmp/mii-sample/model-in-image__WLS-v1 directory. The model in this directory references the web application in your archive, configures a WebLogic Server Administration Server, and configures a WebLogic cluster. It consists of only two files, model.10.properties, a file with a single property, and, model.10.yaml, a YAML file with your WebLogic configuration.\nHere is the WLS model.10.properties:\nCLUSTER_SIZE=5 Here is the WLS model.10.yaml:\ndomainInfo: AdminUserName: \u0026#39;@@SECRET:__weblogic-credentials__:username@@\u0026#39; AdminPassword: \u0026#39;@@SECRET:__weblogic-credentials__:password@@\u0026#39; ServerStartMode: \u0026#39;prod\u0026#39; topology: Name: \u0026#39;@@ENV:CUSTOM_DOMAIN_NAME@@\u0026#39; AdminServerName: \u0026#39;admin-server\u0026#39; Cluster: \u0026#39;cluster-1\u0026#39;: DynamicServers: ServerTemplate: \u0026#39;cluster-1-template\u0026#39; ServerNamePrefix: \u0026#39;managed-server\u0026#39; DynamicClusterSize: \u0026#39;@@PROP:CLUSTER_SIZE@@\u0026#39; MaxDynamicClusterSize: \u0026#39;@@PROP:CLUSTER_SIZE@@\u0026#39; MinDynamicClusterSize: \u0026#39;0\u0026#39; CalculatedListenPorts: false Server: \u0026#39;admin-server\u0026#39;: ListenPort: 7001 ServerTemplate: \u0026#39;cluster-1-template\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 appDeployments: Application: myapp: SourcePath: \u0026#39;wlsdeploy/applications/myapp-v1\u0026#39; ModuleType: ear Target: \u0026#39;cluster-1\u0026#39; The model file:\n  Defines a WebLogic domain with:\n Cluster cluster-1 Administration Server admin-server An EAR application, targeted to cluster-1, located in the WDT archive ZIP file at wlsdeploy/applications/myapp-v1    Leverages macros to inject external values:\n The property file CLUSTER_SIZE property is referenced in the model YAML file DynamicClusterSize and MaxDynamicClusterSize fields using a PROP macro. The model file domain name is injected using a custom environment variable named CUSTOM_DOMAIN_NAME using an ENV macro.  You set this environment variable later in this sample using an env field in its Domain. This conveniently provides a simple way to deploy multiple differently named domains using the same model image.   The model file administrator user name and password are set using a weblogic-credentials secret macro reference to the WebLogic credential secret.  This secret is in turn referenced using the webLogicCredentialsSecret field in the Domain. The weblogic-credentials is a reserved name that always dereferences to the owning Domain actual WebLogic credentials secret name.      A Model in Image image can contain multiple properties files, archive ZIP files, and YAML files but in this sample you use just one of each. For a complete discussion of Model in Images model file naming conventions, file loading order, and macro syntax, see Model files files in the Model in Image user documentation.\nCreating the image with WIT At this point, you have staged all of the files needed for the image model-in-image:WLS-v1; they include:\n /tmp/mii-sample/model-images/weblogic-deploy.zip /tmp/mii-sample/model-images/model-in-image__WLS-v1/model.10.yaml /tmp/mii-sample/model-images/model-in-image__WLS-v1/model.10.properties /tmp/mii-sample/model-images/model-in-image__WLS-v1/archive.zip  If you don’t see the weblogic-deploy.zip file, then you missed a step in the prerequisites.\nNow, you use the Image Tool to create a Docker image named model-in-image:WLS-v1 with a FROM clause that references a base WebLogic image. You’ve already set up this tool during the prerequisite steps.\nRun the following commands to create the model image and verify that it worked:\n$ cd /tmp/mii-sample/model-images $ ./imagetool/bin/imagetool.sh update \\  --tag model-in-image:WLS-v1 \\  --fromImage container-registry.oracle.com/middleware/weblogic:12.2.1.4 \\  --wdtModel ./model-in-image__WLS-v1/model.10.yaml \\  --wdtVariables ./model-in-image__WLS-v1/model.10.properties \\  --wdtArchive ./model-in-image__WLS-v1/archive.zip \\  --wdtModelOnly \\  --wdtDomainType WLS \\  --chown oracle:root If you don’t see the imagetool directory, then you missed a step in the prerequisites.\nThe preceding command runs the WebLogic Image Tool in its Model in Image mode, and does the following:\n Builds the final Docker image as a layer on the container-registry.oracle.com/middleware/weblogic:12.2.1.4 base image. Copies the WDT ZIP file that’s referenced in the WIT cache into the image.  Note that you cached WDT in WIT using the keyword latest when you set up the cache during the sample prerequisites steps. This lets WIT implicitly assume it’s the desired WDT version and removes the need to pass a -wdtVersion flag.   Copies the specified WDT model, properties, and application archives to image location /u01/wdt/models.  When the command succeeds, you should see output like the following:\n[INFO ] Build successful. Build time=36s. Image tag=model-in-image:WLS-v1 Verify the image is available in the local Docker server with the following command.\n$ docker images | grep WLS-v1 model-in-image WLS-v1 012d3bfa3536 5 days ago 1.13GB  You may run into a Dockerfile parsing error if your Docker buildkit is enabled, see Troubleshooting - WebLogic Image Tool failure.\n Pushing the image to Azure Container Registry AKS can pull Docker images from any container registry, but the easiest integration is to use Azure Container Registry (ACR). In this section, we will create a new Azure Container Registry, connect it to our pre-existing AKS cluster and push the Docker image built in the preceding section to it. For complete details, see Azure Container Registry documentation.\nLet\u0026rsquo;s create an instance of ACR in the same resource group we used for AKS. We will use the environment variables used during the steps above. For simplicity, we use the resource group name as the name of the ACR instance.\n$ az acr create --resource-group $AKS_PERS_RESOURCE_GROUP --name $AKS_PERS_RESOURCE_GROUP --sku Basic --admin-enabled true Closely examine the JSON output from this command. Save the value of the loginServer property aside. It will look something like the following.\n\u0026#34;loginServer\u0026#34;: \u0026#34;contosoresourcegroup1610068510.azurecr.io\u0026#34;, Use this value to sign in to the ACR instance. Note that because you are signing in with the az cli, you do not need a password because your identity is already conveyed by having done az login previously.\n$ export AKS_PERS_ACR=\u0026lt;you-ACR-loginServer\u0026gt; $ az acr login --name $AKS_PERS_ACR Ensure Docker is running on your local machine. Run the following commands to tag and push the image to your ACR.\n$ docker tag model-in-image:WLS-v1 $AKS_PERS_ACR/$AKS_PERS_ACR:model-in-image-aks $ docker push $AKS_PERS_ACR/$AKS_PERS_ACR:model-in-image-aks The push refers to repository [contosorgresourcegroup1610068510.azurecr.io/contosorgresourcegroup1610068510.azurecr.io] model-in-image-aks: digest: sha256:208217afe336053e4c524caeea1a415ccc9cc73b206ee58175d0acc5a3eeddd9 size: 2415 Finally, connect AKS to the ACR. For more details on connecting ACR to an existing AKS, see Configure ACR integration for existing AKS clusters.\n$ az aks update --name $AKS_CLUSTER_NAME --resource-group $AKS_PERS_RESOURCE_GROUP --attach-acr $AKS_PERS_RESOURCE_GROUP If you see an error that seems related to you not being an Owner on this subscription, please refer to the troubleshooting section Cannot attach ACR due to not being Owner of subscription.\nSuccessful output will be a JSON object with the entry \u0026quot;type\u0026quot;: \u0026quot;Microsoft.ContainerService/ManagedClusters\u0026quot;.\nCreate WebLogic domain In this section, you will deploy the new image to the namespace sample-domain1-ns, including the following steps:\n Create a namespace for the WebLogic domain. Upgrade the operator to manage the WebLogic domain namespace. Create a Secret containing your WebLogic administrator user name and password. Create a Secret containing your Model in Image runtime encryption password:  All Model in Image domains must supply a runtime encryption Secret with a password value. The runtime encryption password is used to encrypt configuration that is passed around internally by the operator. The value must be kept private but can be arbitrary; you can optionally supply a different secret value every time you restart the domain.   Deploy a Domain YAML file that references the new image. Wait for the domain’s Pods to start and reach their ready state.  Namespace Create a namespace that can host one or more domains:\n$ kubectl create namespace sample-domain1-ns ## label the domain namespace so that the operator can autodetect and create WebLogic Server pods. $ kubectl label namespace sample-domain1-ns weblogic-operator=enabled Kubernetes Secrets for WebLogic First, create the secrets needed by the WLS type model domain. For more on secrets in the context of running domains, see Prepare to run a domain. In this case, you have two secrets.\nRun the following kubectl commands to deploy the required secrets:\n$ kubectl -n sample-domain1-ns create secret generic \\  sample-domain1-weblogic-credentials \\  --from-literal=username=weblogic --from-literal=password=welcome1 $ kubectl -n sample-domain1-ns label secret \\  sample-domain1-weblogic-credentials \\  weblogic.domainUID=sample-domain1 $ kubectl -n sample-domain1-ns create secret generic \\  sample-domain1-runtime-encryption-secret \\  --from-literal=password=welcome1 $ kubectl -n sample-domain1-ns label secret \\  sample-domain1-runtime-encryption-secret \\  weblogic.domainUID=sample-domain1 Some important details about these secrets:\n  The WebLogic credentials secret:\n It is required and must contain username and password fields. It must be referenced by the spec.webLogicCredentialsSecret field in your Domain resource YAML file. For complete details about the Domain resource, see the Domain resource reference. It also must be referenced by macros in the domainInfo.AdminUserName and domainInfo.AdminPassWord fields in your model.10.yaml file.    The Model WDT runtime encrytion secret:\n This is a special secret required by Model in Image. It must contain a password field. It must be referenced using the spec.model.runtimeEncryptionSecret field in your Domain resource YAML file. It must remain the same for as long as the domain is deployed to Kubernetes but can be changed between deployments. It is used to encrypt data as it\u0026rsquo;s internally passed using log files from the domain\u0026rsquo;s introspector job and on to its WebLogic Server pods.    Deleting and recreating the secrets:\n You must delete a secret before creating it, otherwise the create command will fail if the secret already exists. This allows you to change the secret when using the kubectl create secret command.    You name and label secrets using their associated domainUID for two reasons:\n To make it obvious which secrets belong to which domains. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all resources associated with a domain.    Kubernetes Secrets for Docker Deploy a corresponding Kubernetes docker secret to the same namespace to access the image during domain creation.\nUse kubernetes/samples/scripts/create-kubernetes-secrets/create-docker-credentials-secret.sh to create the secret. Please invoke the script with the -h option to see the available switches and usage.\n$ cd weblogic-kubernetes-operator $ cd kubernetes/samples/scripts/create-kubernetes-secrets $ ./create-docker-credentials-secret.sh -h Get the password for the ACR and store it in the Kubernetes secret.\n$ az acr credential show --name $AKS_PERS_ACR The login server endpoint suffix '.azurecr.io' is automatically omitted. { \u0026quot;passwords\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;password\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;f02Ls3jqnNQ0ToXIoyY2g8oJrVk0w5P/\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;password2\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;qbZx1bZT7=rha7Ta6Wa0zfCZqoNMNoj1\u0026quot; } ], \u0026quot;username\u0026quot;: \u0026quot;contosoresourcegroup1610068510\u0026quot; } $ export AKS_PERS_ACR_PASSWORD=\u0026lt;the-password-from-your-output\u0026gt; Use the create-docker-credentials-secret.sh script to store the ACR credentials as a Kubernetes secret.\n# cd kubernetes/samples/scripts/create-kubernetes-secrets $ export SECRET_NAME_DOCKER=\u0026#34;regsecret\u0026#34; $ ./create-docker-credentials-secret.sh -s ${SECRET_NAME_DOCKER} -e $AKS_PERS_RESOURCE_GROUP -p $AKS_PERS_ACR_PASSWORD -u $AKS_PERS_RESOURCE_GROUP -d $AKS_PERS_ACR -n sample-domain1-ns secret/regsecret created The secret regsecret has been successfully created in the sample-domain1-ns namespace. Domain resource Now, you create a Domain YAML file. Think of the Domain YAML file as the way to configure some aspects of your WebLogic domain using Kubernetes. The operator uses the Kubernetes \u0026ldquo;custom resource\u0026rdquo; feature to define a Kubernetes resource type called Domain. For more on the Domain Kubernetes resource, see Domain Resource. For more on custom resources see the Kubernetes documentation.\nWe provide a sample file at kubernetes/samples/scripts/create-weblogic-domain/model-in-image/domain-resources/WLS/mii-initial-d1-WLS-v1.yaml, copy it to a file called /tmp/mii-sample/mii-initial.yaml.\n$ cd kubernetes/samples/scripts/create-weblogic-domain/model-in-image/domain-resources/WLS $ cp mii-initial-d1-WLS-v1.yaml /tmp/mii-sample/mii-initial.yaml Modify the Domain YAML with your values.\n   Name in YAML file Example value Notes     spec.image $AKS_PERS_ACR/$AKS_PERS_ACR:model-in-image-aks Must be the same as the value to which you pushed the image to by running the command docker push $AKS_PERS_ACR/$AKS_PERS_ACR:model-in-image-aks.   spec.imagePullSecrets.name regsecret Make sure its value is the same value with ${SECRET_NAME_DOCKER}.    Run the following command to create the domain custom resource:\n$ kubectl apply -f /tmp/mii-sample/mii-initial.yaml Successful output will look like:\ndomain.weblogic.oracle/sample-domain1 created Verify the WebLogic Server pods are all running:\n$ kubectl get pods -n sample-domain1-ns --watch NAME READY STATUS RESTARTS AGE sample-domain1-introspector-xwpbn 0/1 ContainerCreating 0 0s sample-domain1-introspector-xwpbn 1/1 Running 0 1s sample-domain1-introspector-xwpbn 0/1 Completed 0 66s sample-domain1-introspector-xwpbn 0/1 Terminating 0 67s sample-domain1-introspector-xwpbn 0/1 Terminating 0 67s sample-domain1-admin-server 0/1 Pending 0 0s sample-domain1-admin-server 0/1 Pending 0 0s sample-domain1-admin-server 0/1 ContainerCreating 0 0s sample-domain1-admin-server 0/1 Running 0 2s sample-domain1-admin-server 1/1 Running 0 42s sample-domain1-managed-server1 0/1 Pending 0 0s sample-domain1-managed-server1 0/1 Pending 0 0s sample-domain1-managed-server1 0/1 ContainerCreating 0 0s sample-domain1-managed-server2 0/1 Pending 0 0s sample-domain1-managed-server2 0/1 Pending 0 0s sample-domain1-managed-server2 0/1 ContainerCreating 0 0s sample-domain1-managed-server2 0/1 Running 0 3s sample-domain1-managed-server2 1/1 Running 0 40s sample-domain1-managed-server1 0/1 Running 0 53s sample-domain1-managed-server1 1/1 Running 0 93s # The success deployment should be: $ kubectl get all -n sample-domain1-ns NAME READY STATUS RESTARTS AGE pod/sample-domain1-admin-server 1/1 Running 0 16m pod/sample-domain1-managed-server1 1/1 Running 0 15m pod/sample-domain1-managed-server2 1/1 Running 0 15m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/sample-domain1-admin-server ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 16m service/sample-domain1-cluster-cluster-1 ClusterIP 10.0.188.60 \u0026lt;none\u0026gt; 8001/TCP 15m service/sample-domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 15m service/sample-domain1-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 15m It may take you up to 10 minutes to deploy all pods, please wait and make sure everything is ready.\nInvoke the web application Create Azure load balancer Create the Azure public standard load balancer to access the WebLogic Server Administration Console and applications deployed in the cluster.\nUse the configuration file in kubernetes/samples/scripts/create-weblogic-domain-on-azure-kubernetes-service/model-in-image/admin-lb.yaml to create a load balancer service for the Administration Server. If you are choosing not to use the predefined YAML file and instead created a new one with customized values, then substitute the following content with you domain values.\n  Click here to view YAML content.   apiVersion: v1 kind: Service metadata: name: sample-domain1-admin-server-external-lb namespace: sample-domain1-ns spec: ports: - name: default port: 7001 protocol: TCP targetPort: 7001 selector: weblogic.domainUID: sample-domain1 weblogic.serverName: admin-server sessionAffinity: None type: LoadBalancer    Use the configuration file in kubernetes/samples/scripts/create-weblogic-domain-on-azure-kubernetes-service/model-in-image/cluster-lb.yaml to create a load balancer service for the managed servers. If you are choosing not to use the predefined YAML file and instead created new one with customized values, then substitute the following content with you domain values.\n  Click here to view YAML content.   apiVersion: v1 kind: Service metadata: name: sample-domain1-cluster-1-lb namespace: sample-domain1-ns spec: ports: - name: default port: 8001 protocol: TCP targetPort: 8001 selector: weblogic.domainUID: sample-domain1 weblogic.clusterName: cluster-1 sessionAffinity: None type: LoadBalancer    Create the load balancer services using the following command:\n$ cd kubernetes/samples/scripts/create-weblogic-domain-on-azure-kubernetes-service/model-in-image $ kubectl apply -f admin-lb.yaml service/sample-domain1-admin-server-external-lb created $ kubectl apply -f cluster-lb.yaml service/sample-domain1-cluster-1-external-lb created Get the external IP addresses of the Administration Server and cluster load balancers (please wait for the external IP addresses to be assigned):\n$ kubectl get svc -n sample-domain1-ns --watch NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE sample-domain1-admin-server ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 8m33s sample-domain1-admin-server-external-lb LoadBalancer 10.0.184.118 52.191.234.149 7001:30655/TCP 2m30s sample-domain1-cluster-1-lb LoadBalancer 10.0.76.7 52.191.235.71 8001:30439/TCP 2m25s sample-domain1-cluster-cluster-1 ClusterIP 10.0.118.225 \u0026lt;none\u0026gt; 8001/TCP 7m53s sample-domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 7m53s sample-domain1-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 7m52s In the example, the URL to access the Administration Server is: http://52.191.234.149:7001/console. IMPORTANT: You must ensure that any Network Security Group rules that govern access to the console allow inbound traffic on port 7001. The default user name for the Administration Console is weblogic and the default password is welcome1. Please change this for production deployments.\nIf the WLS Administration Console is still not available, use kubectl describe domain to check domain status.\n$ kubectl describe domain domain1 Make sure the status of cluster-1 is ServersReady and Available.\n  Click here to view the example domain status.   Name: sample-domain1 Namespace: sample-domain1-ns Labels: weblogic.domainUID=sample-domain1 Annotations: \u0026lt;none\u0026gt; API Version: weblogic.oracle/v8 Kind: Domain Metadata: Creation Timestamp: 2020-11-30T05:40:11Z Generation: 1 Resource Version: 9346 Self Link: /apis/weblogic.oracle/v8/namespaces/sample-domain1-ns/domains/sample-domain1 UID: 9f10a602-714a-46c5-8dcb-815616b587af Spec: Admin Server: Server Start State: RUNNING Clusters: Cluster Name: cluster-1 Replicas: 2 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Start State: RUNNING Configuration: Model: Domain Type: WLS Runtime Encryption Secret: sample-domain1-runtime-encryption-secret Domain Home: /u01/domains/sample-domain1 Domain Home Source Type: FromModel Image: docker.io/sleepycat2/wls-on-aks:model-in-image Image Pull Policy: IfNotPresent Image Pull Secrets: Name: regsecret Include Server Out In Pod Log: true Replicas: 1 Restart Version: 1 Server Pod: Env: Name: CUSTOM_DOMAIN_NAME Value: domain1 Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx512m Resources: Requests: Cpu: 250m Memory: 768Mi Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: sample-domain1-weblogic-credentials Status: Clusters: Cluster Name: cluster-1 Maximum Replicas: 5 Minimum Replicas: 0 Ready Replicas: 2 Replicas: 2 Replicas Goal: 2 Conditions: Last Transition Time: 2020-11-30T05:45:15.493Z Reason: ServersReady Status: True Type: Available Introspect Job Failure Count: 0 Replicas: 2 Servers: Desired State: RUNNING Health: Activation Time: 2020-11-30T05:44:15.652Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: aks-pool1model-71528953-vmss000001 Server Name: admin-server State: RUNNING Cluster Name: cluster-1 Desired State: RUNNING Health: Activation Time: 2020-11-30T05:44:54.699Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: aks-pool1model-71528953-vmss000000 Server Name: managed-server1 State: RUNNING Cluster Name: cluster-1 Desired State: RUNNING Health: Activation Time: 2020-11-30T05:45:07.211Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: aks-pool1model-71528953-vmss000001 Server Name: managed-server2 State: RUNNING Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server3 Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server4 Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server5 Start Time: 2020-11-30T05:40:11.709Z Events: \u0026lt;none\u0026gt;    Access the application Access the Administration Console using the admin load balancer IP address, http://52.191.234.149:7001/console\nAccess the sample application using the cluster load balancer IP address.\n## Access the sample application using the cluster load balancer IP (52.191.235.71) $ curl http://52.191.235.71:8001/myapp_war/index.jsp \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version 'v1' of the mii-sample JSP web-app. Welcome to WebLogic Server 'managed-server1'! domain UID = 'sample-domain1' domain name = 'domain1' Found 1 local cluster runtime: Cluster 'cluster-1' Found 0 local data sources: ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; $ curl http://52.191.235.71:8001/myapp_war/index.jsp \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version 'v1' of the mii-sample JSP web-app. Welcome to WebLogic Server 'managed-server2'! domain UID = 'sample-domain1' domain name = 'domain1' Found 1 local cluster runtime: Cluster 'cluster-1' Found 0 local data sources: ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; Rolling updates Naturally, you will want to deploy newer versions of the EAR application, located in the WDT archive ZIP file at wlsdeploy/applications/myapp-v1. To learn how to do this, follow the steps in Update 3.\nClean up resources Run the following commands to clean up resources.\n$ az group delete --yes --no-wait --name $AKS_PERS_RESOURCE_GROUP $ az group delete --yes --no-wait --name \u0026#34;MC_$AKS_PERS_RESOURCE_GROUP\u0026#34;_\u0026#34;$AKS_CLUSTER_NAME\u0026#34;_\u0026#34;$AKS_PERS_LOCATION\u0026#34; $ az ad sp delete --id $SP_APP_ID  Troubleshooting For troubleshooting advice, see Troubleshooting.\nUseful links  Model in Image user documentation Model in Image sample  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/azure-kubernetes-service/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "Troubleshooting.",
	"content": " Access Administration Console: Possible causes for Administration Console inaccessibility Domain debugging Pod Error: How to get details of the pod error WebLogic Image Tool failure WebLogic Kubernetes Operator installation failure  System pods are pending WebLogic Kubernetes Operator ErrImagePull   WSL2 bad timestamp Cannot attach ACR due to not being Owner of subscription Virtual Machine size is not supported  Get pod error details You may get the following message while creating the WebLogic domain: \u0026quot;the job status is not Completed!\u0026quot;\nstatus on iteration 20 of 20 pod domain1-create-weblogic-sample-domain-job-nj7wl status is Init:0/1 The create domain job is not showing status completed after waiting 300 seconds. Check the log output for errors. Error from server (BadRequest): container \u0026#34;create-weblogic-sample-domain-job\u0026#34; in pod \u0026#34;domain1-create-weblogic-sample-domain-job-nj7wl\u0026#34; is waiting to start: PodInitializing [ERROR] Exiting due to failure - the job status is not Completed! You can get further error details by running kubectl describe pod, as shown here:\n$ kubectl describe pod \u0026lt;your-pod-name\u0026gt; This is an output example:\n$ kubectl describe pod domain1-create-weblogic-sample-domain-job-nj7wl Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 4m2s default-scheduler Successfully assigned default/domain1-create-weblogic-sample-domain-job-qqv6k to aks-nodepool1-58449474-vmss000001 Warning FailedMount 119s kubelet, aks-nodepool1-58449474-vmss000001 Unable to mount volumes for pod \u0026quot;domain1-create-weblogic-sample-domain-job-qqv6k_default(15706980-73cb-11ea-b804-b2c91b494b00)\u0026quot;: timeout expired waiting for volumes to attach or mount for pod \u0026quot;default\u0026quot;/\u0026quot;domain1-create-weblogic-sample-domain-job-qqv6k\u0026quot;. list of unmounted volumes=[weblogic-sample-domain-storage-volume]. list of unattached volumes=[create-weblogic-sample-domain-job-cm-volume weblogic-sample-domain-storage-volume weblogic-credentials-volume default-token-zr7bq] Warning FailedMount 114s (x9 over 4m2s) kubelet, aks-nodepool1-58449474-vmss000001 MountVolume.SetUp failed for volume \u0026quot;wls-azurefile\u0026quot; : Couldn't get secret default/azure-secrea Fail to access Administration Console Here are some common reasons for this failure, along with some tips to help you investigate.\n  Create WebLogic domain job fails\nCheck the deploy log and find the failure details with kubectl describe pod podname. Please go Getting pod error details.\n  Process of starting the servers is still running\nCheck with kubectl get svc and if domainUID-admin-server, domainUID-managed-server1, and domainUID-managed-server2 are not listed, we need to wait some more for the Administration Server to start.\n  The following output is an example of when the Administration Server has started.\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE domain1-admin-server ClusterIP None \u0026lt;none\u0026gt; 30012/TCP,7001/TCP 7m3s domain1-admin-server-ext NodePort 10.0.78.211 \u0026lt;none\u0026gt; 7001:30701/TCP 7m3s domain1-admin-server-external-lb LoadBalancer 10.0.6.144 40.71.233.81 7001:32758/TCP 7m32s domain1-cluster-1-lb LoadBalancer 10.0.29.231 52.142.39.152 8001:31022/TCP 7m30s domain1-cluster-cluster-1 ClusterIP 10.0.80.134 \u0026lt;none\u0026gt; 8001/TCP 1s domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 1s domain1-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 1s internal-weblogic-operator-svc ClusterIP 10.0.1.23 \u0026lt;none\u0026gt; 8082/TCP 9m59s kubernetes ClusterIP 10.0.0.1 \u0026lt;none\u0026gt; 443/TCP 16m If services are up but the WLS Administration Console is still not available, use kubectl describe domain to check domain status.\n$ kubectl describe domain domain1 Make sure the status of cluster-1 is ServersReady and Available. The status of admin-server, managed-server1, and managed-server2 should be RUNNING. Otherwise, the cluster is likely still in the process of becoming fully ready.\n  Click here to view the example status.   Status: Clusters: Cluster Name: cluster-1 Maximum Replicas: 5 Minimum Replicas: 1 Ready Replicas: 2 Replicas: 2 Replicas Goal: 2 Conditions: Last Transition Time: 2020-07-06T05:39:32.539Z Reason: ServersReady Status: True Type: Available Replicas: 2 Servers: Desired State: RUNNING Node Name: aks-nodepool1-11471722-vmss000001 Server Name: admin-server State: RUNNING Cluster Name: cluster-1 Desired State: RUNNING Node Name: aks-nodepool1-11471722-vmss000001 Server Name: managed-server1 State: RUNNING Cluster Name: cluster-1 Desired State: RUNNING Node Name: aks-nodepool1-11471722-vmss000001 Server Name: managed-server2 State: RUNNING Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server3 Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server4 Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server5    Domain debugging For some suggestions for debugging problems with Model in Image after your Domain YAML file is deployed, see Debugging.\nWSL2 bad timestamp If you are running with WSL2, you may run into the bad timestamp issue, which blocks Azure CLI. You may see the following error:\n$ kubectl get pod Unable to connect to the server: x509: certificate has expired or is not yet valid: current time 2020-11-25T15:58:10+08:00 is before 2020-11-27T04:25:04Z You can run the following command to update WSL2 system time:\n# Fix the outdated systime time $ sudo hwclock -s # Check systime time $ data Fri Nov 27 13:07:14 CST 2020 Timeout for the operator installation You may run into a timeout while installing the operator and get the following error:\n$ helm install weblogic-operator kubernetes/charts/weblogic-operator \\  --namespace sample-weblogic-operator-ns \\  --set serviceAccount=sample-weblogic-operator-sa \\  --set \u0026#34;enableClusterRoleBinding=true\u0026#34; \\  --set \u0026#34;domainNamespaceSelectionStrategy=LabelSelector\u0026#34; \\  --set \u0026#34;domainNamespaceLabelSelector=weblogic-operator\\=enabled\u0026#34; \\ --wait Error: timed out waiting for the condition Make sure you are working with the main branch. Remove the operator and install again.\n$ helm uninstall weblogic-operator -n sample-weblogic-operator-ns release \u0026quot;weblogic-operator\u0026quot; uninstalled Check out main and install the operator.\n$ cd weblogic-kubernetes-operator $ git checkout main $ helm install weblogic-operator kubernetes/charts/weblogic-operator \\  --namespace sample-weblogic-operator-ns \\  --set serviceAccount=sample-weblogic-operator-sa \\  --set \u0026#34;enableClusterRoleBinding=true\u0026#34; \\  --set \u0026#34;domainNamespaceSelectionStrategy=LabelSelector\u0026#34; \\  --set \u0026#34;domainNamespaceLabelSelector=weblogic-operator\\=enabled\u0026#34; \\  --wait WebLogic Image Tool failure If your version of WIT is older than 1.9.8, you will get an error running ./imagetool/bin/imagetool.sh if the Docker buildkit is enabled.\nHere is the warning message shown:\nfailed to solve with frontend dockerfile.v0: failed to create LLB definition: failed to parse stage name \u0026#34;WDT_BUILD\u0026#34;: invalid reference format: repository name must be lowercase To resolve the error, either upgrade to a newer version of WIT or disable the Docker buildkit with the following commands and run the imagetool command again.\n$ export DOCKER_BUILDKIT=0 $ export COMPOSE_DOCKER_CLI_BUILD=0 WebLogic Kubernetes Operator installation failure Currently, we meet two cases that block the operator installation:\n The system pods in the AKS cluster are pending. The operator image is unavailable.  Follow these steps to dig into the error.\nThe AKS cluster system pods are pending If system pods in the AKS cluster are pending, it will block the operator installation.\nThis is an error example with warning message no nodes available to schedule pods.\n$ kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE default weblogic-operator-c5c78b8b5-ssvqk 0/1 Pending 0 13m kube-system coredns-79766dfd68-wcmkd 0/1 Pending 0 3h22m kube-system coredns-autoscaler-66c578cddb-tc946 0/1 Pending 0 3h22m kube-system dashboard-metrics-scraper-6f5fb5c4f-9f5mb 0/1 Pending 0 3h22m kube-system kubernetes-dashboard-849d5c99ff-xzknj 0/1 Pending 0 3h22m kube-system metrics-server-7f5b4f6d8c-bqzrn 0/1 Pending 0 3h22m kube-system tunnelfront-765bf6df59-msj27 0/1 Pending 0 3h22m sample-weblogic-operator-ns weblogic-operator-f86b879fd-v2xrz 0/1 Pending 0 35m $ kubectl describe pod weblogic-operator-f86b879fd-v2xrz -n sample-weblogic-operator-ns Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 71s (x25 over 36m) default-scheduler no nodes available to schedule pods If you run into this error, remove the AKS cluster and create a new one.\nRun the kubectl get pod -A to make sure all the system pods are running.\n$ kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-79766dfd68-ch5b9 1/1 Running 0 3h44m kube-system coredns-79766dfd68-sxk4g 1/1 Running 0 3h43m kube-system coredns-autoscaler-66c578cddb-s5qm5 1/1 Running 0 3h44m kube-system dashboard-metrics-scraper-6f5fb5c4f-wtckh 1/1 Running 0 3h44m kube-system kube-proxy-fwll6 1/1 Running 0 3h42m kube-system kube-proxy-kq6wj 1/1 Running 0 3h43m kube-system kube-proxy-t2vbb 1/1 Running 0 3h43m kube-system kubernetes-dashboard-849d5c99ff-hrz2w 1/1 Running 0 3h44m kube-system metrics-server-7f5b4f6d8c-snnbt 1/1 Running 0 3h44m kube-system omsagent-8tf4j 1/1 Running 0 3h43m kube-system omsagent-n9b7k 1/1 Running 0 3h42m kube-system omsagent-rcmgr 1/1 Running 0 3h43m kube-system omsagent-rs-787ff54d9d-w7tp5 1/1 Running 0 3h44m kube-system tunnelfront-794845c84b-v9f98 1/1 Running 0 3h44m WebLogic Kubernetes Operator ErrImagePull If you got an error of ErrImagePull from pod status, use docker pull to check the operator image. If an error occurs, you can switch to a version that is greater than 3.1.1.\n$ docker pull ghcr.io/oracle/weblogic-kubernetes-operator:\u0026lt;version\u0026gt; # Example: pull 3.1.1. $ docker pull ghcr.io/oracle/weblogic-kubernetes-operator:3.1.1 3.1.1: Pulling from oracle/weblogic-kubernetes-operator 980316e41237: Pull complete c980371d97ea: Pull complete db19c8ff0d12: Pull complete 550f44317ae5: Pull complete be2e701f5ee0: Pull complete 1cb891615559: Pull complete 4f4fb700ef54: Pull complete Digest: sha256:6b060ec1989fcb26e1acb0d0b906d81fce6b8ec2e0a30fa2b9d9099290eb6416 Status: Downloaded newer image for ghcr.io/oracle/weblogic-kubernetes-operator:3.1.1 ghcr.io/oracle/weblogic-kubernetes-operator:3.1.1 Cannot attach ACR due to not being Owner of subscription If you\u0026rsquo;re unable to create an ACR and you\u0026rsquo;re using a service principal, you can use manual Role Assignments to grant access to the ACR as described in Azure Container Registry authentication with service principals.\nFirst, find the objectId of the service principal used when the AKS cluster was created. You will need the output from az ad sp create-for-rbac, which you were directed to save to a file. Within the output, you need the value of the name property. It will start with http. Get the objectId with this command.\n$ az ad sp show --id http://\u0026lt;your-name-from-the-saved-output\u0026gt; | grep objectId \u0026quot;objectId\u0026quot;: \u0026quot;nror4p30-qnoq-4129-o89r-p60n71805npp\u0026quot;, Next, assign the acrpull role to that service principal with this command.\n$ az role assignment create --assignee-object-id \u0026lt;your-objectId-from-above\u0026gt; --scope $AKS_PERS_RESOURCE_GROUP --role acrpull { \u0026quot;canDelegate\u0026quot;: null, \u0026quot;condition\u0026quot;: null, ... \u0026quot;type\u0026quot;: \u0026quot;Microsoft.Authorization/roleAssignments\u0026quot; } After you do this, re-try the command that gave the error.\nVirtual Machine size is not supported If you run into the following error when creating the AKS cluster, please use an available VM size in the region.\n$ az aks create \\  --resource-group $AKS_PERS_RESOURCE_GROUP \\  --name $AKS_CLUSTER_NAME \\  --node-count 2 \\  --generate-ssh-keys \\  --nodepool-name nodepool1 \\  --node-vm-size Standard_DS2_v2 \\  --location $AKS_PERS_LOCATION \\  --service-principal $SP_APP_ID \\  --client-secret $SP_CLIENT_SECRET BadRequestError: Operation failed with status: 'Bad Request'. Details: Virtual Machine size: 'Standard_DS2_v2' is not supported for subscription subscription-id in location 'eastus'. The available VM sizes are 'basic_a0,basic_a1,basic_a2,basic_a3,basic_a4,standard_a2'. Please refer to aka.ms/aks-vm-sizes for the details. ResourceNotFoundError: The Resource 'Microsoft.ContainerService/managedClusters/wlsaks1613726008' under resource group 'wlsresourcegroup1613726008' was not found. For more details please go to https://aka.ms/ARMResourceNotFoundFix As shown in the example, you can use standard_a2; pay attention to the CPU and memory of that size; make sure it meets your memory requirements.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/faq/boot-identity-not-valid/",
	"title": "Boot identity not valid",
	"tags": [],
	"description": "One or more WebLogic Server instances in my domain will not start and I see errors in the server log like this: Boot identity not valid.",
	"content": " One or more WebLogic Server instances in my domain will not start and I see errors in the server log like this:\n\u0026lt;Feb 6, 2020 12:05:35,550 AM GMT\u0026gt; \u0026lt;Authentication denied: Boot identity not valid. The user name or password or both from the boot identity file (boot.properties) is not valid. The boot identity may have been changed since the boot identity file was created. Please edit and update the boot identity file with the proper values of username and password. The first time the updated boot identity file is used to start the server, these new values are encrypted.\u0026gt;\n When you see these kinds of errors, it typically means that the user name and password provided in the weblogicCredentialsSecret are incorrect. Prior to operator version 2.5.0, this error could have also indicated that the WebLogic domain directory\u0026rsquo;s security configuration files have changed in an incompatible way between when the operator scanned the domain directory, which occurs during the \u0026ldquo;introspection\u0026rdquo; phase, and when the server instance attempted to start. There is now a separate validation for that condition described in the Domain secret mismatch FAQ entry.\nCheck that the user name and password credentials stored in the Kubernetes Secret, referenced by weblogicCredentialsSecret, contain the expected values for an account with administrative privilege for the WebLogic domain. Then stop all WebLogic Server instances in the domain before restarting so that the operator will repeat its introspection and generate the corrected boot.properties files.\nIf the \u0026ldquo;Boot identity not valid\u0026rdquo; error is still not resolved, and the error is logged on a WebLogic Managed Server, then the server may be failing to contact the Administration Server. Check carefully for any errors or exceptions logged before the \u0026ldquo;Boot identity not valid\u0026rdquo; error in the Managed Server log file and look for indications of communication failure. For example:\n  If you have enabled SSL for the Administration Server default channel or have configured an administration port on the Administration Server, and the SSL port is using the demo identity certificates, then a Managed Server may fail to establish an SSL connection to the Administration Server due to a hostname verification exception (such as the \u0026ldquo;SSLKeyException: Hostname verification failed\u0026rdquo;). For non-production environments, you can turn off the hostname verification by setting the -Dweblogic.security.SSL.ignoreHostnameVerification=true property in the Java options for starting the WebLogic Server.\nNOTE: Turning off hostname verification leaves WebLogic Server vulnerable to man-in-the-middle attacks. Oracle recommends leaving hostname verification on in production environments.\n  For other SSL-related errors, make sure the keystores and passwords are specified correctly in the Java options for starting the WebLogic Server. See Configuring SSL for details on configuring SSL in the Oracle WebLogic Server environment.\n  If the DNS hostname of the Administration Server can\u0026rsquo;t be resolved during Managed Server startup with errors such as \u0026ldquo;java.net.UnknownHostException: domain1-admin-server\u0026rdquo;, then check the DNS server pod logs for any errors and take corrective actions to resolve those errors.\n  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/cicd/choose-an-approach/",
	"title": "Choose an approach",
	"tags": [],
	"description": "How to choose an approach.",
	"content": "Let\u0026rsquo;s review what we have discussed and talk about when we might want to use various approaches. We can start by asking ourselves questions like these:\n  Can you make the desired change with a configuration override or Model in Image ConfigMap?\nWhen your domain home source type is Domain in PV or Domain in Image, the operator allows you to inject a number of configuration overrides into your pods before starting any servers in the domain.\nWhen your domain home source type is Model in Image, you can inject model updates before starting any servers in the domain for any type of update, or even while your domain is running for most types of updates. Model in Image runtime model updates are propagated by restarting (rolling) the running WebLogic Servers.\nA good example of a change would be changing the settings for a data source. For example, you may wish to have a larger connection pool in your production environment than you do in your development/test environments. You probably also want to have different credentials. You may want to change the service name, and so on. All of these kinds of updates can be made with configuration overrides for Domain in PV and Domain in Image, and with model updates for Model in Image. These are placed in a Kubernetes ConfigMap, meaning that they are outside of the image, so they do not require rebuilding the image. If all of your changes fit into this category, it is probably much better to just use configuration overrides for Domain in PV and Domain in Image, and use model updates for Model in Image.\n  Are you only changing the WebLogic configuration, for example, deploying or updating an application, changing a resource configuration in a way that is not supported by configuration overrides, Model in Image model updates, and such?\nIf your changes fit into this category, and you have used the \u0026ldquo;domain-in-image\u0026rdquo; approach and the image layering model, then you only need to update the top layer of your image. This is relatively easy compared to making changes in lower layers. You could create a new layer with the changes, or you could rebuild/replace the existing top layer with a new one. Which approach you choose depends mainly on whether you need to maintain the same domain encryption keys or not.\n  Do you need to be able to do a rolling restart?\nIf you need to do a rolling restart with Domain in Image, for example, to maintain the availability of your applications, then you need to make sure that a new domain layer has the same domain encryption keys. You cannot perform a rolling restart of a domain if the new members have a different encryption key.\nIf you need to do a rolling restart with Model in Image or Domain in PV, the domain encryption keys are not a concern. In Model in Image, the keys are generated by the operator at runtime before the first WebLogic Server pod is started. In Domain in PV, the keys are generated once, the first time the domain is started, and remain in the domain home within the PV.\n  Do you need to mutate something in a lower layer, for example, patch WebLogic, the JDK, or Linux?\nIf you need to make an update in a lower layer, then you will need to rebuild that layer and all of the layers above it. This means that you will need to rebuild the domain layer. In the case of Domain in Image, you will need to determine if you need to keep the same domain encryption keys.\n  The diagram below summarizes these concerns in a decision tree for the “domain-in-image” case:\nIf you are using the \u0026ldquo;domain-on-PV\u0026rdquo; or \u0026ldquo;model-in-image\u0026rdquo; approach, many of these concerns become moot because you have an effective separation between your domain and the image. There is still the possibility that an update in the image could affect your domain; for example, if you updated the JDK, you may need to update some of your domain scripts to reflect the new JDK path.\nHowever, in this scenario, your environment is much closer to what you are probably used to in a traditional (non-Kubernetes) environment, and you will probably find that all of the practices you used from that pre-Kubernetes environment are directly applicable here too, with just some small modifications. For example, applying a WebLogic patch would now involve building a new image.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/introduction/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "An architectural overview of the operator runtime and related resources.",
	"content": "Contents  Overall architecture Domain architecture Domain UID Network name predictability Domain state stored outside container images  Overall architecture The operator consists of the following parts:\n The operator runtime, which is a process that:  Runs in a container deployed into a Kubernetes Pod and that monitors one or more Kubernetes namespaces. Performs the actual management tasks for domain resources deployed to these namespaces.   A Helm chart for installing the operator runtime and its related resources. A Kubernetes custom resource definition (CRD) that, when installed, enables the Kubernetes API server and the operator to monitor and manage domain resource instances. Domain resources that reference WebLogic domain configuration, a WebLogic install, and anything else necessary to run the domain. A variety of samples for preparing or packaging WebLogic domains for running in Kubernetes.  The operator is packaged in a container image which you can access using the following docker pull commands:\n$ docker pull ghcr.io/oracle/weblogic-kubernetes-operator:3.3.2 For more details on acquiring the operator image and prerequisites for installing the operator, consult the Quick Start guide.\nThe operator registers a Kubernetes custom resource definition called domain.weblogic.oracle (shortname domain, plural domains). More details about the Domain type defined by this CRD, including its schema, are available here.\nThe diagram below shows the general layout of high-level components, including optional components, in a Kubernetes cluster that is hosting WebLogic domains and the operator:\nThe Kubernetes cluster has several namespaces. Components may be deployed into namespaces as follows:\n One or more operators, each deployed into its own namespace.  There can be more than one operator in a Kubernetes cluster but only a single operator per namespace. Each operator is configured with the specific namespaces that it is responsible for.  The operator will not take any action on any domain that is not in one of the namespaces the operator is configured to manage. Multiple operators cannot manage the same namespace. The operator can be configured to monitor its own namespace.   There is no limit on the number of domains or namespaces that an operator can manage. If the Elastic Stack integration option is configured to monitor the operator, then a Logstash pod will also be deployed in the operator’s namespace.   WebLogic domain resources deployed into various namespaces.  There can be more than one domain in a namespace, if desired. Every domain resource must be configured with a domain unique identifier.   Customers are responsible for load balancer configuration, which will typically be in the same namespace with domains or in a shared namespace. Customers are responsible for Elasticsearch and Kibana deployments that may be used to monitor WebLogic server and pod logs.  Domain UID Every domain resource must be configured with a domain unique identifier which is a string and may also be called a Domain UID, domainUID, or DOMAIN_UID depending on the context. This value is distinct and need not match the domain name from the WebLogic domain configuration. The operator will use this as a name prefix for the domain related resources that it creates for you (such as services and pods).\nA Domain UID is set on a domain resource using spec.domainUID, and defaults to the value of metadata.name. The spec.domainUID domain resource field is usually left unset in order to take advantage of this default.\nIt is recommended that a Domain UID be configured to be unique across all Kubernetes namespaces and even across different Kubernetes clusters in order to assist in future work to identify related domains in active-passive scenarios across data centers; however, it is only required that this value be unique within a namespace, similarly to the names of Kubernetes resources.\nAs a convention, any resource that is associated with a particular Domain UID is typically given a Kubernetes label named weblogic.domainUID that is assigned to that UID. If the operator creates a resource for you on behalf of a particular domain, it will follow this convention. For example, to see all pods created with the weblogic.domainUID label in a Kubernetes cluster try: kubectl get pods -l weblogic.domainUID --all-namespaces=true --show-labels=true.\nA Domain UID may be up to 45 characters long. For more details about Domain UID name requirements, see Meet Kubernetes resource name restrictions.\nDomain architecture The diagram below shows how the various parts of a WebLogic domain are manifest in Kubernetes by the operator.\nThis diagram shows the following details:\n An optional, persistent volume is created by the customer using one of the available providers. If the persistent volume is shared across the domain or members of a cluster, then the chosen provider must support “Read Write Many” access mode. The shared state on the persistent volume may include the domain directory, the applications directory, a directory for storing logs, and a directory for any file-based persistence stores. A pod is created for the WebLogic Server Administration Server. This pod is named DOMAIN_UID-wlservername and is labeled with weblogic.domainUID, weblogic.serverName, and weblogic.domainName. One container runs in this pod. WebLogic Node Manager and Administration Server processes are run inside this container. The Node Manager process is used as an internal implementation detail for the liveness probe which we will descibe in more detail later, for patching, and to provide monitoring and control capabilities to the Administration Console. It is not intended to be used for other purposes, and it may be removed in some future release. A ClusterIP type service is created for the Administration Server pod. This service provides a stable, well-known network (DNS) name for the Administration Server. This name is derived from the domainUID and the Administration Server name as described here, and it is known before starting up any pod. The Administration Server ListenAddress is set to this well-known name. ClusterIP type services are only visible inside the Kubernetes cluster. They are used to provide the well-known names that all of the servers in a domain use to communicate with each other. This service is labeled with weblogic.domainUID and weblogic.domainName. A NodePort type service is optionally created for the Administration Server pod. This service provides HTTP access to the Administration Server to clients that are outside the Kubernetes cluster. This service is intended to be used to access the WebLogic Server Administration Console or for the T3 protocol for WLST connections. This service is labeled with weblogic.domainUID and weblogic.domainName. A pod is created for each WebLogic Server Managed Server. These pods are named DOMAIN_UID-wlservername and are labeled with weblogic.domainUID, weblogic.serverName, and weblogic.domainName. One container runs in each pod. WebLogic Node Manager and Managed Server processes are run inside each of these containers. The Node Manager process is used as an internal implementation detail for the liveness probe which we will describe in more detail later. It is not intended to be used for other purposes, and it may be removed in some future release. A ClusterIP type service is created for each Managed Server pod as described here. These services are intended to be used to access applications running on the Managed Servers. These services are labeled with weblogic.domainUID and weblogic.domainName. A ClusterIP type service is also created for each WebLogic cluster as described here. Customers can expose these services using a load balancer or NodePort type service to expose these endpoints outside the Kubernetes cluster. A PodDisruptionBudget is created for each WebLogic cluster. These pod disruption budgets are labeled with weblogic.domainUID, weblogic.clusterName and weblogic.domainName. An Ingress may optionally be created by the customer for each WebLogic cluster. An Ingress provides load balanced HTTP access to all Managed Servers in that WebLogic cluster. The load balancer updates its routing table for an Ingress every time a Managed Server in the WebLogic cluster becomes “ready” or ceases to be able to service requests, such that the Ingress always points to just those Managed Servers that are able to handle user requests.  Kubernetes requires that the names of some resource types follow the DNS label standard as defined in DNS Label Names and RFC 1123. Therefore, the operator enforces that the names of the Kubernetes resources do not exceed Kubernetes limits (see Meet Kubernetes resource name restrictions).\n The diagram below shows the components inside the containers running WebLogic Server instances:\nThe Domain specifies a container image, defaulting to container-registry.oracle.com/middleware/weblogic:12.2.1.4. All containers running WebLogic Server use this same image. Depending on the use case, this image could contain the WebLogic Server product binaries or also include the domain directory. During a rolling event caused by a change to the Domain\u0026rsquo;s image field, containers will be using a mix of the updated value of the image field and its previous value.\n Within the container, the following aspects are configured by the operator:\n The ENTRYPOINT is configured by a script that starts up a Node Manager process, and then uses WLST to request that Node Manager start the server. Node Manager is used to start servers so that the socket connection to the server will be available to obtain server status even when the server is unresponsive. This is used by the liveness probe. The liveness probe is configured to check that a server is alive by querying the Node Manager process. By default, the liveness probe is configured to check liveness every 45 seconds and to timeout after 5 seconds. If a pod fails the liveness probe, Kubernetes will restart that container. For details about liveness probe customization, see Liveness probe customization. The readiness probe is configured to use the WebLogic Server ReadyApp framework. The readiness probe determines if a server is ready to accept user requests. The readiness probe is used to determine when a server should be included in a load balancer\u0026rsquo;s endpoints, in the case of a rolling restart, when a restarted server is fully started, and for various other purposes. For details about readiness probe customization, see Readiness probe customization. A shutdown hook is configured that will execute a script that performs a graceful shutdown of the server. This ensures that servers have an opportunity to shut down cleanly before they are killed.  Network name predictability The operator deploys services with predictable well-defined DNS names for each WebLogic server and cluster in your WebLogic configuration. The name of a WebLogic server service is DOMAIN_UID-wlservername and the name of a WebLogic server cluster is DOMAIN_UID-cluster-wlclustername, all in lowercase, with underscores _ converted to hyphens -.\nThe operator also automatically overrides the ListenAddress fields in each running WebLogic Server to match its service name in order to ensure that the servers will always be able to find each other.\nFor details, see Meet Kubernetes resource name restrictions.\nDomain state stored outside container images The operator expects (and requires) that all state that is expected to outlive the life of a pod be stored outside of the images that are used to run the domain. This means either in a persistent file system, or in a database. The WebLogic configuration, that is, the domain directory and the applications directory may come from the image or a persistent volume. However, other state, such as file-based persistent stores, and such, must be stored on a persistent volume or in a database. All of the containers that are participating in the WebLogic domain use the same image, and take on their personality; that is, which server they execute, at startup time. Each Pod mounts storage, according to the Domain, and has access to the state information that it needs to fulfill its role in the domain.\nIt is worth providing some background information on why this approach was adopted, in addition to the fact that this separation is consistent with other existing operators (for other products) and the Kubernetes “cattle, not pets” philosophy when it comes to containers.\nThe external state approach allows the operator to treat the images as essentially immutable, read-only, binary images. This means that the image needs to be pulled only once, and that many domains can share the same image. This helps to minimize the amount of bandwidth and storage needed for WebLogic Server images.\nThis approach also eliminates the need to manage any state created in a running container, because all of the state that needs to be preserved is written into either the persistent volume or a database backend. The containers and pods are completely throwaway and can be replaced with new containers and pods, as necessary. This makes handling failures and rolling restarts much simpler because there is no need to preserve any state inside a running container.\nWhen users wish to apply a binary patch to WebLogic Server, it is necessary to create only a single new, patched image. If desired, any domains that are running may be updated to this new patched image with a rolling restart, because there is no state in the containers.\nIt is envisaged that in some future release of the operator, it will be desirable to be able to “move” or “copy” domains in order to support scenarios like Kubernetes federation, high availability, and disaster recovery. Separating the state from the running containers is seen as a way to greatly simplify this feature, and to minimize the amount of data that would need to be moved over the network, because the configuration is generally much smaller than the size of WebLogic Server images.\nThe team developing the operator felt that these considerations provided adequate justification for adopting the external state approach.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/accessing-the-domain/wlst/",
	"title": "Use WLST",
	"tags": [],
	"description": "You can use the WebLogic Scripting Tool (WLST) to manage a domain running in Kubernetes.",
	"content": "You can use the WebLogic Scripting Tool (WLST) to manage a domain running in Kubernetes.\nTo give WLST access to a domain running in Kubernetes, you can:\n Use kubectl exec Use a NodePort Use port forwarding  Use kubectl exec You can use the kubectl exec command to start an interactive WLST session within a pod or to remotely run a WLST script on a pod. Typically, this is the preferred method.\nFor example, if a domainUID is sample-domain1, its Administration Server is named admin-server and is configured with default port 7001, and its pods are running in namespace sample-domain1-ns, then you can start an interactive WLST session this way:\n$ kubectl -n sample-domain1-ns exec -it sample-domain1-admin-server /bin/bash [oracle@sample-domain1-admin-server oracle]$ wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt; connect(\u0026#39;myusername\u0026#39;,\u0026#39;mypassword\u0026#39;,\u0026#39;t3://sample-domain1-admin-server:7001\u0026#39;) Connecting to t3://sample-domain1-admin-server:7001 with userid myusername ... Successfully connected to Admin Server \u0026#34;admin-server\u0026#34; that belongs to domain \u0026#34;base_domain\u0026#34;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/base_domain/serverConfig/\u0026gt; exit() Exiting WebLogic Scripting Tool. [oracle@sample-domain1-admin-server oracle]$ exit $ Use a NodePort If you are setting up WLST access through a NodePort and your external port is not going to be the same as the port number on the WebLogic Administration Server Pod, then see Enabling WLST access when local and remote ports do not match for an additional required setup step.\n A NodePort can expose a WebLogic T3 or administrative channel outside the Kubernetes cluster. For domain security considerations, see External network access security.\n You can configure an Administration Server to expose an externally accessible NodePort using these two steps:\n Configure a Network Access Point (custom channel) with the T3 protocol on the Administration Server. Expose this channel on a NodePort service using the domain.spec.adminServer.adminService.channels attribute.  Here is an example snippet of a WebLogic domain config.xml file for T3 channel T3Channel defined for an Administration Server named admin-server:\n\u0026lt;server\u0026gt; \u0026lt;name\u0026gt;admin-server\u0026lt;/name\u0026gt; \u0026lt;listen-port\u0026gt;7001\u0026lt;/listen-port\u0026gt; \u0026lt;listen-address/\u0026gt; \u0026lt;network-access-point\u0026gt; \u0026lt;name\u0026gt;T3Channel\u0026lt;/name\u0026gt; \u0026lt;protocol\u0026gt;t3\u0026lt;/protocol\u0026gt; \u0026lt;public-address\u0026gt;kubernetes001\u0026lt;/public-address\u0026gt; \u0026lt;listen-port\u0026gt;30012\u0026lt;/listen-port\u0026gt; \u0026lt;public-port\u0026gt;30012\u0026lt;/public-port\u0026gt; \u0026lt;/network-access-point\u0026gt; \u0026lt;/server\u0026gt; Here is an example snippet of a domain resource that sets up a NodePort for the channel:\nspec: adminServer: adminService: channels: - channelName: T3Channel nodePort: 30012 If you set the nodePort: value to 0, then Kubernetes will choose an open port for you.\nFor more details on exposing the T3 channel using a NodePort service, run the kubectl explain domain.spec.adminServer.adminService.channels command or see the domain resource schema and documentation.\nFor example, if a domainUID is domain1, the Administration Server name is admin-server, and you have set up a NodePort service on external port 30012 using the domain.spec.adminServer.adminService.channels attribute, then the service would be called:\ndomain1-admin-server-ext This service will be in the same namespace as the domain, and its external port number can be obtained by checking its nodePort field:\n$ kubectl get service domain1-admin-server-ext -n mynamespace -o jsonpath=\u0026#39;{.spec.ports[0].nodePort}\u0026#39; 30012 If the Kubernetes node machine address is kubernetes001, then WLST can connect to the WebLogic Server Administration Server pod through the NodePort as shown below:\n$ $ORACLE_HOME/oracle_common/common/bin/wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt; connect(\u0026#39;myusername\u0026#39;,\u0026#39;mypassword\u0026#39;,\u0026#39;t3://kubernetes001:30012\u0026#39;) Connecting to t3://kubernetes001:30012 with userid myusername ... Successfully connected to Admin Server \u0026#34;admin-server\u0026#34; that belongs to domain \u0026#34;base_domain\u0026#34;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/base_domain/serverConfig/\u0026gt; exit() Exiting WebLogic Scripting Tool. Use port forwarding One way to provide external access to WLST is to forward network traffic from a local port on your local machine to the administration port of an Administration Server Pod. See these instructions.\nPort forwarding can expose a WebLogic T3 or administrative channel outside the Kubernetes cluster. For domain security considerations, see External network access security.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/security/encryption/",
	"title": "Encryption",
	"tags": [],
	"description": "WebLogic domain encryption and the operator",
	"content": "Contents  Introspector encryption Encryption of Kubernetes Secrets Additional reading  Introspector encryption The operator has an introspection job that handles WebLogic domain encryption. The introspection job also addresses the use of Kubernetes Secrets with configuration overrides. For additional information on the configuration handling, see Configuration overrides.\nThe introspection job also creates a boot.properties file that is made available to the pods in the WebLogic domain. The credential used for the WebLogic domain is kept in a Kubernetes Secret which follows the naming pattern \u0026lt;domainUID\u0026gt;-weblogic-credentials, where \u0026lt;domainUID\u0026gt; is the unique identifier of the domain, for example, mydomain-weblogic-credentials.\nFor more information about the WebLogic credentials secret, see Secrets.\n Encryption of Kubernetes Secrets To better protect your credentials and private keys, the Kubernetes cluster should be set up with encryption. Please see the Kubernetes documentation about encryption at rest for secret data and using a KMS provider for data encryption.\n Additional reading  Encryption of values for WebLogic configuration overrides  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/domains/domain-home-in-image/",
	"title": "Domain home in image",
	"tags": [],
	"description": "Sample for creating a WebLogic domain home inside an image, and the domain resource YAML file for deploying the generated WebLogic domain.",
	"content": "The sample scripts demonstrate the creation of a WebLogic domain home in an image using WebLogic Image Tool (WIT). The sample scripts have the option of putting the WebLogic domain log, server logs, server output files, and the Node Manager logs on an existing Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC). The scripts also generate the domain resource YAML file, which can then be used by the scripts or used manually to start the Kubernetes artifacts of the corresponding domain, including the WebLogic Server pods and services.\nPrerequisites Before you begin, read this document, Domain resource.\nThe following prerequisites must be met prior to running the create domain script:\n The WebLogic Image Tool requires that JAVA_HOME is set to a Java JDK version 8 or later. The operator requires an image with either Oracle WebLogic Server 12.2.1.3.0 with patch 29135930 applied, or Oracle WebLogic Server 12.2.1.4.0, or Oracle WebLogic Server 14.1.1.0.0. The existing WebLogic Server image, container-registry.oracle.com/middleware/weblogic:12.2.1.3, has all the necessary patches applied. For details on how to obtain or create the image, see WebLogic Server images. Create a Kubernetes Namespace for the domain unless you intend to use the default namespace. If logHomeOnPV is enabled, create the Kubernetes PersistentVolume where the log home will be hosted, and the Kubernetes PersistentVolumeClaim for the domain in the same Kubernetes Namespace. For samples to create a PV and PVC, see Create sample PV and PVC. Create a Kubernetes Secret for the WebLogic administrator credentials that contains the fields username and password, and make sure that the secret name matches the value specified for weblogicCredentialsSecretName; see Configuration parameters below. For example:  $ cd ./kubernetes/samples/scripts/create-weblogic-domain-credentials $ create-weblogic-credentials.sh -u \u0026lt;username\u0026gt; -p \u0026lt;password\u0026gt; -d domain1 -n default -s domain1-weblogic-credentials NOTE: Using this example, you would configure weblogicCredentialsSecretName to be domain1-weblogic-credentials.\nUse the script to create a domain The create-domain.sh script generates a new container image on each run with a new domain home and a different internal domain secret in it. To prevent having disparate images with different domain secrets in the same domain, we strongly recommend that a new domain uses a domainUID that is different from any of the active domains, or that you delete the existing Domain using the following command and wait until all the WebLogic Server instance Pods are terminated before you create a Domain with the same domainUID: $ kubectl delete domain [domainUID] -n [domainNamespace]\n The sample for creating domains is in this directory:\n$ cd kubernetes/samples/scripts/create-weblogic-domain/domain-home-in-image Make a copy of the create-domain-inputs.yaml file, update it with the correct values, and run the create script, pointing it at your inputs file and an output directory, along with user name and password for the WebLogic administrator:\n$ ./create-domain.sh \\  -u \u0026lt;username\u0026gt; \\  -p \u0026lt;password\u0026gt; \\  -i create-domain-inputs.yaml \\  -o /\u0026lt;path to output-directory\u0026gt;  The create-domain.sh script and its inputs file are for demonstration purposes only; its contents and the domain resource file that it generates for you might change without notice. In production, we strongly recommend that you use the WebLogic Image Tool and WebLogic Deploy Tooling (when applicable), and directly work with domain resource files instead.\n The script will perform the following steps:\n  Create a directory for the generated properties and Kubernetes YAML files for this domain if it does not already exist. The pathname is /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, its contents will be removed.\n  Create a properties file, domain.properties, in the directory that is created above. This properties file will be used to create a sample WebLogic Server domain. The domain.properties file will be removed upon successful completion of the script.\n  Download the latest WebLogic Deploy Tooling (WDT) and WebLogic Image Tool installer ZIP files to your /tmp/dhii-sample/tools directory. WIT is required to create your Domain in Image container images, and WDT is required if using wdt mode. Visit the GitHub WebLogic Deploy Tooling Releases and WebLogic Image Tool Releases web pages to determine the latest release version for each.\n  Set up the WebLogic Image Tool in the \u0026lt;toolsDir\u0026gt;/imagetool directory, where \u0026lt;toolsDir\u0026gt; is the directory specified in the toolsDir parameter in the inputs YAML file. Set the WIT cache store location to the \u0026lt;tools\u0026gt;/imagetool-cache directory and put a wdt_\u0026lt;WDT_VERSION\u0026gt; entry in the tool\u0026rsquo;s cache, which points to the path of the WDT ZIP file installer. For more information about the WIT cache, see the WIT Cache documentation.\n  If the optional -n option and an encryption key is provided, invoke the WDT Encrypt Model Tool in a container running the image specified in domainHomeImageBase parameter in your inputs file to encrypt the password properties in domain.properties file. Note that this password encryption step is skipped if the value of the mode parameter in the inputs YAML file is wlst because the feature is provided by WDT.\n  Invoke the WebLogic Image Tool to create a new WebLogic Server domain based on the WebLogic image specified in the domainHomeImageBase parameter from your inputs file. The new WebLogic Server domain is created using one of the following options based on the value of the mode parameter in the inputs YAML file:\n If the value of the mode parameter is wdt, the WDT model specified in the createDomainWdtModel parameter and the WDT variables in domain.properties file are used by the WebLogic Image Tool to create the new WebLogic Server domain. If the value of the mode parameter is wlst, the offline WLST script specified in the createDomainWlstScript parameter is run to create the new WebLogic Server domain.    The generated image is tagged with the image parameter provided in your inputs file.\nOracle strongly recommends storing the image containing the domain home as private in the registry (for example, Oracle Cloud Infrastructure Registry, GitHub Container Registry, and such) because this image contains sensitive information about the domain, including keys and credentials that are used to access external resources (for example, the data source password). For more information, see WebLogic domain in image protection.\n   Create a Kubernetes domain resource YAML file, domain.yaml, in the directory that is created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command.\n  $ kubectl apply -f /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml As a convenience, using the -e option, the script can optionally create the domain object, which in turn results in the creation of the corresponding WebLogic Server pods and services. This option should be used in a single node Kubernetes cluster only.\nFor a multi-node Kubernetes cluster, make sure that the generated image is available on all nodes before creating the domain resource YAML file using the kubectl apply -f command.\nThe usage of the create script is as follows:\n$ sh create-domain.sh -h usage: create-domain.sh -o dir -i file -u username -p password [-n encryption-key] [-e] [-v] [-h] -i Parameter inputs file, must be specified. -o Output directory for the generated properties and YAML files, must be specified. -u WebLogic administrator user name for the WebLogic domain. -p WebLogic administrator Password for the WebLogic domain. -e Also create the resources in the generated YAML files, optional. -v Validate the existence of persistentVolumeClaim, optional. -n Encryption key for encrypting passwords in the WDT model and properties files, optional. -h Help If you copy the sample scripts to a different location, make sure that you copy everything in the \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts directory together into the target directory, maintaining the original directory hierarchy.\nThe default domain created by the script has the following characteristics:\n An Administration Server named admin-server listening on port 7001. A dynamic cluster named cluster-1 of size 5. Two Managed Servers, named managed-server1 and managed-server2, listening on port 8001. No applications deployed. A T3 channel.  If you run the sample from a machine that is remote to the Kubernetes cluster, and you need to push the new image to a registry that is local to the cluster, you need to do the following (also, see the image property in the Configuration parameters table.):\n Set the image property in the inputs file to the target image name (including the registry hostname, port, and the tag, if needed). If you want Kubernetes to pull the image from a private registry, create a Kubernetes Secret to hold your credentials and set the imagePullSecretName property in the inputs file to the name of the secret. The Kubernetes Secret must be in the same namespace where the domain will be running. For more information, see WebLogic domain in image protection.\n  Run the create-domain.sh script without the -e option. Push the image to the target registry. Run the following command to create the domain:  $ kubectl apply -f /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml The domain creation inputs can be customized by editing create-domain-inputs.yaml.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Definition Default     sslEnabled Boolean indicating whether to enable SSL for each WebLogic Server instance. false   adminPort Port number of the Administration Server inside the Kubernetes cluster. 7001   adminServerSSLPort SSL port number of the Administration Server inside the Kubernetes cluster. 7002   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. admin-server   clusterName Name of the WebLogic cluster instance to generate for the domain. cluster-1   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainWdtModel WDT model YAML file that the create domain script uses to create a WebLogic domain when using wdt mode. This value is ignored when the mode is set to wlst. wdt/wdt_model_dynamic.yaml   createDomainWlstScript WLST script that the create domain script uses to create a WebLogic domain when using wlst mode. This value is ignored when the mode is set to wdt (which is the default mode). wlst/create-wls-domain.py   domainHome Domain home directory of the WebLogic domain to be created in the generated WebLogic Server image. /u01/oracle/user_projects/domains/\u0026lt;domainUID\u0026gt;   domainHomeImageBase Base WebLogic binary image used to build the WebLogic domain image. The operator requires either Oracle WebLogic Server 12.2.1.3.0 with patch 29135930 applied, or Oracle WebLogic Server 12.2.1.4.0, or Oracle WebLogic Server 14.1.1.0.0. The existing WebLogic Server image, container-registry.oracle.com/middleware/weblogic:12.2.1.3, has all the necessary patches applied. For details on how to obtain or create the image, see WebLogic Server images. container-registry.oracle.com/middleware/weblogic:12.2.1.3   domainPVMountPath Mount path of the domain persistent volume. This parameter is required if logHomeOnPV is true. Otherwise, it is ignored. /shared   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Domain. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes Service name. domain1   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   httpAccessLogInLogHome Boolean indicating if server HTTP access log files should be written to the same directory as logHome if logHomeOnPV is true. Otherwise, server HTTP access log files will be written to the directory specified in the WebLogic domain home configuration. true   image WebLogic Server image that the operator uses to start the domain. The create domain scripts generate a WebLogic Server image with a domain home in it. By default, the scripts tag the generated WebLogic Server image as domain-home-in-image, and use it plus the tag that is obtained from the domainHomeImageBase to set the image element in the generated domain resource YAML file. If this property is set, the create domain scripts will use the value specified, instead of the default value, to tag the generated image and set the image in the domain resource YAML file. A unique value is required for each domain that is created using the scripts. If you are running the sample scripts from a machine that is remote to the Kubernetes cluster where the domain is going to be running, you need to set this property to the image name that is intended to be used in a registry local to that Kubernetes cluster. You also need to push the image to that registry before starting the domain using the kubectl create -f or kubectl apply -f command. domain-home-in-image:\u0026lt;tag from domainHomeImageBase\u0026gt;   imagePullPolicy WebLogic Server image pull policy. Legal values are IfNotPresent, Always, or Never. IfNotPresent   imagePullSecretName Name of the Kubernetes Secret to access the container registry to pull the WebLogic Server image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server .out int the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to initially start for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). If sslEnabled is set to true and the WebLogic demo certificate is used, add -Dweblogic.security.SSL.ignoreHostnameVerification=true to allow the managed servers to connect to the Administration Server while booting up. The WebLogic generated demo certificate in this environment typically contains a host name that is different from the runtime container\u0026rsquo;s host name. -Dweblogic.StdoutDebugEnabled=false   logHomeOnPV Specifies whether the log home is stored on the persistent volume. If set to true, then you must specify the logHome, persistentVolumeClaimName, and domainPVMountPath parameters. false   logHome The in-pod location for domain log, server logs, server out, Node Manager log, introspector out, and server HTTP access log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. This parameter is required if logHomeOnPV is true. Otherwise, it is ignored. /shared/logs/domain1   managedServerNameBase Base string used to generate Managed Server names. managed-server   managedServerPort Port number for each Managed Server. 8001   managedServerSSLPort SSL port number for each Managed Server. 8002   mode Whether to use the WDT model specified in createDomainWdtModel or the offline WLST script specified in createDomainWlstScript to create a WebLogic domain. Legal values are wdt or wlst. wdt   namespace Kubernetes Namespace in which to create the domain. default   persistentVolumeClaimName Name of the persistent volume claim. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. This parameter is required if logHomeOnPV is true. Otherwise, it is ignored. domain1-weblogic-sample-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the network access point. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only, in a single server (all-in-one) Kubernetes Deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster.   weblogicCredentialsSecretName Name of the Kubernetes Secret for the Administration Server user name and password. domain1-weblogic-credentials   serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.   toolsDir The directory where WebLogic Deploy Tool and WebLogic Image Tool are installed. The script will install these tools to this directory if they are not already installed. /tmp/dhii-sample/tools   wdtVersion Version of the WebLogic Deploy Tool to be installed by the script. This can be a specific version, such as 1.9.10, or LATEST. LATEST   witVersion Version of the WebLogic Image Tool to be installed by the script. This can be a specific version, such as 1.9.10, or LATEST. LATEST    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the inputs YAML file. Those properties include the adminServerName, clusterName, and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes Service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create a WebLogic domain home and associated Kubernetes resources for a domain that has only one cluster. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. Also, the generated domain resource YAML file can be modified to cover more use cases.\nVerify the results The create script will verify that the domain was created, and will report failure if there was any error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nNote that the example results below use the default Kubernetes Namespace. If you are using a different namespace, you need to replace NAMESPACE in the example kubectl commands with the actual Kubernetes Namespace.\nGenerated YAML files with the default inputs The content of the generated domain.yaml:\n# Copyright (c) 2017, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026#34;weblogic.oracle/v8\u0026#34; kind: Domain metadata: name: domain1 namespace: default labels: weblogic.domainUID: domain1 spec: # The WebLogic Domain Home domainHome: /u01/oracle/user_projects/domains/domain1 # Set domain home type to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: Image # The WebLogic Server image that the operator uses to start the domain image: \u0026#34;domain-home-in-image:12.2.1.4\u0026#34; # imagePullPolicy defaults to \u0026#34;Always\u0026#34; if image version is :latest imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: domain1-weblogic-credentials # Whether to include the server out file into the pod\u0026#39;s stdout, default is true includeServerOutInPodLog: true # Whether to enable log home # logHomeEnabled: false # The in-pod location for domain log, server logs, server out, introspector out, and Node Manager log files # logHome: /shared/logs/domain1 # serverStartPolicy legal values are \u0026#34;NEVER\u0026#34;, \u0026#34;IF_NEEDED\u0026#34;, or \u0026#34;ADMIN_ONLY\u0026#34; # This determines which WebLogic Servers the operator will start up when it discovers this Domain # - \u0026#34;NEVER\u0026#34; will not start any server in the domain # - \u0026#34;ADMIN_ONLY\u0026#34; will start up only the administration server (no managed servers will be started) # - \u0026#34;IF_NEEDED\u0026#34; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026#34;IF_NEEDED\u0026#34; serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false\u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-Djava.security.egd=file:/dev/./urandom \u0026#34; # volumes: # - name: weblogic-domain-storage-volume # persistentVolumeClaim: # claimName: domain1-weblogic-sample-pvc # volumeMounts: # - mountPath: /shared # name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026#34;RUNNING\u0026#34; or \u0026#34;ADMIN\u0026#34; # \u0026#34;RUNNING\u0026#34; means the listed server will be started up to \u0026#34;RUNNING\u0026#34; mode # \u0026#34;ADMIN\u0026#34; means the listed server will be start up to \u0026#34;ADMIN\u0026#34; mode serverStartState: \u0026#34;RUNNING\u0026#34; # adminService: # channels: # The Admin Server\u0026#39;s NodePort # - channelName: default # nodePort: 30701 # Uncomment to export the T3Channel as a service # - channelName: T3Channel # clusters is used to configure the desired behavior for starting member servers of a cluster. # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: cluster-1 serverStartState: \u0026#34;RUNNING\u0026#34; replicas: 2 # The number of managed servers to start for unlisted clusters # replicas: 1 Verify the domain To confirm that the domain was created, use this command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\nHere is an example of the output of this command:\n$ kubectl describe domain domain1 Name: domain1 Namespace: default Labels: weblogic.domainUID=domain1 Annotations: \u0026lt;none\u0026gt; API Version: weblogic.oracle/v2 Kind: Domain Metadata: Cluster Name: Creation Timestamp: 2019-01-10T14:29:37Z Generation: 1 Resource Version: 3698533 Self Link: /apis/weblogic.oracle/v2/namespaces/default/domains/domain1 UID: 28655979-14e4-11e9-b751-fa163e855ac8 Spec: Admin Server: Server Pod: Annotations: Container Security Context: Env: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Clusters: Cluster Name: cluster-1 Cluster Service: Annotations: Labels: Replicas: 2 Server Pod: Annotations: Container Security Context: Env: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Domain Home: /u01/oracle/user_projects/domains/domain1 Domain Home In Image: true Image: domain-home-in-image:12.2.1.4 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Managed Servers: Server Pod: Annotations: Container Security Context: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Xms64m -Xmx256m Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: domain1-weblogic-credentials Status: Conditions: Last Transition Time: 2019-01-10T14:31:10.681Z Reason: ServersReady Status: True Type: Available Servers: Health: Activation Time: 2019-01-10T14:30:47.432Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: admin-server State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-01-10T14:32:01.467Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: managed-server1 State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-01-10T14:32:04.532Z Overall Health: ok Subsystems: Node Name: slc16ffk Server Name: managed-server2 State: RUNNING Start Time: 2019-01-10T14:29:37.455Z Events: \u0026lt;none\u0026gt; In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued very soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. The operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Use the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE domain1-admin-server 1/1 Running 0 30m domain1-managed-server1 1/1 Running 0 29m domain1-managed-server2 1/1 Running 0 29m Verify the services Use the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command:\n$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE domain1-admin-server ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 32m domain1-cluster-cluster-1 ClusterIP 10.99.151.142 \u0026lt;none\u0026gt; 8001/TCP 31m domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 31m domain1-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 22m Delete the domain The generated YAML file in the /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt; directory can be used to delete the Kubernetes resource. Use the following command to delete the domain:\n$ kubectl delete -f domain.yaml Delete the generated image. When no longer needed, delete the generated image. If the image is in a local repository, use the following command to delete an image tagged with domain-home-in-image:12.2.1.4:\n$ docker rmi domain-home-in-image:12.2.1.4 Delete the tools directory. When no longer needed, delete the directory where WebLogic Deploy Tool and WebLogic Image Tool are installed. By default, they are installed under /tmp/dhii-sample/tools directory.\n$ rm -rf /tmp/dhii-sample/tools/ "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/domains/",
	"title": "Domains",
	"tags": [],
	"description": "These samples show various choices for working with domains.",
	"content": "These samples show various choices for working with domains.\n Manually  Sample for creating the domain custom resource manually.\n Domain home on a PV  Sample for creating a WebLogic domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated WebLogic domain.\n Domain home in image  Sample for creating a WebLogic domain home inside an image, and the domain resource YAML file for deploying the generated WebLogic domain.\n Model in image  Sample for supplying a WebLogic Deploy Tooling (WDT) model that the operator expands into a full domain home during runtime.\n FMW Infrastructure domain on a PV  Sample for creating an FMW Infrastructure domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated WebLogic domain.\n FMW Infrastructure domain home in image  Sample for creating an FMW Infrastructure domain home inside an image, and the domain resource YAML file for deploying the generated WebLogic domain.\n Delete resources associated with the domain  Delete the Kubernetes resources associated with the domain created while executing the samples.\n Domain lifecycle operations  Start and stop Managed Servers, clusters, and domains.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/domains/model-in-image/update1/",
	"title": "Update 1",
	"tags": [],
	"description": "",
	"content": "This use case demonstrates dynamically adding a data source to your running domain by updating your model and rolling your domain. It demonstrates several features of WDT and Model in Image:\n The syntax used for updating a model is the same syntax you use for creating the original model. A domain\u0026rsquo;s model can be updated dynamically by supplying a model update in a file in a Kubernetes ConfigMap. Model updates can be as simple as changing the value of a single attribute, or more complex, such as adding a JMS Server.  For a detailed discussion of model updates, see Runtime Updates in the Model in Image user guide.\nThe operator does not support all possible dynamic model updates. For model update limitations, consult Runtime Updates in the Model in Image user docs, and carefully test any model update before attempting a dynamic update in production.\n Here are the steps:\n  Ensure that you have a running domain.\nMake sure you have deployed the domain from the Initial use case.\n  Create a data source model YAML file.\nCreate a WDT model snippet for a data source (or use the example provided). Make sure that its target is set to cluster-1, and that its initial capacity is set to 0.\nThe reason for the latter is to prevent the data source from causing a WebLogic Server startup failure if it can\u0026rsquo;t find the database, which would be likely to happen because you haven\u0026rsquo;t deployed one (unless you\u0026rsquo;re using the JRF path through the sample).\nHere\u0026rsquo;s an example data source model configuration that meets these criteria:\nresources: JDBCSystemResource: mynewdatasource: Target: \u0026#39;cluster-1\u0026#39; JdbcResource: JDBCDataSourceParams: JNDIName: [ jdbc/mydatasource1, jdbc/mydatasource2 ] GlobalTransactionsProtocol: TwoPhaseCommit JDBCDriverParams: DriverName: oracle.jdbc.xa.client.OracleXADataSource URL: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-datasource-secret:url@@\u0026#39; PasswordEncrypted: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-datasource-secret:password@@\u0026#39; Properties: user: Value: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-datasource-secret:user@@\u0026#39; oracle.net.CONNECT_TIMEOUT: Value: 5000 oracle.jdbc.ReadTimeout: Value: 30000 JDBCConnectionPoolParams: InitialCapacity: 0 MaxCapacity: \u0026#39;@@SECRET:@@ENV:DOMAIN_UID@@-datasource-secret:max-capacity@@\u0026#39; TestTableName: SQL ISVALID TestConnectionsOnReserve: true Place the above model snippet in a file named /tmp/mii-sample/mydatasource.yaml and then use it in the later step where you deploy the model ConfigMap, or alternatively, use the same data source that\u0026rsquo;s provided in /tmp/mii-sample/model-configmaps/datasource/model.20.datasource.yaml.\n  Create the data source secret.\nThe data source references a new secret that needs to be created. Run the following commands to create the secret:\n$ kubectl -n sample-domain1-ns create secret generic \\  sample-domain1-datasource-secret \\  --from-literal=\u0026#39;user=sys as sysdba\u0026#39; \\  --from-literal=\u0026#39;password=incorrect_password\u0026#39; \\  --from-literal=\u0026#39;max-capacity=1\u0026#39; \\  --from-literal=\u0026#39;url=jdbc:oracle:thin:@oracle-db.default.svc.cluster.local:1521/devpdb.k8s\u0026#39; $ kubectl -n sample-domain1-ns label secret \\  sample-domain1-datasource-secret \\  weblogic.domainUID=sample-domain1 We deliberately specify an incorrect password and a low maximum pool capacity because we will demonstrate dynamically correcting the data source attributes in the Update 4 use case without requiring rolling the domain.\nYou name and label secrets using their associated domain UID for two reasons:\n To make it obvious which secret belongs to which domains. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all the resources associated with a domain.    Create a ConfigMap with the WDT model that contains the data source definition.\nRun the following commands:\n$ kubectl -n sample-domain1-ns create configmap sample-domain1-wdt-config-map \\  --from-file=/tmp/mii-sample/model-configmaps/datasource $ kubectl -n sample-domain1-ns label configmap sample-domain1-wdt-config-map \\  weblogic.domainUID=sample-domain1  If you\u0026rsquo;ve created your own data source file, then substitute the file name in the --from-file= parameter (we suggested /tmp/mii-sample/mydatasource.yaml earlier). Note that the -from-file= parameter can reference a single file, in which case it puts the designated file in the ConfigMap, or it can reference a directory, in which case it populates the ConfigMap with all of the files in the designated directory.  You name and label the ConfigMap using its associated domain UID for two reasons:\n To make it obvious which ConfigMap belong to which domains. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all resources associated with a domain.    Update your Domain YAML file to refer to the ConfigMap and Secret.\n  Option 1: Update a copy of your Domain YAML file from the Initial use case.\n  In the Initial use case, we suggested creating a Domain YAML file named /tmp/mii-sample/mii-initial.yaml or using the /tmp/mii-sample/domain-resources/WLS/mii-initial-d1-WLS-v1.yaml file that is supplied with the sample.\n  We suggest copying the original Domain YAML file and naming the copy /tmp/mii-sample/mii-update1.yaml before making any changes.\n  Working on a copy is not strictly necessary, but it helps keep track of your work for the different use cases in this sample and provides you a backup of your previous work.\n    Add the secret to its spec.configuration.secrets stanza:\nspec: ... configuration: ... secrets: - sample-domain1-datasource-secret (Leave any existing secrets in place.)\n  Change its spec.configuration.model.configMap to look like the following:\nspec: ... configuration: ... model: ... configMap: sample-domain1-wdt-config-map   Apply your changed Domain YAML file:\n Note: Before you deploy the domain custom resource, determine if you have Kubernetes cluster worker nodes that are remote to your local machine. If so, then you need to put the Domain YAML file\u0026rsquo;s image in a location that these nodes can access and you may also need to modify your Domain YAML file to reference the new location. See Ensuring your Kubernetes cluster can access images.\n $ kubectl apply -f /tmp/mii-sample/mii-update1.yaml     Option 2: Use the updated Domain YAML file that is supplied with the sample:\n Note: Before you deploy the domain custom resource, determine if you have Kubernetes cluster worker nodes that are remote to your local machine. If so, then you need to put the Domain YAML file\u0026rsquo;s image in a location that these nodes can access and you may also need to modify your Domain YAML file to reference the new location. See Ensuring your Kubernetes cluster can access images.\n $ kubectl apply -f /tmp/miisample/domain-resources/WLS/mii-update1-d1-WLS-v1-ds.yaml     Restart (\u0026lsquo;roll\u0026rsquo;) the domain.\nNow that the data source is deployed in a ConfigMap and its secret is also deployed, and you have applied an updated Domain YAML file with its spec.configuration.model.configMap and spec.configuration.secrets referencing the ConfigMap and secret, tell the operator to roll the domain.\nWhen a model domain restarts, it will rerun its introspector job in order to regenerate its configuration, and it will also pass the configuration changes found by the introspector to each restarted server. One way to cause a running domain to restart is to change the domain\u0026rsquo;s spec.restartVersion. To do this:\n  Option 1: Edit your domain custom resource.\n Call kubectl -n sample-domain1-ns edit domain sample-domain1. Edit the value of the spec.restartVersion field and save.  The field is a string; typically, you use a number in this field and increment it with each restart.      Option 2: Dynamically change your domain using kubectl patch.\n  To get the current restartVersion call:\n$ kubectl -n sample-domain1-ns get domain sample-domain1 \u0026#39;-o=jsonpath={.spec.restartVersion}\u0026#39;   Choose a new restart version that\u0026rsquo;s different from the current restart version.\n The field is a string; typically, you use a number in this field and increment it with each restart.    Use kubectl patch to set the new value. For example, assuming the new restart version is 2:\n$ kubectl -n sample-domain1-ns patch domain sample-domain1 --type=json \u0026#39;-p=[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/restartVersion\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2\u0026#34; }]\u0026#39;     Option 3: Use the sample helper script.\n Call /tmp/mii-sample/utils/patch-restart-version.sh -n sample-domain1-ns -d sample-domain1. This will perform the same kubectl get and kubectl patch commands as Option 2.      Wait for the roll to complete.\nNow that you\u0026rsquo;ve started a domain roll, you\u0026rsquo;ll need to wait for it to complete if you want to verify that the data source was deployed.\n  One way to do this is to call kubectl get pods -n sample-domain1-ns --watch and wait for the pods to cycle back to their ready state.\n  Alternatively, you can run /tmp/mii-sample/utils/wl-pod-wait.sh -p 3. This is a utility script that provides useful information about a domain\u0026rsquo;s pods and waits for them to reach a ready state, reach their target restartVersion, and reach their target image before exiting.\n  Click here to display the `wl-pod-wait.sh` usage.   $ ./wl-pod-wait.sh -?  Usage: wl-pod-wait.sh [-n mynamespace] [-d mydomainuid] \\ [-p expected_pod_count] \\ [-t timeout_secs] \\ [-q] Exits non-zero if 'timeout_secs' is reached before 'pod_count' is reached. Parameters: -d \u0026lt;domain_uid\u0026gt; : Defaults to 'sample-domain1'. -n \u0026lt;namespace\u0026gt; : Defaults to 'sample-domain1-ns'. -p 0 : Wait until there are no running WebLogic Server pods for a domain. The default. -p \u0026lt;pod_count\u0026gt; : Wait until all of the following are true for exactly 'pod_count' WebLogic Server pods in the domain: - ready - same 'weblogic.domainRestartVersion' label value as the domain resource's 'spec.restartVersion' - same 'weblogic.introspectVersion' label value as the domain resource's 'spec.introspectVersion' - same image as the the domain resource's image -t \u0026lt;timeout\u0026gt; : Timeout in seconds. Defaults to '1000'. -q : Quiet mode. Show only a count of wl pods that have reached the desired criteria. -? : This help.      Click here to view sample output from `wl-pod-wait.sh` that shows a rolling domain.    @@ [2020-04-30T13:53:19][seconds=0] Info: Waiting up to 1000 seconds for exactly '3' WebLogic Server pods to reach the following criteria: @@ [2020-04-30T13:53:19][seconds=0] Info: ready='true' @@ [2020-04-30T13:53:19][seconds=0] Info: image='model-in-image:WLS-v1' @@ [2020-04-30T13:53:19][seconds=0] Info: domainRestartVersion='2' @@ [2020-04-30T13:53:19][seconds=0] Info: namespace='sample-domain1-ns' @@ [2020-04-30T13:53:19][seconds=0] Info: domainUID='sample-domain1' @@ [2020-04-30T13:53:19][seconds=0] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:53:19][seconds=0] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-introspector-wlkpr' '' '' '' 'Pending' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:53:20][seconds=1] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:53:20][seconds=1] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-introspector-wlkpr' '' '' '' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:54:18][seconds=59] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:54:18][seconds=59] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------------------- ------- ----------------------- ------ ----------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-introspector-wlkpr' '' '' '' 'Succeeded' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:54:19][seconds=60] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:54:19][seconds=60] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:54:31][seconds=72] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:54:31][seconds=72] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v1' 'false' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:54:40][seconds=81] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:54:40][seconds=81] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:54:52][seconds=93] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:54:52][seconds=93] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:54:58][seconds=99] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:54:58][seconds=99] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'false' 'Pending' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:55:00][seconds=101] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:55:00][seconds=101] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'false' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:55:12][seconds=113] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:55:12][seconds=113] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'false' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:55:24][seconds=125] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:55:24][seconds=125] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'false' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:55:33][seconds=134] Info: '1' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:55:33][seconds=134] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:55:34][seconds=135] Info: '1' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:55:34][seconds=135] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v1' 'false' 'Pending' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:55:40][seconds=141] Info: '1' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:55:40][seconds=141] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:55:44][seconds=145] Info: '1' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:55:44][seconds=145] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'false' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:56:25][seconds=186] Info: '2' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:56:25][seconds=186] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:56:26][seconds=187] Info: '2' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:56:26][seconds=187] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v1' 'false' 'Pending' @@ [2020-04-30T13:56:30][seconds=191] Info: '2' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:56:30][seconds=191] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:56:34][seconds=195] Info: '2' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:56:34][seconds=195] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '2' 'model-in-image:WLS-v1' 'false' 'Pending' @@ [2020-04-30T13:57:09][seconds=230] Info: '3' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-04-30T13:57:09][seconds=230] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '2' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-04-30T13:57:09][seconds=230] Info: Success!        After your domain is running, you can call the sample web application to determine if the data source was deployed.\nSend a web application request to the ingress controller:\n$ curl -s -S -m 10 -H \u0026#39;host: sample-domain1-cluster-cluster-1.mii-sample.org\u0026#39; \\  http://localhost:30305/myapp_war/index.jsp Or, if Traefik is unavailable and your Administration Server pod is running, you can run kubectl exec:\n$ kubectl exec -n sample-domain1-ns sample-domain1-admin-server -- bash -c \\  \u0026#34;curl -s -S -m 10 http://sample-domain1-cluster-cluster-1:8001/myapp_war/index.jsp\u0026#34; You will see something like the following:\n\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v1\u0026#39; of the mii-sample JSP web-app. Welcome to WebLogic Server \u0026#39;managed-server1\u0026#39;! domain UID = \u0026#39;sample-domain1\u0026#39; domain name = \u0026#39;domain1\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found min threads constraint runtime named \u0026#39;SampleMinThreads\u0026#39; with configured count: 1 Found max threads constraint runtime named \u0026#39;SampleMaxThreads\u0026#39; with configured count: 10 Found 1 local data source: Datasource \u0026#39;mynewdatasource\u0026#39;: State=\u0026#39;Running\u0026#39;, testPool=\u0026#39;Failed\u0026#39; ---TestPool Failure Reason--- NOTE: Ignore \u0026#39;mynewdatasource\u0026#39; failures until the sample\u0026#39;s Update 4 use case. --- ... ... invalid host/username/password ... ----------------------------- ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;   A TestPool Failure is expected because we will demonstrate dynamically correcting the data source attributes in Update 4.\nIf you see an error other than the expected TestPool Failure, then consult Debugging in the Model in Image user guide.\nIf you plan to run the Update 3 or Update 4 use case, then leave your domain running.\nTo remove the resources you have created in the samples, see Cleanup.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/developerguide/building/",
	"title": "Building",
	"tags": [],
	"description": "",
	"content": "The operator is built using Apache Maven. The build machine will also need to have Docker installed.\nTo build the operator, issue the following command in the project directory:\n$ mvn clean install This will compile the source files, build JAR files containing the compiled classes and libraries needed to run the operator, and will also execute all of the unit tests.\nContributions must conform to coding and formatting standards.\nBuilding the operator container image These commands should be executed in the project root directory:\n$ ./buildDockerImage.sh -t weblogic-kubernetes-operator:some-tag Replace \u0026lt;version\u0026gt; with the version of the project found in the pom.xml file in the project root directory.\nWe recommend that you use a tag other than latest, to make it easy to distinguish your image. In the example above, the tag could be the GitHub ID of the developer.\nRunning the operator from an IDE The operator can be run from an IDE, which is useful for debugging. In order to do so, the machine running the IDE must be configured with a Kubernetes configuration file in ~/.kube/config or in a location pointed to by the KUBECONFIG environment variable.\nConfigure the IDE to run the class oracle.kubernetes.operator.Main.\nYou may need to create a directory called /operator on your machine. Please be aware that the operator code is targeted to Linux, and although it will run fine on macOS, it will probably not run on other operating systems. If you develop on another operating system, you should deploy the operator to a Kubernetes cluster and use remote debugging instead.\nRunning the operator in a Kubernetes cluster If you\u0026rsquo;re not running Kubernetes on your development machine, you\u0026rsquo;ll need to make the container image available to a registry visible to your Kubernetes cluster. Either docker push the image to a private registry or upload your image to a machine running Docker and Kubernetes as follows:\nOn your build machine:\n$ docker save weblogic-kubernetes-operator:some-tag \u0026gt; operator.tar $ scp operator.tar YOUR_USER@YOUR_SERVER:/some/path/operator.tar On the Kubernetes server:\n$ docker load \u0026lt; /some/path/operator.tar Use the Helm charts to install the operator.\nIf the operator\u0026rsquo;s behavior or pod log is insufficient to diagnose and resolve failures, then you can connect a Java debugger to the operator using the debugging options.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-operators/the-rest-api/",
	"title": "Use the operator&#39;s REST services",
	"tags": [],
	"description": "Use the operator&#39;s REST services.",
	"content": "The operator provides a REST server which you can use to get a list of WebLogic domains and clusters and to initiate scaling operations. Swagger documentation for the REST API is available here.\nYou can access most of the REST services using GET, for example:\n To obtain a list of domains, send a GET request to the URL /operator/latest/domains To obtain a list of clusters in a domain, send a GET request to the URL /operator/latest/domains/\u0026lt;domainUID\u0026gt;/clusters  All of the REST services require authentication. Callers must pass in a valid token header and a CA certificate file. In previous operator versions, the operator performed authentication and authorization checks using the Kubernetes token review and subject access review APIs, and then updated the Domain resource using the operator\u0026rsquo;s privileges. Now, by default, the operator will use the caller\u0026rsquo;s bearer token to perform the underlying update to the Domain resource using the caller\u0026rsquo;s privileges and thus delegating authentication and authorization checks directly to the Kubernetes API Server (see REST interface configuration).\nWhen using the operator\u0026rsquo;s REST services to scale up or down a WebLogic cluster, you may need to grant patch access to the user or service account associated with the caller\u0026rsquo;s bearer token. This can be done with an RBAC ClusterRoleBinding between the user or service account and the ClusterRole that defines the permissions for the WebLogic domains resource.\n Callers should pass in the Accept:/application/json header.\nTo protect against Cross Site Request Forgery (CSRF) attacks, the operator REST API requires that you send in a X-Requested-By header when you invoke a REST endpoint that makes a change (for example, when you POST to the /scale endpoint). The value is an arbitrary name such as MyClient. For example, when using curl:\n$ curl ... -H X-Requested-By:MyClient ... -X POST .../scaling If you do not pass in the X-Requested-By header, then you\u0026rsquo;ll get a 400 (bad request) response without any details explaining why the request is bad. The X-Requested-By header is not needed for requests that only read, for example, when you GET any of the operator\u0026rsquo;s REST endpoints.\nBefore using the sample script below, you must:\n Update it to ensure it has the correct service account, namespaces, and such, and it points to the values.yaml that you used to install the operator (so that it can get the certificates). Add your operator\u0026rsquo;s certificate to your operating system\u0026rsquo;s trust store (see below). If you are using a self-signed certificate and your client is macOS, you may need to update the version of curl you have installed. The version of CURL that ships with macOS High Sierra (curl 7.54.0 (x86_64-apple-darwin17.0) libcurl/7.54.0 LibreSSL/2.0.20 zlib/1.2.11 nghttp2/1.24.0) has known issues with self-signed certificates. Oracle recommends curl 7.63.0 (x86_64-apple-darwin17.7.0) libcurl/7.63.0 SecureTransport zlib/1.2.11, which can be installed with brew install curl.  How to add your certificate to your operating system trust store For macOS, find the certificate in Finder, and double-click on it. This will add it to your keystore and open Keychain Access. Find the certificate in Keychain Access and double-click on it to open the details. Open the \u0026ldquo;Trust\u0026rdquo; pull-down menu and set the value of \u0026ldquo;When using this certificate\u0026rdquo; to \u0026ldquo;Always Trust\u0026rdquo;, then close the detail window. Enter your password when prompted.\nFor Oracle Linux, run the script below, once to copy the certificate into /tmp/operator.cert.pem, then run these commands to add the certificate to the trust store:\n$ sudo cp /tmp/operator.cert.pem /etc/pki/ca-trust/source/anchors/ $ sudo update-ca-trust enable; sudo update-ca-trust extract $ openssl x509 -noout -hash -in /tmp/operator.cert.pem $ sudo ln -s /etc/pki/ca-trust/source/anchors/operator.cert.pem /etc/pki/tls/certs/e242d2da.0 In the final command, the file name e242d2da.0 should be the output of the previous command plus the suffix .0.\nFor other operating systems, consult your operating system\u0026rsquo;s documentation (or Google).\nSample operator REST client script Here is a small, sample BASH script that may help to prepare the necessary token, certificates, and such, to call the operator\u0026rsquo;s REST services. Please read the important caveats above before using this script:\n#!/bin/bash KUBERNETES_SERVER=$1 URL_TAIL=$2 REST_PORT=`kubectl get services -n weblogic-operator -o jsonpath=\u0026#39;{.items[?(@.metadata.name == \u0026#34;external-weblogic-operator-svc\u0026#34;)].spec.ports[?(@.name == \u0026#34;rest\u0026#34;)].nodePort}\u0026#39;` REST_ADDR=\u0026#34;https://${KUBERNETES_SERVER}:${REST_PORT}\u0026#34; SECRET=`kubectl get serviceaccount weblogic-operator -n weblogic-operator -o jsonpath=\u0026#39;{.secrets[0].name}\u0026#39;` ENCODED_TOKEN=`kubectl get secret ${SECRET} -n weblogic-operator -o jsonpath=\u0026#39;{.data.token}\u0026#39;` TOKEN=`echo ${ENCODED_TOKEN} | base64 --decode` OPERATOR_CERT_DATA=`kubectl get secret -n weblogic-operator weblogic-operator-external-rest-identity -o jsonpath=\u0026#39;{.data.tls\\.crt}\u0026#39;` OPERATOR_CERT_FILE=\u0026#34;/tmp/operator.cert.pem\u0026#34; echo ${OPERATOR_CERT_DATA} | base64 --decode \u0026gt; ${OPERATOR_CERT_FILE} cat ${OPERATOR_CERT_FILE} echo \u0026#34;Ready to call operator REST APIs\u0026#34; STATUS_CODE=`curl \\  -v \\  --cacert ${OPERATOR_CERT_FILE} \\  -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\  -H Accept:application/json \\  -X GET ${REST_ADDR}/${URL_TAIL} \\  -o curl.out \\  --stderr curl.err \\  -w \u0026#34;%{http_code}\u0026#34;` cat curl.err cat curl.out | jq .  You can use the -k option to bypass the check to verify that the operator\u0026rsquo;s certificate is trusted (instead of curl --cacert), but this is insecure.\n To use this script, pass in the Kubernetes server address and then the URL you want to call. The script assumes jq is installed and uses it to format the response. This can be removed if desired. The script also prints out quite a bit of useful debugging information, in addition to the response. Here is an example of the output of this script:\n$ ./rest.sh kubernetes001 operator/latest/domains/domain1/clusters Ready to call operator REST APIs Note: Unnecessary use of -X or --request, GET is already inferred. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0* Trying 10.139.151.214... * TCP_NODELAY set * Connected to kubernetes001 (10.1.2.3) port 31001 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH * error setting certificate verify locations, continuing anyway: * CAfile: /tmp/operator.cert.pem CApath: none * TLSv1.2 (OUT), TLS handshake, Client hello (1): } [512 bytes data] * TLSv1.2 (IN), TLS handshake, Server hello (2): { [81 bytes data] * TLSv1.2 (IN), TLS handshake, Certificate (11): { [799 bytes data] * TLSv1.2 (IN), TLS handshake, Server key exchange (12): { [413 bytes data] * TLSv1.2 (IN), TLS handshake, Server finished (14): { [4 bytes data] * TLSv1.2 (OUT), TLS handshake, Client key exchange (16): } [150 bytes data] * TLSv1.2 (OUT), TLS change cipher, Client hello (1): } [1 bytes data] * TLSv1.2 (OUT), TLS handshake, Finished (20): } [16 bytes data] * TLSv1.2 (IN), TLS change cipher, Client hello (1): { [1 bytes data] * TLSv1.2 (IN), TLS handshake, Finished (20): { [16 bytes data] * SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256 * ALPN, server did not agree to a protocol * Server certificate: * subject: CN=weblogic-operator * start date: Jan 18 16:30:01 2018 GMT * expire date: Jan 16 16:30:01 2028 GMT * issuer: CN=weblogic-operator * SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway. \u0026gt; GET /operator/latest/domains/domain1/clusters HTTP/1.1 \u0026gt; Host: kubernetes001:31001 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJ3ZWJsb2dpYy1vcGVyYXRvciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQ (truncated) 1vcGVyYXRvcjp3ZWJsb2dpYy1vcGVyYXRvciJ9.NgaGR0NbzbJpVXguQDjRKyDBnNTqwgwPEXv3NjWwMcaf0OlN54apHubdrIx6KYz9ONGz-QeTLnoMChFY7oWA6CBfbvjt-GQX6JvdoJYxsQo1pt-E6sO2YvqTFE4EG-gpEDaiCE_OjZ_bBpJydhIiFReToA3-mxpDAUK2_rUfkWe5YEaLGMWoYQfXPAykzFiH4vqIi_tzzyzNnGxI2tUcBxNh3tzWFPGXKhzG18HswiwlFU5pe7XEYv4gJbvtV5tlGz7YdmH74Rc0dveV-54qHD_VDC5M7JZVh0ZDlyJMAmWe4YcdwNQQNGs91jqo1-JEM0Wj8iQSDE3cZj6MB0wrdg \u0026gt; Accept:application/json \u0026gt; 0 0 0 0 0 0 0 0 --:--:-- 0:00:01 --:--:-- 0\u0026lt; HTTP/1.1 200 OK \u0026lt; Content-Type: application/json \u0026lt; Content-Length: 463 \u0026lt; { [463 bytes data] 100 463 100 463 0 0 205 0 0:00:02 0:00:02 --:--:-- 205 * Connection #0 to host kubernetes001 left intact { \u0026#34;links\u0026#34;: [ { \u0026#34;rel\u0026#34;: \u0026#34;self\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;href\u0026#34;: \u0026#34;/operator/latest/domains/domain1/clusters\u0026#34; }, { \u0026#34;rel\u0026#34;: \u0026#34;canonical\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;href\u0026#34;: \u0026#34;/operator/latest/domains/domain1/clusters\u0026#34; }, { \u0026#34;rel\u0026#34;: \u0026#34;parent\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;href\u0026#34;: \u0026#34;/operator/latest/domains/domain1\u0026#34; } ], \u0026#34;items\u0026#34;: [ { \u0026#34;links\u0026#34;: [ { \u0026#34;rel\u0026#34;: \u0026#34;self\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;href\u0026#34;: \u0026#34;/operator/latest/domains/domain1/clusters/cluster-1\u0026#34; }, { \u0026#34;rel\u0026#34;: \u0026#34;canonical\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;href\u0026#34;: \u0026#34;/operator/latest/domains/domain1/clusters/cluster-1\u0026#34; } ], \u0026#34;cluster\u0026#34;: \u0026#34;cluster-1\u0026#34; } ] } "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/domain-lifecycle/scaling/",
	"title": "Scaling",
	"tags": [],
	"description": "The operator provides several ways to initiate scaling of WebLogic clusters.",
	"content": "WebLogic Server supports two types of clustering configurations, configured and dynamic. Configured clusters are created by defining each individual Managed Server instance. In dynamic clusters, the Managed Server configurations are generated from a single, shared template. With dynamic clusters, when additional server capacity is needed, new server instances can be added to the cluster without having to configure them individually. Also, unlike configured clusters, scaling up of dynamic clusters is not restricted to the set of servers defined in the cluster but can be increased based on runtime demands. For more information on how to create, configure, and use dynamic clusters in WebLogic Server, see Dynamic Clusters.\nFor more in-depth information on support for scaling WebLogic clusters in Kubernetes, see WebLogic Dynamic Clusters on Kubernetes.\nThe operator provides several ways to initiate scaling of WebLogic clusters, including:\n On-demand, updating the Domain directly (using kubectl). Calling the operator\u0026rsquo;s REST scale API, for example, from curl. Using a WLDF policy rule and script action to call the operator\u0026rsquo;s REST scale API. Using a Prometheus alert action to call the operator\u0026rsquo;s REST scale API.  On-demand, updating the Domain directly The easiest way to scale a WebLogic cluster in Kubernetes is to simply edit the replicas field of a Domain. This can be done by using kubectl. More specifically, you can modify the Domain directly by using the kubectl edit command. For example:\n$ kubectl edit domain domain1 -n [namespace] Here we are editing a Domain named domain1. The kubectl edit command will open the Domain definition in an editor and allow you to modify the replicas value directly. Once committed, the operator will be notified of the change and will immediately attempt to scale the corresponding cluster by reconciling the number of running pods/Managed Server instances with the replicas value specification.\nspec: ... clusters: - clusterName: cluster-1 replicas: 1 ... Alternatively, you can specify a default replicas value for all the clusters. If you do this, then you don\u0026rsquo;t need to list the cluster in the Domain (unless you want to customize another property of the cluster).\nspec: ... replicas: 1 ... In addition, see the helper scripts in the Domain lifecycle sample scripts section.\nCalling the operator\u0026rsquo;s REST scale API Scaling up or scaling down a WebLogic cluster provides increased reliability of customer applications as well as optimization of resource usage. In Kubernetes environments, scaling WebLogic clusters involves scaling the number of corresponding Pods in which Managed Server instances are running. Because the operator manages the life cycle of a WebLogic domain, the operator exposes a REST API that allows an authorized actor to request scaling of a WebLogic cluster.\nThe following URL format is used for describing the resources for scaling (up and down) a WebLogic cluster:\nhttp(s)://${OPERATOR_ENDPOINT}/operator/\u0026lt;version\u0026gt;/domains/\u0026lt;domainUID\u0026gt;/clusters/\u0026lt;clusterName\u0026gt;/scale For example:\nhttp(s)://${OPERATOR_ENDPOINT}/operator/v1/domains/domain1/clusters/cluster-1/scale In this URL format:\n OPERATOR_ENDPOINT is the host and port of the operator REST endpoint (internal or external). \u0026lt;version\u0026gt; denotes the version of the REST resource. \u0026lt;domainUID\u0026gt; is the unique identifier of the WebLogic domain. \u0026lt;clusterName\u0026gt; is the name of the WebLogic cluster to be scaled.  The /scale REST endpoint accepts an HTTP POST request and the request body supports the JSON \u0026quot;application/json\u0026quot; media type. The request body will be a simple name-value item named managedServerCount; for example:\n{ \u0026#34;managedServerCount\u0026#34;: 3 } The managedServerCount value designates the number of Managed Server instances to scale to. On a successful scaling request, the REST interface will return an HTTP response code of 204 (“No Content”).\nWhen you POST to the /scale REST endpoint, you must send the following headers:\n X-Requested-By request value. The value is an arbitrary name such as MyClient. Authorization: Bearer request value. The value of the Bearer token is the WebLogic domain service account token.  For example, when using curl:\n$ curl -v -k -H X-Requested-By:MyClient -H Content-Type:application/json -H Accept:application/json -H \u0026#34;Authorization:Bearer ...\u0026#34; -d \u0026#39;{ \u0026#34;managedServerCount\u0026#34;: 3 }\u0026#39; https://.../scaling If you omit the header, you\u0026rsquo;ll get a 400 (bad request) response. If you omit the Bearer Authentication header, then you\u0026rsquo;ll get a 401 (Unauthorized) response. If the service account or user associated with the Bearer token does not have permission to patch the WebLogic domain resource, then you\u0026rsquo;ll get a 403 (Forbidden) response.\nTo resolve a 403 (Forbidden) response, when calling the operator\u0026rsquo;s REST scaling API, you may need to add the patch request verb to the cluster role associated with the WebLogic domains resource. The example ClusterRole definition below grants get, list, patch and update access to the WebLogic domains resource\n kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: weblogic-domain-cluster-role rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;services/status\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;weblogic.oracle\u0026#34;] resources: [\u0026#34;domains\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;patch\u0026#34;, update\u0026#34;] --- Operator REST endpoints The WebLogic Kubernetes Operator can expose both an internal and external REST HTTPS endpoint. The internal REST endpoint is only accessible from within the Kubernetes cluster. The external REST endpoint is accessible from outside the Kubernetes cluster. The internal REST endpoint is enabled by default and thus always available, whereas the external REST endpoint is disabled by default and only exposed if explicitly configured. Detailed instructions for configuring the external REST endpoint are available here.\nRegardless of which endpoint is being invoked, the URL format for scaling is the same.\n What does the operator do in response to a scaling request? When the operator receives a scaling request, it will:\n Perform an authentication and authorization check to verify that the specified user is allowed to perform the specified operation on the specified resource. Validate that the specified domain, identified by domainUID, exists. Validate that the WebLogic cluster, identified by clusterName, exists. Verify that the specified WebLogic cluster has a sufficient number of configured servers or sufficient dynamic cluster size to satisfy the scaling request. Initiate scaling by setting the replicas field within the corresponding Domain, which can be done in either:  A cluster entry, if defined within its cluster list. At the domain level, if not defined in a cluster entry.    In response to a change to either replicas field, in the Domain, the operator will increase or decrease the number of Managed Server instance Pods to match the desired replica count.\nUsing a WLDF policy rule and script action to call the operator\u0026rsquo;s REST scale API The WebLogic Diagnostics Framework (WLDF) is a suite of services and APIs that collect and surface metrics that provide visibility into server and application performance. To support automatic scaling of WebLogic clusters in Kubernetes, WLDF provides the Policies and Actions component, which lets you write policy expressions for automatically executing scaling operations on a cluster. These policies monitor one or more types of WebLogic Server metrics, such as memory, idle threads, and CPU load. When the configured threshold in a policy is met, the policy is triggered, and the corresponding scaling action is executed. The WebLogic Kubernetes Operator project provides a shell script, scalingAction.sh, for use as a Script Action, which illustrates how to issue a request to the operator’s REST endpoint.\nConfigure automatic scaling of WebLogic clusters in Kubernetes with WLDF The following steps are provided as a guideline on how to configure a WLDF Policy and Script Action component for issuing scaling requests to the operator\u0026rsquo;s REST endpoint:\n  Copy the scalingAction.sh script to $DOMAIN_HOME/bin/scripts so that it\u0026rsquo;s accessible within the Administration Server pod. For more information, see Configuring Script Actions in Configuring and Using the Diagnostics Framework for Oracle WebLogic Server.\n  Configure a WLDF policy and action as part of a diagnostic module targeted to the Administration Server. For information about configuring the WLDF Policies and Actions component, see Configuring Policies and Actions in Configuring and Using the Diagnostics Framework for Oracle WebLogic Server.\na. Configure a WLDF policy with a rule expression for monitoring WebLogic Server metrics, such as memory, idle threads, and CPU load for example.\nb. Configure a WLDF script action and associate the scalingAction.sh script.\n  Important notes about the configuration properties for the Script Action:\nThe scalingAction.sh script requires access to the SSL certificate of the operator’s endpoint and this is provided through the environment variable INTERNAL_OPERATOR_CERT.\nThe operator’s SSL certificate can be found in the internalOperatorCert entry of the operator’s ConfigMap weblogic-operator-cm:\nFor example:\n#\u0026gt; kubectl describe configmap weblogic-operator-cm -n weblogic-operator ... Data ==== internalOperatorCert: ---- LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR3akNDQXFxZ0F3SUJBZ0lFRzhYT1N6QU... ... The scalingAction.sh script accepts a number of customizable parameters:\n  action - scaleUp or scaleDown (Required)\n  domain_uid - WebLogic domain unique identifier (Required)\n  cluster_name - WebLogic cluster name (Required)\n  kubernetes_master - Kubernetes master URL, default=https://kubernetes\n  Set this to https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT} when invoking scalingAction.sh from the Administration Server pod.\n   access_token - Service Account Bearer token for authentication and authorization for access to REST Resources\n  wls_domain_namespace - Kubernetes Namespace in which the WebLogic domain is defined, default=default\n  operator_service_name - WebLogic Kubernetes Operator Service name of the REST endpoint, default=internal-weblogic-operator-service\n  operator_service_account - Kubernetes Service Account name for the operator, default=weblogic-operator\n  operator_namespace – Namespace in which the operator is deployed, default=weblogic-operator\n  scaling_size – Incremental number of Managed Server instances by which to scale up or down, default=1\n  You can use any of the following tools to configure policies for diagnostic system modules:\n WebLogic Server Administration Console WLST REST JMX application  A more in-depth description and example on using WLDF\u0026rsquo;s Policies and Actions component for initiating scaling requests through the operator\u0026rsquo;s REST endpoint can be found in the blogs:\n Automatic Scaling of WebLogic Clusters on Kubernetes WebLogic Dynamic Clusters on Kubernetes  Create ClusterRoleBindings to allow a namespace user to query WLS Kubernetes cluster information The script scalingAction.sh, specified in the WLDF script action above, needs the appropriate RBAC permissions granted for the service account user (in the namespace in which the WebLogic domain is deployed) in order to query the Kubernetes API server for both configuration and runtime information of the Domain. The following is an example YAML file for creating the appropriate Kubernetes ClusterRole bindings:\nIn the example ClusterRoleBinding definition below, the WebLogic domain is deployed to a namespace weblogic-domain. Replace the namespace value with the name of the namespace in which the WebLogic domain is deployed in your Kubernetes environment.\n kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: weblogic-domain-cluster-role rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;services/status\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;weblogic.oracle\u0026#34;] resources: [\u0026#34;domains\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;patch\u0026#34;, update\u0026#34;] --- # # creating role-bindings for cluster role # kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: domain-cluster-rolebinding subjects: - kind: ServiceAccount name: default namespace: weblogic-domain apiGroup: \u0026#34;\u0026#34; roleRef: kind: ClusterRole name: weblogic-domain-cluster-role apiGroup: \u0026#34;rbac.authorization.k8s.io\u0026#34; --- # # creating role-bindings # kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: weblogic-domain-operator-rolebinding namespace: weblogic-operator subjects: - kind: ServiceAccount name: default namespace: weblogic-domain apiGroup: \u0026#34;\u0026#34; roleRef: kind: ClusterRole name: cluster-admin apiGroup: \u0026#34;rbac.authorization.k8s.io\u0026#34; --- Using a Prometheus alert action to call the operator\u0026rsquo;s REST scale API In addition to using the WebLogic Diagnostic Framework for automatic scaling of a dynamic cluster, you can use a third-party monitoring application like Prometheus. Please read the following blog for details about Using Prometheus to Automatically Scale WebLogic Clusters on Kubernetes.\nHelpful tips Debugging scalingAction.sh The scalingAction.sh script was designed to be executed within a container of the Administration Server Pod because the associated diagnostic module is targeted to the Administration Server.\nThe easiest way to verify and debug the scalingAction.sh script is to open a shell on the running Administration Server pod and execute the script on the command line.\nThe following example illustrates how to open a bash shell on a running Administration Server pod named domain1-admin-server and execute the scriptAction.sh script. It assumes that:\n The domain home is in /u01/oracle/user-projects/domains/domain1 (that is, the domain home is inside an image). The Dockerfile copied scalingAction.sh to /u01/oracle/user-projects/domains/domain1/bin/scripts/scalingAction.sh.  $ kubectl exec -it domain1-admin-server /bin/bash $ cd /u01/oracle/user-projects/domains/domain1/bin/scripts \u0026amp;\u0026amp; \\  ./scalingAction.sh A log, scalingAction.log, will be generated in the same directory in which the script was executed and can be examined for errors.\nExample on accessing the external REST endpoint The easiest way to test scaling using the external REST endpoint is to use a command-line tool like curl. Using curl to issue an HTTPS scale request requires these mandatory header properties:\n Bearer Authorization token SSL certificate for the operator\u0026rsquo;s external REST endpoint X-Requested-By header value  The following shell script is an example of how to issue a scaling request, with the necessary HTTP request header values, using curl. This example assumes the operator and Domain YAML file are configured with the following fields in Kubernetes:\n Operator properties:  externalRestEnabled: true externalRestHttpsPort: 31001 operator\u0026rsquo;s namespace: weblogic-operator operator\u0026rsquo;s hostname is the same as the host shell script is executed on.   Domain fields:  WebLogic cluster name: ApplicationCluster Domain UID: domain1    #!/bin/sh # Setup properties  ophost=`uname -n` opport=31001 #externalRestHttpsPort cluster=cluster-1 size=3 #New cluster size ns=weblogic-operator # Operator NameSpace sa=weblogic-operator # Operator ServiceAccount domainuid=domain1 # Retrieve service account name for given namespace  sec=`kubectl get serviceaccount ${sa} -n ${ns} -o jsonpath=\u0026#39;{.secrets[0].name}\u0026#39;` #echo \u0026#34;Secret [${sec}]\u0026#34; # Retrieve base64 encoded secret for the given service account  enc_token=`kubectl get secret ${sec} -n ${ns} -o jsonpath=\u0026#39;{.data.token}\u0026#39;` #echo \u0026#34;enc_token [${enc_token}]\u0026#34; # Decode the base64 encoded token  token=`echo ${enc_token} | base64 --decode` #echo \u0026#34;token [${token}]\u0026#34; # clean up any temporary files rm -rf operator.rest.response.body operator.rest.stderr operator.cert.pem # Retrieve SSL certificate from the Operator\u0026#39;s external REST endpoint `openssl s_client -showcerts -connect ${ophost}:${opport} \u0026lt;/dev/null 2\u0026gt;/dev/null | openssl x509 -outform PEM \u0026gt; operator.cert.pem` echo \u0026#34;Rest EndPoint url https://${ophost}:${opport}/operator/v1/domains/${domainuid}/clusters/${cluster}/scale\u0026#34; # Issue \u0026#39;curl\u0026#39; request to external REST endpoint  curl --noproxy \u0026#39;*\u0026#39; -v --cacert operator.cert.pem \\ -H \u0026#34;Authorization: Bearer ${token}\u0026#34; \\ -H Accept:application/json \\ -H \u0026#34;Content-Type:application/json\u0026#34; \\ -H \u0026#34;X-Requested-By:WLDF\u0026#34; \\ -d \u0026#34;{\\\u0026#34;managedServerCount\\\u0026#34;: $size}\u0026#34; \\ -X POST https://${ophost}:${opport}/operator/v1/domains/${domainuid}/clusters/${cluster}/scale \\ -o operator.rest.response.body \\ --stderr operator.rest.stderr "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/persistent-storage/",
	"title": "Persistent storage",
	"tags": [],
	"description": "",
	"content": "This document outlines how to set up a Kubernetes PersistentVolume and PersistentVolumeClaim which can be used as storage for WebLogic domain homes and log files. A PersistentVolume can be shared by multiple WebLogic domains or dedicated to a particular domain.\nPrerequisites The following prerequisites must be fulfilled before proceeding with the creation of the volume:\n Create a Kubernetes Namespace for the PersistentVolumeClaim unless the intention is to use the default namespace. Note that a PersistentVolumeClaim has to be in the same namespace as the Domain that uses it. Make sure that all the servers in the WebLogic domain are able to reach the storage location. Make sure that the host directory that will be used, already exists and has the appropriate file permissions set.  Storage locations PersistentVolumes can point to different storage locations, for example NFS servers or a local directory path. The list of available options is listed in the Kubernetes documentation.\nNote regarding HostPath: In a single-node Kubernetes cluster, such as may be used for testing or proof of concept activities, HOST_PATH provides the simplest configuration. In a multinode Kubernetes cluster, a HOST_PATH that is located on shared storage mounted by all nodes in the Kubernetes cluster is the simplest configuration. If nodes do not have shared storage, then NFS is probably the most widely available option. There are other options listed in the referenced table.\nThe PersistentVolume for the domain must be created using the appropriate tools before running the script to create the domain. In the simplest case, namely the HOST_PATH provider, this means creating a directory on the Kubernetes master and ensuring that it has the correct permissions:\n$ mkdir -m 777 -p /path/to/domain1PersistentVolume Note regarding NFS: In the current GA version, the OCI Container Engine for Kubernetes supports network block storage that can be shared across nodes with access permission RWOnce (meaning that only one can write, others can read only). At this time, the WebLogic on Kubernetes domain created by the WebLogic Kubernetes Operator, requires a shared file system to store the WebLogic domain configuration, which MUST be accessible from all the pods across the nodes. As a workaround, you need to install an NFS server on one node and share the file system across all the nodes.\nCurrently, we recommend that you use NFS version 3.0 for running WebLogic Server on OCI Container Engine for Kubernetes. During certification, we found that when using NFS 4.0, the servers in the WebLogic domain went into a failed state intermittently. Because multiple threads use NFS (default store, diagnostics store, Node Manager, logging, and domain_home), there are issues when accessing the file store. These issues are removed by changing the NFS to version 3.0.\nPersistentVolume GID annotation The HOST_PATH directory permissions can be made more secure by using a Kubernetes annotation on the PersistentVolume that provides the group identifier (GID) which will be added to pods using the PersistentVolume.\nFor example, if the GID of the directory is 6789, then the directory can be updated to remove permissions other than for the user and group along with the PersistentVolume being annotated with the specified GID:\n$ chmod 770 /path/to/domain1PersistentVolume $ kubectl annotate pv domain1-weblogic-sample-pv pv.beta.kubernetes.io/gid=6789 After the domain is created and servers are running, the group ownership of the PersistentVolume files can be updated to the specified GID which will provide read access to the group members. Typically, files created from a pod onto the PersistentVolume will have UID 1000 and GID 1000 which is the oracle user from the WebLogic Server image.\nAn example of updating the group ownership on the PersistentVolume would be as follows:\n$ cd /path/to/domain1PersistentVolume $ sudo chgrp 6789 applications domains logs stores $ sudo chgrp -R 6789 domains/ $ sudo chgrp -R 6789 logs/ YAML files Persistent volumes and claims are described in YAML files. For each PersistentVolume, you should create one PersistentVolume YAML file and one PersistentVolumeClaim YAML file. In the example below, you will find two YAML templates, one for the volume and one for the claim. As stated above, they either can be dedicated to a specific domain, or shared across multiple domains. For the use cases where a volume will be dedicated to a particular domain, it is a best practice to label it with weblogic.domainUID=[domain name]. This makes it easy to search for, and clean up resources associated with that particular domain.\nFor sample YAML templates, refer to the PersistentVolumes example.\nKubernetes resources After you have written your YAML files, use them to create the PersistentVolume by creating Kubernetes resources using the kubectl create -f command:\n$ kubectl create -f pv.yaml $ kubectl create -f pvc.yaml Verify the results To confirm that the PersistentVolume was created, use these commands:\n$ kubectl describe pv [persistent volume name] $ kubectl describe pvc -n NAMESPACE [persistent volume claim name] Common problems This section provides details of common problems that might occur while running the script and how to resolve them.\nPersistentVolume provider not configured correctly Possibly the most common problem experienced during testing was the incorrect configuration of the PersistentVolume provider. The PersistentVolume must be accessible to all Kubernetes Nodes, and must be able to be mounted as Read/Write/Many. If this is not the case, the PersistentVolume creation will fail.\nThe simplest case is where the HOST_PATH provider is used. This can be either with one Kubernetes Node, or with the HOST_PATH residing in shared storage available at the same location on every node (for example, on an NFS mount). In this case, the path used for the PersistentVolume must have its permission bits set to 777.\nFurther reading  See the blog, How to run WebLogic clusters on the Oracle Cloud Infrastructure Container Engine for Kubernetes.  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/reference/domain-resource/",
	"title": "Domain resource",
	"tags": [],
	"description": "Use this document to set up and configure your own Domain YAML file.",
	"content": "View the Domain reference document here.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/quickstart/get-images/",
	"title": "Get images",
	"tags": [],
	"description": "",
	"content": "Get these images and put them into your local registry.   Pull the operator image:\n$ docker pull ghcr.io/oracle/weblogic-kubernetes-operator:3.3.2   Pull the Traefik ingress controller image:\n$ docker pull traefik:2.2.1   Obtain the WebLogic Server image from the Oracle Container Registry.\na. First time users, follow these directions.\nb. Find and then pull the WebLogic 12.2.1.4 install image:\n$ docker pull container-registry.oracle.com/middleware/weblogic:12.2.1.4  The WebLogic Server image, weblogic:12.2.1.3, has all the necessary patches applied. The WebLogic Server image, weblogic:12.2.1.4, does not require any additional patches.\n   Copy the image to all the nodes in your cluster, or put it in a container registry that your cluster can access.\n  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/prepare/",
	"title": "Setup checklist",
	"tags": [],
	"description": "Follow these steps to set up your environment.",
	"content": "  Fulfill the operator prerequisite requirements.\n  Set up Kubernetes.\n  Optional. Enable Istio.\n  Install Helm.\n  Get the operator image from the GitHub Container Registry.\n  Install the operator.\n  Optional. Run a database. For example, run an Oracle database inside Kubernetes.\n  Optional. Load balance with an ingress controller or a web server. For information about the current capabilities and setup instructions for each of the supported load balancers, see the WebLogic Operator Load Balancer Samples.\n  Optional. Configure Kibana and Elasticsearch. You can send the operator logs to Elasticsearch, to be displayed in Kibana. Use this sample script to configure Elasticsearch and Kibana deployments and services.\n  Optional. Create persistent file storage. For example, a Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC).\n  Set up your domain. For information, see Choose a domain home source type.\n  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/faq/domain-secret-mismatch/",
	"title": "Domain secret mismatch",
	"tags": [],
	"description": "One or more WebLogic Server instances in my domain will not start and the domain resource `status` or the pod log reports errors like this: Domain secret mismatch.",
	"content": " One or more WebLogic Server instances in my domain will not start and the Domain status or the pod log reports errors like this:\nDomain secret mismatch. The domain secret in DOMAIN_HOME/security/SerializedSystemIni.dat where DOMAIN_HOME=$DOMAIN_HOME does not match the domain secret found by the introspector job. WebLogic requires that all WebLogic Servers in the same domain share the same domain secret.\n When you see these kinds of errors, it means that the WebLogic domain directory\u0026rsquo;s security configuration files have changed in an incompatible way between when the operator scanned the domain directory, which occurs during the \u0026ldquo;introspection\u0026rdquo; phase, and when the server instance attempted to start.\nTo understand the \u0026ldquo;incompatible domain security configuration\u0026rdquo; type of failure, it\u0026rsquo;s important to review the contents of the WebLogic domain directory. Each WebLogic domain directory contains a security subdirectory that contains a file called SerializedSystemIni.dat. This file contains security data to bootstrap the WebLogic domain, including a domain-specific encryption key.\nDuring introspection, the operator generates a Kubernetes Job that runs a pod in the domain\u0026rsquo;s Kubernetes Namespace and with the same Kubernetes ServiceAccount that will be used later to run the Administration Server. This pod has access to the Kubernetes secret referenced by weblogicCredentialsSecret and encrypts these values with the domain-specific encryption key so that the secured value can be injected in to the boot.properties files when starting server instances.\nWhen the domain directory is changed such that the domain-specific encryption key is different, the boot.properties entries generated during introspection will now be invalid.\nThis can happen in a variety of ways, depending on the domain home source type.\nDomain in Image Rolling to an image containing new or unrelated domain directory The error occurs while rolling pods have containers based on a new container image that contains an entirely new or unrelated domain directory.\nThe problem is that WebLogic cannot support server instances being part of the same WebLogic domain if the server instances do not all share the same domain-specific encryption key. Additionally, operator introspection currently happens only when starting servers following a total shutdown. Therefore, the boot.properites files generated from introspecting the image containing the original domain directory will be invalid when used with a container started with the updated container image containing the new or unrelated domain directory.\nThe solution is to follow either the recommended CI/CD guidelines so that the original and new container images contain domain directories with consistent domain-specific encryption keys and bootstrapping security details, or to perform a total shutdown of the domain so that introspection reoccurs as servers are restarted.\nFull domain shutdown and restart The error occurs while starting servers after a full domain shutdown.\nIf your development model generates new container images with new and unrelated domain directories and then tags those images with the same tag, then different Kubernetes worker nodes may have different images under the same tag in their individual, local container repositories.\nThe simplest solution is to set imagePullPolicy to Always; however, the better solution would be to design your development pipeline to generate new container image tags on every build and to never reuse an existing tag.\nDomain in PV Completely replacing the domain directory The error occurs while starting servers when the domain directory change was made while other servers were still running.\nIf completely replacing the domain directory, then you must stop all running servers.\nBecause all servers will already be stopped, there is no requirement that the new contents of the domain directory be related to the previous contents of the domain directory. When starting servers again, the operator will perform its introspection of the domain directory. However, you may want to preserve the domain directory security configuration including the domain-specific encryption key and, in that case, you should follow a similar pattern as is described in the CI/CD guidelines for the domain in a container image model to preserve the original security-related domain directory files.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/cicd/mutate-the-domain-layer/",
	"title": "Mutate the domain layer",
	"tags": [],
	"description": "How to mutate the domain layer.",
	"content": "If you need to mutate the domain layer, and keep the same domain encryption keys, then there are some choices about how to implement that, as alluded to previously. Let\u0026rsquo;s explore those in some more detail now.\nThe first option is to implement each mutation as a delta to the previous state. This is conceptually similar to how immutable objects (like Java Strings) are implemented, a \u0026ldquo;copy on write\u0026rdquo; approach applied to the domain configuration as a unit. This does have the advantage that it is simple to implement, but the disadvantage that your builds would depend on the previous good build and this is somewhat contrary to typical CI/CD practices. You also have to work out what to do with the bad builds, or \u0026ldquo;holes\u0026rdquo; in the sequence.\nAn alternative is to capture a \u0026ldquo;primordial state\u0026rdquo; of the domain before starting the sequence. In practical terms, this might mean creating a very simple domain with no applications or resources in it, and \u0026ldquo;saving\u0026rdquo; it before ever starting any servers. This primordial domain (let’s call it t=0) would then be used to build each mutation. So each state is built from t=0, plus all of the changes up to that point.\nSaid another way, each build would start with t=0 as the base image and extend it. This eliminates the need to keep each intermediate state, and would also likely have benefits when you remove things from the domain, because you would not have \u0026ldquo;lost\u0026rdquo; (\u0026ldquo;whited out\u0026rdquo; is the image layer term) space in the intermediate layers. Although, these layers tend to be relatively small, so this is possibly not a big issue.\nThis approach is probably an improvement. It does get interesting though when you update a lower layer, for example when you patch WebLogic or update the JDK. When this happens, you need to create another base image, shown in the diagram as v2 t-0. All of the mutations in this new chain are based on this new base image. So that still leaves us with the problem of how to take the domain from the first series (v1 t=0 to t=3) and \u0026ldquo;copy\u0026rdquo; it across to the second series (v2).\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/accessing-the-domain/port-forward/",
	"title": "Use port forwarding",
	"tags": [],
	"description": "You can use port forwarding to access WebLogic Server Administration Consoles and WLST.",
	"content": "Contents  Overview Set up Administration Server network channels for port forward access Port forward to an Administration Server Pod   Port forward example Port forward notes and warnings Enabling WLST access when local and remote ports do not match Terminating port forwarding    Overview Beginning with WebLogic Kubernetes Operator version 3.3.2, or earlier if you are using an Istio-enabled domain, you can use the kubectl port-forward command to set up external access for the WebLogic Server Administration Console, the Remote Console, and WLST. This approach is particularly useful for managing WebLogic from a private local network without exposing WebLogic\u0026rsquo;s ports to a public network.\nHere are the steps:\n Set up WebLogic Administration Server network channels for port forward access. If you are setting up WLST access and your port forwarding local port is not going to be the same as the port number on the WebLogic Administration Server Pod, then see enabling WLST access when local and remote ports do not match for an additional required setup step. Be sure to review the port forward notes and warnings first, and then run a port forwarding command. Use the WebLogic Server Administration Console, the Remote Console, or WLST with the port forwarding command\u0026rsquo;s local address. Finally, terminate port forwarding.  Externally exposing administrative, RMI, or T3 capable WebLogic channels using a Kubernetes NodePort, load balancer, port forwarding, or a similar method can create an insecure configuration. For more information, see External network access security.\n Set up Administration Server network channels for port forward access To enable a kubectl port-forward command to communicate with a WebLogic Administration Server Pod, the operator must modify the Administration Server configuration to add network channels (Network Access Points) with a localhost address for each existing administration protocol capable port. This behavior depends on your version and domain resource configuration:\n  If Istio is not enabled on the domain, then, for operator versions 3.3.2 and later, this behavior is configurable on the domain resource using the domain.spec.adminServer.adminChannelPortForwardingEnabled domain resource attribute. This attribute is enabled by default in operator versions 4.0 and later, and is disabled by default in versions prior to 4.0. For details about this attribute, run the kubectl explain domain.spec.adminServer.adminChannelPortForwardingEnabled command or see the domain resource schema.\n  For Istio-enabled domains, the operator already adds a network channel with a localhost listen address for each existing port. This means that no additional configuration is required to enable port forwarding when Istio is enabled. For more details, see How Istio-enabled domains differ from regular domains.\n  If your domain is already running, and you have made configuration changes, then you will need to rerun its introspector job and ensure that the admin pod restarts for the configuration changes to take effect.\n When administration channel port forwarding is enabled, the operator automatically adds the following network channels (also known as Network Access Points) to the WebLogic Administration Server Pod:\n   Description Channel Name Listen Address Port Protocol     When there\u0026rsquo;s no administration port or administration channels configured, and a non-SSL default channel exists internal-t3 localhost Server listening port t3   When there\u0026rsquo;s no administration port or administration channels configured, and an SSL default channel exists internal-t3s localhost Server SSL listening port t3s   When an administration port is enabled internal-admin localhost WebLogic administration port admin   When one or more custom administration channels are configured internal-admin${index} (where ${index} is a number that starts with 1 and increases by 1 for each custom administration channel) localhost Custom administration port admin    Port forward to an Administration Server Pod If you have set up WebLogic Administration Server network channels for port forward access, then you can run a kubectl port-forward command to access such a channel. The command:\n By default, opens a port of your choosing on localhost on the machine where you run the command. Forwards traffic from this location to an address and port on the pod. For administrative access, you need to forward to a port on the Administration Server Pod that accepts administration traffic.  Port forwarding occurs as long as the remote pod and the command are running. If you exit the command, then the forwarding stops and the local port and address is freed. If the pod cycles or shuts down, then the forwarding also stops and the command must be rerun after the pod recovers in order for the forwarding to restart.\nThe kubectl port-forward command has the following syntax:\nkubectl port-forward K8S_RESOURCE_TYPE/K8S_RESOURCE_NAME [options] LOCAL_PORT:REMOTE_PORT_ON_RESOURCE For detailed usage, see port-forward in the Kubernetes reference documentation and run kubectl port-forward -h.\nFor examples, notes, and warnings, see the Port forward example and Port forward notes and warnings.\nPort forward example For example, if you have a WebLogic domain with:\n Domain UID domain1 Namespace mynamespace An Administration Server named admin-server listening on non-SSL listen port 7001 No administration port or administration channels configured on the Administration Server  And, you have set up WebLogic Administration Server network channels for port forward access for this domain, then you can run either of the following commands to forward local port 32015 to the Administration Server Pod:\nkubectl port-forward pods/domain1-admin-server -n mynamespace 32015:7001 or\nkubectl port-forward service/domain1-admin-server -n mynamespace 32015:7001 The command output will be similar to the following:\nForwarding from 127.0.0.1:32015 -\u0026gt; 7001 In this example:\n  You can access the WebLogic Server Administration Console at the http://localhost:32015/console URL by using a browser on the machine where you run the kubectl port-forward command.\n  You can use WLST on the machine where you run the kubectl port-forward command to connect to t3://localhost:32015 as shown:\n$ $ORACLE_HOME/oracle_common/common/bin/wlst.sh Initializing WebLogic Scripting Tool (WLST) ... Welcome to WebLogic Server Administration Scripting Shell Type help() for help on available commands wls:/offline\u0026gt; connect(\u0026#39;myadminuser\u0026#39;,\u0026#39;myadminpassword\u0026#39;,\u0026#39;t3://localhost:32015\u0026#39;) Connecting to t3://localhost:32015 with userid myadminuser ... Successfully connected to Admin Server \u0026#34;admin-server\u0026#34; that belongs to domain \u0026#34;base_domain\u0026#34;. Warning: An insecure protocol was used to connect to the server. To ensure on-the-wire security, the SSL port or Admin port should be used instead. wls:/base_domain/serverConfig/\u0026gt; exit()   Port forward notes and warnings   Security warning: A port-forward connection can expose a WebLogic T3 or administrative channel outside the Kubernetes cluster. For domain security considerations, see External network access security.\n  Working with administration or SSL ports: If a WebLogic administration port is configured and enabled on the Administration Server, then you will need to forward to this port instead of its pod\u0026rsquo;s other ports. In this case, the Administration Console access requires using the secure https protocol and WLST access requires using t3s protocol. Similarly, if forwarding to an SSL port, then this requires using the https and t3s protocols for Administration Console and WLST access respectively.\n  Recovering from pod failures and restarts: A port-forward connection terminates after the pod instance fails or restarts. You can rerun the same command to establish a new forwarding session and resume forwarding.\n  Using WLST when the local port differs from the remote port: If you are setting up WLST access and your port forwarding local port is not going to be the same as the port number on the WebLogic Administration Server Pod, then see Enabling WLST access when local and remote ports do not match for an additional required setup step.\n  Specifying a custom local IP address for port forwarding:\nSpecifying a custom local IP address for port forwarding allows you to run WLST or your browser on a different machine than the port forward command.\n You can use the --address option of the kubectl port-forward command to listen on specific IP addresses instead of, or in addition to, localhost.\nThe --address option only accepts numeric IP addresses or localhost (comma-separated) as a value.\nFor example, to enable Administration Console access at http://my-ip-address:32015/console, the command:\nkubectl port-forward --address my-ip-address pods/domain1-admin-server -n mynamespace 32015:7001 See kubectl port-forward -h for help and examples.\n  Optionally let kubectl port-forward choose the local port:\nIf you don\u0026rsquo;t specify a local port in your port forward command, for example:\nkubectl port-forward pods/domain1-admin-server -n mynamespace :7001 Then the command finds a local port number that is not in use, and its output is similar to:\nForwarding from 127.0.0.1:63753 -\u0026gt; 7001 And the Administration Console is accessible using the http://localhost:63753/console URL.\n  Enabling WLST access when local and remote ports do not match If a local (forwarded) port number is not the same as the administration port number, then WLST access will not work by default and you may see a BEA-000572 RJVM error in the Administration Server logs:\n\u0026lt;Aug 30, 2021 9:33:24,753 PM GMT\u0026gt; \u0026lt;Error\u0026gt; \u0026lt;RJVM\u0026gt; \u0026lt;BEA-000572\u0026gt; \u0026lt;The server rejected a connection attempt JVMMessage from: \u0026#39;-2661445766084484528C:xx.xx.xx.xxR:-5905806878036317188S:domain1-admin-server:domain1:admin-server\u0026#39; to: \u0026#39;0B:xx.xx.xx.xx:[-1,-1,32015,-1,-1,-1,-1]\u0026#39; cmd: \u0026#39;CMD_IDENTIFY_REQUEST\u0026#39;, QOS: \u0026#39;102\u0026#39;, responseId: \u0026#39;-1\u0026#39;, invokableId: \u0026#39;-1\u0026#39;, flags: \u0026#39;JVMIDs Sent, TX Context Not Sent, 0x1\u0026#39;, abbrev offset: \u0026#39;114\u0026#39; probably due to an incorrect firewall configuration or administrative command.\u0026gt;\u0026lt; To work around the problem, configure the JAVA_OPTIONS environment variable for your Administration Server with the -Dweblogic.rjvm.enableprotocolswitch=true system property in your domain resource YAML. For more information on this switch, refer to MOS \u0026lsquo;Doc 860340.1\u0026rsquo;.\nTerminating port forwarding A port-forward connection is active only while the kubectl port-forward command is running.\nYou can terminate the port-forward connection by pressing CTRL+C in the terminal where the port forward command is running.\nIf you run the command in the background, then you can kill the process with the kill -9 \u0026lt;pid\u0026gt; command. For example:\n$ ps -ef | grep port-forward oracle 27072 25312 1 21:45 pts/3 00:00:00 kubectl -n mynamespace port-forward pods/domain1-admin-server 32015:7001 oracle 27944 11417 0 21:45 pts/1 00:00:00 grep --color=auto port-forward $ kill -9 27072 "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/security/service-accounts/",
	"title": "Service accounts",
	"tags": [],
	"description": "Kubernetes ServiceAccounts for the operator",
	"content": "WebLogic Kubernetes Operator ServiceAccounts When the operator is installed, the Helm chart property, serviceAccount, can be specified where the value contains the name of the Kubernetes ServiceAccount in the namespace in which the operator will be installed. For more information about the Helm chart, see the Operator Helm configuration values.\nThe operator will use this ServiceAccount when calling the Kubernetes API server and the appropriate access controls will be created for this ServiceAccount by the operator\u0026rsquo;s Helm chart.\nFor more information about access controls, see RBAC.\n If the operator\u0026rsquo;s service account cannot have the privileges to access the cluster-level resources, such as CustomResourceDefinitions, Namespaces and PersistentVolumes, consider using a dedicated namespace for each operator and the domains that the operator manages. See the dedicated setting in Operator Helm configuration values.\n In order to display the ServiceAccount used by the operator, where the operator was installed using the Helm release name weblogic-operator, look for the serviceAccount value using the Helm command:\n$ helm get values --all weblogic-operator Additional reading  Helm service account Operator Helm chart service account configuration  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/domains/model-in-image/",
	"title": "Model in image",
	"tags": [],
	"description": "Sample for supplying a WebLogic Deploy Tooling (WDT) model that the operator expands into a full domain home during runtime.",
	"content": "Contents  Introduction  Model in Image domain types (WLS, JRF, and Restricted JRF) Use cases Sample directory structure Ensuring your Kubernetes cluster can access images   References Sample steps  Prerequisites for all domain types Additional prerequisites for JRF domains Initial: Deploying an initial WebLogic domain Update 1: Dynamically adding a data source using a model ConfigMap and a domain restart (roll) Update 2: Deploying an additional domain Update 3: Updating an application using an updated image and a domain restart (roll) Update 4: Dynamically updating the WebLogic configuration without restarting (rolling) servers Cleanup    Introduction This sample demonstrates deploying a Model in Image domain home source type. Unlike Domain in PV and Domain in Image, Model in Image eliminates the need to pre-create your WebLogic domain home prior to deploying your Domain YAML file. Instead, Model in Image uses a WebLogic Deploy Tooling (WDT) model to specify your WebLogic configuration.\nWDT models are a convenient and simple alternative to WebLogic Scripting Tool (WLST) configuration scripts and templates. They compactly define a WebLogic domain using YAML files and support including application archives in a ZIP file. The WDT model format is described in the open source, WebLogic Deploy Tooling GitHub project, and the required directory structure for a WDT archive is specifically discussed here.\nFor more information on Model in Image, see the Model in Image user guide. For a comparison of Model in Image to other domain home source types, see Choose a domain home source type.\nModel in Image domain types (WLS, JRF, and Restricted JRF) There are three types of domains supported by Model in Image: a standard WLS domain, an Oracle Fusion Middleware Infrastructure Java Required Files (JRF) domain, and a RestrictedJRF domain. This sample demonstrates the WLS and JRF types.\nThe JRF domain path through the sample includes additional steps required for JRF: deploying an infrastructure database, initializing the database using the Repository Creation Utility (RCU) tool, referencing the infrastructure database from the WebLogic configuration, setting an Oracle Platform Security Services (OPSS) wallet password, and exporting/importing an OPSS wallet file. JRF domains may be used by Oracle products that layer on top of WebLogic Server, such as SOA and OSB. Similarly, RestrictedJRF domains may be used by Oracle layered products, such as Oracle Communications products.\nUse cases This sample demonstrates five Model in Image use cases:\n  Initial: An initial WebLogic domain with the following characteristics:\n Image model-in-image:WLS-v1 with:  A WebLogic installation A WebLogic Deploy Tooling (WDT) installation A WDT archive with version v1 of an exploded Java EE web application A WDT model with:  A WebLogic Administration Server A WebLogic cluster A reference to the web application     Kubernetes Secrets:  WebLogic credentials Required WDT runtime password   A Domain with:  metadata.name and weblogic.domainUID label set to sample-domain1 spec.domainHomeSourceType: FromModel spec.image: model-in-image:WLS-v1 References to the secrets      Update 1: Demonstrates updating the initial domain by dynamically adding a data source using a model ConfigMap and then restarting (rolling) the domain to propagate the change.\n Image model-in-image:WLS-v1:  Same image as Initial use case   Kubernetes Secrets:  Same as Initial use case, plus secrets for data source credentials and URL   Kubernetes ConfigMap with:  A WDT model for a data source targeted to the cluster   A Domain, same as Initial use case, plus:  spec.model.configMap referencing the ConfigMap References to data source secrets      Update 2: Demonstrates deploying a second domain (similar to the Update 1 use case domain).\n Image model-in-image:WLS-v1:  Same image as the Initial and Update 1 use cases   Kubernetes Secrets and ConfigMap:  Similar to the Update 1 use case, except names and labels are decorated with a new domain UID   A Domain, similar to Update 1 use case, except:  Its metadata.name and weblogic.domainUid label become sample-domain2 instead of sample-domain1 Its secret/ConfigMap references are decorated with sample-domain2 instead of sample-domain1 Has a changed env variable that sets a new domain name      Update 3: Demonstrates deploying an updated image with an updated application to the Update 1 use case domain and then restarting (rolling) its domain to propagate the change.\n Image model-in-image:WLS-v2, similar to model-in-image:WLS-v1 image with:  An updated web application v2 at the myapp-v2 directory path instead of myapp-v1 An updated model that points to the new web application path   Kubernetes Secrets and ConfigMap:  Same as the Update 1 use case   A Domain:  Same as the Update 1 use case, except spec.image is model-in-image:WLS-v2      Update 4: Demonstrates dynamically updating the running Update 1 or Update 3 WebLogic domain configuration without requiring a domain restart (roll).\n Image model-in-image:WLS-v1 or model-in-image:WLS-v2:  Same image as Update 1 or Update 3 use cases   Kubernetes ConfigMap with:  A WDT model for Work Manager minimum and maximum threads constraints, plus the same data source as the Update 1 use case   Kubernetes Secrets:  Same as the Update 1 and Update 3 use case, except: An updated data source secret with a new password and an increased maximum pool capacity   A Domain, same as Update 1 or Update 3 use case, plus:  spec.configuration.model.onlineUpdate set to enabled: true      Sample directory structure The sample contains the following files and directories:\n   Location Description     domain-resources JRF and WLS Domain YAML files.   archives Source code location for WebLogic Deploy Tooling application ZIP archives.   model-images Staging for each model image\u0026rsquo;s WDT YAML files, WDT properties, and WDT archive ZIP files. The directories in model images are named for their respective images.   model-configmaps/datasource Staging files for a model ConfigMap that configures a data source.   model-configmaps/workmanager Staging files for a model ConfigMap that configures the Work Manager threads constraints.   ingresses Ingress resources.   utils/wl-pod-wait.sh Utility script for watching the pods in a domain reach their expected restartVersion, introspectVersion, image name, and ready state.   utils/patch-introspect-version.sh Utility script for updating a running domain spec.introspectVersion field (which causes it to \u0026rsquo;re-instrospect' and \u0026lsquo;roll\u0026rsquo; only if non-dynamic attributes are updated).   utils/patch-restart-version.sh Utility script for updating a running domain spec.restartVersion field (which causes it to \u0026rsquo;re-instrospect' and \u0026lsquo;roll\u0026rsquo;).   utils/patch-enable-online-update.sh Utility script for updating a running domain spec.configuration.model.onlineUpdate field to enabled: true (which enables the online update feature).   utils/opss-wallet.sh Utility script for exporting or importing a JRF domain OPSS wallet file.    Ensuring your Kubernetes cluster can access images If you run the sample from a machine that is remote to one or more of your Kubernetes cluster worker nodes, then you need to ensure that the images you create can be accessed from any node in the cluster.\nFor example, if you have permission to put the image in a container registry that the cluster can also access, then:\n After you\u0026rsquo;ve created an image:  docker tag the image with a target image name (including the registry host name, port, repository name, and the tag, if needed). docker push the tagged image to the target repository.   Before you deploy a Domain:  Modify the Domain YAML file\u0026rsquo;s image: value to match the image tag for the image in the repository. If the repository requires a login, then also deploy a corresponding Kubernetes docker secret to the same namespace that the Domain will use, and modify the Domain YAML file\u0026rsquo;s imagePullSecrets: to reference this secret.    Alternatively, if you have access to the local image cache on each worker node in the cluster, then you can use a Docker command to save the image to a file, copy the image file to each worker node, and use a docker command to load the image file into the node\u0026rsquo;s image cache.\nFor more information, see the Cannot pull image FAQ.\nReferences For references to the relevant user documentation, see:\n Model in Image user documentation WebLogic Deploy Tooling WebLogic Image Tool  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/rest/",
	"title": "REST APIs",
	"tags": [],
	"description": "Sample for generating a self-signed certificate and private key that can be used for the operator&#39;s external REST API.",
	"content": "Sample to create certificate and key When a user enables the operator\u0026rsquo;s external REST API (by setting externalRestEnabled to true when installing or upgrading the operator Helm chart), the user also needs to provide the certificates and private key used for the SSL/TLS identity on the external REST API endpoint by creating a kubernetes tls secret and using that secret\u0026rsquo;s name with the operator Helm chart values.\nThis sample script generates a self-signed certificate and private key that can be used for the operator\u0026rsquo;s external REST API when experimenting with the operator.\nThe certificate and key generated with this script should not be used in a production environment.\n The syntax of the script is:\n$ kubernetes/samples/scripts/rest/generate-external-rest-identity.sh \\  -a \u0026lt;SANs\u0026gt; -n \u0026lt;operator-namespace\u0026gt; [-s \u0026lt;secret-name\u0026gt;] Where \u0026lt;SANs\u0026gt; lists the subject alternative names to put into the generated self-signed certificate for the external operator REST HTTPS interface, \u0026lt;operator-namespace\u0026gt; should match the namespace where the operator will be installed, and optionally the secret name, which defaults to weblogic-operator-external-rest-identity.\nYou should include the addresses of all masters and load balancers (for example, what a client specifies to access the external REST endpoint) in the subject alternative name list. In addition, each name must be prefaced by DNS: for a host name, or IP: for an address, as in this example:\n-a \u0026quot;DNS:myhost,DNS:localhost,IP:127.0.0.1\u0026quot; The external certificate and key can be changed after installation of the operator. For more information, see Updating operator external certificates.\nThe following script will create the tls secret named weblogic-operator-identity in the namespace weblogic-operator-ns, using a self-signed certificate and private key:\n$ echo \u0026#34;externalRestEnabled: true\u0026#34; \u0026gt; my_values.yaml $ generate-external-rest-identity.sh \\  -a \u0026#34;DNS:${HOSTNAME},DNS:localhost,IP:127.0.0.1\u0026#34; \\  -n weblogic-operator-ns -s weblogic-operator-identity \u0026gt;\u0026gt; my_values.yaml $ kubectl -n weblogic-operator-ns describe secret weblogic-operator-identity $ helm install my_operator kubernetes/charts/weblogic-operator \\  --namespace weblogic-operator-ns --values my_values.yaml --wait "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/domains/model-in-image/update2/",
	"title": "Update 2",
	"tags": [],
	"description": "",
	"content": "This use case demonstrates concurrently deploying a domain that is similar to the Update 1 use case domain to the same sample-domain1-ns namespace, but with a different domain UID, a different WebLogic domain name, and a different WebLogic domain encryption key. It does this by:\n Using the same image, image model YAML file, and application archive as the Initial and Update 1 use cases. Using the same model update ConfigMap source file as the Update 1 use case (a data source). Using a different (unique) domain UID, sample-domain2, for the new domain. Using a different (unique) domain name, domain2, for the different domains. Deploying secrets and a model update ConfigMap that are uniquely labeled and named for the new domain.  Note that this use case shows Model in Image\u0026rsquo;s unique ability to quickly deploy a copy of a WebLogic domain that has a different WebLogic domain name and domain encryption key. This is a useful capability that is not supported by the Domain in Image domain home source type:\n  Domain in Image does not support overriding the domain name, but different domain names are necessary when two domains need to interoperate. This use case takes advantage of model macros to ensure that its two different domains have a different domain name:\n First, you define the domain name in the model YAML file using the @@ENV:CUSTOM_DOMAIN_NAME@@ environment variable macro. Second, you set the value of the CUSTOM_DOMAIN_NAME environment variable to be different using the env stanza in each Domain\u0026rsquo;s YAML file.    Domain in Image requires that its images embed a WebLogic security/SerializedSystemIni.dat domain encryption key that cannot be changed for the image (see Why layering matters in CI/CD considerations). This necessarily means that two Domain in Image domains that share the same image can decrypt each other\u0026rsquo;s encrypted passwords. On the other hand, a Model in Image\u0026rsquo;s domain encryption key is not embedded in the image and instead, is dynamically and uniquely created each time the domain is started.\n  Oracle requires interoperating WebLogic domains to have different domain names. This is necessary when two domains communicate, or when a WebLogic Server or WebLogic Java client concurrently connects to multiple domains.\n Here are the steps for this use case:\n  Make sure you have deployed the domain from the Update 1 use case.\n  Create a ConfigMap with the WDT model that contains the data source definition.\nRun the following commands:\n$ kubectl -n sample-domain1-ns create configmap sample-domain2-wdt-config-map \\  --from-file=/tmp/mii-sample/model-configmaps/datasource $ kubectl -n sample-domain1-ns label configmap sample-domain2-wdt-config-map \\  weblogic.domainUID=sample-domain2 If you\u0026rsquo;ve created your own data source file in the Update 1 use case, then substitute the file name in the --from-file= parameter (we suggested /tmp/mii-sample/mydatasource.yaml earlier). Note that the -from-file= parameter can reference a single file, in which case it puts the designated file in the ConfigMap, or it can reference a directory, in which case it populates the ConfigMap with all of the files in the designated directory.\nObservations:\n We are leaving the namespace sample-domain1-ns unchanged for the ConfigMap because you will deploy domain sample-domain2 to the same namespace as sample-domain1. You name and label the ConfigMap using its associated domain UID for two reasons:  To make it obvious which ConfigMap belongs to which domain. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all resources associated with a domain.   You use a different ConfigMap for the new domain for two reasons:  To make it easier to keep the life cycle and/or CI/CD process for the two domains simple and independent. To \u0026lsquo;future proof\u0026rsquo; the new domain so that changes to the original domain or new domain\u0026rsquo;s ConfigMap can be independent.      Create the secrets that are referenced by the WDT model files in the image and ConfigMap; they also will be referenced by the Domain YAML file.\nRun the following commands:\n# spec.webLogicCredentialsSecret $ kubectl -n sample-domain1-ns create secret generic \\  sample-domain2-weblogic-credentials \\  --from-literal=username=weblogic --from-literal=password=welcome1 $ kubectl -n sample-domain1-ns label secret \\  sample-domain2-weblogic-credentials \\  weblogic.domainUID=sample-domain2 # spec.configuration.model.runtimeEncryptionSecret $ kubectl -n sample-domain1-ns create secret generic \\  sample-domain2-runtime-encryption-secret \\  --from-literal=password=my_runtime_password $ kubectl -n sample-domain1-ns label secret \\  sample-domain2-runtime-encryption-secret \\  weblogic.domainUID=sample-domain2 # referenced by spec.configuration.secrets and by the data source model YAML in the ConfigMap $ kubectl -n sample-domain1-ns create secret generic \\  sample-domain2-datasource-secret \\  --from-literal=\u0026#39;user=sys as sysdba\u0026#39; \\  --from-literal=\u0026#39;password=incorrect_password\u0026#39; \\  --from-literal=\u0026#39;max-capacity=1\u0026#39; \\  --from-literal=\u0026#39;url=jdbc:oracle:thin:@oracle-db.default.svc.cluster.local:1521/devpdb.k8s\u0026#39; $ kubectl -n sample-domain1-ns label secret \\  sample-domain2-datasource-secret \\  weblogic.domainUID=sample-domain2 Observations:\n We are leaving the namespace sample-domain1-ns unchanged for each secret because you will deploy domain sample-domain2 to the same namespace as sample-domain1. You name and label the secrets using their associated domain UID for two reasons:  To make it obvious which secret belongs to which domain. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all resources associated with a domain.   You use a different set of secrets for the new domain for two reasons:  To make it easier to keep the life cycle and/or CI/CD process for the two domains simple and independent. To \u0026lsquo;future proof\u0026rsquo; the new domain so that changes to the original domain\u0026rsquo;s secrets or new domain\u0026rsquo;s secrets can be independent.   We deliberately specify an incorrect password and a low maximum pool capacity in the data source secret because we will demonstrate dynamically correcting the data source attributes for sample-domain1 in the Update 4 use case.  If you\u0026rsquo;re following the JRF path through the sample, then you also need to deploy the additional secret referenced by macros in the JRF model RCUDbInfo clause, plus an OPSS wallet password secret. For details about the uses of these secrets, see the Model in Image user documentation. Note that we are using the RCU prefix FMW2 for this domain, because the first domain is already using FMW1.\n  Click here for the commands for deploying additional secrets for JRF.   $ kubectl -n sample-domain1-ns create secret generic \\  sample-domain2-rcu-access \\  --from-literal=rcu_prefix=FMW2 \\  --from-literal=rcu_schema_password=Oradoc_db1 \\  --from-literal=rcu_db_conn_string=oracle-db.default.svc.cluster.local:1521/devpdb.k8s $ kubectl -n sample-domain1-ns label secret \\  sample-domain2-rcu-access \\  weblogic.domainUID=sample-domain2 $ kubectl -n sample-domain1-ns create secret generic \\  sample-domain2-opss-wallet-password-secret \\  --from-literal=walletPassword=welcome1 $ kubectl -n sample-domain1-ns label secret \\  sample-domain2-opss-wallet-password-secret \\  weblogic.domainUID=sample-domain2      Set up a Domain YAML file that is similar to your Update 1 use case Domain YAML file but with a different domain UID, domain name, model update ConfigMap reference, and Secret references:\n  Option 1: Update a copy of your Domain YAML file from the Update 1 use case.\n  In the Update 1 use case, we suggested creating a file named /tmp/mii-sample/mii-update1.yaml or using the /tmp/mii-sample/domain-resources/WLS/mii-update1-d1-WLS-v1-ds.yaml file that is supplied with the sample.\n  We suggest copying this Domain YAML file and naming the copy /tmp/mii-sample/mii-update2.yaml before making any changes.\n  Working on a copy is not strictly necessary, but it helps keep track of your work for the different use cases in this sample and provides you a backup of your previous work.\n    Change the /tmp/mii-sample/mii-update2.yaml Domain YAML file name and weblogic.domainUID label to sample-domain2.\nThe final result will look something like this:\napiVersion: \u0026#34;weblogic.oracle/v8\u0026#34; kind: Domain metadata: name: sample-domain2 namespace: sample-domain1-ns labels: weblogic.domainUID: sample-domain2  NOTE: We are leaving the namespace sample-domain1-ns unchanged because you will be deploying domain sample-domain2 to the same namespace as sample-domain1.\n   Change the /tmp/mii-sample/mii-update2.yaml Domain YAML file\u0026rsquo;s CUSTOM_DOMAIN_NAME environment variable from domain1 to domain2.\nThe model file in the image uses macro @@ENV:CUSTOM_DOMAIN_NAME@@ to reference this environment variable when setting its domain name.\nSpecifically, change the corresponding Domain spec.serverPod.env YAML file stanza to look something like this:\n... spec: ... serverPod: ... env: - name: CUSTOM_DOMAIN_NAME value: \u0026#34;domain2\u0026#34; ...   Change the /tmp/mii-sample/mii-update2.yaml Domain YAML file\u0026rsquo;s spec.domainHome value to /u01/domains/sample-domain2. The corresponding YAML file stanza will look something like this:\n... spec: ... domainHome: /u01/domains/sample-domain2 ... (This change is not strictly needed, but it is a helpful convention to decorate a WebLogic domain\u0026rsquo;s home directory with its domain name or domain UID.)\n  Change the /tmp/mii-sample/mii-update2.yaml secret references in the spec.webLogicCredentialsSecret and spec.configuration.secrets stanzas to reference this use case\u0026rsquo;s secrets. Specifically, change:\nspec: ... webLogicCredentialsSecret: name: sample-domain1-weblogic-credentials ... configuration: ... secrets: - sample-domain1-datasource-secret ... model: ... runtimeEncryptionSecret: sample-domain1-runtime-encryption-secret To this:\nspec: ... webLogicCredentialsSecret: name: sample-domain2-weblogic-credentials ... configuration: ... secrets: - sample-domain2-datasource-secret ... model: ... runtimeEncryptionSecret: sample-domain2-runtime-encryption-secret  NOTE: If you are following the JRF path through the sample, similarly change your spec.configuration.opss.walletPasswordSecret and the RCU secret name referenced in spec.configuration.secrets.\n   Change the Domain YAML file\u0026rsquo;s spec.configuration.model.configMap value from sample-domain1-wdt-config-map to sample-domain2-wdt-config-map. The corresponding YAML file stanza will look something like this:\nspec: ... configuration: ... model: ... configMap: sample-domain2-wdt-config-map   Now, compare your original and changed Domain YAML files to double check your changes.\n$ diff /tmp/mii-sample/mii-update1.yaml /tmp/mii-sample/mii-update2.yaml 9c9 \u0026lt; name: sample-domain1 --- \u0026gt; name: sample-domain2 13c13 \u0026lt; weblogic.domainUID: sample-domain1 --- \u0026gt; weblogic.domainUID: sample-domain2 21c21 \u0026lt; domainHome: /u01/domains/sample-domain1 --- \u0026gt; domainHome: /u01/domains/sample-domain2 36c36 \u0026lt; name: sample-domain1-weblogic-credentials --- \u0026gt; name: sample-domain2-weblogic-credentials 46c46 \u0026lt; #logHome: /shared/logs/sample-domain1 --- \u0026gt; #logHome: /shared/logs/sample-domain2 61c61 \u0026lt; value: \u0026quot;domain1\u0026quot; --- \u0026gt; value: \u0026quot;domain2\u0026quot; 71c71 \u0026lt; # claimName: sample-domain1-weblogic-sample-pvc --- \u0026gt; # claimName: sample-domain2-weblogic-sample-pvc 110c110 \u0026lt; configMap: sample-domain1-wdt-config-map --- \u0026gt; configMap: sample-domain2-wdt-config-map 113c113 \u0026lt; runtimeEncryptionSecret: sample-domain1-runtime-encryption-secret --- \u0026gt; runtimeEncryptionSecret: sample-domain2-runtime-encryption-secret 118c118 \u0026lt; - sample-domain1-datasource-secret --- \u0026gt; - sample-domain2-datasource-secret    Click here to see additional \u0026#39;diff\u0026#39; expected for the JRF path through the sample.    ``` \u0026lt; walletPasswordSecret: sample-domain1-opss-wallet-password-secret --- \u0026gt; walletPasswordSecret: sample-domain2-opss-wallet-password-secret 130c130 \u0026lt; #walletFileSecret: sample-domain1-opss-walletfile-secret --- \u0026gt; #walletFileSecret: sample-domain2-opss-walletfile-secret ```     NOTE: The diff should not contain a namespace change. You are deploying domain sample-domain2 to the same namespace as sample-domain1 (namespace sample-domain1-ns).\n   Apply your changed Domain YAML file:\n Note: Before you deploy the Domain YAML file, determine if you have Kubernetes cluster worker nodes that are remote to your local machine. If so, you need to put the Domain\u0026rsquo;s image in a location that these nodes can access and you may also need to modify your Domain YAML file to reference the new location. See Ensuring your Kubernetes cluster can access images.\n $ kubectl apply -f /tmp/mii-sample/mii-update2.yaml     Option 2: Use the updated Domain YAML file that is supplied with the sample:\n Note: Before you deploy the Domain YAML file, determine if you have Kubernetes cluster worker nodes that are remote to your local machine. If so, you need to put the Domain\u0026rsquo;s image in a location that these nodes can access and you may also need to modify your Domain YAML file to reference the new location. See Ensuring your Kubernetes cluster can access images.\n $ kubectl apply -f /tmp/miisample/domain-resources/WLS/mii-update2-d2-WLS-v1-ds.yaml     Wait for sample-domain2 to start.\nIf you run kubectl get pods -n sample-domain1-ns --watch, then you will see the introspector job for sample-domain2 run and your WebLogic Server pods start. The output will look something like this:\n  Click here to expand.   $ kubectl get pods -n sample-domain1-ns --watch NAME READY STATUS RESTARTS AGE sample-domain1-admin-server 1/1 Running 0 5d2h sample-domain1-managed-server1 1/1 Running 1 5d2h sample-domain1-managed-server2 1/1 Running 2 5d2h sample-domain2-introspector-plssr 0/1 Pending 0 0s sample-domain2-introspector-plssr 0/1 Pending 0 0s sample-domain2-introspector-plssr 0/1 ContainerCreating 0 0s sample-domain2-introspector-plssr 1/1 Running 0 2s sample-domain2-introspector-plssr 0/1 Completed 0 69s sample-domain2-introspector-plssr 0/1 Terminating 0 71s sample-domain2-introspector-plssr 0/1 Terminating 0 71s sample-domain2-admin-server 0/1 Pending 0 0s sample-domain2-admin-server 0/1 Pending 0 0s sample-domain2-admin-server 0/1 ContainerCreating 0 0s sample-domain2-admin-server 0/1 Running 0 1s sample-domain2-admin-server 1/1 Running 0 34s sample-domain2-managed-server1 0/1 Pending 0 0s sample-domain2-managed-server1 0/1 Pending 0 0s sample-domain2-managed-server1 0/1 ContainerCreating 0 0s sample-domain2-managed-server2 0/1 Pending 0 0s sample-domain2-managed-server2 0/1 Pending 0 0s sample-domain2-managed-server2 0/1 ContainerCreating 0 0s sample-domain2-managed-server1 0/1 Running 0 1s sample-domain2-managed-server2 0/1 Running 0 1s sample-domain2-managed-server1 1/1 Running 0 45s sample-domain2-managed-server2 1/1 Running 0 45s    For a more detailed view of this activity, you can instead call /tmp/mii-sample/utils/wl-pod-wait.sh -n sample-domain1-ns -d sample-domain2 -p 3. The output will look something like this:\n  Click here to expand.   $ ./wl-pod-wait.sh -n sample-domain1-ns -d sample-domain2 -p 3 @@ [2020-05-13T17:06:00][seconds=1] Info: Waiting up to 1000 seconds for exactly '3' WebLogic Server pods to reach the following criteria: @@ [2020-05-13T17:06:00][seconds=1] Info: ready='true' @@ [2020-05-13T17:06:00][seconds=1] Info: image='model-in-image:WLS-v1' @@ [2020-05-13T17:06:00][seconds=1] Info: domainRestartVersion='1' @@ [2020-05-13T17:06:00][seconds=1] Info: namespace='sample-domain1-ns' @@ [2020-05-13T17:06:00][seconds=1] Info: domainUID='sample-domain2' @@ [2020-05-13T17:06:00][seconds=1] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-13T17:06:00][seconds=1] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------------------- ------- ----- ----- --------- 'sample-domain2-introspector-plssr' '' '' '' 'Running' @@ [2020-05-13T17:07:03][seconds=64] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-13T17:07:03][seconds=64] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------------------- ------- ----- ----- ----------- 'sample-domain2-introspector-plssr' '' '' '' 'Succeeded' @@ [2020-05-13T17:07:06][seconds=67] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-13T17:07:06][seconds=67] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE ---- ------- ----- ----- ----- @@ [2020-05-13T17:07:14][seconds=75] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-13T17:07:14][seconds=75] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE ----------------------------- ------- ----------------------- ------- --------- 'sample-domain2-admin-server' '1' 'model-in-image:WLS-v1' 'false' 'Running' @@ [2020-05-13T17:07:47][seconds=108] Info: '1' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-13T17:07:47][seconds=108] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain2-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain2-managed-server1' '1' 'model-in-image:WLS-v1' 'false' 'Pending' @@ [2020-05-13T17:07:49][seconds=110] Info: '1' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-13T17:07:49][seconds=110] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain2-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain2-managed-server1' '1' 'model-in-image:WLS-v1' 'false' 'Running' 'sample-domain2-managed-server2' '1' 'model-in-image:WLS-v1' 'false' 'Pending' @@ [2020-05-13T17:07:50][seconds=111] Info: '1' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-13T17:07:50][seconds=111] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain2-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain2-managed-server1' '1' 'model-in-image:WLS-v1' 'false' 'Running' 'sample-domain2-managed-server2' '1' 'model-in-image:WLS-v1' 'false' 'Running' @@ [2020-05-13T17:08:32][seconds=153] Info: '2' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-13T17:08:32][seconds=153] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain2-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain2-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain2-managed-server2' '1' 'model-in-image:WLS-v1' 'false' 'Running' @@ [2020-05-13T17:08:34][seconds=155] Info: '3' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-13T17:08:34][seconds=155] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain2-admin-server' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain2-managed-server1' '1' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain2-managed-server2' '1' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-05-13T17:08:34][seconds=155] Info: Success!      After the sample-domain2 domain is running, you can call its sample web application to verify that it\u0026rsquo;s fully active.\nSend a web application request to the ingress controller for sample-domain2:\n$ curl -s -S -m 10 -H \u0026#39;host: sample-domain2-cluster-cluster-1.mii-sample.org\u0026#39; \\  http://localhost:30305/myapp_war/index.jsp Or, if Traefik is unavailable and your domain2 Administration Server pod is running, you can run kubectl exec:\n$ kubectl exec -n sample-domain1-ns sample-domain2-admin-server -- bash -c \\  \u0026#34;curl -s -S -m 10 http://sample-domain2-cluster-cluster-1:8001/myapp_war/index.jsp\u0026#34; You will see something like the following:\n\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v1\u0026#39; of the mii-sample JSP web-app. Welcome to WebLogic Server \u0026#39;managed-server1\u0026#39;! domain UID = \u0026#39;sample-domain2\u0026#39; domain name = \u0026#39;domain2\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found min threads constraint runtime named \u0026#39;SampleMinThreads\u0026#39; with configured count: 1 Found max threads constraint runtime named \u0026#39;SampleMaxThreads\u0026#39; with configured count: 10 Found 1 local data source: Datasource \u0026#39;mynewdatasource\u0026#39;: State=\u0026#39;Running\u0026#39;, testPool=\u0026#39;Failed\u0026#39; ---TestPool Failure Reason--- NOTE: Ignore \u0026#39;mynewdatasource\u0026#39; failures until the sample\u0026#39;s Update 4 use case. --- ... ... invalid host/username/password ... ----------------------------- ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;   A TestPool Failure is expected because we will demonstrate dynamically correcting the data source attributes for sample-domain1 in Update 4.\nIf you see an error other than the expected TestPool Failure, then consult Debugging in the Model in Image user guide.\nYou will not be using the sample-domain2 domain again in this sample; if you wish, you can shut it down now by calling kubectl -n sample-domain1-ns delete domain sample-domain2.\nTo remove the resources you have created in the samples, see Cleanup.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/developerguide/integration-tests/",
	"title": "Integration tests",
	"tags": [],
	"description": "",
	"content": "The project includes integration tests that can be run against a Kubernetes cluster. If you want to use these tests, you will need to provide your own Kubernetes cluster. The Kubernetes cluster must meet the version number requirements and have Helm installed. Ensure that the operator image is in a container registry visible to the Kubernetes cluster.\nYou will need to obtain the kube.config file for an administrative user and make it available on the machine running the build. To run the tests, update the KUBECONFIG environment variable to point to your config file and then execute:\n$ mvn clean verify -P java-integration-tests For more detailed information, see How to run the Java integration tests .\nWhen you run the integrations tests, they do a cleanup of any operator or domains on that cluster.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/developerguide/branching/",
	"title": "Branching",
	"tags": [],
	"description": "",
	"content": "The main branch is protected and contains source for the latest completed features and bug fixes. While this branch contains active work, we expect to keep it always \u0026ldquo;ready to release.\u0026rdquo; Therefore, longer running feature work will be performed on specific branches, such as feature/dynamic-clusters.\nBecause we want to balance separating destabilizing work into feature branches against the possibility of later difficult merges, we encourage developers working on features to pull out any necessary refactoring or improvements that are general purpose into their own shorter-lived branches and create pull requests to main when these smaller work items are completed.\nAll commits to main must pass the integration test suite. Please run these tests locally before submitting a pull request. Additionally, each push to a branch in our GitHub repository triggers a run of a subset of the integration tests with the results visible here.\nPlease submit pull requests to the main branch unless you are collaborating on a feature and have another target branch. Please see details on the Oracle Contributor Agreement (OCA) and guidelines for pull requests in Contribute to the operator.\nWe will create git tags for each release candidate and generally available (GA) release of the operator.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/introduction/get-help/",
	"title": "Get help",
	"tags": [],
	"description": "Where to get help, submit suggestions, or submit issues.",
	"content": "Oracle Slack We have a closely monitored public Slack channel where you can get in touch with us to ask questions about using the operator or give us feedback or suggestions about what features and improvements you would like to see. We can also create dedicated private channels upon request. We would love to hear from you.\nTo join our public channel, please visit this site to get an invitation. The invitation email will include details of how to access our Slack workspace. After you are logged in, please come to #operator and say, \u0026ldquo;hello!\u0026rdquo;\nGitHub issues To quickly get help, we strongly recommend using Oracle Slack. If this is insufficient, then feel free to submit a GitHub Issue on the WebLogic Kubernetes Operator project. All issues are public.\nOracle support To access Oracle support, see WebLogic Server Certifications on Kubernetes in My Oracle Support Doc ID 2349228.1.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/platforms/environments/",
	"title": "Supported platforms",
	"tags": [],
	"description": "See the operator supported environments.",
	"content": "Oracle Cloud Infrastructure The Oracle Global Pricing and Licensing site provides details about licensing practices and policies. WebLogic Server and the operator are supported on \u0026ldquo;Authorized Cloud Environments\u0026rdquo; as defined in this Oracle licensing policy and this list of eligible products.\nThe official document that defines the supported configurations is here.\nIn accordance with these policies, the operator and WebLogic Server are supported on Oracle Cloud Infrastructure using Oracle Container Engine for Kubernetes, or in a cluster running Oracle Linux Container Services for use with Kubernetes on OCI Compute, and on \u0026ldquo;Authorized Cloud Environments\u0026rdquo;.\nOracle Linux Cloud Native Environment (OLCNE) Oracle Linux Cloud Native Environment is a fully integrated suite for the development and management of cloud-native applications. Based on Open Container Initiative (OCI) and Cloud Native Computing Foundation (CNCF) standards, Oracle Linux Cloud Native Environment delivers a simplified framework for installations, updates, upgrades, and configuration of key features for orchestrating microservices.\nWebLogic Server and the WebLogic Kubernetes Operator are certified and supported on Oracle Linux Cloud Native Environment:\n Operator v2.6.0 is certified on OLCNE 1.1 and v3.2.5 is certified on OLCNE 1.3. Operator v3.2.5 provides certified support of OLCNE 1.3 with Kubernetes 1.20.6 and CRI-O 1.20.2.  Microsoft Azure Kubernetes Service Azure Kubernetes Service (AKS) is a hosted Kubernetes environment. The WebLogic Kubernetes Operator, Oracle WebLogic Sever 12c, and Oracle Fusion Middleware Infrastructure 12c are fully supported and certified on Azure Kubernetes Service (as per the documents referenced above).\nAKS support and limitations:\n Both Domain in Image and Domain in PV domain home source types are supported. For Domain in PV, we support Azure Files volumes accessed through a persistent volume claim; see here. Azure Load Balancers are supported when provisioned using a Kubernetes Service of type=LoadBalancer. Oracle databases running in Oracle Cloud Infrastructure are supported for Fusion Middleware Infrastructure MDS data stores only when accessed through an OCI FastConnect. Windows Server containers are not currently supported, only Linux containers.  VMware Tanzu Kubernetes Service Tanzu Kubernetes Grid (TKG) is a managed Kubernetes Service that lets you quickly deploy and manage Kubernetes clusters. The WebLogic Kubernetes Operator and Oracle WebLogic Sever are fully supported and certified on VMware Tanzu Kubernetes Grid Multicloud 1.1.3 (with vSphere 6.7U3).\nTKG support and limitations:\n Both Domain in Image and Model in Image domain home source types are supported. Domain in PV is not supported. VSphere CSI driver supports only volumes with Read-Write-Once policy. This does not allow writing stores on PV.  For applications requiring HA, use JMS and JTA stores in the database.   The ingress used for certification is NGINX, with MetalLB load balancer.  OpenShift Operator 2.0.1+ is certified for use on OpenShift Container Platform 3.11.43+, with Kubernetes 1.11.5+.\nOperator 2.5.0+ is certified for use on OpenShift Container Platform 4.3.0+ with Kubernetes 1.16.2+.\nTo accommodate OpenShift security requirements:\n For security requirements to run WebLogic in OpenShift, see the OpenShift chapter in the Security section. Beginning with operator version 3.3.2, specify the kubernetesPlatorm Helm chart property with value OpenShift. For more information, see Operator Helm configuration values.  Important note about development-focused Kubernetes distributions There are a number of development-focused distributions of Kubernetes, like kind, Minikube, Minishift, and so on. Often these run Kubernetes in a virtual machine on your development machine. We have found that these distributions present some extra challenges in areas like:\n Separate container image caches, making it necessary to save/load images to move them between Docker file systems Default virtual machine file sizes and resource limits that are too small to run WebLogic or hold the necessary images Storage providers that do not always support the features that the operator or WebLogic rely on Load balancing implementations that do not always support the features that the operator or WebLogic rely on  As such, we do not recommend using these distributions to run the operator or WebLogic, and we do not provide support for WebLogic or the operator running in these distributions.\nWe have found that Docker for Desktop does not seem to suffer the same limitations, and we do support that as a development/test option.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/quickstart/install/",
	"title": "Install the operator and ingress controller",
	"tags": [],
	"description": "",
	"content": "Use Helm to install the operator and Traefik ingress controller. First, set up Helm:\n$ helm repo add traefik https://containous.github.io/traefik-helm-chart/ --force-update Create a Traefik ingress controller. Create a namespace for the ingress controller.\n$ kubectl create namespace traefik Use the values.yaml file in the sample but set kubernetes.namespaces specifically.\n$ helm install traefik-operator traefik/traefik \\  --namespace traefik \\  --values kubernetes/samples/charts/traefik/values.yaml \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34; Install the operator.   Create a namespace for the operator:\n$ kubectl create namespace sample-weblogic-operator-ns   Create a service account for the operator in the operator\u0026rsquo;s namespace:\n$ kubectl create serviceaccount -n sample-weblogic-operator-ns sample-weblogic-operator-sa   Use helm to install and start the operator from the directory you just cloned:\n$ helm install sample-weblogic-operator kubernetes/charts/weblogic-operator \\  --namespace sample-weblogic-operator-ns \\  --set image=ghcr.io/oracle/weblogic-kubernetes-operator:3.3.2 \\  --set serviceAccount=sample-weblogic-operator-sa \\  --set \u0026#34;enableClusterRoleBinding=true\u0026#34; \\  --set \u0026#34;domainNamespaceSelectionStrategy=LabelSelector\u0026#34; \\  --set \u0026#34;domainNamespaceLabelSelector=weblogic-operator\\=enabled\u0026#34; \\  --wait This Helm release deploys the operator and configures it to manage Domains in any Kubernetes namespace with the label, \u0026ldquo;weblogic-operator=enabled\u0026rdquo;. Because of the \u0026ldquo;enableClusterRoleBinding\u0026rdquo; option, the operator will have privilege in all Kubernetes namespaces. This simplifies adding and removing managed namespaces as you will only have to adjust labels on those namespaces. If you want to limit the operator\u0026rsquo;s privilege to just the set of namespaces that it will manage, then remove this option, but this will mean that the operator only has privilege in the set of namespaces that match the selection strategy at the time the Helm release was installed or upgraded.\nNote: Prior to version 3.1.0, the operator\u0026rsquo;s Helm chart only supported configuring the namespaces that the operator would manage using a list of namespaces. The chart now supports specifying namespaces using a label selector, regular expression, or list. Review the available Helm configuration values.\n  Verify that the operator\u0026rsquo;s pod is running, by listing the pods in the operator\u0026rsquo;s namespace. You should see one for the operator.\n$ kubectl get pods -n sample-weblogic-operator-ns   Verify that the operator is up and running by viewing the operator pod\u0026rsquo;s log:\n$ kubectl logs -n sample-weblogic-operator-ns -c weblogic-operator deployments/weblogic-operator   "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/domain-lifecycle/introspection/",
	"title": "Domain introspection",
	"tags": [],
	"description": "This document describes domain introspection in the Oracle WebLogic Server in Kubernetes environment.",
	"content": "Contents  Overview When introspection occurs automatically Initiating introspection Failed introspection Introspection use cases  Overview This document describes domain introspection, when it occurs automatically, and how and when to initiate additional introspections of the domain configuration in the Oracle WebLogic Server in Kubernetes environment.\nIn order to manage the operation of WebLogic domains in Kubernetes, the Oracle WebLogic Kubernetes Operator analyzes the WebLogic domain configuration using an \u0026ldquo;introspection\u0026rdquo; job. This Job will be named DOMAIN_UID-introspector, will be run in the same namespace as the Domain, and must successfully complete before the operator will begin to start WebLogic Server instances. Because each of the domain home source types are different (for instance, Domain in PV uses a domain home on a PersistentVolume while Model in Image generates the domain home dynamically from a WDT model), the Pod created by this Job will be as similar as possible to the Pod that will later be generated for the Administration Server. This guarantees that the operator is analyzing the same WebLogic domain configuration that WebLogic Server instances will use.\nIntrospection ensures that:\n The operator is aware of domain topology from the WebLogic domain configuration, including servers, clusters, network access points, listen addresses, and other configurations. The operator can generate configuration overrides to adjust the WebLogic domain configuration to match the Kubernetes environment, such as modifying listen addresses. For Model in Image, the operator can generate the WebLogic domain home, including the final domain configuration. For Domain in PV and Domain in Image, the operator can use any customer-provided configuration overrides along with the operator-generated overrides to generate the final configuration overrides.  When introspection occurs automatically Introspection automatically occurs when:\n The operator is starting a WebLogic Server instance when there are currently no other servers running. This occurs when the operator first starts servers for a domain or when starting servers following a full domain shutdown. For Model in Image, the operator determines that at least one WebLogic Server instance that is currently running must be shut down and restarted. This could be a rolling of one or more clusters, the shut down and restart of one or more WebLogic Server instances, or a combination.  Initiating introspection Sometimes, such as for the use cases described below, it is desirable to explicitly initiate introspection. To initiate introspection, change the value of your Domain introspectVersion field.\nSet introspectVersion to a new value.\nkind: Domain metadata: name: domain1 spec: introspectVersion: \u0026#34;2\u0026#34; ... As with restartVersion, the introspectVersion field has no required format; however, we recommend using a value likely to be unique such as a continually increasing number or a timestamp.\nBeginning with operator 3.1.0, if a domain resource\u0026rsquo;s spec.introspectVersion is set, each of the domain\u0026rsquo;s WebLogic Server pods will have a label with the key weblogic.introspectVersion to indicate the introspectVersion at which the pod is running.\nName: domain1-admin-server Namespace: domain1-ns Labels: weblogic.createdByOperator=true weblogic.domainName=domain1 weblogic.domainRestartVersion=abcdef weblogic.domainUID=domain1 weblogic.introspectVersion=12345 weblogic.serverName=admin-server When a domain\u0026rsquo;s spec.introspectVersion is changed, the weblogic.introspectVersion label of each WebLogic Server pod is updated to the new introspectVersion value, either when the operator restarts the pod or when the operator determines that the pod does not need to be restarted.\nFailed introspection Sometimes the Kubernetes Job, named DOMAIN_UID-introspector, created for the introspection will fail.\nWhen introspection fails, the operator will not start any WebLogic Server instances. If this is not the initial introspection and there are already WebLogic Server instances running, then a failed introspection will leave the existing WebLogic Server instances running without making any changes to the operational state of the domain.\nThe introspection will be periodically retried and then will eventually timeout with the Domain status indicating the processing failed. To recover from a failed state, correct the underlying problem and update the introspectVersion.\nPlease review the details for diagnosing introspection failures related to configuration overrides or Model in Image domain home generation.\nThe introspector log is mirrored to the Domain resource spec.logHome directory when spec.logHome is configured and spec.logHomeEnabled is true.\n Introspection use cases The following sections describe typical use cases for rerunning the introspector.\nAdding clusters or Managed Servers to a Domain in PV configuration When you have an existing WebLogic domain home on a persistent volume (Domain in PV) and you currently have WebLogic Server instances running, it is now possible to define new WebLogic clusters or Managed Servers in the domain configuration and start these new instances without affecting the life cycle of any WebLogic Server instances that are already running.\nPrior to operator 3.0.0, this was not possible because there was no mechanism to initiate introspection other than a full domain shut down and restart and so the operator was unaware of the new clusters or Managed Servers. Now, after updating the domain configuration, you can initiate introspection by changing the introspectVersion.\nFor instance, if you had a domain configuration with a single cluster named \u0026ldquo;cluster-1\u0026rdquo; then your Domain YAML file may have content like this:\nspec: ... clusters: - clusterName: cluster-1 replicas: 3 ... If you modified your WebLogic domain configuration (using the console or WLST) to add a new dynamic cluster named \u0026ldquo;cluster-2\u0026rdquo;, then you could immediately start cluster members of this new cluster by updating your Domain YAML file like this:\nspec: ... clusters: - clusterName: cluster-1 replicas: 3 - clusterName: cluster-2 replicas: 2 introspectVersion: \u0026#34;2\u0026#34; ... When this updated Domain YAML file is applied, the operator will initiate a new introspection of the domain configuration during which it will learn about the additional WebLogic cluster and then the operator will continue to start WebLogic Server instances that are members of this new cluster. In this case, the operator will start two Managed Servers that are members of the cluster named \u0026ldquo;cluster-2\u0026rdquo;.\nDistributing changes to configuration overrides The operator supports customer-provided configuration overrides. These configuration overrides, which are supported with Domain in PV or Domain in Image, allow you to override elements of the domain configuration, such as data source URL\u0026rsquo;s or credentials.\nWith operator 3.0.0, you can now change the configuration overrides and distribute these new configuration overrides to already running WebLogic Server instances. To do this, update the ConfigMap that contains the configuration overrides or update one or more of the Secrets referenced by those configuration overrides and then initiate introspection by changing the introspectVersion field.\nWe have introduced a new field, called overrideDistributionStrategy and located under configuration, that controls whether updated configuration overrides are distributed dynamically to already running WebLogic Server instances or if the new configuration overrides are only applied when servers are started or restarted.\nThe default value for overrideDistributionStrategy is DYNAMIC, which means that new configuration overrides are distributed dynamically to already running WebLogic Server instances.\nAlternately, you can set overrideDistributionStrategy to ON_RESTART, which means that the new configuration overrides will not be distributed to already running WebLogic Server instances, but will instead be applied only to servers as they start or restart. Use of this value will not cause WebLogic Server instances to restart absent changes to other fields, such as restartVersion.\nChanges to configuration overrides distributed to running WebLogic Server instances can only take effect if the corresponding WebLogic configuration MBean attribute is \u0026ldquo;dynamic\u0026rdquo;. For instance, the Data Source \u0026ldquo;passwordEncrypted\u0026rdquo; attribute is dynamic while the \u0026ldquo;Url\u0026rdquo; attribute is non-dynamic.\n Distributing changes to running Model in Image domains The operator supports rerunning the introspector in order to propagate model updates to a running Model in Image domain.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/faq/node-heating/",
	"title": "Node heating problem",
	"tags": [],
	"description": "The operator creates a Pod for each WebLogic Server instance that is started. The Kubernetes Scheduler then selects a Node for each Pod. Because the default scheduling algorithm gives substantial weight to selecting a Node where the necessary container images have already been pulled, this often results in Kubernetes running many of the Pods for WebLogic Server instances on the same Node while other Nodes are not fairly utilized. This is commonly known as the Node heating problem.",
	"content": "The WebLogic Kubernetes Operator creates a Pod for each WebLogic Server instance that is started. The Kubernetes Scheduler then selects a Node for each Pod. Because the default scheduling algorithm gives substantial weight to selecting a Node where the necessary container images have already been pulled, this often results in Kubernetes running many of the Pods for WebLogic Server instances on the same Node while other Nodes are not fairly utilized. This is commonly known as the \u0026ldquo;Node heating problem.\u0026rdquo;\nOne solution is to ensure that all necessary container images are available on worker Nodes as part of node provisioning. When the necessary container images are available on each worker Node, the Kubernetes Scheduler will instead select a Node based on other factors such as available CPU and memory or a simple round-robin.\nThe operator team recommends a different solution that is based on inter-pod affinity and anti-affinity. This solution has the advantage of both resolving the Node heating problem and of explicitly directing the Kubernetes Scheduler to spread the Pods for WebLogic Server instances from a given cluster or domain more widely across the available Nodes. Inter-pod affinity and anti-affinity are features of the Kubernetes Scheduler that allow the scheduler to choose a Node for a new Pod based on details of the Pods that are already running. For WebLogic Server use cases, the intent will often be for anti-affinity with the Pods for other WebLogic Server instances so that server instances spread over the available Nodes.\nTo use these features, edit the Domain Custom Resource to add content to the serverPod element, in this case at the scope of a cluster, as shown in the following example:\nclusters: - clusterName: cluster-1 serverStartState: \u0026#34;RUNNING\u0026#34; serverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026#34;weblogic.clusterName\u0026#34; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; Because the serverPod element here is scoped to a cluster, the content of the affinity element will be added to the Pod generated for each WebLogic Server instance that is a member of this WebLogic cluster. This inter-pod anti-affinity statement expresses a preference that the scheduler select a Node for the new Pod avoiding, as much as possible, Nodes that already have Pods with the label \u0026ldquo;weblogic.clusterName\u0026rdquo; and the name of this cluster. Note that the weight is set to 100, which is the maximum weight, so that this term will outweigh any possible preference for a Node based on availability of container images.\nIt is possible to express many other scheduling preferences or constraints. The following example similarly expresses an anti-affinity, but changes the test to have all WebLogic Server instances in the domain prefer to run on Nodes where there is not already a Pod for a running instance:\nserverPod: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026#34;weblogic.domainUID\u0026#34; operator: In values: - $(DOMAIN_UID) topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; Details about how the operator generates Pods for WebLogic Server instances, including details about labels and variable substitution, are available here.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/model-in-image/",
	"title": "Model in Image",
	"tags": [],
	"description": "",
	"content": "  Overview  Introduction to Model in Image, description of its runtime behavior, and references.\n Usage  Steps for creating and deploying Model in Image images and their associated Domain YAML files.\n Auxiliary images  Auxiliary images are an alternative approach for supplying a domain\u0026#39;s model files or other types of files.\n Model files  Model file requirements, macros, and loading order.\n Runtime updates  Updating a running Model in Image domain\u0026#39;s images and model files.\n Debugging  Debugging a deployed Model in Image domain.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/domains/fmw-domain/",
	"title": "FMW Infrastructure domain on a PV",
	"tags": [],
	"description": "Sample for creating an FMW Infrastructure domain home on an existing PV or PVC, and the domain resource YAML file for deploying the generated WebLogic domain.",
	"content": "The sample scripts demonstrate the creation of an FMW Infrastructure domain home on an existing Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC). The scripts also generate the domain resource YAML file, which can then be used to start the Kubernetes artifacts of the corresponding domain. Optionally, the scripts start up the domain, and WebLogic Server pods and services.\nPrerequisites Before you begin, read this document, Domain resource.\nThe following prerequisites must be met prior to running the create domain script:\n Make sure the WebLogic Kubernetes Operator is running. The operator requires an image with either FMW Infrastructure 12.2.1.3.0 with patch 29135930 applied or FMW Infrastructure 12.2.1.4.0. For details on how to obtain or create the image, refer to FMW Infrastructure domains. Create a Kubernetes Namespace for the domain unless you intend to use the default namespace. In the same Kubernetes Namespace, create the Kubernetes PersistentVolume (PV) where the domain home will be hosted, and the Kubernetes PersistentVolumeClaim (PVC) for the domain. For samples to create a PV and PVC, see Create sample PV and PVC. By default, the create-domain.sh script creates a domain with the domainUID set to domain1 and expects the PVC domain1-weblogic-sample-pvc to be present. You can create domain1-weblogic-sample-pvc using create-pv-pvc.sh with an inputs file that has the domainUID set to domain1. Create the Kubernetes Secrets username and password of the administrative account in the same Kubernetes namespace as the domain. Unless you are creating a Restricted-JRF domain, you also need to:  Configure access to your database. For details, see here. Create a Kubernetes Secret with the RCU credentials. For details, refer to this document.    Use the script to create a domain The sample for creating domains is in this directory:\n$ cd kubernetes/samples/scripts/create-fmw-infrastructure-domain/domain-home-on-pv Make a copy of the create-domain-inputs.yaml file, update it with the correct values, and run the create script, pointing it at your inputs file and an output directory:\n$ ./create-domain.sh \\  -i create-domain-inputs.yaml \\  -o /\u0026lt;path to output-directory\u0026gt;  The create-domain.sh script and its inputs file are for demonstration purposes only; its contents and the domain resource file that it generates for you might change without notice. In production, we strongly recommend that you use the WebLogic Image Tool and WebLogic Deploy Tooling (when applicable), and directly work with domain resource files instead.\n The script will perform the following steps:\n  Create a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The path name is /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, its contents must be removed before using this script.\n  Create a Kubernetes Job that will start up a utility FMW Infrastructure container and run offline WLST scripts to create the domain on the shared storage.\n  Run and wait for the job to finish.\n  Create a Kubernetes domain resource YAML file, domain.yaml, in the directory that is created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command:\n$ kubectl apply -f /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml   Create a convenient utility script, delete-domain-job.yaml, to clean up the domain home created by the create script.\n  As a convenience, using the -e option, the script can optionally create the domain object, which in turn results in the creation of the corresponding WebLogic Server pods and services as well.\nThe usage of the create script is as follows:\n$ sh create-domain.sh -h usage: create-domain.sh -o dir -i file [-e] [-v] [-h] -i Parameter inputs file, must be specified. -o Output directory for the generated YAML files, must be specified. -e Also create the resources in the generated YAML files, optional. -v Validate the existence of persistentVolumeClaim, optional. -h Help If you copy the sample scripts to a different location, make sure that you copy everything in the \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts directory together into the target directory, maintaining the original directory hierarchy.\nThe default domain created by the script has the following characteristics:\n An Administration Server named admin-server listening on port 7001. A configured cluster named cluster-1 of size 5. Five Managed Servers, named managed-server1, managed-server2, and so on, listening on port 8001. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;. No applications deployed. No data sources or JMS resources. A T3 channel.  The domain creation inputs can be customized by editing create-domain-inputs.yaml.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Definition Default     adminPort Port number of the Administration Server inside the Kubernetes cluster. 7001   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. admin-server   clusterName Name of the WebLogic cluster instance to generate for the domain. cluster-1   configuredManagedServerCount Number of Managed Server instances to generate for the domain. 5   createDomainFilesDir Directory on the host machine to locate all the files to create a WebLogic domain, including the script that is specified in the createDomainScriptName property. By default, this directory is set to the relative path wlst, and the create script will use the built-in WLST offline scripts in the wlst directory to create the WebLogic domain. It can also be set to the relative path wdt, and then the built-in WDT scripts will be used instead. An absolute path is also supported to point to an arbitrary directory in the file system. The built-in scripts can be replaced by the user-provided scripts or model files as long as those files are in the specified directory. Files in this directory are put into a Kubernetes ConfigMap, which in turn is mounted to the createDomainScriptsMountPath, so that the Kubernetes Pod can use the scripts and supporting files to create a domain home. wlst   createDomainScriptsMountPath Mount path where the create domain scripts are located inside a pod. The create-domain.sh script creates a Kubernetes Job to run the script (specified in the createDomainScriptName property) in a Kubernetes Pod to create a domain home. Files in the createDomainFilesDir directory are mounted to this location in the pod, so that the Kubernetes Pod can use the scripts and supporting files to create a domain home. /u01/weblogic   createDomainScriptName Script that the create domain script uses to create a WebLogic domain. The create-domain.sh script creates a Kubernetes Job to run this script to create a domain home. The script is located in the in-pod directory that is specified in the createDomainScriptsMountPath property. If you need to provide your own scripts to create the domain home, instead of using the built-it scripts, you must use this property to set the name of the script that you want the create domain job to run. create-domain-job.sh   domainHome Home directory of the WebLogic domain. If not specified, the value is derived from the domainUID as /shared/domains/\u0026lt;domainUID\u0026gt;. /shared/domains/domain1   domainPVMountPath Mount path of the domain persistent volume. /shared   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Domain. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes Service name. domain1   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   fmwDomainType FMW Infrastructure Domain Type. Legal values are JRF or RestrictedJRF. JRF   httpAccessLogInLogHome Boolean indicating if server HTTP access log files should be written to the same directory as logHome. Otherwise, server HTTP access log files will be written to the directory specified in the WebLogic domain home configuration. true   image FMW Infrastructure image. The operator requires FMW Infrastructure 12.2.1.3.0 with patch 29135930 applied or FMW Infrastructure 12.2.1.4.0. For details on how to obtain or create the image, see FMW Infrastructure domains. container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4   imagePullPolicy WebLogic Server image pull policy. Legal values are IfNotPresent, Always, or Never. IfNotPresent   imagePullSecretName Name of the Kubernetes Secret to access the container registry to pull the WebLogic Server image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server .out in the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to start initially for the domain. 2   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, Node Manager log, introspector out, and server HTTP access log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /shared/logs/domain1   managedServerNameBase Base string used to generate Managed Server names. managed-server   managedServerPort Port number for each Managed Server. 8001   namespace Kubernetes Namespace in which to create the domain. default   persistentVolumeClaimName Name of the persistent volume claim. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. domain1-weblogic-sample-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the network access point. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only, in a single server (all-in-one) Kubernetes Deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster.   weblogicCredentialsSecretName Name of the Kubernetes Secret for the Administration Server user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. domain1-weblogic-credentials   serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.   rcuSchemaPrefix The schema prefix to use in the database, for example SOA1. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. domain1   rcuDatabaseURL The database URL. database:1521/service   rcuCredentialsSecret The Kubernetes Secret containing the database credentials. domain1-rcu-credentials    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the create-inputs.yaml file. Those properties include the adminServerName, clusterName, and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes Service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create a WebLogic domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain resource YAML file could also be modified to cover more use cases.\nVerify the results The create script will verify that the domain was created, and will report failure if there was any error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nNote that the example results below use the default Kubernetes Namespace. If you are using a different namespace, you need to replace NAMESPACE in the example kubectl commands with the actual Kubernetes Namespace.\nGenerated YAML files with the default inputs The content of the generated domain.yaml:\n# Copyright (c) 2017, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026#34;weblogic.oracle/v8\u0026#34; kind: Domain metadata: name: fmw-domain namespace: default labels: weblogic.domainUID: fmw-domain spec: # The WebLogic Domain Home domainHome: /shared/domains/fmw-domain # Set domain home type to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: PersistentVolume # The WebLogic Server image that the Operator uses to start the domain image: \u0026#34;container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4\u0026#34; # imagePullPolicy defaults to \u0026#34;Always\u0026#34; if image version is :latest imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: fmw-domain-weblogic-credentials # Whether to include the server out file into the pod\u0026#39;s stdout, default is true includeServerOutInPodLog: true # Whether to enable log home logHomeEnabled: true # The in-pod location for domain log, server logs, server out, introspector out, and Node Manager log files logHome: /shared/logs/fmw-domain # serverStartPolicy legal values are \u0026#34;NEVER\u0026#34;, \u0026#34;IF_NEEDED\u0026#34;, or \u0026#34;ADMIN_ONLY\u0026#34; # This determines which WebLogic Servers the Operator will start up when it discovers this Domain # - \u0026#34;NEVER\u0026#34; will not start any server in the domain # - \u0026#34;ADMIN_ONLY\u0026#34; will start up only the administration server (no managed servers will be started) # - \u0026#34;IF_NEEDED\u0026#34; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026#34;IF_NEEDED\u0026#34; serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false\u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-Djava.security.egd=file:/dev/./urandom \u0026#34; volumes: - name: weblogic-domain-storage-volume persistentVolumeClaim: claimName: fmw-domain-weblogic-pvc volumeMounts: - mountPath: /shared name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026#34;RUNNING\u0026#34; or \u0026#34;ADMIN\u0026#34; # \u0026#34;RUNNING\u0026#34; means the listed server will be started up to \u0026#34;RUNNING\u0026#34; mode # \u0026#34;ADMIN\u0026#34; means the listed server will be start up to \u0026#34;ADMIN\u0026#34; mode serverStartState: \u0026#34;RUNNING\u0026#34; adminService: channels: # The Admin Server\u0026#39;s NodePort - channelName: default nodePort: 30731 # Uncomment to export the T3Channel as a service # - channelName: T3Channel # clusters is used to configure the desired behavior for starting member servers of a cluster.  # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: cluster-1 serverStartState: \u0026#34;RUNNING\u0026#34; replicas: 2 # The number of managed servers to start for unlisted clusters # replicas: 1 Verify the domain To confirm that the domain was created, use this command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\nHere is an example of the output of this command:\nName: fmw-domain Namespace: default Labels: weblogic.domainUID=fmw-domain Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026quot;apiVersion\u0026quot;:\u0026quot;weblogic.oracle/v4\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Domain\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;labels\u0026quot;:{\u0026quot;weblogic.domainUID\u0026quot;:\u0026quot;fmw-domain\u0026quot;,\u0026quot;weblogic.reso... API Version: weblogic.oracle/v4 Kind: Domain Metadata: Creation Timestamp: 2019-04-18T00:11:15Z Generation: 23 Resource Version: 5904947 Self Link: /apis/weblogic.oracle/v4/namespaces/default/domains/fmw-domain UID: 7b3477e2-616e-11e9-ab7b-000017024fa2 Spec: Admin Server: Admin Service: Annotations: Channels: Channel Name: default Node Port: 30731 Labels: Server Pod: Annotations: Container Security Context: Containers: Env: Init Containers: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Clusters: Cluster Name: cluster-1 Cluster Service: Annotations: Labels: Replicas: 4 Server Pod: Annotations: Container Security Context: Containers: Env: Init Containers: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Volumes: Server Service: Annotations: Labels: Server Start State: RUNNING Domain Home: /shared/domains/fmw-domain Domain Home In Image: false Image: container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Log Home: /shared/logs/fmw-domain Log Home Enabled: true Managed Servers: Server Pod: Annotations: Container Security Context: Containers: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom Init Containers: Labels: Liveness Probe: Node Selector: Pod Security Context: Readiness Probe: Resources: Limits: Requests: Volume Mounts: Mount Path: /shared Name: weblogic-domain-storage-volume Volumes: Name: weblogic-domain-storage-volume Persistent Volume Claim: Claim Name: fmw-domain-weblogic-pvc Server Service: Annotations: Labels: Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: fmw-domain-weblogic-credentials Status: Conditions: Last Transition Time: 2019-04-18T00:14:34.322Z Reason: ServersReady Status: True Type: Available Modified: true Replicas: 4 Servers: Cluster Name: cluster-1 Health: Activation Time: 2019-04-18T00:18:46.787Z Overall Health: ok Subsystems: Node Name: mark Server Name: managed-server4 State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-04-18T00:18:46.439Z Overall Health: ok Subsystems: Node Name: mark Server Name: managed-server3 State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-04-18T00:14:19.227Z Overall Health: ok Subsystems: Node Name: mark Server Name: managed-server2 State: RUNNING Cluster Name: cluster-1 Health: Activation Time: 2019-04-18T00:14:17.747Z Overall Health: ok Subsystems: Node Name: mark Server Name: managed-server1 State: RUNNING Health: Activation Time: 2019-04-18T00:13:13.836Z Overall Health: ok Subsystems: Node Name: mark Server Name: admin-server State: RUNNING Start Time: 2019-04-18T00:11:15.306Z Events: \u0026lt;none\u0026gt; In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued very soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. The operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Use the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE fmw-domain-admin-server 1/1 Running 0 15h fmw-domain-managed-server1 1/1 Running 0 15h fmw-domain-managed-server2 1/1 Running 0 15h fmw-domain-managed-server3 1/1 Running 0 15h fmw-domain-managed-server4 1/1 Running 0 15h Verify the services Use the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command:\n$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE fmw-domain-admin-server ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 15h fmw-domain-admin-server-ext NodePort 10.101.26.42 \u0026lt;none\u0026gt; 7001:30731/TCP 15h fmw-domain-cluster-cluster-1 ClusterIP 10.107.55.188 \u0026lt;none\u0026gt; 8001/TCP 15h fmw-domain-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 15h fmw-domain-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 15h fmw-domain-managed-server3 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 15h fmw-domain-managed-server4 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 15h Delete the generated domain home Sometimes in production, but most likely in testing environments, you might want to remove the domain home that is generated using the create-domain.sh script. Do this by running the generated delete domain job script in the /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt; directory.\n$ kubectl create -f delete-domain-job.yaml "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/cicd/how-to-copy-domains/",
	"title": "Copy domains",
	"tags": [],
	"description": "How to copy domains.",
	"content": "The recommended approach to save a copy of a Domain in Image or Domain in PV domain is to simply ZIP (or tar) the domain directory. However, there is a very important caveat with this recommendation - when you unzip the domain, it must go back into exactly the same location (Domain Home) in the (new) file system. Using this approach will maintain the same domain encryption key.\nThe best practice/recommended approach is to create a \u0026ldquo;primordial domain\u0026rdquo; which does not contain any applications or resources, and to create a ZIP file of this domain before starting any servers.\n The domain ZIP file must be created before starting servers.\n When servers are started the first time, they will encrypt various other data. Make sure that you create the ZIP file before starting servers for the first time. The primordial domain ZIP file should be stored in a safe place where the CI/CD can get it when needed, for example in a secured Artifactory repository (or something similar).\nRemember, anyone who gets access to this ZIP file can get access to the domain encryption key, so it needs to be protected appropriately.\n Every time you run your CI/CD pipeline to create a new mutation of the domain, it should retrieve and unzip the primordial domain first, and then apply changes to that domain using tools like WDT or WLST (see here).\n Always use external state.\n You should always keep state outside the image. This means that you should use JDBC stores for leasing tables, JMS and Transaction stores, EJB timers, JMS queues, and so on. This ensures that data will not be lost when a container is destroyed.\nWe recommend that state be kept in a database to take advantage of built-in database server HA, and the fact that disaster recovery of sites across all but the shortest distances almost always requires using a single database server to consolidate and replicate data (DataGuard).\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/ingress/",
	"title": "Ingress",
	"tags": [],
	"description": "Ingress controllers and load balancer sample scripts.",
	"content": "The WebLogic Kubernetes Operator supports NGINX, Traefik, Voyager, and Apache. We provide samples that demonstrate how to install and configure each one.\nFor production environments, we recommend NGINX, Voyager, Traefik (2.2.1 or later) ingress controllers, Apache, or the load balancer provided by your cloud provider.\n The samples are located in following folders:\n Traefik Voyager NGINX Apache-samples/custom-sample Apache-samples/default-sample Ingress-per-domain Apache-webtier  The Apache-webtier script contains a Helm chart that is used in the Apache samples.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/domains/model-in-image/update3/",
	"title": "Update 3",
	"tags": [],
	"description": "",
	"content": "The Update 3 use case demonstrates deploying an updated WebLogic application to the running Update 1 use case domain using an updated image.\nIn the use case, you will:\n Create an image model-in-image:WLS-v2 that is similar to the currently active model-in-image:WLS-v1 image, but with the following updates:  An updated web application v2 at the myapp-v2 directory path within the WDT application archive instead of myapp-v1. An updated model YAML file within the image that points to the new web application path.   Apply an updated Domain YAML file that references the new image while still referencing the original Update 1 use case secrets and model ConfigMap.  After the updated Domain YAML file is applied, the operator will:\n Rerun the introspector job and generate a new domain home based on the new model. Restart the domain\u0026rsquo;s Administration Server pod so that it loads the new image and new domain home. Roll the domain\u0026rsquo;s cluster servers one at a time so that they each load the new image, new domain home, and revised application.  Finally, you will call the application to verify that its revision is active.\nNote that the old version of the application v1 remains in the new image\u0026rsquo;s archive but is unused. We leave it there to demonstrate that the old version can remain in case you want to revert to it. After the new image is applied, you can revert by modifying your model\u0026rsquo;s configuration.model.configMap to override the related application path in your image model.\nHere are the steps for this use case:\n  Make sure you have deployed the domain from the Update 1 use case.\n  Create an updated image.\nRecall that a goal of the Initial use case was to demonstrate using the WebLogic Image Tool to create an image named model-in-image:WLS-v1 from files that were staged in /tmp/mii-sample/model-images/model-in-image:WLS-v1/. The staged files included a web application in a WDT ZIP archive, and WDT model configuration for a WebLogic Administration Server called admin-server and a WebLogic cluster called cluster-1. The final image was called model-in-image:WLS-v1 and, in addition to having a copy of the staged files in its /u01/wdt/models directory, also contained a WebLogic installation and a WebLogic Deploy Tooling installation.\nIn this use case, you will follow similar steps to the Initial use case in order to create a new image with an updated application and model, plus deploy the updated model and application to the running Update 1 use case domain.\n  Understanding your updated WDT archive.\nThe updated archive for this use case is in directory /tmp/mii-sample/archives/archive-v2. You will use it to create an archive ZIP file for the image. This archive is similar to the /tmp/mii-sample/archives/archive-v1 from the Initial use case with the following differences:\n It includes an updated version of the application in ./wlsdeploy/applications/myapp-v2 (while keeping the original application in directory ./wlsdeploy/applications/myapp-v1). The application in ./wlsdeploy/applications/myapp-v2/myapp_war/index.jsp contains a single difference from the original application: it changes the line out.println(\u0026quot;Hello World! This is version 'v1' of the mii-sample JSP web-app.\u0026quot;); to out.println(\u0026quot;Hello World! This is version 'v2' of the mii-sample JSP web-app.\u0026quot;);.  For additional information about archives, see Understanding your first archive in the Initial use case.\n  Stage a ZIP file of the WDT archive.\n Note: If you are using JRF in this sample, substitute JRF for each occurrence of WLS in the paths below.\n When you create your updated image, you will use the files in staging directory /tmp/mii-sample/model-in-image__WLS-v2. In preparation, you need it to contain a ZIP file of the new WDT application archive.\nRun the following commands to create your application archive ZIP file and put it in the expected directory:\n# Delete existing archive.zip in case we have an old leftover version $ rm -f /tmp/mii-sample/model-images/model-in-image__WLS-v2/archive.zip # Move to the directory which contains the source files for our new archive $ cd /tmp/mii-sample/archives/archive-v2 # Zip the archive to the location will later use when we run the WebLogic Image Tool $ zip -r /tmp/mii-sample/model-images/model-in-image__WLS-v2/archive.zip wlsdeploy   Understanding your staged model files.\nThe WDT model YAML file and properties for this use case have already been staged for you to directory /tmp/mii-sample/model-in-image__WLS-v2.\nThe model.10.yaml file in this directory has an updated path wlsdeploy/applications/myapp-v2 that references the updated web application in your archive, but is otherwise identical to the model staged for the original image. The final related YAML file stanza looks like this:\nappDeployments: Application: myapp: SourcePath: \u0026#39;wlsdeploy/applications/myapp-v2\u0026#39; ModuleType: ear Target: \u0026#39;cluster-1\u0026#39; If you would like to review the entire original model before this change, see Staging model files in the Initial use case.\n  Create a new image from your staged model files using WIT.\n Note: If you are using JRF in this sample, substitute JRF for each occurrence of WLS in the imagetool command line below, plus substitute container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 for the --fromImage value.\n At this point, you have staged all of the files needed for image model-in-image:WLS-v2; they include:\n /tmp/mii-sample/model-images/weblogic-deploy.zip /tmp/mii-sample/model-images/model-in-image__WLS-v2/model.10.yaml /tmp/mii-sample/model-images/model-in-image__WLS-v2/model.10.properties /tmp/mii-sample/model-images/model-in-image__WLS-v2/archive.zip  If you don\u0026rsquo;t see the weblogic-deploy.zip file, then you missed a step in the prerequisites.\nNow, you use the Image Tool to create an image named model-in-image:WLS-v2 that\u0026rsquo;s layered on a base WebLogic image. You\u0026rsquo;ve already set up this tool during the prerequisite steps.\nRun the following commands to create the model image and verify that it worked:\n$ cd /tmp/mii-sample/model-images $ ./imagetool/bin/imagetool.sh update \\  --tag model-in-image:WLS-v2 \\  --fromImage container-registry.oracle.com/middleware/weblogic:12.2.1.4 \\  --wdtModel ./model-in-image__WLS-v2/model.10.yaml \\  --wdtVariables ./model-in-image__WLS-v2/model.10.properties \\  --wdtArchive ./model-in-image__WLS-v2/archive.zip \\  --wdtModelOnly \\  --wdtDomainType WLS \\  --chown oracle:root If you don\u0026rsquo;t see the imagetool directory, then you missed a step in the prerequisites.\nThis command runs the WebLogic Image Tool in its Model in Image mode, and does the following:\n Builds the final container image as a layer on the container-registry.oracle.com/middleware/weblogic:12.2.1.4 base image. Copies the WDT ZIP file that\u0026rsquo;s referenced in the WIT cache into the image.  Note that you cached WDT in WIT using the keyword latest when you set up the cache during the sample prerequisites steps. This lets WIT implicitly assume it\u0026rsquo;s the desired WDT version and removes the need to pass a -wdtVersion flag.   Copies the specified WDT model, properties, and application archives to image location /u01/wdt/models.  When the command succeeds, it will end with output like the following:\n[INFO ] Build successful. Build time=36s. Image tag=model-in-image:WLS-v2 Also, if you run the docker images command, then you will see an image named model-in-image:WLS-v2.\n Note: If you have Kubernetes cluster worker nodes that are remote to your local machine, then you need to put the image in a location that these nodes can access. See Ensuring your Kubernetes cluster can access images.\n     Deploy resources - Introduction   Set up and apply a Domain YAML file that is similar to your Update 1 use case Domain YAML file but with a different image:\n Note: If you are using JRF in this sample, substitute JRF for each occurrence of WLS in the paths, files, and image names below.\n   Option 1: Update a copy of your Domain YAML file from the Update 1 use case.\n  In the Update 1 use case, we suggested creating a file named /tmp/mii-sample/mii-update1.yaml or using the /tmp/mii-sample/domain-resources/WLS/mii-update1-d1-WLS-v1-ds.yaml file that is supplied with the sample.\n  We suggest copying this Domain YAML file and naming the copy /tmp/mii-sample/mii-update3.yaml before making any changes.\n  Working on a copy is not strictly necessary, but it helps keep track of your work for the different use cases in this sample and provides you a backup of your previous work.\n    Change the /tmp/mii-sample/mii-update3.yaml Domain YAML file\u0026rsquo;s image field to reference model-in-image:WLS-v2 instead of model-in-image:WLS-v1.\nThe final result will look something like this:\n... spec: ... image: \u0026#34;model-in-image:WLS-v2\u0026#34;   Apply your changed Domain YAML file:\n Note: Before you deploy the domain custom resource, determine if you have Kubernetes cluster worker nodes that are remote to your local machine. If so, you need to put the Domain YAML file\u0026rsquo;s image in a location that these nodes can access and you may also need to modify your Domain YAML file to reference the new location. See Ensuring your Kubernetes cluster can access images.\n $ kubectl apply -f /tmp/mii-sample/mii-update3.yaml     Option 2: Use the updated Domain YAML file that is supplied with the sample:\n Note: Before you deploy the Domain YAML file, determine if you have Kubernetes cluster worker nodes that are remote to your local machine. If so, you need to put the Domain YAML file\u0026rsquo;s image in a location that these nodes can access and you may also need to modify your Domain YAML file to reference the new location. See Ensuring your Kubernetes cluster can access images.\n $ kubectl apply -f /tmp/miisample/domain-resources/WLS/mii-update3-d1-WLS-v2-ds.yaml     Wait for the roll to complete.\nNow that you\u0026rsquo;ve applied a Domain YAML file with an updated image, the operator will automatically rerun the domain\u0026rsquo;s introspector job in order to generate a new domain home, and then will restart (\u0026lsquo;roll\u0026rsquo;) each of the domain\u0026rsquo;s pods so that they use the new domain home and the new image. You\u0026rsquo;ll need to wait for this roll to complete before you can verify that the new image and its associated new application have been deployed.\n  One way to do this is to call kubectl get pods -n sample-domain1-ns --watch and wait for the pods to cycle back to their ready state.\n  Alternatively, you can run /tmp/mii-sample/utils/wl-pod-wait.sh -p 3. This is a utility script that provides useful information about a domain\u0026rsquo;s pods and waits for them to reach a ready state, reach their target restartVersion, and reach their target image before exiting.\n  Click here to display the `wl-pod-wait.sh` usage.   $ ./wl-pod-wait.sh -?  Usage: wl-pod-wait.sh [-n mynamespace] [-d mydomainuid] \\ [-p expected_pod_count] \\ [-t timeout_secs] \\ [-q] Exits non-zero if 'timeout_secs' is reached before 'pod_count' is reached. Parameters: -d \u0026lt;domain_uid\u0026gt; : Defaults to 'sample-domain1'. -n \u0026lt;namespace\u0026gt; : Defaults to 'sample-domain1-ns'. -p 0 : Wait until there are no running WebLogic Server pods for a domain. The default. -p \u0026lt;pod_count\u0026gt; : Wait until all of the following are true for exactly 'pod_count' WebLogic Server pods in the domain: - ready - same 'weblogic.domainRestartVersion' label value as the domain resource's 'spec.restartVersion' - same 'weblogic.introspectVersion' label value as the domain resource's 'spec.introspectVersion' - same image as the the domain resource's image -t \u0026lt;timeout\u0026gt; : Timeout in seconds. Defaults to '1000'. -q : Quiet mode. Show only a count of wl pods that have reached the desired criteria. -? : This help.      Click here to view sample output from `wl-pod-wait.sh` that shows a rolling domain.   $ ./wl-pod-wait.sh -n sample-domain1-ns -d sample-domain1 -p 3 @@ [2020-05-14T17:28:47][seconds=1] Info: Waiting up to 1000 seconds for exactly '3' WebLogic Server pods to reach the following criteria: @@ [2020-05-14T17:28:47][seconds=1] Info: ready='true' @@ [2020-05-14T17:28:47][seconds=1] Info: image='model-in-image:WLS-v2' @@ [2020-05-14T17:28:47][seconds=1] Info: domainRestartVersion='1' @@ [2020-05-14T17:28:47][seconds=1] Info: namespace='sample-domain1-ns' @@ [2020-05-14T17:28:47][seconds=1] Info: domainUID='sample-domain1' @@ [2020-05-14T17:28:47][seconds=1] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-14T17:28:47][seconds=1] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-introspector-g5kzn' '' '' '' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '2' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-05-14T17:29:39][seconds=53] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-14T17:29:39][seconds=53] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '2' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-05-14T17:29:50][seconds=64] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-14T17:29:50][seconds=64] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '2' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-05-14T17:29:58][seconds=72] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-14T17:29:58][seconds=72] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v2' 'false' 'Pending' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '2' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-05-14T17:29:59][seconds=73] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-14T17:29:59][seconds=73] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v2' 'false' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '2' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-05-14T17:30:30][seconds=104] Info: '1' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-14T17:30:30][seconds=104] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'true' 'Running' 'sample-domain1-managed-server2' '2' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-05-14T17:31:13][seconds=147] Info: '1' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-14T17:31:13][seconds=147] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server1' '2' 'model-in-image:WLS-v1' 'false' 'Running' 'sample-domain1-managed-server2' '2' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-05-14T17:31:15][seconds=149] Info: '1' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-14T17:31:15][seconds=149] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server2' '2' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-05-14T17:31:41][seconds=175] Info: '1' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-14T17:31:41][seconds=175] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v2' 'false' 'Pending' 'sample-domain1-managed-server2' '2' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-05-14T17:31:42][seconds=176] Info: '1' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-14T17:31:42][seconds=176] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v2' 'false' 'Running' 'sample-domain1-managed-server2' '2' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-05-14T17:32:21][seconds=215] Info: '2' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-14T17:32:21][seconds=215] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server2' '2' 'model-in-image:WLS-v1' 'true' 'Running' @@ [2020-05-14T17:32:31][seconds=225] Info: '2' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-14T17:32:31][seconds=225] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server2' '2' 'model-in-image:WLS-v1' 'false' 'Running' @@ [2020-05-14T17:32:40][seconds=234] Info: '2' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-14T17:32:40][seconds=234] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v2' 'true' 'Running' @@ [2020-05-14T17:32:51][seconds=245] Info: '2' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-14T17:32:51][seconds=245] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v2' 'false' 'Pending' @@ [2020-05-14T17:32:52][seconds=246] Info: '2' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-14T17:32:52][seconds=246] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------- --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v2' 'false' 'Running' @@ [2020-05-14T17:33:25][seconds=279] Info: '3' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-05-14T17:33:25][seconds=279] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME VERSION IMAGE READY PHASE -------------------------------- ------- ----------------------- ------ --------- 'sample-domain1-admin-server' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server1' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server2' '1' 'model-in-image:WLS-v2' 'true' 'Running' @@ [2020-05-14T17:33:25][seconds=279] Info: Success!        After your domain roll is complete, you can call the sample web application to determine if the updated application was deployed.\nWhen the application is invoked, it will contain an output string like Hello World! This is version 'v2' of the mii-sample JSP web-app..\nSend a web application request to the ingress controller:\n$ curl -s -S -m 10 -H \u0026#39;host: sample-domain1-cluster-cluster-1.mii-sample.org\u0026#39; \\  http://localhost:30305/myapp_war/index.jsp Or, if Traefik is unavailable and your Administration Server pod is running, you can run kubectl exec:\n$ kubectl exec -n sample-domain1-ns sample-domain1-admin-server -- bash -c \\  \u0026#34;curl -s -S -m 10 http://sample-domain1-cluster-cluster-1:8001/myapp_war/index.jsp\u0026#34; You will see something like the following:\n\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v2\u0026#39; of the mii-sample JSP web-app. Welcome to WebLogic Server \u0026#39;managed-server1\u0026#39;! domain UID = \u0026#39;sample-domain1\u0026#39; domain name = \u0026#39;domain1\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found min threads constraint runtime named \u0026#39;SampleMinThreads\u0026#39; with configured count: 1 Found max threads constraint runtime named \u0026#39;SampleMaxThreads\u0026#39; with configured count: 10 Found 1 local data source: Datasource \u0026#39;mynewdatasource\u0026#39;: State=\u0026#39;Running\u0026#39;, testPool=\u0026#39;Failed\u0026#39; ---TestPool Failure Reason--- NOTE: Ignore \u0026#39;mynewdatasource\u0026#39; failures until the sample\u0026#39;s Update 4 use case. --- ... ... invalid host/username/password ... ----------------------------- ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;   A TestPool Failure is expected because we will demonstrate dynamically correcting the data source attributes in Update 4.\nIf you see an error other than the expected TestPool Failure, then consult Debugging in the Model in Image user guide.\nIf you plan to run the Update 4 use case, then leave your domain running.\nTo remove the resources you have created in the samples, see Cleanup.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/developerguide/coding-standards/",
	"title": "Coding standards",
	"tags": [],
	"description": "",
	"content": "This project has adopted the following coding standards:\n Code will be formated using Oracle / WebLogic standards, which are identical to the Google Java Style. Javadoc must be provided for all public packages, classes, and methods, and must include all parameters and returns. Javadoc is not required for methods that override or implement methods that are already documented. All non-trivial methods should include LOGGER.entering() and LOGGER.exiting() calls. The LOGGER.exiting() call should include the value that is going to be returned from the method, unless that value includes a credential or other sensitive information. All logged messages must be internationalized using the resource bundle src/main/resources/Operator.properties and using a key itemized in src/main/java/oracle/kubernetes/operator/logging/MessageKeys.java. After operator initialization, all operator work must be implemented using the asynchronous call model (described below). In particular, worker threads must not use sleep() or IO or lock-based blocking methods.  Code formatting plugins The following IDE plugins are available to assist with following the code formatting standards\nIntelliJ An IntelliJ plugin is available from the plugin repository.\nThe plugin will be enabled by default. To disable it in the current project, go to File \u0026gt; Settings... \u0026gt; google-java-format Settings (or IntelliJ IDEA \u0026gt; Preferences... \u0026gt; Other Settings \u0026gt; google-java-format Settings on macOS) and uncheck the \u0026ldquo;Enable google-java-format\u0026rdquo; checkbox.\nTo disable it by default in new projects, use File \u0026gt; Other Settings \u0026gt; Default Settings....\nWhen enabled, it will replace the normal \u0026ldquo;Reformat Code\u0026rdquo; action, which can be triggered from the \u0026ldquo;Code\u0026rdquo; menu or with the Ctrl-Alt-L (by default) keyboard shortcut.\nThe import ordering is not handled by this plugin, unfortunately. To fix the import order, download the IntelliJ Java Google Style file and import it into File→Settings→Editor→Code Style.\nEclipse An Eclipse plugin can be downloaded from the releases page. Drop it into the Eclipse drop-ins folder to activate the plugin.\nThe plugin adds a google-java-format formatter implementation that can be configured in Eclipse \u0026gt; Preferences \u0026gt; Java \u0026gt; Code Style \u0026gt; Formatter \u0026gt; Formatter Implementation.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/security/rbac/",
	"title": "RBAC",
	"tags": [],
	"description": "Operator role-based authorization",
	"content": "Contents  Overview Operator RBAC definitions  Role and RoleBinding naming conventions ClusterRole and ClusterRoleBinding naming conventions   RoleBindings ClusterRoleBindings  Overview The operator assumes that certain Kubernetes Roles are created in the Kubernetes cluster. The operator Helm chart creates the required ClusterRoles, ClusterRoleBindings, Roles, and RoleBindings for the ServiceAccount that is used by the operator. The operator will also attempt to verify that the RBAC settings are correct when the operator starts running.\nFor more information about the Kubernetes ServiceAccount used by the operator, see Service Accounts.\n The general design goal is to provide the operator with the minimum amount of permissions that the operator requires and to favor built-in roles over custom roles where it make sense to use the Kubernetes built-in roles.\nFor more information about Kubernetes Roles, see the Kubernetes RBAC documentation.\n Operator RBAC definitions To display the Kubernetes Roles and related Bindings used by the operator where the operator was installed using the Helm release name weblogic-operator, look for the Kubernetes objects, Role, RoleBinding, ClusterRole, and ClusterRoleBinding, when using the Helm status command:\n$ helm status weblogic-operator Assuming the operator was installed into the namespace weblogic-operator-ns with a target namespaces of domain1-ns, the following commands can be used to display a subset of the Kubernetes Roles and related RoleBindings:\n$ kubectl describe clusterrole \\  weblogic-operator-ns-weblogic-operator-clusterrole-general $ kubectl describe clusterrolebinding \\  weblogic-operator-ns-weblogic-operator-clusterrolebinding-general $ kubectl -n weblogic-operator-ns \\  describe role weblogic-operator-role $ kubectl -n domain1-ns \\  describe rolebinding weblogic-operator-rolebinding-namespace Kubernetes Role and RoleBinding naming conventions The following naming pattern is used for the Role and RoleBinding objects:\n weblogic-operator-\u0026lt;type\u0026gt;-\u0026lt;optional-role-name\u0026gt;  Using:\n \u0026lt;type\u0026gt; as the kind of Kubernetes object:  role rolebinding   \u0026lt;optional-role-name\u0026gt; as an optional name given to the Role or RoleBinding  For example: namespace    A complete name for an operator created Kubernetes RoleBinding would be:\n weblogic-operator-rolebinding-namespace\n Kubernetes ClusterRole and ClusterRoleBinding naming conventions The following naming pattern is used for the ClusterRole and ClusterRoleBinding objects:\n \u0026lt;operator-ns\u0026gt;-weblogic-operator-\u0026lt;type\u0026gt;-\u0026lt;role-name\u0026gt;  Using:\n \u0026lt;operator-ns\u0026gt; as the namespace in which the operator is installed  For example: weblogic-operator-ns   \u0026lt;type\u0026gt; as the kind of Kubernetes object:  clusterrole clusterrolebinding   \u0026lt;role-name\u0026gt; as the name given to the Role or RoleBinding  For example: general    A complete name for an operator created Kubernetes ClusterRoleBinding would be:\n weblogic-operator-ns-weblogic-operator-clusterrolebinding-general\n RoleBindings Assuming that the operator was installed into the Kubernetes Namespace weblogic-operator-ns, and a target namespace for the operator is domain1-ns, the following RoleBinding entries are mapped to a Role or ClusterRole granting permission to the operator.\n   RoleBinding Mapped to Role Resource Access Notes     weblogic-operator-rolebinding weblogic-operator-role Edit: secrets, configmaps, events The RoleBinding is created in the namespace weblogic-operator-ns 1   weblogic-operator-rolebinding-namespace Operator Cluster Role namespace Read: secrets, pods/log, pods/exec The RoleBinding is created in the namespace domain1-ns 2     Edit: configmaps, events, pods, services, jobs.batch, poddisruptionbudgets.policy      Create: pods/exec     ClusterRoleBindings Assuming that the operator was installed into the Kubernetes Namespace weblogic-operator-ns, the following ClusterRoleBinding entries are mapped to a ClusterRole granting permission to the operator.\nNote: The operator names in table below represent the \u0026lt;role-name\u0026gt; from the cluster names section.\n   ClusterRoleBinding Mapped to Cluster Role Resource Access Notes     Operator general Operator general Read: namespaces 3     Edit: customresourcedefinitions      Update: domains (weblogic.oracle), domains/status      Create: tokenreviews, selfsubjectrulesreviews    Operator nonresource Operator nonresource Get: /version/* 1   Operator discovery Kubernetes system:discovery See: Kubernetes Discovery Roles 3   Operator auth-delegator Kubernetes system:auth-delegator See: Kubernetes Component Roles 3      The binding is assigned to the operator ServiceAccount. \u0026#x21a9;\u0026#xfe0e;\n The binding is assigned to the operator ServiceAccount in each namespace that the operator is configured to manage. See Managing domain namespaces \u0026#x21a9;\u0026#xfe0e;\n The binding is assigned to the operator ServiceAccount. In addition, the Kubernetes RBAC resources that the operator installation actually set up will be adjusted based on the value of the dedicated setting. By default, the dedicated value is set to false, those security resources are created as ClusterRole and ClusterRoleBindings. If the dedicated value is set to true, those resources will be created as Roles and RoleBindings in the namespace of the operator. \u0026#x21a9;\u0026#xfe0e;\n   "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/configoverrides/",
	"title": "Configuration overrides",
	"tags": [],
	"description": "",
	"content": "Contents  Overview Prerequisites Typical overrides Unsupported overrides Overrides distribution Override template names and syntax  Override template names Override template schemas Override template macros Override template syntax special requirements Override template samples   Step-by-step guide Debugging Internal design flow   Overview Configuration overrides can only be used in combination with Domain in Image and Domain in PV domains. For Model in Image domains, use Model in Image Runtime Updates instead.\n Use configuration overrides (also called situational configuration) to customize a Domain in Image or Domain in PV domain\u0026rsquo;s WebLogic domain configuration without modifying the domain\u0026rsquo;s actual config.xml or system resource files. For example, you may want to override a JDBC data source XML module user name, password, and URL so that it references a local database.\nYou can use overrides to customize domains as they are moved from QA to production, are deployed to different sites, or are even deployed multiple times at the same site. Beginning with operator version 3.0.0, you can now modify configuration overrides for running WebLogic Server instances and have these new overrides take effect dynamically. There are limitations to the WebLogic configuration attributes that can be modified by overrides and only changes to dynamic configuration MBean attributes may be changed while a server is running. Other changes, specifically overrides to non-dynamic MBeans, must be applied when servers are starting or restarting.\nHow do you specify overrides?  Make sure your domain home meets the prerequisites. See Prerequisites. Make sure your overrides are supported. See Typical overrides and Unsupported overrides. Create a Kubernetes ConfigMap that contains:  Override templates (also known as situational configuration templates), with names and syntax as described in Override template names and syntax. A file named version.txt that contains the exact string 2.0.   Set your Domain configuration.overridesConfigMap field to the name of this ConfigMap. If templates leverage secret macros:  Create Kubernetes Secrets that contain template macro values. Set your domain configuration.secrets to reference the aforementioned Secrets.   If your configuration overrides modify non-dynamic MBean attributes and you currently have WebLogic Server instances from this domain running:  Decide if the changes you are making to non-dynamic MBean attributes can be applied by rolling the affected clusters or Managed Server instances or if the change required a full domain shutdown. If a full domain shut down is requried, stop all running WebLogic Server instance Pods in your domain and then restart them. (See Starting and stopping servers.) Otherwise, simply restart your domain, which includes rolling clusters. (See Restarting servers.)   Verify your overrides are taking effect. (See Debugging).  For a detailed walk-through of these steps, see the Step-by-step guide.\nHow do overrides work during runtime?  Configuration overrides are processed during the operator\u0026rsquo;s introspection phase. Introspection automatically occurs when:  The operator is starting a WebLogic Server instance when there are currently no other servers running. This occurs when the operator first starts servers for a domain or when starting servers following a full domain shutdown. For Model in Image, the operator determines that at least one WebLogic Server instance that is currently running must be shut down and restarted. This could be a rolling of one or more clusters, the shut down and restart of one or more WebLogic Server instances, or a combination.   You can initiate introspection by changing the value of the Domain introspectVersion field. For configuration overrides and during introspection, the operator will:  Resolve any macros in your override templates. Place expanded override templates in the optconfig directory located in each WebLogic domain home directory.   When the WebLogic Server instances start, they will:  Automatically load the override files from the optconfig directory. Use the override values in the override files instead of the values specified in their config.xml or system resource XML files.   WebLogic Server instances monitor the files in the optconfig directory so that if these files change while the server is running, WebLogic will detect and use the new configuration values based on the updated contents of these files. This only works for changes to configuration overrides related to dynamic configuration MBean attributes.  For a detailed walk-through of the runtime flow, see the Internal design flow.\n Prerequisites   Configuration overrides can be used in combination with Domain in Image and Domain in PV domains. For Model in Image domains (introduced in 3.0.0), use Model in Image Runtime Updates instead.\n  A WebLogic domain home must not contain any configuration overrides XML file in its optconfig directory that was not placed there by the operator. Any existing configuration overrides XML files in this directory will be deleted and replaced by your operator override templates, if any.\n  If you want to override a JDBC, JMS, or WLDF (diagnostics) module, then the original module must be located in your domain home config/jdbc, config/jms, and config/diagnostics directory, respectively. These are the default locations for these types of modules.\n   Typical overrides Typical attributes for overrides include:\n User names, passwords, and URLs for:  JDBC data sources JMS bridges, foreign servers, and SAF   Network channel external/public addresses  For remote RMI clients (T3, JMS, EJB) For remote WLST clients   Network channel external/public ports  For remote RMI clients (T3, JMS, EJB)   Debugging Tuning (MaxMessageSize, and such)  See overrides distribution for a discussion of distributing new or changed configuration overrides to already running WebLogic Server instances.\n Unsupported overrides IMPORTANT: The operator does not support customer-provided overrides in the following areas.\n Domain topology (cluster members) Network channel listen address, port, and enabled configuration Server and domain log locations Default or custom file store directories when domain.spec.dataHome is set Node Manager related configuration Changing any existing MBean name Adding or removing a module (for example, a JDBC module)  Specifically, do not use custom overrides for:\n Adding or removing:  Servers Clusters Network Access Points (custom channels) Modules   Changing any of the following:  Dynamic cluster size Default, SSL, and Admin channel Enabled, listen address, and port Network Access Point (custom channel), listen address, or port Server and domain log locations \u0026ndash; use the domain.spec.logHome setting instead and ensure that domain.spec.logHomeEnabled is set to true Default or custom file store directories when domain.spec.dataHome is set Node Manager access credentials Any existing MBean name (for example, you cannot change the domain name)    Note that it\u0026rsquo;s supported, even expected, to override network access point public or external addresses and ports. Also note that external access to JMX (MBean) or online WLST requires that the network access point internal port and external port match (external T3 or HTTP tunneling access to JMS, RMI, or EJBs don\u0026rsquo;t require port matching).\nThe behavior when using an unsupported override is undefined.\nOverrides distribution The operator generates the final configuration overrides, combining customer-provided configuration overrides and operator-generated overrides, during the operator\u0026rsquo;s introspection phase. These overrides are then used when starting or restarting WebLogic Server instances. Starting with operator version 3.0.0, these overrides can also be distributed and applied to already running WebLogic Server instances.\nFor Domain in PV, the ability to change WebLogic domain configuration using traditional management transactions involving the Administration Console or WLST can be combined with the ability to initiate a repeat introspection and distribute updated configuration overrides. This combination supports use cases such as defining a new WebLogic cluster and then immediately starting Managed Server cluster members.\n Override template names and syntax Overrides leverage a built-in WebLogic feature called \u0026ldquo;Configuration Overriding\u0026rdquo; which is often informally called \u0026ldquo;Situational Configuration.\u0026rdquo; Configuration overriding consists of XML formatted files that closely resemble the structure of WebLogic config.xml and system resource module XML files. In addition, the attribute fields in these files can embed add, replace, and delete verbs to specify the desired override action for the field.\nOverride template names The operator requires a different file name format for override templates than WebLogic\u0026rsquo;s built-in configuration overrides feature. It converts the names to the format required by the configuration overrides feature when it moves the templates to the domain home optconfig directory. The following table describes the format:\n   Original Configuration Required Override Name     config.xml config.xml   JMS module jms-MODULENAME.xml   JDBC module jdbc-MODULENAME.xml   Diagnostics module diagnostics-MODULENAME.xml    A MODULENAME must correspond to the MBean name of a system resource defined in your original config.xml file. It\u0026rsquo;s not possible to add a new module by using overrides. If you need your overrides to set up a new module, then have your original configuration specify \u0026lsquo;skeleton\u0026rsquo; modules that can be overridden.\nOverride template schemas An override template must define the exact schemas required by the configuration overrides feature. The schemas vary based on the file type you wish to override.\nconfig.xml\n\u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;d:domain xmlns:d=\u0026#34;http://xmlns.oracle.com/weblogic/domain\u0026#34; xmlns:f=\u0026#34;http://xmlns.oracle.com/weblogic/domain-fragment\u0026#34; xmlns:s=\u0026#34;http://xmlns.oracle.com/weblogic/situational-config\u0026#34;\u0026gt; ... \u0026lt;/d:domain\u0026gt; jdbc-MODULENAME.xml\n\u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;jdbc:jdbc-data-source xmlns:jdbc=\u0026#34;http://xmlns.oracle.com/weblogic/jdbc-data-source\u0026#34; xmlns:f=\u0026#34;http://xmlns.oracle.com/weblogic/jdbc-data-source-fragment\u0026#34; xmlns:s=\u0026#34;http://xmlns.oracle.com/weblogic/situational-config\u0026#34;\u0026gt; ... \u0026lt;/jdbc:jdbc-data-source\u0026gt; jms-MODULENAME.xml\n\u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;jms:weblogic-jms xmlns:jms=\u0026#34;http://xmlns.oracle.com/weblogic/weblogic-jms\u0026#34; xmlns:f=\u0026#34;http://xmlns.oracle.com/weblogic/weblogic-jms-fragment\u0026#34; xmlns:s=\u0026#34;http://xmlns.oracle.com/weblogic/situational-config\u0026#34; \u0026gt; ... \u0026lt;/jms:weblogic-jms\u0026gt; diagnostics-MODULENAME.xml\n\u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;wldf:wldf-resource xmlns:wldf=\u0026#34;http://xmlns.oracle.com/weblogic/weblogic-diagnostics\u0026#34; xmlns:f=\u0026#34;http://xmlns.oracle.com/weblogic/weblogic-diagnostics-fragment\u0026#34; xmlns:s=\u0026#34;http://xmlns.oracle.com/weblogic/situational-config\u0026#34; \u0026gt; ... \u0026lt;/wldf:wldf-resource\u0026gt; Override template macros The operator supports embedding macros within override templates. This helps make your templates flexibly handle multiple use cases, such as specifying a different URL, user name, and password for a different deployment.\nTwo types of macros are supported, environment variable macros and secret macros:\n  Environment variable macros have the syntax ${env:ENV-VAR-NAME}, where the supported environment variables include DOMAIN_UID, DOMAIN_NAME, DOMAIN_HOME, and LOG_HOME.\n  Secret macros have the syntax ${secret:SECRETNAME.SECRETKEY} and ${secret:SECRETNAME.SECRETKEY:encrypt}.\n  The secret macro SECRETNAME field must reference the name of a Kubernetes Secret, and the SECRETKEY field must reference a key within that Secret. For example, if you have created a Secret named dbuser with a key named username that contains the value scott, then the macro ${secret:dbuser.username} will be replaced with the word scott before the template is copied into its WebLogic Server instance Pod.\nSECURITY NOTE: Use the :encrypt suffix in a secret macro to encrypt its replacement value with the WebLogic WLST encrypt command (instead of leaving it at its plain text value). This is useful for overriding MBean attributes that expect encrypted values, such as the password-encrypted field of a data source, and is also useful for ensuring that a custom overrides configuration file the operator places in the domain home does not expose passwords in plain-text.\n Override template syntax special requirements Check each item below for best practices and to ensure custom overrides configuration takes effect:\n Reference the name of the current bean and each parent bean in any hierarchy you override.  Note that the combine-mode verbs (add and replace) should be omitted for beans that are already defined in your original domain home configuration.  See Override template samples for examples.     Use replace and add verbs as follows:  If you are adding a new bean that doesn\u0026rsquo;t already exist in your original domain home config.xml, then specify add on the MBean itself and on each attribute within the bean.  See the server-debug stanza in Override template samples for an example.   If you are adding a new attribute to an existing bean in the domain home config.xml, then the attribute needs an add verb.  See the max-message-size stanza in Override template samples for an example.   If you are changing the value of an existing attribute within a domain home config.xml, then the attribute needs a replace verb.  See the public-address stanza in Override template samples for an example.     When overriding config.xml:  The XML namespace (xmlns: in the XML) must be exactly as specified in Override template schemas.  For example, use d: to reference config.xml beans and attributes, f: for add and replace domain-fragment verbs, and s: to reference the configuration overrides schema.   Avoid specifying the domain name stanza, as this may cause some overrides to be ignored (for example, server-template scoped overrides).   When overriding modules:  It is a best practice to use XML namespace abbreviations jms:, jdbc:, and wldf: respectively for JMS, JDBC, and WLDF (diagnostics) module override files. A module must already exist in your original configuration if you want to override it; it\u0026rsquo;s not possible to add a new module by using overrides. If you need your overrides to set up a new module, then have your original configuration specify \u0026lsquo;skeleton\u0026rsquo; modules that can be overridden. See Overriding a data source module for best practice advice. Note that similar advice applies generally to other module types.   Consider having your original configuration reference invalid user names, passwords, and URLs:  If your original (non-overridden) configuration references non-working user names, passwords, and URLS, then this helps guard against accidentally deploying a working configuration that\u0026rsquo;s invalid for the intended environment. For example, if your base configuration references a working QA database, and there is some mistake in setting up overrides, then it\u0026rsquo;s possible the running servers will connect to the QA database when you deploy to your production environment.    Override template samples Here are some sample template override files.\nOverriding config.xml The following config.xml override file demonstrates:\n Setting the max-message-size field on a WebLogic Server named admin-server. It assumes the original config.xml does not define this value, and so uses add instead of replace. Sets the public-address and public-port fields with values obtained from a Secret named test-host with keys hostname and port. It assumes the original config.xml already sets these fields, and so uses replace instead of add. Sets two debug settings. It assumes the original config.xml does not have a server-debug stanza, so it uses add throughout the entire stanza.  \u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;d:domain xmlns:d=\u0026#34;http://xmlns.oracle.com/weblogic/domain\u0026#34; xmlns:f=\u0026#34;http://xmlns.oracle.com/weblogic/domain-fragment\u0026#34; xmlns:s=\u0026#34;http://xmlns.oracle.com/weblogic/situational-config\u0026#34; \u0026gt; \u0026lt;d:server\u0026gt; \u0026lt;d:name\u0026gt;admin-server\u0026lt;/d:name\u0026gt; \u0026lt;d:max-message-size f:combine-mode=\u0026#34;add\u0026#34;\u0026gt;78787878\u0026lt;/d:max-message-size\u0026gt; \u0026lt;d:server-debug f:combine-mode=\u0026#34;add\u0026#34;\u0026gt; \u0026lt;d:debug-server-life-cycle f:combine-mode=\u0026#34;add\u0026#34;\u0026gt;true\u0026lt;/d:debug-server-life-cycle\u0026gt; \u0026lt;d:debug-jmx-core f:combine-mode=\u0026#34;add\u0026#34;\u0026gt;true\u0026lt;/d:debug-jmx-core\u0026gt; \u0026lt;/d:server-debug\u0026gt; \u0026lt;d:network-access-point\u0026gt; \u0026lt;d:name\u0026gt;T3Channel\u0026lt;/d:name\u0026gt; \u0026lt;d:public-address f:combine-mode=\u0026#34;replace\u0026#34;\u0026gt;${secret:test-host.hostname}\u0026lt;/d:public-address\u0026gt; \u0026lt;d:public-port f:combine-mode=\u0026#34;replace\u0026#34;\u0026gt;${secret:test-host.port}\u0026lt;/d:public-port\u0026gt; \u0026lt;/d:network-access-point\u0026gt; \u0026lt;/d:server\u0026gt; \u0026lt;/d:domain\u0026gt; Overriding a data source module The following jdbc-testDS.xml override template demonstrates setting the URL, user name, and password-encrypted fields of a JDBC module named testDS by using secret macros. The generated configuration overrides that replaces the macros with secret values will be located in the DOMAIN_HOME/optconfig/jdbc directory. The password-encrypted field will be populated with an encrypted value because it uses a secret macro with an :encrypt suffix. The Secret is named dbsecret and contains three keys: url, username, and password.\nBest practices for data source modules and their overrides:\n A data source module must already exist in your original configuration if you want to override it; it\u0026rsquo;s not possible to add a new module by using overrides. If you need your overrides to set up a new module, then have your original configuration specify \u0026lsquo;skeleton\u0026rsquo; modules that can be overridden. See the next two bulleted items for the typical contents of a skeleton data source module. Set your original (non-overridden) URL, username, and password to invalid values. This helps prevent accidentally starting a server without overrides, and then having the data source successfully connect to a database that\u0026rsquo;s wrong for the current environment. For example, if these attributes are set to reference a QA database in your original configuration, then a mistake configuring overrides in your production Kubernetes Deployment could cause your production applications to use your QA database. Set your original (non-overridden) JDBCConnectionPoolParams MinCapacity and InitialCapacity to 0, and set your original DriverName to a reference an existing JDBC Driver. This ensures that you can still successfully boot a server even when you have configured invalid URL/username/password values, your database isn\u0026rsquo;t running, or you haven\u0026rsquo;t specified your overrides yet.  \u0026lt;?xml version=\u0026#39;1.0\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;jdbc:jdbc-data-source xmlns:jdbc=\u0026#34;http://xmlns.oracle.com/weblogic/jdbc-data-source\u0026#34; xmlns:f=\u0026#34;http://xmlns.oracle.com/weblogic/jdbc-data-source-fragment\u0026#34; xmlns:s=\u0026#34;http://xmlns.oracle.com/weblogic/situational-config\u0026#34;\u0026gt; \u0026lt;jdbc:name\u0026gt;testDS\u0026lt;/jdbc:name\u0026gt; \u0026lt;jdbc:jdbc-driver-params\u0026gt; \u0026lt;jdbc:url f:combine-mode=\u0026#34;replace\u0026#34;\u0026gt;${secret:dbsecret.url}\u0026lt;/jdbc:url\u0026gt; \u0026lt;jdbc:properties\u0026gt; \u0026lt;jdbc:property\u0026gt; \u0026lt;jdbc:name\u0026gt;user\u0026lt;/jdbc:name\u0026gt; \u0026lt;jdbc:value f:combine-mode=\u0026#34;replace\u0026#34;\u0026gt;${secret:dbsecret.username}\u0026lt;/jdbc:value\u0026gt; \u0026lt;/jdbc:property\u0026gt; \u0026lt;/jdbc:properties\u0026gt; \u0026lt;jdbc:password-encrypted f:combine-mode=\u0026#34;replace\u0026#34;\u0026gt;${secret:dbsecret.password:encrypt}\u0026lt;/jdbc:password-encrypted\u0026gt; \u0026lt;/jdbc:jdbc-driver-params\u0026gt; \u0026lt;/jdbc:jdbc-data-source\u0026gt;  Step-by-step guide  Make sure your domain home meets the prerequisites. See Prerequisites. Make sure your overrides are supported. See Typical overrides, Overrides distribution, and Unsupported overrides. Create a directory containing (A) a set of configuration overrides templates for overriding the MBean properties you want to replace and (B) a version.txt file.  This directory must not contain any other files. The version.txt file must contain exactly the string 2.0.  Note: This version.txt file must stay 2.0 even when you are updating your templates from a previous deployment.   Templates must not override the settings listed in Unsupported overrides. Templates must be formatted and named as per Override template names and syntax. Templates can embed macros that reference environment variables or Kubernetes Secrets. See Override template macros.   Create a Kubernetes ConfigMap from the directory of templates.   The ConfigMap must be in the same Kubernetes Namespace as the domain.\n  If the ConfigMap is going to be used by a single DOMAIN_UID, then we recommend adding the weblogic.domainUID=\u0026lt;mydomainuid\u0026gt; label to help track the resource.\n  For example, assuming ./mydir contains your version.txt and situation configuration template files:\n$ kubectl -n MYNAMESPACE create cm MYCMNAME --from-file ./mydir $ kubectl -n MYNAMESPACE label cm MYCMNAME weblogic.domainUID=DOMAIN_UID    Create any Kubernetes Secrets referenced by a template \u0026ldquo;secret macro\u0026rdquo;.   Secrets can have multiple keys (files) that can hold either cleartext or base64 values. We recommend that you use base64 values for passwords by using Opaque type secrets in their data field, so that they can\u0026rsquo;t be easily read at a casual glance. For more information, see https://kubernetes.io/docs/concepts/configuration/secret/.\n  Secrets must be in the same Kubernetes Namespace as the domain.\n  If a Secret is going to be used by a single DOMAIN_UID, then we recommend adding the weblogic.domainUID=\u0026lt;mydomainuid\u0026gt; label to help track the resource.\n  For example:\n$ kubectl -n MYNAMESPACE create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret $ kubectl -n MYNAMESPACE label secret my-secret weblogic.domainUID=DOMAIN_UID    Configure the name of the ConfigMap in the Domain YAML file configuration.overridesConfigMap field. Configure the names of each Secret in Domain YAML file.  If the Secret contains the WebLogic admin username and password keys, then set the Domain YAML file webLogicCredentialsSecret field. For all other Secrets, add them to the Domain YAML file configuration.secrets field. Note: This must be in an array format even if you only add one Secret (see the sample Domain YAML below).   Changes to configuration overrides, including the contents of the ConfigMap containing the override templates or the contents of referenced Secrets, do not take effect until the operator runs or repeats its introspection of the WebLogic domain configuration. If your configuration overrides modify non-dynamic MBean attributes and you currently have WebLogic Server instances from this domain running:  Decide if the changes you are making to non-dynamic MBean attributes can be applied by rolling the affected clusters or Managed Server instances, or if the change requires a full domain shutdown. (See Overrides distribution) If a full domain shut down is requried, stop all running WebLogic Server instance Pods in your domain and then restart them. (See Starting and stopping servers.) Otherwise, simply restart your domain, which includes rolling clusters. (See Restarting servers.)   See Debugging for ways to check if the configuration overrides are taking effect or if there are errors.  Example Domain YAML:\napiVersion: \u0026#34;weblogic.oracle/v8\u0026#34; kind: Domain metadata: name: domain1 namespace: default labels: weblogic.domainUID: domain1 spec: [ ... ] webLogicCredentialsSecret: name: domain1-wl-credentials-secret configuration: overridesConfigMap: domain1-overrides-config-map secrets: [my-secret, my-other-secret] [ ... ]  Debugging Use this information to verify that your overrides are taking effect or if there are errors.\nBackground notes:\n  The WebLogic Server Administration Console will not reflect any override changes.\n You cannot use the Console to verify that overrides are taking effect. Instead, you can check overrides using WLST; see the wlst.sh script below for details.    Incorrect override files may be silently accepted without warnings or errors.\n For example, WebLogic Server instance Pods may fully start regardless of XML override syntax errors or if the specified name of an MBean is incorrect. So, it is important to make sure that the template files are correct in a QA environment, otherwise, WebLogic Servers may start even though critically required overrides are failing to take effect.    Some incorrect overrides may be detected on WebLogic Server versions that support the weblogic.SituationalConfig.failBootOnError system property (not applicable to WebLogic Server 12.2.1.3.0).\n If the system property is supported, then, by default, WebLogic Server will fail to boot if it encounters a syntax error while loading configuration overrides files. If you want to start up WebLogic Servers with incorrectly formatted override files, then disable this check by setting the FAIL_BOOT_ON_SITUATIONAL_CONFIG_ERROR environment variable in the Kubernetes containers for the WebLogic Servers to false.    Debugging steps:\n  Make sure that you\u0026rsquo;ve followed each step in the Step-by-step guide.\n  If WebLogic Server instance Pods do not come up at all, then:\n  Examine your Domain resource status: kubectl -n MYDOMAINNAMESPACE describe domain MYDOMAIN\n  Check events for the Domain: kubectl -n MY_NAMESPACE get events --sort-by='.lastTimestamp'. For more information, see Domain events.\n  Check the introspector job and its log.\n In the domain\u0026rsquo;s namespace, see if you can find a job named DOMAIN_UID-introspector and a corresponding pod named something like DOMAIN_UID-introspector-xxxx. If so, examine:  kubectl -n MYDOMAINNAMESPACE describe job INTROSPECTJOBNAME kubectl -n MYDOMAINNAMESPACE logs INTROSPECTPODNAME   The introspector log is mirrored to the Domain resource spec.logHome directory when spec.logHome is configured and spec.logHomeEnabled is true.    Check the operator log for Warning/Error/Severe messages.\n kubectl -n MYOPERATORNAMESPACE logs OPERATORPODNAME      If WebLogic Server instance Pods do start, then:\n Search your Administration Server Pod\u0026rsquo;s kubectl log for the keyword situational, for example, kubectl logs MYADMINPOD | grep -i situational.  The only WebLogic Server log lines that match should look something like:  \u0026lt;Dec 14, 2018 12:20:47 PM UTC\u0026gt; \u0026lt;Info\u0026gt; \u0026lt;Management\u0026gt; \u0026lt;BEA-141330\u0026gt; \u0026lt;Loading situational configuration file: /shared/domains/domain1/optconfig/custom-situational-config.xml\u0026gt; This line indicates a configuration overrides file has been loaded.   If the search yields Warning or Error lines, then the format of the custom configuration overrides template is incorrect, and the Warning or Error text should describe the problem. Note: The following exception may show up in the server logs when overriding JDBC modules. It is not expected to affect runtime behavior, and can be ignored: java.lang.NullPointerException at weblogic.management.provider.internal.situationalconfig.SituationalConfigManagerImpl.registerListener(SituationalConfigManagerImpl.java:227) at weblogic.management.provider.internal.situationalconfig.SituationalConfigManagerImpl.start(SituationalConfigManagerImpl.java:319) ... at weblogic.management.configuration.DomainMBeanImpl.setJDBCSystemResources(DomainMBeanImpl.java:11444) ...    Look in your DOMAIN_HOME/optconfig directory.  This directory, or a subdirectory within this directory, should contain each of your custom configuration overrides files. If it doesn\u0026rsquo;t, then this likely indicates that your Domain YAML file configuration.overridesConfigMap was not set to match your custom override ConfigMap name, or that your custom override ConfigMap does not contain your override files.      If the Administration Server Pod does start but fails to reach ready state or tries to restart:\n Check for this message  WebLogic Server failed to start due to missing or invalid situational configuration files in the Administration Server Pod\u0026rsquo;s kubectl log  This suggests that the Administration Server failure to start may have been caused by errors found in a configuration override file.  Lines containing the String situational may be found in the Administration Server Pod log to provide more hints. For example:   \u0026lt;Jun 20, 2019 3:48:45 AM GMT\u0026gt; \u0026lt;Warning\u0026gt; \u0026lt;Management\u0026gt; \u0026lt;BEA-141323\u0026gt; \u0026lt;The situational config file has an invalid format, it is being ignored: XMLSituationalConfigFile[/shared/domains/domain1/optconfig/jdbc/testDS-0527-jdbc-situational-config.xml] because org.xml.sax.SAXParseException; lineNumber: 8; columnNumber: 3; The element type \u0026quot;jdbc:jdbc-driver-params\u0026quot; must be terminated by the matching end-tag \u0026quot;\u0026lt;/jdbc:jdbc-driver-params\u0026gt;\u0026quot;.  The warning message suggests a syntax error is found in the provided configuration override file for the testDS JDBC data source.     Check for pod-related Kubernetes events: kubectl -n MY_NAMESPACE get events --sort-by='.lastTimestamp'.    If you\u0026rsquo;d like to verify that the configuration overrides are taking effect in the WebLogic MBean tree, then one way to do this is to compare the server config and domain config MBean tree values.\n The domain config value should reflect the original value in your domain home configuration. The server config value should reflect the overridden value. For example, assuming your DOMAIN_UID is domain1, and your domain contains a WebLogic Server named admin-server, then:  $ kubectl exec -it domain1-admin-server /bin/bash $ wlst.sh \u0026gt; connect(MYADMINUSERNAME, MYADMINPASSWORD, \u0026#39;t3://domain1-admin-server:7001\u0026#39;) \u0026gt; domainConfig() \u0026gt; get(\u0026#39;/Servers/admin-server/MaxMessageSize\u0026#39;) \u0026gt; serverConfig() \u0026gt; get(\u0026#39;/Servers/admin-server/MaxMessageSize\u0026#39;) \u0026gt; exit()   To cause the WebLogic configuration overrides feature to produce additional debugging information in the WebLogic Server logs, configure the JAVA_OPTIONS environment variable in your Domain YAML file with:\n-Dweblogic.debug.DebugSituationalConfig=true -Dweblogic.debug.DebugSituationalConfigDumpXml=true    Internal design flow  The operator generates the final configuration overrides, which include the merging of operator-generated overrides and the processing of any customer-provided configuration overrides templates and Secrets, during its introspection phase. The operator creates a Kubernetes Job for introspection named DOMAIN_UID-introspector. The introspector Job\u0026rsquo;s Pod:  Mounts the Kubernetes ConfigMap and Secrets specified by using the operator Domain configuration.overridesConfigMap, webLogicCredentialsSecret, and configuration.secrets fields. Reads the mounted configuration overrides templates from the ConfigMap and expands them to create the actual configuration overrides files for the domain:  It expands some fixed replaceable values (for example, ${env:DOMAIN_UID}). It expands referenced Secrets by reading the value from the corresponding mounted secret file (for example, ${secret:mysecret.mykey}). It optionally encrypts secrets using offline WLST to encrypt the value - useful for passwords (for example, ${secret:mysecret.mykey:encrypt}). It returns expanded configuration overrides files to the operator. It reports any errors when attempting expansion to the operator.     The operator runtime:  Reads the expanded configuration overrides files or errors from the introspector. And, if the introspector reported no errors, it:  Puts configuration overrides files in one or more ConfigMaps whose names start with DOMAIN_UID-weblogic-domain-introspect-cm. Mounts these ConfigMaps into the WebLogic Server instance Pods.   Otherwise, if the introspector reported errors, it:  Logs warning, error, or severe messages. Will not start WebLogic Server instance Pods; however, any already running Pods are preserved.     The startServer.sh script in the WebLogic Server instance Pods:  Copies the expanded configuration overrides files to a special location where the WebLogic runtime can find them:  config.xml overrides are copied to the optconfig directory in its domain home. Module overrides are copied to the optconfig/jdbc, optconfig/jms, or optconfig/diagnostics directory.   Deletes any configuration overrides files in the optconfig directory that do not have corresponding template files in the ConfigMap.   WebLogic Servers read their overrides from their domain home\u0026rsquo;s optconfig directory. If WebLogic Server instance Pods are already running when introspection is repeated and this new introspection generates different configuration overrides then:  After the operator updates the ConfigMap, Kubernetes modifies the mounted files in running containers to match the new contents of the ConfigMap. The rate of this periodic sync of ConfigMap data by kubelet is configurable, but defaults to 10 seconds. If overridesDistributionStrategy is DYNAMIC, then the livenessProbe.sh script, which is already periodically invoked by Kubernetes, will perform the same actions as startServer.sh to update the files in optconfig. WebLogic Server instances monitor the files in optconfig and dynamically update the active configuration based on the current contents of the configuration overrides files. Otherwise, if the overridesDistributionStrategy is ON_RESTART, then the updated files at the ConfigMap\u0026rsquo;s mount point are not copied to optconfig while the WebLogic Server instance is running and, therefore, don\u0026rsquo;t affect the active configuration.    Changes to configuration overrides distributed to running WebLogic Server instances can only take effect if the corresponding WebLogic configuration MBean attribute is \u0026ldquo;dynamic\u0026rdquo;. For instance, the Data Source passwordEncrypted attribute is dynamic while the Url attribute is non-dynamic.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-operators/",
	"title": "Manage operators",
	"tags": [],
	"description": "Helm is used to create and deploy necessary operator resources and to run the operator in a Kubernetes cluster. Use the operator&#39;s Helm chart to install and manage the operator.",
	"content": "Overview Helm is a framework that helps you manage Kubernetes applications, and Helm charts help you define and install Helm applications into a Kubernetes cluster. The operator\u0026rsquo;s Helm chart is located in the kubernetes/charts/weblogic-operator directory.\nInstall Helm Helm manages releases (installations) of your charts. For detailed instructions on installing Helm, see https://github.com/helm/helm.\nOperator\u0026rsquo;s Helm chart configuration The operator Helm chart is pre-configured with default values for the configuration of the operator.\nYou can override these values by doing one of the following:\n Creating a custom YAML file with only the values to be overridden, and specifying the --value option on the Helm command line. Overriding individual values directly on the Helm command line, using the --set option.  You can find out the configuration values that the Helm chart supports, as well as the default values, using this command:\n$ helm inspect values kubernetes/charts/weblogic-operator The available configuration values are explained by category in Operator Helm configuration values.\nHelm commands are explained in more detail in Useful Helm operations.\nOptional: Configure the operator\u0026rsquo;s external REST HTTPS interface The operator can expose an external REST HTTPS interface which can be accessed from outside the Kubernetes cluster. As with the operator\u0026rsquo;s internal REST interface, the external REST interface requires an SSL/TLS certificate and private key that the operator will use as the identity of the external REST interface (see below).\nTo enable the external REST interface, configure these values in a custom configuration file, or on the Helm command line:\n Set externalRestEnabled to true. Set externalRestIdentitySecret to the name of the Kubernetes tls secret that contains the certificates and private key. Optionally, set externalRestHttpsPort to the external port number for the operator REST interface (defaults to 31001).  For more detailed information, see the REST interface configuration values.\nSample SSL certificate and private key for the REST interface For testing purposes, the WebLogic Kubernetes Operator project provides a sample script that generates a self-signed certificate and private key for the operator external REST interface. The generated certificate and key are stored in a Kubernetes tls secret and the sample script outputs the corresponding configuration values in YAML format. These values can be added to your custom YAML configuration file, for use when the operator\u0026rsquo;s Helm chart is installed.\nThe sample script should not be used in a production environment because typically a self-signed certificate for external communication is not considered safe. A certificate signed by a commercial certificate authority is more widely accepted and should contain valid host names, expiration dates, and key constraints.\n For more detailed information about the sample script and how to run it, see the REST APIs.\nOptional: Elastic Stack (Elasticsearch, Logstash, and Kibana) integration The operator Helm chart includes the option of installing the necessary Kubernetes resources for Elastic Stack integration.\nYou are responsible for configuring Kibana and Elasticsearch, then configuring the operator Helm chart to send events to Elasticsearch. In turn, the operator Helm chart configures Logstash in the operator deployment to send the operator\u0026rsquo;s log contents to that Elasticsearch location.\nElastic Stack per-operator configuration As part of the Elastic Stack integration, Logstash configuration occurs for each deployed operator instance. You can use the following configuration values to configure the integration:\n Set elkIntegrationEnabled is true to enable the integration. Set logStashImage to override the default version of Logstash to be used (logstash:6.2). Set elasticSearchHost and elasticSearchPort to override the default location where Elasticsearch is running (elasticsearch2.default.svc.cluster.local:9201). This will configure Logstash to send the operator\u0026rsquo;s log contents there.  For more detailed information, see the Operator Helm configuration values.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/quickstart/prepare/",
	"title": "Prepare for a domain",
	"tags": [],
	"description": "",
	"content": "  Create and label a namespace that can host one or more domains:\n$ kubectl create namespace sample-domain1-ns $ kubectl label ns sample-domain1-ns weblogic-operator=enabled   Configure Traefik to manage ingresses created in this namespace:\n$ helm upgrade traefik-operator traefik/traefik \\  --namespace traefik \\  --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik,sample-domain1-ns}\u0026#34;   If you have reached this point while following the \u0026ldquo;Model in Image\u0026rdquo; sample, please stop here and return to the sample instructions.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/faq/fan/",
	"title": "Disabling Fast Application Notifications",
	"tags": [],
	"description": "To support Fast Application Notifications (FAN), Oracle databases configure GRID (Oracle Grid Infrastructure). GRID is typically associated with (and required by) Oracle RAC databases but can also be used in other configurations.  Oracle Autonomous Database-Serverless (ATP-S) does not provide GRID.",
	"content": "To support Fast Application Notifications (FAN), Oracle databases configure GRID (Oracle Grid Infrastructure). GRID is typically associated with (and required by) Oracle RAC databases but can also be used in other configurations. Oracle Autonomous Database-Serverless (ATP-S) does not provide GRID.\nWhen connecting to a database that does not have GRID, the only type of WebLogic Server data source that is supported is the Generic Data Sources. Multi Data Sources and Active GridLink data sources cannot be used because they work with RAC.\nWebLogic Server 12.2.1.3.0 shipped with the 12.2.0.1 Oracle driver. When connecting with this driver to a database that does not have GRID, you will encounter the following exception (however, the 18.3 driver does not have this problem):\noracle.simplefan.impl.FanManager configure SEVERE: attempt to configure ONS in FanManager failed with oracle.ons.NoServersAvailable: Subscription time out To correct the problem, you must disable FAN, in one of two places:\n Through a system property at the domain, cluster, or server level.  To do this, edit the Domain Custom Resource to set the system property oracle.jdbc.fanEnabled to false as shown in the following example:\nserverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false -Doracle.jdbc.fanEnabled=false\u0026#34; - name: WLSDEPLOY_PROPERTIES value: \u0026#34;-Doracle.jdbc.fanEnabled=false\u0026#34; Configure the data source connection pool properties.  The following WLST script adds the oracle.jdbc.fanEnabled property, set to false, to an existing data source.\nfmwDb = \u0026#39;jdbc:oracle:thin:@\u0026#39; + db print \u0026#39;fmwDatabase \u0026#39; + fmwDb cd(\u0026#39;/JDBCSystemResource/LocalSvcTblDataSource/JdbcResource/LocalSvcTblDataSource\u0026#39;) cd(\u0026#39;JDBCDriverParams/NO_NAME_0\u0026#39;) set(\u0026#39;DriverName\u0026#39;, \u0026#39;oracle.jdbc.OracleDriver\u0026#39;) set(\u0026#39;URL\u0026#39;, fmwDb) set(\u0026#39;PasswordEncrypted\u0026#39;, dbPassword) stbUser = dbPrefix + \u0026#39;_STB\u0026#39; cd(\u0026#39;Properties/NO_NAME_0/Property/user\u0026#39;) set(\u0026#39;Value\u0026#39;, stbUser) ls() cd(\u0026#39;../..\u0026#39;) ls() create(\u0026#39;oracle.jdbc.fanEnabled\u0026#39;,\u0026#39;Property\u0026#39;) ls() cd(\u0026#39;Property/oracle.jdbc.fanEnabled\u0026#39;) set(\u0026#39;Value\u0026#39;, \u0026#39;false\u0026#39;) ls() After changing the data source connection pool configuration, for the attribute to take effect, make sure to undeploy and redeploy the data source, or restart WebLogic Server.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/domains/fmw-domain-home-in-image/",
	"title": "FMW Infrastructure domain home in image",
	"tags": [],
	"description": "Sample for creating an FMW Infrastructure domain home inside an image, and the domain resource YAML file for deploying the generated WebLogic domain.",
	"content": "The sample scripts demonstrate the creation of a FMW Infrastructure domain home in an image using WebLogic Image Tool (WIT). The sample scripts have the option of putting the WebLogic domain log, server logs, server output files, and the Node Manager logs on an existing Kubernetes PersistentVolume (PV) and PersistentVolumeClaim (PVC). The scripts also generate the domain resource YAML file, which can then be used by the scripts or used manually to start the Kubernetes artifacts of the corresponding domain, including the WebLogic Server pods and services.\nPrerequisites Before you begin, read this document, Domain resource.\nThe following prerequisites must be met prior to running the create domain script:\n Make sure the WebLogic Kubernetes Operator is running. The operator requires an image with either FMW Infrastructure 12.2.1.3.0 with patch 29135930 applied or FMW Infrastructure 12.2.1.4.0. For details on how to obtain or create the image, refer to FMW Infrastructure domains. Create a Kubernetes Namespace for the domain unless you intend to use the default namespace. If logHomeOnPV is enabled, create the Kubernetes PersistentVolume where the log home will be hosted, and the Kubernetes PersistentVolumeClaim for the domain in the same Kubernetes Namespace. For samples to create a PV and PVC, see Create sample PV and PVC. Create the Kubernetes Secrets username and password of the administrative account in the same Kubernetes namespace as the domain. Unless you are creating a Restricted-JRF domain, you also need to:  Configure access to your database. For details, see here. Create a Kubernetes Secret with the RCU credentials. For details, refer to this document.    Use the script to create a domain The sample for creating domains is in this directory:\n$ cd kubernetes/samples/scripts/create-fmw-infrastructure-domain/domain-home-in-image Make a copy of the create-domain-inputs.yaml file, update it with the correct values. If fwmDomainType is JRF, also update the input files with configurations for accessing the RCU database schema, including rcuSchemaPrefix, rcuSchemaPassword, rcuDatabaseURL, and rcuCredentialSecrets. Run the create script, pointing it at your inputs file and an output directory, along with user name and password for the WebLogic administrator, and if creating a JFR FMW domain, also provide the password for the RCU schema:\n$ ./create-domain.sh \\  -u \u0026lt;username\u0026gt; \\  -p \u0026lt;password\u0026gt; \\  -q \u0026lt;RCU schema password\u0026gt; \\  -i create-domain-inputs.yaml \\  -o /\u0026lt;path to output-directory\u0026gt;  The create-domain.sh script and its inputs file are for demonstration purposes only; its contents and the domain resource file that it generates for you might change without notice. In production, we strongly recommend that you use the WebLogic Image Tool and WebLogic Deploy Tooling (when applicable), and directly work with domain resource files instead.\n The script will perform the following steps:\n  Create a directory for the generated Kubernetes YAML files for this domain if it does not already exist. The path name is /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;. If the directory already exists, its contents must be removed before using this script.\n  Create a properties file, domain.properties, in the directory that is created above. This properties file will be used to create a sample FMW Infrastructure domain. The domain.properties file will be removed upon successful completion of the script.\n  Download the latest WebLogic Deploy Tooling (WDT) and WebLogic Image Tool installer ZIP files to your /tmp/dhii-sample/tools directory. WIT is required to create your Domain in Image container images, and WDT is required if using wdt mode. Visit the GitHub WebLogic Deploy Tooling Releases and WebLogic Image Tool Releases web pages to determine the latest release version for each.\n  Set up the WebLogic Image Tool in the \u0026lt;toolsDir\u0026gt;/imagetool directory, where \u0026lt;toolsDir\u0026gt; is the directory specified in the toolsDir parameter in the inputs YAML file. Set the WIT cache store location to the \u0026lt;tools\u0026gt;/imagetool-cache directory and put a wdt_\u0026lt;WDT_VERSION\u0026gt; entry in the tool\u0026rsquo;s cache, which points to the path of the WDT ZIP file installer. For more information about the WIT cache, see the WIT Cache documentation.\n  If the optional -n option and an encryption key is provided, invoke the WDT Encrypt Model Tool in a container running the image specified in domainHomeImageBase parameter in your inputs file to encrypt the password properties in domain.properties file. Note that this password encryption step is skipped if the value of the mode parameter in the inputs YAML file is wlst because the feature is provided by WDT.\n  Invoke the WebLogic Image Tool to create a new FWM Infrastructure domain based on the FMW Infrastructure image specified in the domainHomeImageBase parameter from your inputs file. The new WebLogic Server domain is created using one of the following options based on the value of the mode parameter in the inputs YAML file:\n If the value of the mode parameter is wdt, the WDT model specified in the createDomainWdtModel parameter and the WDT variables in domain.properties file are used by the WebLogic Image Tool to create the new WebLogic Server domain. If the value of the mode parameter is wlst, the offline WLST script specified in the createDomainWlstScript parameter is run to create the new WebLogic Server domain.    The generated image is tagged with the image parameter provided in your inputs file.\nOracle strongly recommends storing the image containing the domain home as private in the registry (for example, Oracle Cloud Infrastructure Registry, GitHub Container Registry, and such) because this image contains sensitive information about the domain, including keys and credentials that are used to access external resources (for example, the data source password). For more information, see WebLogic domain in image protection.\n   Create a Kubernetes domain resource YAML file, domain.yaml, in the directory that is created above. This YAML file can be used to create the Kubernetes resource using the kubectl create -f or kubectl apply -f command:\n$ kubectl apply -f /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt;/domain.yaml   As a convenience, using the -e option, the script can optionally create the domain object, which in turn results in the creation of the corresponding WebLogic Server pods and services as well.\nThe usage of the create script is as follows:\n$ sh create-domain.sh -h usage: create-domain.sh -o dir -i file -u username -p password [-q rcuSchemaPassword] [-b buildNetworkParam] [-n encryption-key] [-e] [-v] [-h] -i Parameter inputs file, must be specified. -o Output directory for the generated YAML files, must be specified. -u WebLogic administrator user name for the WebLogic domain. -p WebLogic administrator Password for the WebLogic domain. -q Password for the RCU schema. Required for JRF FMW domain type. -e Also create the resources in the generated YAML files, optional. -v Validate the existence of persistentVolumeClaim, optional. -n Encryption key for encrypting passwords in the WDT model and properties files, optional. -b Value to be used in the buildNetwork parameter when invoking WebLogic Image Tool, optional. -h Help If you copy the sample scripts to a different location, make sure that you copy everything in the \u0026lt;weblogic-kubernetes-operator-project\u0026gt;/kubernetes/samples/scripts directory together into the target directory, maintaining the original directory hierarchy.\nThe default domain created by the script has the following characteristics:\n An Administration Server named admin-server listening on port 7001. A configured cluster named cluster-1 of size 3. Three Managed Servers, named managed-server1, managed-server2, and so on, listening on port 8001. Log files that are located in /shared/logs/\u0026lt;domainUID\u0026gt;. No applications deployed. No data sources or JMS resources. A T3 channel.  The domain creation inputs can be customized by editing create-domain-inputs.yaml.\nConfiguration parameters The following parameters can be provided in the inputs file.\n   Parameter Definition Default     adminPort Port number of the Administration Server inside the Kubernetes cluster. 7001   adminNodePort Port number of the Administration Server outside the Kubernetes cluster. 30701   adminServerName Name of the Administration Server. admin-server   clusterName Name of the WebLogic cluster instance to generate for the domain. cluster-1   configuredManagedServerCount Number of Managed Server instances to generate for the domain. This value is ignored when using WDT with a model that creates configured cluster where the number of Managed Server instances is determined by the WDT model specified by createDomainWdtModel. 5   createDomainWdtModel WDT model YAML file that the create domain script uses to create a WebLogic domain when using wdt mode. This value is ignored when the mode is set to wlst. wdt/wdt_model_configured.yaml or wdt/wdt_model_restricted_jrf_configured.yaml depending on the value of fmwDomainType   createDomainWlstScript WLST script that the create domain script uses to create a WebLogic domain when using wlst mode. This value is ignored when the mode is set to wdt (which is the default mode). ../../common/createFMWJRFDomain.py or ../../common/createFMWRestrictedJRFDomain.py depending on the value of fmwDomainType   domainHome Domain home directory of the WebLogic domain to be created in the generated WebLogic Server image. /u01/oracle/user_projects/domains/\u0026lt;domainUID\u0026gt;   domainHomeImageBase Base OracleFMWInfrastructure binary image used to build the OracleFMWInfrastructure domain image. The operator requires FMW Infrastructure 12.2.1.3.0 with patch 29135930 applied or FMW Infrastructure 12.2.1.4.0. For details on how to obtain or create the image, see FMW Infrastructure domains. container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4   domainPVMountPath Mount path of the domain persistent volume. /shared   domainUID Unique ID that will be used to identify this particular domain. Used as the name of the generated WebLogic domain as well as the name of the Domain. This ID must be unique across all domains in a Kubernetes cluster. This ID cannot contain any character that is not valid in a Kubernetes Service name. domain1   exposeAdminNodePort Boolean indicating if the Administration Server is exposed outside of the Kubernetes cluster. false   exposeAdminT3Channel Boolean indicating if the T3 administrative channel is exposed outside the Kubernetes cluster. false   fmwDomainType FMW Infrastructure Domain Type. Legal values are JRF or RestrictedJRF. JRF   httpAccessLogInLogHome Boolean indicating if server HTTP access log files should be written to the same directory as logHome. Otherwise, server HTTP access log files will be written to the directory specified in the WebLogic domain home configuration. true   image Oracle FMW Infrastructure Server image that the operator uses to start the domain. The create domain scripts generate a Oracle FMW Infrastructure Server image with a domain home in it. By default, the scripts tag the generated Oracle FMW Infrastructure Server image as domain-home-in-image, and use it plus the tag that is obtained from the domainHomeImageBase to set the image element in the generated domain resource YAML file. If this property is set, the create domain scripts will use the value specified, instead of the default value, to tag the generated image and set the image in the domain resource YAML file. A unique value is required for each domain that is created using the scripts. If you are running the sample scripts from a machine that is remote to the Kubernetes cluster where the domain is going to be running, you need to set this property to the image name that is intended to be used in a registry local to that Kubernetes cluster. You also need to push the image to that registry before starting the domain using the kubectl create -f or kubectl apply -f command. domain-home-in-image:\u0026lt;tag from domainHomeImageBase\u0026gt;   imagePullPolicy Oracle FMW Infrastructure Server image pull policy. Legal values are IfNotPresent, Always, or Never. IfNotPresent   imagePullSecretName Name of the Kubernetes Secret to access the container registry to pull the WebLogic Server image. The presence of the secret will be validated when this parameter is specified.    includeServerOutInPodLog Boolean indicating whether to include the server .out in the pod\u0026rsquo;s stdout. true   initialManagedServerReplicas Number of Managed Servers to start initially for the domain. 1   javaOptions Java options for starting the Administration Server and Managed Servers. A Java option can have references to one or more of the following pre-defined variables to obtain WebLogic domain information: $(DOMAIN_NAME), $(DOMAIN_HOME), $(ADMIN_NAME), $(ADMIN_PORT), and $(SERVER_NAME). -Dweblogic.StdoutDebugEnabled=false   logHome The in-pod location for the domain log, server logs, server out, Node Manager log, introspector out, and server HTTP access log files. If not specified, the value is derived from the domainUID as /shared/logs/\u0026lt;domainUID\u0026gt;. /shared/logs/domain1   managedServerNameBase Base string used to generate Managed Server names. managed-server   managedServerPort Port number for each Managed Server. 8001   mode Whether to use the WDT model specified in createDomainWdtModel or the offline WLST script specified in createDomainWlstScript to create a WebLogic domain. Legal values are wdt or wlst. wdt   namespace Kubernetes Namespace in which to create the domain. default   persistentVolumeClaimName Name of the persistent volume claim. If not specified, the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-sample-pvc. domain1-weblogic-sample-pvc   productionModeEnabled Boolean indicating if production mode is enabled for the domain. true   serverStartPolicy Determines which WebLogic Server instances will be started. Legal values are NEVER, IF_NEEDED, ADMIN_ONLY. IF_NEEDED   t3ChannelPort Port for the T3 channel of the network access point. 30012   t3PublicAddress Public address for the T3 channel. This should be set to the public address of the Kubernetes cluster. This would typically be a load balancer address. For development environments only, in a single server (all-in-one) Kubernetes Deployment, this may be set to the address of the master, or at the very least, it must be set to the address of one of the worker nodes. If not provided, the script will attempt to set it to the IP address of the Kubernetes cluster.   weblogicCredentialsSecretName Name of the Kubernetes Secret for the Administration Server user name and password. If not specified, then the value is derived from the domainUID as \u0026lt;domainUID\u0026gt;-weblogic-credentials. domain1-weblogic-credentials   serverPodCpuRequest, serverPodMemoryRequest, serverPodCpuCLimit, serverPodMemoryLimit The maximum amount of compute resources allowed, and minimum amount of compute resources required, for each server pod. Please refer to the Kubernetes documentation on Managing Compute Resources for Containers for details. Resource requests and resource limits are not specified.   rcuCredentialsSecret The Kubernetes Secret containing the database credentials. domain1-rcu-credentials   rcuDatabaseURL The database URL. database:1521/service   rcuSchemaPrefix The schema prefix to use in the database, for example SOA1. You may wish to make this the same as the domainUID in order to simplify matching domains to their RCU schemas. domain1   toolsDir The directory where WebLogic Deploy Tool and WebLogic Image Tool are installed. The script will install these tools to this directory if they are not already installed. /tmp/dhii-sample/tools   wdtVersion Version of the WebLogic Deploy Tool to be installed by the script. This can be a specific version, such as 1.9.10, or LATEST. LATEST   witVersion Version of the WebLogic Image Tool to be installed by the script. This can be a specific version, such as 1.9.10, or LATEST. LATEST    Note that the names of the Kubernetes resources in the generated YAML files may be formed with the value of some of the properties specified in the inputs YAML file. Those properties include the adminServerName, clusterName, and managedServerNameBase. If those values contain any characters that are invalid in a Kubernetes Service name, those characters are converted to valid values in the generated YAML files. For example, an uppercase letter is converted to a lowercase letter and an underscore (\u0026quot;_\u0026quot;) is converted to a hyphen (\u0026quot;-\u0026quot;).\nThe sample demonstrates how to create a FMW Infrastructure domain home and associated Kubernetes resources for a domain that has one cluster only. In addition, the sample provides the capability for users to supply their own scripts to create the domain home for other use cases. The generated domain resource YAML file could also be modified to cover more use cases.\nVerify the results The create script will verify that the domain was created, and will report failure if there was any error. However, it may be desirable to manually verify the domain, even if just to gain familiarity with the various Kubernetes objects that were created by the script.\nNote that the example results below use the default Kubernetes Namespace. If you are using a different namespace, you need to replace NAMESPACE in the example kubectl commands with the actual Kubernetes Namespace.\nGenerated YAML files with the default inputs The content of the generated domain.yaml:\n# Copyright (c) 2017, 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026#34;weblogic.oracle/v8\u0026#34; kind: Domain metadata: name: fmwdomain namespace: default labels: weblogic.domainUID: fmwdomain spec: # The WebLogic Domain Home domainHome: /u01/oracle/user_projects/domains/fmwdomain # The domain home source type # Set to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image domainHomeSourceType: Image # The WebLogic Server image that the Operator uses to start the domain image: \u0026#34;domain-home-in-image:12.2.1.4\u0026#34; # imagePullPolicy defaults to \u0026#34;Always\u0026#34; if image version is :latest imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of # how to create that Secret at the end of this file) webLogicCredentialsSecret: name: fmwdomain-weblogic-credentials # Whether to include the server out file into the pod\u0026#39;s stdout, default is true includeServerOutInPodLog: true # Whether to enable log home # logHomeEnabled: false # Whether to write HTTP access log file to log home # httpAccessLogInLogHome: true # The in-pod location for domain log, server logs, server out, introspector out, and Node Manager log files # logHome: /shared/logs/domain1 # An (optional) in-pod location for data storage of default and custom file stores. # If not specified or the value is either not set or empty (e.g. dataHome: \u0026#34;\u0026#34;) then the # data storage directories are determined from the WebLogic domain home configuration. dataHome: \u0026#34;\u0026#34; # serverStartPolicy legal values are \u0026#34;NEVER\u0026#34;, \u0026#34;IF_NEEDED\u0026#34;, or \u0026#34;ADMIN_ONLY\u0026#34; # This determines which WebLogic Servers the Operator will start up when it discovers this Domain # - \u0026#34;NEVER\u0026#34; will not start any server in the domain # - \u0026#34;ADMIN_ONLY\u0026#34; will start up only the administration server (no managed servers will be started) # - \u0026#34;IF_NEEDED\u0026#34; will start all non-clustered servers, including the administration server and clustered servers up to the replica count serverStartPolicy: \u0026#34;IF_NEEDED\u0026#34; serverPod: # an (optional) list of environment variable to be set on the servers env: - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false\u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m \u0026#34; # volumes: # - name: weblogic-domain-storage-volume # persistentVolumeClaim: # claimName: weblogic-sample-pvc # volumeMounts: # - mountPath: /shared # name: weblogic-domain-storage-volume # adminServer is used to configure the desired behavior for starting the administration server. adminServer: # serverStartState legal values are \u0026#34;RUNNING\u0026#34; or \u0026#34;ADMIN\u0026#34; # \u0026#34;RUNNING\u0026#34; means the listed server will be started up to \u0026#34;RUNNING\u0026#34; mode # \u0026#34;ADMIN\u0026#34; means the listed server will be start up to \u0026#34;ADMIN\u0026#34; mode serverStartState: \u0026#34;RUNNING\u0026#34; adminService: channels: # The Admin Server\u0026#39;s NodePort - channelName: default nodePort: 30701 # Uncomment to export the T3Channel as a service # - channelName: T3Channel serverPod: # an (optional) list of environment variable to be set on the admin servers env: - name: USER_MEM_ARGS value: \u0026#34;-Djava.security.egd=file:/dev/./urandom -Xms512m -Xmx1024m \u0026#34; # clusters is used to configure the desired behavior for starting member servers of a cluster.  # If you use this entry, then the rules will be applied to ALL servers that are members of the named clusters. clusters: - clusterName: cluster-1 serverStartState: \u0026#34;RUNNING\u0026#34; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026#34;weblogic.clusterName\u0026#34; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; replicas: 1 # The number of managed servers to start for unlisted clusters # replicas: 1 # Istio # configuration: # istio: # enabled: # readinessPort: Verify the domain To confirm that the domain was created, use this command:\n$ kubectl describe domain DOMAINUID -n NAMESPACE Replace DOMAINUID with the domainUID and NAMESPACE with the actual namespace.\nHere is an example of the output of this command:\nName: fmwdomain Namespace: default Labels: weblogic.domainUID=fmwdomain Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026quot;apiVersion\u0026quot;:\u0026quot;weblogic.oracle/v8\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Domain\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;labels\u0026quot;:{\u0026quot;weblogic.domainUID\u0026quot;:\u0026quot;fmwdomain\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;fmwdom... API Version: weblogic.oracle/v8 Kind: Domain Metadata: Creation Timestamp: 2021-05-11T17:13:56Z Generation: 1 Resource Version: 39039230 Self Link: /apis/weblogic.oracle/v8/namespaces/default/domains/fmwdomain UID: 447a6d1f-cb6f-4884-82bf-c7b3dd1f898d Spec: Admin Server: Admin Service: Channels: Channel Name: default Node Port: 30701 Server Pod: Env: Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms512m -Xmx1024m Server Start State: RUNNING Clusters: Cluster Name: cluster-1 Replicas: 1 Server Pod: Affinity: Pod Anti Affinity: Preferred During Scheduling Ignored During Execution: Pod Affinity Term: Label Selector: Match Expressions: Key: weblogic.clusterName Operator: In Values: $(CLUSTER_NAME) Topology Key: kubernetes.io/hostname Weight: 100 Server Start State: RUNNING Data Home: Domain Home: /u01/oracle/user_projects/domains/fmwdomain Domain Home Source Type: Image Image: domain-home-in-image:12.2.1.4 Image Pull Policy: IfNotPresent Include Server Out In Pod Log: true Server Pod: Env: Name: JAVA_OPTIONS Value: -Dweblogic.StdoutDebugEnabled=false Name: USER_MEM_ARGS Value: -Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx1024m Server Start Policy: IF_NEEDED Web Logic Credentials Secret: Name: fmwdomain-weblogic-credentials Status: Clusters: Cluster Name: cluster-1 Maximum Replicas: 3 Minimum Replicas: 0 Ready Replicas: 1 Replicas: 1 Replicas Goal: 1 Conditions: Last Transition Time: 2021-05-11T17:16:42.202063Z Reason: ServersReady Status: True Type: Available Introspect Job Failure Count: 0 Replicas: 1 Servers: Desired State: RUNNING Health: Activation Time: 2021-05-11T17:15:43.355000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: alai-1 Server Name: admin-server State: RUNNING Cluster Name: cluster-1 Desired State: RUNNING Health: Activation Time: 2021-05-11T17:16:36.586000Z Overall Health: ok Subsystems: Subsystem Name: ServerRuntime Symptoms: Node Name: alai-1 Server Name: managed-server1 State: RUNNING Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server2 Cluster Name: cluster-1 Desired State: SHUTDOWN Server Name: managed-server3 Start Time: 2021-05-11T17:13:56.329656Z Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal DomainProcessingCompleted 59m Successfully completed processing domain resource fmwdomain In the Status section of the output, the available servers and clusters are listed. Note that if this command is issued very soon after the script finishes, there may be no servers available yet, or perhaps only the Administration Server but no Managed Servers. The operator will start up the Administration Server first and wait for it to become ready before starting the Managed Servers.\nVerify the pods Use the following command to see the pods running the servers:\n$ kubectl get pods -n NAMESPACE Here is an example of the output of this command:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE fmwdomain-admin-server 1/1 Running 0 14m fmwdomain-managed-server1 1/1 Running 0 12m Verify the services Use the following command to see the services for the domain:\n$ kubectl get services -n NAMESPACE Here is an example of the output of this command:\n$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE fmwdomain-admin-server ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 15h fmwdomain-admin-server-ext NodePort 10.101.26.42 \u0026lt;none\u0026gt; 7001:30731/TCP 15h fmwdomain-cluster-cluster-1 ClusterIP 10.107.55.188 \u0026lt;none\u0026gt; 8001/TCP 15h fmwdomain-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 15h Delete the domain The generated YAML file in the /\u0026lt;path to output-directory\u0026gt;/weblogic-domains/\u0026lt;domainUID\u0026gt; directory can be used to delete the Kubernetes resource. Use the following command to delete the domain:\n$ kubectl delete -f domain.yaml Delete the generated image. When no longer needed, delete the generated image. If the image is in a local repository, use the following command to delete an image tagged with domain-home-in-image:12.2.1.4:\n$ docker rmi domain-home-in-image:12.2.1.4 Delete the tools directory. When no longer needed, delete the directory where WebLogic Deploy Tool and WebLogic Image Tool are installed. By default, they are installed under /tmp/dhii-sample/tools directory.\n$ rm -rf /tmp/dhii-sample/tools/ Troubleshooting Message: Failed to build JDBC Connection object\nIf the WebLogic Image Tool failed to create a domain and the following error is seen in the output:\nConfiguring the Service Table DataSource... fmwDatabase jdbc:oracle:thin:@172.18.0.2:30012/devpdb.k8s Getting Database Defaults... Error: getDatabaseDefaults() failed. Do dumpStack() to see details. Error: runCmd() failed. Do dumpStack() to see details. Problem invoking WLST - Traceback (innermost last): File \u0026#34;/u01/oracle/createFMWDomain.py\u0026#34;, line 332, in ? File \u0026#34;/u01/oracle/createFMWDomain.py\u0026#34;, line 44, in createInfraDomain File \u0026#34;/u01/oracle/createFMWDomain.py\u0026#34;, line 151, in extendDomain File \u0026#34;/tmp/WLSTOfflineIni1609018487056199846.py\u0026#34;, line 267, in getDatabaseDefaults File \u0026#34;/tmp/WLSTOfflineIni1609018487056199846.py\u0026#34;, line 19, in command Failed to build JDBC Connection object: at com.oracle.cie.domain.script.jython.CommandExceptionHandler.handleException(CommandExceptionHandler.java:69) at com.oracle.cie.domain.script.jython.WLScriptContext.handleException(WLScriptContext.java:3085) at com.oracle.cie.domain.script.jython.WLScriptContext.runCmd(WLScriptContext.java:738) at sun.reflect.GeneratedMethodAccessor131.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) com.oracle.cie.domain.script.jython.WLSTException: com.oracle.cie.domain.script.jython.WLSTException: Got exception when auto configuring the schema component(s) with data obtained from shadow table: Failed to build JDBC Connection object: First, verify that the JDBC connection URL shown in the output is correct. Update the rcuDatabaseURL parameter in the inputs YAML file to the correct value if necessary.\nIf the JDBC connection URL is correct, it is possible that the container in which the WebLogic Image Tool is running for creating a WebLogic domain, is not using the correct networking stack. The optional -b option in the create-domain.sh script can be used to specify the networking mode for the RUN instruction during image build. For example, to use the host\u0026rsquo;s network stack, invoke create-domain.sh with -b host. Please refer to Docker Network Settings references for supported networking options.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/cicd/tools/",
	"title": "Tools",
	"tags": [],
	"description": "Tools that are available to build CI/CD pipelines.",
	"content": "WebLogic Deploy Tooling (WDT) You can use several of the WDT tools in a CI/CD pipeline. For example, the createDomain tool creates a new domain based on a simple model, and updateDomain (and deployApps) uses the same model concept to update an existing domain (preserving the same domain encryption key). The deployApps tool is very similar to the updateDomain tool, but limits what can be updated to application-related configuration attributes such as data sources and application archives. The model used by these tools is a sparse set of attributes needed to create or update the domain. A model can be as sparse as providing only the WebLogic Server administrative password, although not very interesting. A good way to get a jumpstart on a model is to use the discoverDomain tool in WDT which builds a model based on an existing domain.\nA Model in Image domain takes advantage of WDT by letting you specify an operator domain directly with a model instead of requiring that you supply a domain home.\n Other than the tools themselves, there are three components to the WDT tools:\n The Domain Model - Metadata model describing the desired domain.\nThe metadata domain model can be written in YAML or JSON and is documented here. The Archive ZIP - Binaries to supplement the model.\nAll binaries needed to supplement the model must be specified in an archive file, which is just a ZIP file with a specific directory structure. Optionally, the model can be stored inside the ZIP file, if desired. Any binaries not already on the target system must be in the ZIP file so that the tooling can extract them in the target domain. The Properties File - A standard Java properties file.\nA property file used to provide values to placeholders in the model.  WDT Create Domain Samples  (Docker) A sample for creating a domain in a container image with WDT can be found here. (Kubernetes) A similar sample of creating a domain in an image with WDT can be found in the WebLogic Kubernetes Operator project for creating a domain-in-image with WDT. (Kubernetes) A Model in Image sample for supplying an image that contains a WDT model only, instead of a domain home. In this case, the operator generates the domain home for you at runtime.  WebLogic Scripting Tool (WLST) You can use WLST scripts to create and update domain homes in a CI/CD pipeline for Domain in Image and Domain in PV domains. We recommend that you use offline WLST for this purpose. There may be some scenarios where it is necessary to use WLST online, but we recommend that you do that as an exception only, and when absolutely necessary.\nIf you do not already have WLST scripts, we recommend that you consider using WebLogic Deploy Tooling (WDT) instead. It provides a more declarative approach to domain creation, whereas WLST is more of an imperative scripting language. WDT provides advantages like being able to use the same model with different versions of WebLogic, whereas you may need to update WLST scripts manually when migrating to a new version of WebLogic for example.\nWebLogic pack and unpack tools WebLogic Server provides tools called \u0026ldquo;pack\u0026rdquo; and \u0026ldquo;unpack\u0026rdquo; that can be used to \u0026ldquo;clone\u0026rdquo; a domain home. These tools do not preserve the domain encryption key. You can use these tools to make copies of Domain in PV and Domain in Image domain homes in scenarios when you do not need the same domain encryption key. See Creating Templates and Domains Using the Pack and Unpack Commands.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/domain-lifecycle/liveness-readiness-probe-customization/",
	"title": "Liveness and readiness probes customization",
	"tags": [],
	"description": "This document describes how to customize the liveness and readiness probes for WebLogic Server instance Pods.",
	"content": "This document describes how to customize the liveness and readiness probes for WebLogic Server instance Pods.\nContents  Liveness probe customization Readiness probe customization  Liveness probe customization The liveness probe is configured to check that a server is alive by querying the Node Manager process. By default, the liveness probe is configured to check liveness every 45 seconds, to timeout after 5 seconds, and to perform the first check after 30 seconds. The default success and failure threshold values are 1. If a pod fails the liveness probe, Kubernetes will restart that container.\nYou can customize the liveness probe initial delay, interval, timeout, and failure threshold using the livenessProbe attribute under the serverPod element of the domain resource.\nFollowing is an example configuration to change the liveness probe interval, timeout, and failure threshold value.\nserverPod: livenessProbe: periodSeconds: 30 timeoutSeconds: 10 failureThreshold: 3 Note: The liveness probe success threshold value must always be 1. See Configure Probes in the Kubernetes documentation for more details.\nAfter the liveness probe script (livenessProbe.sh) performs its normal checks, you can customize the liveness probe by specifying a custom script, which will be invoked by livenessProbe.sh. You can specify the custom script either by using the livenessProbeCustomScript attribute in the domain resource or by setting the LIVENESS_PROBE_CUSTOM_SCRIPT environment variable using the env attribute under the serverPod element (see the configuration examples below). If the custom script fails with a non-zero exit status, the liveness probe will fail and Kubernetes will restart the container.\n The spec.livenessProbeCustomScript domain resource attribute affects all WebLogic Server instance Pods in the domain. The LIVENESS_PROBE_CUSTOM_SCRIPT environment variable takes precedence over the spec.livenessProbeCustomScript domain resource attribute when both are configured, and, like all domain resource environment variables, can be customized on a per domain, per cluster, or even a per server basis. Changes to either the domain resource attribute or the environment variable on a running domain take effect on running WebLogic Server instance Pods when such Pods are restarted (rolled).  Note: The liveness probe custom script option is for advanced usage only and its value is not set by default. If the specified script is not found, then the custom script is ignored and the existing liveness script will perform its normal checks.\nNote: Oracle recommends against having any long running calls (for example, any network calls or executing wlst.sh) in the liveness probe custom script.\nUse the following configuration to specify a liveness probe custom script using the livenessProbeCustomScript domain resource field.\nspec: livenessProbeCustomScript: /u01/customLivenessProbe.sh Use the following configuration to specify the liveness probe custom script using the LIVENESS_PROBE_CUSTOM_SCRIPT environment variable.\nserverPod: env: - name: LIVENESS_PROBE_CUSTOM_SCRIPT value: /u01/customLivenessProbe.sh The following operator-populated environment variables are available for use in the liveness probe custom script, which will be invoked by livenessProbe.sh.\nORACLE_HOME or MW_HOME: The Oracle Fusion Middleware software location as a file system path within the container.\nWL_HOME: The Weblogic Server installation location as a file system path within the container.\nDOMAIN_HOME: The domain home location as a file system path within the container.\nJAVA_HOME: The Java software installation location as a file system path within the container.\nDOMAIN_NAME: The WebLogic Server domain name.\nDOMAIN_UID: The domain unique identifier.\nSERVER_NAME: The WebLogic Server instance name.\nLOG_HOME: The WebLogic log location as a file system path within the container. This variable is available only if its value is set in the configuration.\nNOTES:\n  Additional operator-populated environment variables that are not listed above, are not supported for use in the liveness probe custom script.\n  The custom liveness probe script can call source $DOMAIN_HOME/bin/setDomainEnv.sh if it needs to set up its PATH or CLASSPATH to access WebLogic utilities in its domain.\n  A custom liveness probe must not fail (exit non-zero) when the WebLogic Server instance itself is unavailable. This could be the case when the WebLogic Server instance is booting or about to boot.\n  Readiness probe customization By default, the readiness probe is configured to use the WebLogic Server ReadyApp framework. The ReadyApp framework allows fine customization of the readiness probe by the application\u0026rsquo;s participation in the framework. For more details, see Using the ReadyApp Framework. The readiness probe is used to determine if the server is ready to accept user requests. The readiness is used to determine when a server should be included in a load balancer\u0026rsquo;s endpoints, in the case of a rolling restart, when a restarted server is fully started, and for various other purposes.\nBy default, the readiness probe is configured to check readiness every 5 seconds, to timeout after 5 seconds, and to perform the first check after 30 seconds. The default success and failure thresholds values are 1. You can customize the readiness probe initial delay, interval, timeout, success and failure thresholds using the readinessProbe attribute under the serverPod element of the domain resource.\nFollowing is an example configuration to change readiness probe interval, timeout and failure threshold value.\nserverPod: readinessProbe: periodSeconds: 10 timeoutSeconds: 10 failureThreshold: 3 "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/ingress/",
	"title": "Ingress",
	"tags": [],
	"description": "",
	"content": "Ingresses are one approach provided by Kubernetes to configure load balancers. Depending on the version of Kubernetes you are using, and your cloud provider, you may need to use Ingresses. For more information about Ingresses, see the Ingress documentation.\nWebLogic clusters as backends of an Ingress In an Ingress object, a list of backends are provided for each target that will be load balanced. Each backend is typically a Kubernetes Service, more specifically, a combination of a serviceName and a servicePort.\nWhen the operator creates a WebLogic domain, it also creates a service for each WebLogic cluster in the domain. The operator defines the service such that its selector will match all WebLogic Server pods within the WebLogic cluster which are in the \u0026ldquo;ready\u0026rdquo; state.\nThe name of the service created for a WebLogic cluster follows the pattern \u0026lt;domainUID\u0026gt;-cluster-\u0026lt;clusterName\u0026gt;. For example, if the domainUID is domain1 and the cluster name is cluster-1, the corresponding service will be named domain1-cluster-cluster-1.\nThe service name must comply with standard Kubernetes rules for naming of objects and in particular with DNS-1035:\n A DNS-1035 label must consist of lowercase alphanumeric characters or \u0026lsquo;-\u0026rsquo;, start with an alphabetic character, and end with an alphanumeric character (e.g. my-name, or abc-123, regex used for validation is [a-z]([-a-z0-9]*[a-z0-9])?).\n To comply with these requirements, if the domainUID or the cluster name contains some upper-case characters or underscores, then in the service name the upper-case characters will be converted to lower-case and underscores will be converted to hyphens. For example, if the domainUID is myDomain_1 and the cluster name is myCluster_1, the corresponding service will be named mydomain-1-cluster-mycluster-1.\nThe service, serviceName and servicePort, of a WebLogic cluster will be used in the routing rules defined in the Ingress object and the load balancer will route traffic to the WebLogic Servers within the cluster based on the rules.\nMost common ingress controllers, for example Traefik, Voyager, and NGINX, understand that there are zero or more actual pods behind the service, and they actually build their backend list and route requests to those backends directly, not through the service. This means that requests are properly balanced across the pods, according to the load balancing algorithm in use. Most ingress controllers also subscribe to updates on the service and adjust their internal backend sets when additional pods become ready, or pods enter a non-ready state.\n Steps to set up an ingress load balancer   Install the ingress controller.\nAfter the ingress controller is running, it monitors Ingress resources in a given namespace and acts accordingly.\n  Create Ingress resources.\nIngress resources contain routing rules to one or more backends. An ingress controller is responsible to apply the rules to the underlying load balancer. There are two approaches to create the Ingress resource:\n  Use the Helm chart ingress-per-domain.\nEach ingress provider supports a number of annotations in Ingress resources. This Helm chart allows you to define the routing rules without dealing with the detailed provider-specific annotations. Currently we support two ingress providers: Traefik and Voyager.\n  Create the Ingress resource manually from a YAML file.\nManually create an Ingress YAML file and then apply it to the Kubernetes cluster.\n    Guide and samples for Traefik, Voyager/HAProxy, and NGINX Information about how to install and configure these ingress controllers to load balance WebLogic clusters is provided here:\n Traefik guide Voyager guide NGINX guide  For production environments, we recommend NGINX, Voyager, Traefik (2.2.1 or later) ingress controllers, Apache, or the load balancer provided by your cloud provider.\n Samples are also provided for these two ingress controllers, showing how to manage multiple WebLogic clusters as the backends, using different routing rules, host-routing and path-routing; and TLS termination:\n Traefik samples Voyager samples  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/security/secrets/",
	"title": "Secrets",
	"tags": [],
	"description": "Kubernetes Secrets for the operator",
	"content": "Contents  Domain credentials secret Domain image pull secret Operator image pull secret Operator configuration override secrets Operator external REST interface secret Operator internal REST interface secret  Domain credentials secret The credentials for the WebLogic domain are kept in a Kubernetes Secret where the name of the secret is specified using webLogicCredentialsSecret in the WebLogic Domain resource. Also, the domain credentials secret must be created in the namespace where the Domain will be running.\nFor an example of a WebLogic Domain YAML file using webLogicCredentialsSecret, see Container Image Protection.\n The samples supplied with the operator use a naming convention that follows the pattern \u0026lt;domainUID\u0026gt;-weblogic-credentials, where \u0026lt;domainUID\u0026gt; is the unique identifier of the domain, for example, domain1-weblogic-credentials.\nIf the WebLogic domain will be started in domain1-ns and the \u0026lt;domainUID\u0026gt; is domain1, an example of creating a Kubernetes generic secret is as follows:\n$ kubectl -n domain1-ns create secret generic domain1-weblogic-credentials \\  --from-file=username --from-file=password $ kubectl -n domain1-ns label secret domain1-weblogic-credentials \\  weblogic.domainUID=domain1 weblogic.domainName=domain1  Oracle recommends that you not include unencrypted passwords on command lines. Passwords and other sensitive data can be prompted for or looked up by shell scripts or tooling. For more information about creating Kubernetes Secrets, see the Kubernetes Secrets documentation.\n The operator\u0026rsquo;s introspector job will expect the secret key names to be:\n username password  For example, here is the result when describing the Kubernetes Secret:\n$ kubectl -n domain1-ns describe secret domain1-weblogic-credentials Name: domain1-weblogic-credentials Namespace: domain1-ns Labels: weblogic.domainName=domain1 weblogic.domainUID=domain1 Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== password: 8 bytes username: 8 bytes Domain image pull secret The WebLogic domain that the operator manages can have images that are protected in the registry. The imagePullSecrets setting on the Domain can be used to specify the Kubernetes Secret that holds the registry credentials.\nFor more information, see Container Image Protection.\n Operator image pull secret The Helm chart for installing the operator has an option to specify the image pull secret used for the operator\u0026rsquo;s image when using a private registry. The Kubernetes Secret of type docker-registry should be created in the namespace where the operator is deployed.\nHere is an example of using the helm install command to set the image name and image pull secret:\n$ helm install my-weblogic-operator kubernetes/charts/weblogic-operator \\  --set \u0026#34;image=my.io/my-operator-image:1.0\u0026#34; \\  --set \u0026#34;imagePullSecrets[0].name=my-operator-image-pull-secret\u0026#34; \\  --namespace weblogic-operator-ns \\  --wait  For more information, see Install the operator Helm chart.\n Operator configuration override secrets The operator supports embedding macros within configuration override templates that reference Kubernetes Secrets. These Kubernetes Secrets can be created with any name in the namespace where the Domain will be running. The Kubernetes Secret names are specified using configuration.secrets in the WebLogic Domain resource.\nFor more information, see Configuration overrides.\n Operator external REST interface secret The operator can expose an external REST HTTPS interface which can be accessed from outside the Kubernetes cluster. A Kubernetes tls secret is used to hold the certificates and private key.\nFor more information, see Certificates.\n Operator internal REST interface secret The operator exposes an internal REST HTTPS interface with a self-signed certificate. The certificate is kept in a Kubernetes ConfigMap with the name weblogic-operator-cm using the key internalOperatorCert. The private key is kept in a Kubernetes Secret with the name weblogic-operator-secrets using the key internalOperatorKey. These Kubernetes objects are managed by the operator\u0026rsquo;s Helm chart and are part of the namespace where the operator is installed.\nFor example, to see all the operator\u0026rsquo;s ConfigMaps and secrets when installed into the Kubernetes Namespace weblogic-operator-ns, use:\n$ kubectl -n weblogic-operator-ns get cm,secret "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/elastic-stack/",
	"title": "Elastic Stack",
	"tags": [],
	"description": "",
	"content": "  Operator  Sample for configuring the Elasticsearch and Kibana deployments and services for the operator\u0026#39;s logs.\n WebLogic domain  Sample for using Fluentd for WebLogic domain and operator\u0026#39;s logs.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/domains/model-in-image/update4/",
	"title": "Update 4",
	"tags": [],
	"description": "",
	"content": "This use case demonstrates dynamically changing the Work Manager threads constraint and data source configuration in your running domain without restarting (rolling) running WebLogic Servers. This use case requires that the Update 1 use case has been run and expects that its sample-domain1 domain is deployed and running.\nIn the use case, you will:\n Update the ConfigMap containing the WDT model created in the Update 1 use case with changes to the Work Manager threads constraint configuration. Update the data source secret created in the Update 1 use case to provide the correct password and an increased maximum pool capacity. Update the Domain YAML file to enable the Model in Image online update feature. Update the Domain YAML file to trigger a domain introspection, which applies the new configuration values without restarting servers. Optionally, start a database (to demonstrate that the updated data source attributes have taken effect).  Here are the steps:\n  Make sure that you have deployed the domain from the Update 1 use case, or have deployed an updated version of this same domain from the Update 3 use case.\nThere should be three WebLogic Server pods with names that start with sample-domain1 running in the sample-domain1-ns namespace, a domain named sample-domain1, a ConfigMap named sample-domain1-wdt-config-map, and a Secret named sample-domain1-datasource-secret.\n  Add the Work Manager threads constraint configuration WDT model updates to the existing data source model updates in the Model in Image model ConfigMap.\nIn this step, we will update the model ConfigMap from the Update 1 use case with the desired changes to the minimum and maximum threads constraints.\nHere\u0026rsquo;s an example model configuration that updates the configured count values for the SampleMinThreads minimum threads constraint and SampleMaxThreads maximum threads constraint:\nresources: SelfTuning: MinThreadsConstraint: SampleMinThreads: Count: 2 MaxThreadsConstraint: SampleMaxThreads: Count: 20 Optionally, place the preceding model snippet in a file named /tmp/mii-sample/myworkmanager.yaml and then use it when deploying the updated model ConfigMap, or simply use the same model snippet that\u0026rsquo;s provided in /tmp/mii-sample/model-configmaps/workmanager/model.20.workmanager.yaml.\nRun the following commands:\n$ kubectl -n sample-domain1-ns delete configmap sample-domain1-wdt-config-map $ kubectl -n sample-domain1-ns create configmap sample-domain1-wdt-config-map \\  --from-file=/tmp/mii-sample/model-configmaps/workmanager \\  --from-file=/tmp/mii-sample/model-configmaps/datasource $ kubectl -n sample-domain1-ns label configmap sample-domain1-wdt-config-map \\  weblogic.domainUID=sample-domain1 Notes:\n If you\u0026rsquo;ve created your own model YAML file(s), then substitute the file names in the --from-file= parameters (we suggested /tmp/mii-sample/myworkmanager.yaml and /tmp/mii-sample/mydatasource.xml earlier). The -from-file= parameter can reference a single file, in which case it puts the designated file in the ConfigMap, or it can reference a directory, in which case it populates the ConfigMap with all of the files in the designated directory. It can be specified multiple times on the same command line to load the contents from multiple locations into the ConfigMap. You name and label the ConfigMap using its associated domain UID for two reasons:  To make it obvious which ConfigMap belong to which domains. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all resources associated with a domain.      Update the data source secret that you created in the Update 1 use case with the correct password as well as with an increased maximum pool capacity:\n$ kubectl -n sample-domain1-ns delete secret sample-domain1-datasource-secret $ kubectl -n sample-domain1-ns create secret generic \\  sample-domain1-datasource-secret \\  --from-literal=\u0026#39;user=sys as sysdba\u0026#39; \\  --from-literal=\u0026#39;password=Oradoc_db1\u0026#39; \\  --from-literal=\u0026#39;max-capacity=10\u0026#39; \\  --from-literal=\u0026#39;url=jdbc:oracle:thin:@oracle-db.default.svc.cluster.local:1521/devpdb.k8s\u0026#39; $ kubectl -n sample-domain1-ns label secret \\  sample-domain1-datasource-secret \\  weblogic.domainUID=sample-domain1   Optionally, start the database.\n  If the database is running, then the sample application that we will run at the end of this use case, will verify that your updates to the data source secret took effect.\n  If you are taking the JRF path through the sample, then the database will already be running.\n  If you are taking the WLS path through the sample, then you can deploy the database by:\n Following the first step in Set up and initialize an infrastructure database. This step is titled, \u0026ldquo;Ensure that you have access to the database image, and then create a deployment using it.\u0026rdquo; You can skip the remaining steps (they are only needed for JRF).      Update your Domain YAML file to enable onlineUpdate.\nIf onlineUpdate is enabled for your domain and the only model changes are to WebLogic Domain dynamic attributes, then the operator will attempt to update the running domains online without restarting the servers when you update the domain\u0026rsquo;s introspectVersion.\n  Option 1: Edit your domain custom resource.\n Call kubectl -n sample-domain1-ns edit domain sample-domain1. Add or edit the value of the spec.configuration.model.onlineUpdate stanza so it contains enabled: true and save. The updated domain should look something like this: ... spec: ... configuration: ... model: ... onlineUpdate: enabled: true     Option 2: Dynamically change your domain using kubectl patch. For example:\n$ kubectl -n sample-domain1-ns patch domain sample-domain1 --type=json \u0026#39;-p=[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/configuration/model/onlineUpdate\u0026#34;, \u0026#34;value\u0026#34;: {\u0026#34;enabled\u0026#34; : \u0026#39;true\u0026#39;} }]\u0026#39;   Option 3: Use the sample helper script.\n Call /tmp/mii-sample/utils/patch-enable-online-update.sh -n sample-domain1-ns -d sample-domain1. This will perform the same kubectl patch commands as Option 2.      Prompt the operator to introspect the updated WebLogic domain configuration.\nNow that the updated Work Manager configuration is deployed in an updated model ConfigMap and the updated data source configuration is reflected in the updated data source Secret, we need to have the operator rerun its introspector job in order to regenerate its configuration.\nChange the spec.introspectVersion of the domain to trigger domain introspection. To do this:\n  Option 1: Edit your domain custom resource.\n Call kubectl -n sample-domain1-ns edit domain sample-domain1. Change the value of the spec.introspectVersion field and save. The field is a string; typically, you use a number in this field and increment it.    Option 2: Dynamically change your domain using kubectl patch.\n  Get the current introspectVersion:\n$ kubectl -n sample-domain1-ns get domain sample-domain1 \u0026#39;-o=jsonpath={.spec.introspectVersion}\u0026#39;   Choose a new introspect version that\u0026rsquo;s different from the current introspect version.\n The field is a string; typically, you use a number in this field and increment it.    Use kubectl patch to set the new value. For example, assuming the new introspect version is 2:\n$ kubectl -n sample-domain1-ns patch domain sample-domain1 --type=json \u0026#39;-p=[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/introspectVersion\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2\u0026#34; }]\u0026#39;     Option 3: Use the sample helper script.\n Call /tmp/mii-sample/utils/patch-introspect-version.sh -n sample-domain1-ns -d sample-domain1. This will perform the same kubectl patch command as Option 2.    Because we have set the enabled value in spec.configuration.model.onlineUpdate to true, and all of the model changes we have specified are for WebLogic dynamic configuration attributes, we expect that the domain introspector job will apply the changes to the WebLogic Servers without restarting (rolling) their pods.\n  Wait for the introspector job to run to completion. You can:\n  Call kubectl get pods -n sample-domain1-ns --watch and wait for the introspector pod to get into Terminating state and exit.\nsample-domain1-introspector-vgxxl 0/1 Terminating 0 78s   Alternatively, run /tmp/mii-sample/utils/wl-pod-wait.sh -p 3.\nThis is a utility script that provides useful information about a domain\u0026rsquo;s pods and waits for them to reach a ready state, reach their target restartVersion, reach their target introspectVersion, and reach their target image before exiting.\n  Click here to display the `wl-pod-wait.sh` usage.   $ ./wl-pod-wait.sh -?  Usage: wl-pod-wait.sh [-n mynamespace] [-d mydomainuid] \\ [-p expected_pod_count] \\ [-t timeout_secs] \\ [-q] Exits non-zero if 'timeout_secs' is reached before 'pod_count' is reached. Parameters: -d \u0026lt;domain_uid\u0026gt; : Defaults to 'sample-domain1'. -n \u0026lt;namespace\u0026gt; : Defaults to 'sample-domain1-ns'. -p 0 : Wait until there are no running WebLogic Server pods for a domain. The default. -p \u0026lt;pod_count\u0026gt; : Wait until all of the following are true for exactly 'pod_count' WebLogic Server pods in the domain: - ready - same 'weblogic.domainRestartVersion' label value as the domain resource's 'spec.restartVersion' - same 'weblogic.introspectVersion' label value as the domain resource's 'spec.introspectVersion' - same image as the the domain resource's image -t \u0026lt;timeout\u0026gt; : Timeout in seconds. Defaults to '1000'. -q : Quiet mode. Show only a count of wl pods that have reached the desired criteria. -? : This help.      Click here to view sample output from `wl-pod-wait.sh` that shows the introspector running and that shows each domain pod reach its new `introspectVersion`.   $ ./wl-pod-wait.sh -n sample-domain1-ns -d sample-domain1 -p 3 @@ [2020-11-21T05:55:26][seconds=0] Info: Waiting up to 1000 seconds for exactly '3' WebLogic Server pods to reach the following criteria: @@ [2020-11-21T05:55:26][seconds=0] Info: ready='true' @@ [2020-11-21T05:55:26][seconds=0] Info: image='model-in-image:WLS-v2' @@ [2020-11-21T05:55:26][seconds=0] Info: domainRestartVersion='1' @@ [2020-11-21T05:55:26][seconds=0] Info: introspectVersion='2' @@ [2020-11-21T05:55:26][seconds=0] Info: namespace='sample-domain1-ns' @@ [2020-11-21T05:55:26][seconds=0] Info: domainUID='sample-domain1' @@ [2020-11-21T05:55:26][seconds=0] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-11-21T05:55:26][seconds=0] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME RVERSION IVERSION IMAGE READY PHASE ----------------------------------- -------- -------- ----------------------- ------ --------- 'sample-domain1-admin-server' '1' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-introspector-4gpdj' '' '' '' '' 'Pending' 'sample-domain1-managed-server1' '1' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server2' '1' '1' 'model-in-image:WLS-v2' 'true' 'Running' @@ [2020-11-21T05:55:28][seconds=2] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-11-21T05:55:28][seconds=2] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME RVERSION IVERSION IMAGE READY PHASE ----------------------------------- -------- -------- ----------------------- ------ --------- 'sample-domain1-admin-server' '1' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-introspector-4gpdj' '' '' '' '' 'Running' 'sample-domain1-managed-server1' '1' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server2' '1' '1' 'model-in-image:WLS-v2' 'true' 'Running' @@ [2020-11-21T05:56:53][seconds=87] Info: '0' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-11-21T05:56:53][seconds=87] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME RVERSION IVERSION IMAGE READY PHASE ----------------------------------- -------- -------- ----------------------- ------ ----------- 'sample-domain1-admin-server' '1' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-introspector-4gpdj' '' '' '' '' 'Succeeded' 'sample-domain1-managed-server1' '1' '1' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server2' '1' '1' 'model-in-image:WLS-v2' 'true' 'Running' @@ [2020-11-21T05:56:54][seconds=88] Info: '3' WebLogic Server pods currently match all criteria, expecting '3'. @@ [2020-11-21T05:56:54][seconds=88] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME RVERSION IVERSION IMAGE READY PHASE -------------------------------- -------- -------- ----------------------- ------ --------- 'sample-domain1-admin-server' '1' '2' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server1' '1' '2' 'model-in-image:WLS-v2' 'true' 'Running' 'sample-domain1-managed-server2' '1' '2' 'model-in-image:WLS-v2' 'true' 'Running' @@ [2020-11-21T05:56:54][seconds=88] Info: Success!      If the introspector job fails, then consult Debugging in the Model in Image user guide.\n    Call the sample web application to:\n Determine if the configuration of the minimum and maximum threads constraints have been updated to the new values. Determine if the data source can now contact the database (assuming you deployed the database).  Send a web application request to the ingress controller:\n$ curl -s -S -m 10 -H \u0026#39;host: sample-domain1-cluster-cluster-1.mii-sample.org\u0026#39; \\  http://localhost:30305/myapp_war/index.jsp Or, if Traefik is unavailable and your Administration Server pod is running, you can run kubectl exec:\n$ kubectl exec -n sample-domain1-ns sample-domain1-admin-server -- bash -c \\  \u0026#34;curl -s -S -m 10 http://sample-domain1-cluster-cluster-1:8001/myapp_war/index.jsp\u0026#34; You will see something like the following:\n\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v2\u0026#39; of the mii-sample JSP web-app. Welcome to WebLogic server \u0026#39;managed-server2\u0026#39;! domain UID = \u0026#39;sample-domain1\u0026#39; domain name = \u0026#39;domain1\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found min threads constraint runtime named \u0026#39;SampleMinThreads\u0026#39; with configured count: 2 Found max threads constraint runtime named \u0026#39;SampleMaxThreads\u0026#39; with configured count: 20 Found 1 local data source: Datasource \u0026#39;mynewdatasource\u0026#39;: State=\u0026#39;Running\u0026#39;, testPool=\u0026#39;Passed\u0026#39; ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;   The testPool='Passed' for mynewdatasource verifies that your update to the data source Secret to correct the password succeeded.\nIf you see a testPool='Failed' error, then it is likely you did not deploy the database or your database is not deployed correctly.\nIf you see any other error, then consult Debugging in the Model in Image user guide.\nThis completes the sample scenarios.\nTo remove the resources you have created in the samples, see Cleanup.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/developerguide/code-structure/",
	"title": "Code structure",
	"tags": [],
	"description": "",
	"content": "This project has the following directory structure:\n documentation/latest: This documentation documentation/\u0026lt;numbered directory\u0026gt;: The archived documentation for a previous release documentation/charts: Helm chart repository documentation/swagger: The operator REST API swagger documentation/domains: Reference for Domain resource schema json-schema-generator: Java model to JSON schema generator json-schema-maven-plugin: Maven plugin for schema generator kubernetes/charts: Helm charts kubernetes/samples: All samples, including for WebLogic domain creation integration-tests: JUnit 5 integration test suite operator: Operator runtime swagger-generator: Swagger file generator for the Kubernetes API server and Domain type  Watch package The Watch API in the Kubernetes Java client provides a watch capability across a specific list of resources for a limited amount of time. As such, it is not ideally suited for our use case, where a continuous stream of watches is desired, with watch events generated in real time. The watch-wrapper in this repository extends the default Watch API to provide a continuous stream of watch events until the stream is specifically closed. It also provides resourceVersion tracking to exclude events that have already been seen. The watch-wrapper provides callbacks so events, as they occur, can trigger actions.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/base-images/",
	"title": "WebLogic Server images",
	"tags": [],
	"description": "Create or obtain WebLogic Server images.",
	"content": "Contents  Create or obtain WebLogic Server images Set up secrets to access the Oracle Container Registry Obtain standard images from the Oracle Container Registry Create a custom image with patches applied Create a custom image with your domain inside the image Patch WebLogic Server images Apply patched images to a running domain  Create or obtain WebLogic Server images You will need container images to run your WebLogic domains in Kubernetes. There are two main options available:\n Use an image which contains the WebLogic Server binaries but not the domain, or Use an image which contains both the WebLogic Server binaries and the domain directory.  If you want to use the first option, then you will need to obtain the standard WebLogic Server image from the Oracle Container Registry; see Obtain standard images from the Oracle Container Registry. This image already contains the mandatory patches applied, as described in Create a custom image with patches applied. If you want to use additional patches, you can customize that process to include additional patches.\nIf you want to use the second option, which includes the domain directory inside the image, then you will need to build your own images, as described in Create a custom image with your domain inside the image.\nSet up secrets to access the Oracle Container Registry This version of the operator requires WebLogic Server 12.2.1.3.0 plus patch 29135930; the standard image, container-registry.oracle.com/middleware/weblogic:12.2.1.3, already includes this patch pre-applied. Images for WebLogic Server 12.2.1.4.0 do not require any patches.\n In order for Kubernetes to obtain the WebLogic Server image from the Oracle Container Registry (OCR), which requires authentication, a Kubernetes Secret containing the registry credentials must be created. To create a secret with the OCR credentials, issue the following command:\n$ kubectl create secret docker-registry SECRET_NAME \\  -n NAMESPACE \\  --docker-server=container-registry.oracle.com \\  --docker-username=YOUR_USERNAME \\  --docker-password=YOUR_PASSWORD \\  --docker-email=YOUR_EMAIL In this command, replace the uppercase items with the appropriate values. The SECRET_NAME will be needed in later parameter files. The NAMESPACE must match the namespace where the first domain will be deployed, otherwise, Kubernetes will not be able to find it.\nIt may be preferable to manually pull the image in advance, on each Kubernetes worker node, as described in the next section. If you choose this approach, you do not require the Kubernetes Secret.\nObtain standard images from the Oracle Container Registry The Oracle Container Registry contains images for licensed commercial Oracle software products that you may use in your enterprise. To access the Oracle Registry Server, you must have an Oracle Single Sign-On (SSO) account. The Oracle Container Registry provides a web interface that allows an administrator to authenticate and then to select the images for the software that your organization wishes to use. Oracle Standard Terms and Restrictions terms must be agreed to using the web interface. After the Oracle Standard Terms and Restrictions have been accepted, you can pull images of the software from the Oracle Container Registry using the standard docker pull command.\nTo pull an image from the Oracle Container Registry, in a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms is stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nThe Oracle Container Registry provides a WebLogic Server 12.2.1.3.0 image, which already has the necessary patches applied, and the Oracle WebLogic Server 12.2.1.4.0 and 14.1.1.0.0 images, which do not require any patches.\nFirst, you will need to log in to the Oracle Container Registry:\n$ docker login container-registry.oracle.com Then, you can pull the image with this command:\n$ docker pull container-registry.oracle.com/middleware/weblogic:12.2.1.4 If desired, you can:\n  Check the WLS version with docker run container-registry.oracle.com/middleware/weblogic:12.2.1.4 sh -c 'source $ORACLE_HOME/wlserver/server/bin/setWLSEnv.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; java weblogic.version'\n  Check the WLS patches with docker run container-registry.oracle.com/middleware/weblogic:12.2.1.4 sh -c '$ORACLE_HOME/OPatch/opatch lspatches'\n  Additional information about using this image is available in the Oracle Container Registry.\nCreate a custom image with patches applied The WebLogic Kubernetes Operator and WebLogic Server 12.2.1.3.0 image requires patch 29135930. This patch has some prerequisite patches that will also need to be applied. The WebLogic Server 12.2.1.4.0 image does not require any patches. To create and customize the WebLogic Server image, and apply the required patches, use the WebLogic Image Tool.\nTo use the Image Tool, follow the Setup instructions and Quick Start Guide.\nTo build the WebLogic Server image and apply the patches:\n  Add the Server JRE and the WebLogic Server installer to the cache command.\n$ imagetool cache addInstaller \\ --type=jdk \\ --version=8u241 \\ --path=/home/acmeuser/wls-installers/jre-8u241-linux-x64.tar.gz $ imagetool cache addInstaller \\ --type=wls \\ --version=12.2.1.4.0 \\ --path=/home/acmeuser/wls-installers/fmw_12.2.1.4.0_wls_Disk1_1of1.zip   Use the Create Tool to build the image and apply the patches.\nFor the Create Tool to download the patches, you must supply your My Oracle Support credentials.\n$ imagetool create \\ --tag=weblogic:12.2.1.3 \\ --type=wls \\ --version=12.2.1.3.0 \\ --jdkVersion=8u241 \\ -–patches=29135930_12.2.1.3.0,27117282_12.2.1.3.0 \\ --user=username.mycompany.com \\ --passwordEnv=MYPWD   After the tool creates the image, verify that the image is in your local repository:\n$ docker images   Create a custom image with your domain inside the image You can also create an image with the WebLogic domain inside the image. Samples are provided that demonstrate how to create the image using either:\n WLST to define the domain, or WebLogic Deploy Tooling to define the domain.  In these samples, you will see a reference to a \u0026ldquo;base\u0026rdquo; or FROM image. You should use an image with the mandatory patches installed as this base image. This image could be either the standard container-registry.oracle.com/middleware/weblogic:12.2.1.3 pre-patched image or an image you created using the instructions above. WebLogic Server 12.2.1.4.0 images do not require patches.\nOracle strongly recommends storing a domain image as private in the registry. A container image that contains a WebLogic domain home has sensitive information including keys and credentials that are used to access external resources (for example, the data source password). For more information, see WebLogic domain in container image protection.\n Patch WebLogic Server images Use the WebLogic Image Tool (WIT) to patch WebLogic Server images with quarterly Patch Set Updates (PSUs), which include security fixes, or with one-off patches.\nUse either the WIT create or update command, however, patching using the create command results in a smaller WebLogic Server image size. Note that you will need to download the WebLogic Server 12.2.1.4.0 installer and JDK installer prior to running the create command. For details, see the WIT Quick Start guide.\nExample: Create an image named sample:wls with the WebLogic Server 12.2.1.4.0 slim installer, JDK 8u291, a slim version of the Oracle Linux 7 container image, and latest PSU and recommended CPU and SPU patches applied.\n$ imagetool create --tag sample:wls --type=wlsslim --recommendedPatches --pull --user testuser@xyz.com --password hello --version=12.2.1.4.0 --jdkVersion=8u291 Apply patched images to a running domain When updating the WebLogic binaries of a running domain in Kubernetes with a patched container image, the operator applies the update in a zero downtime fashion. The procedure for the operator to update the running domain differs depending on the domain home source type. One difference between the domain home source types is the location of the domain home:\n Domain in PV: The container image contains the JDK and WebLogic Server binaries. The domain home is located in a Persistent Volume (PV). Model in Image: The container image contains the JDK, WebLogic Server binaries, and a WebLogic Deployment Tooling (WDT) installation, WDT model file, and application archive file. Domain in Image: The container image contains the JDK, WebLogic Server binaries, and domain home.  For Domain in PV, the operator can apply the update to the running domain without modifying the patched container image. For Model in Image (MII) and Domain in Image, before the operator can apply the update, the patched container images must be modified to add the domain home or a WDT model and archive. You use WebLogic Image Tool (WIT) Rebase Image to update the Oracle Home of the original image using the patched Oracle Home from a patched container image.\nIn all three domain home source types, you edit the Domain Resource to inform the operator of the name of the new patched image so that it can manage the update of the WebLogic domain.\nFor a broader description of managing the evolution and mutation of container images to run WebLogic Server in Kubernetes, see CI/CD.\nDomain in PV Edit the Domain Resource image reference with the new image name/tag (for example, oracle/weblogic:12.2.1.4-patched). Then, the operator performs a rolling restart of the WebLogic domain to update the Oracle Home of the servers. For information on server restarts, see Restarting.\nModel in Image Use the WIT rebase command to update the Oracle Home for an existing image with the model and archive files in the image using the patched Oracle Home from a patched container image. Then, the operator performs a rolling update of the domain, updating the Oracle Home of each server pod.\nExample: WIT copies a WDT model and WDT archive from the source image, mydomain:v1, to a new image, mydomain:v2, based on a target image named oracle/weblogic:generic-12.2.1.4.0-patched. Note: Oracle Home and the JDK must be installed in the same directories on each image.\n$ imagetool rebase --tag mydomain:v2 --sourceImage mydomain:v1 --targetImage oracle/weblogic:generic-12.2.1.4.0-patched --wdtModelOnly Then, edit the Domain Resource image reference with the new image name/tag (mydomain:2).\nDomain in Image Use the WIT rebase command to update the Oracle Home for an existing domain image using the patched Oracle Home from a patched container image. Then, the operator performs a rolling update of the domain, updating the Oracle Home of each server pod.\nExample: WIT copies the domain from the source image, mydomain:v1, to a new image, mydomain:v2, based on a target image named oracle/weblogic:12.2.1.4-patched.\nNote: Oracle Home and the JDK must be installed in the same directories on each image.\n$ imagetool rebase --tag mydomain:v2 --sourceImage mydomain:v1 --targetImage oracle/weblogic:12.2.1.4-patched Then, edit the Domain Resource image reference with the new image name/tag (mydomain:2).\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/quickstart/create-domain/",
	"title": "Create a domain",
	"tags": [],
	"description": "",
	"content": "  For use in the following steps:\n Select a user name and password, following the required rules for password creation (at least 8 alphanumeric characters with at least one number or special character). Pick or create a directory to which you can write output.    Create a Kubernetes Secret for the WebLogic domain administrator credentials containing the username and password for the domain, using the create-weblogic-credentials script:\n$ kubernetes/samples/scripts/create-weblogic-domain-credentials/create-weblogic-credentials.sh \\  -u \u0026lt;username\u0026gt; -p \u0026lt;password\u0026gt; -n sample-domain1-ns -d sample-domain1 The sample will create a secret named domainUID-weblogic-credentials where the domainUID is replaced with the value specified by the -d flag. For example, the command above would create a secret named sample-domain1-weblogic-credentials.\n  Create a new image with a domain home, plus create a domain resource that the operator will use to deploy the image, by running the create-domain script.\nThe script\u0026rsquo;s behavior is controlled by an inputs file plus command-line options. The script downloads the WebLogic Image Tool and WebLogic Deploy Tool and uses these tools to create a new image with a domain home. The script also creates a domain resource YAML file that references the image, and, if the -e option is specified, deploys the domain resource to Kubernetes. For a detailed understanding of the steps that the create-domain.sh script performs for you, see the bulleted items under Use the script to create a domain\nThe create-domain.sh script and its inputs file are for demonstration purposes only; its contents and the domain resource file that it generates for you might change without notice. In production, we strongly recommend that you use the WebLogic Image Tool and WebLogic Deploy Tooling (when applicable), and directly work with domain resource files instead.\n First, copy the sample create-domain-inputs.yaml file and update your copy with:\n domainUID: sample-domain1 image: Leave empty unless you need to tag the new image that the script builds to a different name. For example if you are using a remote cluster that will need to pull the image from a container registry, then you should set this value to the fully qualified image name. Note that you will need to push the image manually. weblogicCredentialsSecretName: sample-domain1-weblogic-credentials namespace: sample-domain1-ns domainHomeImageBase: container-registry.oracle.com/middleware/weblogic:12.2.1.4  For example, assuming you named your copy my-inputs.yaml:\n$ cd kubernetes/samples/scripts/create-weblogic-domain/domain-home-in-image $ ./create-domain.sh -i my-inputs.yaml -o /\u0026lt;your output directory\u0026gt; -u \u0026lt;username\u0026gt; -p \u0026lt;password\u0026gt; -e  You need to provide the same WebLogic domain administrator user name and password in the -u and -p options respectively, as you provided when creating the Kubernetes Secret in Step 2.\n For the detailed steps that the create-domain.sh script performs, see Domain Home In Image.\n  Confirm that the operator started the servers for the domain:\na. Use kubectl to show that the Domain was created:\n$ kubectl describe domain sample-domain1 -n sample-domain1-ns b. After a short time, you will see the Administration Server and Managed Servers running.\n$ kubectl get pods -n sample-domain1-ns c. You should also see all the Kubernetes Services for the domain.\n$ kubectl get services -n sample-domain1-ns   Create an ingress for the domain, in the domain namespace, by using the sample Helm chart:\n$ helm install sample-domain1-ingress kubernetes/samples/charts/ingress-per-domain \\  --namespace sample-domain1-ns \\  --set wlsDomain.domainUID=sample-domain1 \\  --set traefik.hostname=sample-domain1.org   To confirm that the ingress controller noticed the new ingress and is successfully routing to the domain\u0026rsquo;s server pods, you can send a request to the URL for the \u0026ldquo;WebLogic ReadyApp framework\u0026rdquo;, as shown in the example below, which will return an HTTP 200 status code.\n$ curl -v -H \u0026#39;host: sample-domain1.org\u0026#39; http://localhost:30305/weblogic/ready  About to connect() to localhost port 30305 (#0) Trying 10.196.1.64... Connected to localhost (10.196.1.64) port 30305 (#0) \u0026gt; GET /weblogic/ HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Accept: */* \u0026gt; host: domain1.org \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Content-Length: 0 \u0026lt; Date: Thu, 20 Dec 2018 14:52:22 GMT \u0026lt; Vary: Accept-Encoding \u0026lt; Connection #0 to host localhost left intact  Depending on where your Kubernetes cluster is running, you may need to open firewall ports or update security lists to allow ingress to this port.\n   To access the WebLogic Server Administration Console:\na. Edit the my-inputs.yaml file (assuming that you named your copy my-inputs.yaml) to set exposedAdminNodePort: true.\nb. Open a browser to http://localhost:30701.\nDo not use the WebLogic Server Administration Console to start or stop servers. See Starting and stopping servers.\n   "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/",
	"title": "Samples",
	"tags": [],
	"description": "",
	"content": "The samples provide demonstrations of how to accomplish common tasks. These samples are provided for educational and demonstration purposes only; they are not intended to be used in production deployments or to be depended upon to create production environments.\n Credentials  Sample for creating a Kubernetes Secret that contains the Administration Server credentials. This Secret can be used in creating a WebLogic Domain YAML file.\n Storage  Sample for creating a PV or PVC that can be used by a Domain YAML file as the persistent storage for the WebLogic domain home or log files.\n Run a database  Run an ephemeral database in Kubernetes that is suitable for sample or basic testing purposes.\n Domains  These samples show various choices for working with domains.\n REST APIs  Sample for generating a self-signed certificate and private key that can be used for the operator\u0026#39;s external REST API.\n Ingress  Ingress controllers and load balancer sample scripts.\n Elastic Stack   Operator Sample for configuring the Elasticsearch and Kibana deployments and services for the operator\u0026#39;s logs. WebLogic domain Sample for using Fluentd for WebLogic domain and operator\u0026#39;s logs.  Azure Kubernetes Service  Sample for using the operator to set up a WLS cluster on the Azure Kubernetes Service.\n Tanzu Kubernetes Service  Sample for using the operator to set up a WLS cluster on the Tanzu Kubernetes Service.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/faq/oci-fss-pv/",
	"title": "Using OCI File Storage (FSS) for persistent volumes",
	"tags": [],
	"description": "If you are running your Kubernetes cluster on Oracle Container Engine for Kubernetes (OKE), and you use OCI File Storage (FSS) for persistent volumes to store the WebLogic domain home, then the file system handling, as demonstrated in the operator persistent volume sample, will require an update to properly initialize the file ownership on the persistent volume when the domain is initially created.",
	"content": "If you are running your Kubernetes cluster on Oracle Container Engine for Kubernetes (commonly known as OKE), and you use OCI File Storage (FSS) for persistent volumes to store the WebLogic domain home, then the file system handling, as demonstrated in the operator persistent volume sample, will require an update to properly initialize the file ownership on the persistent volume when the domain is initially created.\nFile permission handling on persistent volumes can differ between cloud providers and even with the underlying storage handling on Linux based systems. These instructions provide one option to update file ownership used by the standard Oracle images where UID 1000 and GID 1000 typically represent the oracle or opc user. For more information on persistent volume handling, see Persistent storage.\n Failure during domain creation with persistent volume sample The existing sample for creation of a domain home on persistent volume uses a Kubernetes Job to create the domain. The sample uses an initContainers section to change the file ownership which will fail for OCI FSS created volumes used with an OKE cluster.\nThe OCI FSS volume contains some files that are not modifiable thus causing the Kubernetes Job to fail. The failure is seen in the description of the Kubernetes Job pod:\n$ kubectl describe -n domain1-ns pod domain1-create-weblogic-sample-domain-job-wdkvs Init Containers: fix-pvc-owner: Container ID: docker://7051b6abdc296c76e937246df03d157926f2f7477e63b6af3bf65f6ae1ceddee Image: container-registry.oracle.com/middleware/weblogic:12.2.1.3 Image ID: docker-pullable://container-registry.oracle.com/middleware/weblogic@sha256:47dfd4fdf6b56210a6c49021b57dc2a6f2b0d3b3cfcd253af7a75ff6e7421498 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Command: sh -c chown -R 1000:0 /shared State: Terminated Reason: Error Exit Code: 1 Started: Wed, 12 Feb 2020 18:28:53 +0000 Finished: Wed, 12 Feb 2020 18:28:53 +0000 Ready: False Restart Count: 0 Environment: \u0026lt;none\u0026gt; Updating the domain on persistent volume sample In the following snippet of the create-domain-job-template.yaml, you can see the updated command for the init container:\napiVersion: batch/v1 kind: Job metadata: name: %DOMAIN_UID%-create-weblogic-sample-domain-job namespace: %NAMESPACE% spec: template: metadata: ... spec: restartPolicy: Never initContainers: - name: fix-pvc-owner image: %WEBLOGIC_IMAGE% command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;chown 1000:0 %DOMAIN_ROOT_DIR%/. \u0026amp;\u0026amp; find %DOMAIN_ROOT_DIR%/. -maxdepth 1 ! -name \u0026#39;.snapshot\u0026#39; ! -name \u0026#39;.\u0026#39; -print0 | xargs -r -0 chown -R 1000:0\u0026#34;] volumeMounts: - name: weblogic-sample-domain-storage-volume mountPath: %DOMAIN_ROOT_DIR% securityContext: runAsUser: 0 runAsGroup: 0 containers: - name: create-weblogic-sample-domain-job image: %WEBLOGIC_IMAGE% ... Use this new command in your copy of this template file. This will result in the ownership being updated for the expected files only, before the WebLogic domain is created on the persistent volume.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/security/openshift/",
	"title": "OpenShift",
	"tags": [],
	"description": "OpenShift information for the operator",
	"content": "Security requirements to run WebLogic in OpenShift WebLogic Kubernetes Operator images starting with version 3.1 and WebLogic Server images obtained from Oracle Container Registry after August 2020 have an oracle user with UID 1000 with the default group set to root.\nHere is an excerpt from a standard WebLogic Dockerfile that demonstrates how the file system group ownership is configured in the standard WebLogic Server images:\n# Setup filesystem and oracle user# Adjust file permissions, go to /u01 as user \u0026#39;oracle\u0026#39; to proceed with WLS installation# ------------------------------------------------------------RUN mkdir -p /u01 \u0026amp;\u0026amp; \\  chmod 775 /u01 \u0026amp;\u0026amp; \\  useradd -b /u01 -d /u01/oracle -m -s /bin/bash oracle \u0026amp;\u0026amp; \\  chown oracle:root /u01COPY --from=builder --chown=oracle:root /u01 /u01OpenShift, by default, enforces the restricted security context constraint which allocates a high, random UID in the root group for each container. The standard images mentioned above are designed to work with the restricted security context constraint.\nHowever, if you build your own image, have an older version of an image, or obtain an image from another source, it may not have the necessary permissions. You may need to configure similar file system permissions to allow your image to work in OpenShift. Specifically, you need to make sure the following directories have root as their group, and that the group read, write and execute permissions are set (enabled):\n For the operator, /operator and /logs. For WebLogic Server images, /u01 (or the ultimate parent directory of your Oracle Home and domain if you put them in different locations).  If your OpenShift environment has a different default security context constraint, you may need to configure OpenShift to allow use of UID 1000 by creating a security context constraint. Oracle recommends that you define a custom security context constraint that has just the permissions that are required and apply that to WebLogic pods. Oracle does not recommend using the built-in anyuid Security Context Constraint, because it provides more permissions than are needed, and is therefore less secure.\nCreate a custom Security Context Constraint To create a custom security context constraint, create a YAML file with the following content. This example assumes that your OpenShift project is called weblogic and that the service account you will use to run the operator and domains is called weblogic-operator. You should change these in the groups and users sections to match your environment.\nkind: SecurityContextConstraints apiVersion: v1 metadata: name: uid1000 allowHostDirVolumePlugin: false allowHostIPC: false allowHostNetwork: false allowHostPID: false allowHostPorts: false allowPrivilegeEscalation: true allowPrivilegedContainer: false fsGroup: type: MustRunAs groups: - system:serviceaccounts:weblogic readOnlyRootFilesystem: false requiredDropCapabilities: - KILL - MKNOD - SETUID - SETGID runAsUser: type: MustRunAs uid: 1000 seLinuxContext: type: MustRunAs supplementalGroups: type: RunAsAny users: - system:serviceaccount:weblogic:weblogic-operator volumes: - configMap - downwardAPI - emptyDir - persistentVolumeClaim - projected - secret Assuming you called that file uid1000.yaml, you can create the security context constraint using the following command:\n$ oc create -f uid1000.yaml After you have created the security context constraint, you can install the WebLogic Kubernetes Operator. Make sure you use the same service account to which you granted permission in the security context constraint (weblogic-operator in the preceding example). The operator will then run with UID 1000, and any WebLogic domain it creates will also run with UID 1000.\nFor additional information about OpenShift requirements and the operator, see OpenShift.\n Using a dedicated namespace When the user that installs an individual instance of the operator does not have the required privileges to create resources at the Kubernetes cluster level, a dedicated namespace can be used for the operator instance and all the WebLogic domains that it manages. For more details about the dedicated setting, please refer to Operator Helm configuration values.\nSet the Helm chart property kubernetesPlatorm to OpenShift Beginning with operator version 3.3.2, set the operator kubernetesPlatform Helm chart property to OpenShift. This property accommodates OpenShift security requirements. For more information, see Operator Helm configuration values.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/accessing-the-domain/",
	"title": "Access the domain",
	"tags": [],
	"description": "",
	"content": "  Use the Remote Console  Use the WebLogic Remote Console to manage a domain running in Kubernetes.\n Use WLST  You can use the WebLogic Scripting Tool (WLST) to manage a domain running in Kubernetes.\n Use port forwarding  You can use port forwarding to access WebLogic Server Administration Consoles and WLST.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/domains/model-in-image/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "To remove the resources you have created in these samples:\n  Delete the resources associated with the domain.\n$ /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/delete-domain/delete-weblogic-domain-resources.sh -d sample-domain1 $ /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/delete-domain/delete-weblogic-domain-resources.sh -d sample-domain2 This deletes the domain and any related resources that are labeled with the domain UID sample-domain1 and sample-domain2.\nIt leaves the namespace intact, the operator running, the load balancer running (if installed), and the database running (if installed).\n Note: When you delete a domain, the operator will detect your domain deletion and shut down its pods. Wait for these pods to exit before deleting the operator that monitors the sample-domain1-ns namespace. You can monitor this process using the command kubectl get pods -n sample-domain1-ns --watch (ctrl-c to exit).\n   If you set up the Traefik ingress controller:\n$ helm uninstall traefik-operator -n traefik $ kubectl delete namespace traefik   If you set up a database for JRF or the Update 4 use case:\n$ /tmp/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-oracle-db-service/stop-db-service.sh   Delete the operator and its namespace:\n$ helm uninstall sample-weblogic-operator -n sample-weblogic-operator-ns $ kubectl delete namespace sample-weblogic-operator-ns   Delete the domain\u0026rsquo;s namespace:\n$ kubectl delete namespace sample-domain1-ns   Delete the images you may have created in this sample:\n$ docker image rm model-in-image:WLS-v1 $ docker image rm model-in-image:WLS-v2 $ docker image rm model-in-image:JRF-v1 $ docker image rm model-in-image:JRF-v2   "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/domains/delete-domain/",
	"title": "Delete resources associated with the domain",
	"tags": [],
	"description": "Delete the Kubernetes resources associated with the domain created while executing the samples.",
	"content": "After running the samples, you will need to release resources associated with the domain that can then be used for other purposes. The script in this sample demonstrates one approach to releasing these resources.\nUse this script to delete resources associated with the domain $ ./delete-weblogic-domain-resources.sh \\  -d domain-uid[,domain-uid...] \\  [-s max-seconds] \\  [-t] The required option -d takes domain-uid values (separated by commas and no spaces) to identify the resources that should be deleted.\nTo limit the amount of time spent on attempting to delete resources, use -s. The option must be followed by an integer that represents the total number of seconds that will be spent attempting to delete resources. The default number of seconds is 120.\nThe optional -t option shows what the script will delete without executing the deletion.\nTo see the help associated with the script:\n$ ./delete-weblogic-domain-resources.sh -h "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/developerguide/asynchronous-call-model/",
	"title": "Asynchronous call model",
	"tags": [],
	"description": "",
	"content": "Our expectation is that customers will task the operator with managing hundreds of WebLogic domains across dozens of Kubernetes Namespaces. Therefore, we have designed the operator with an efficient user-level threads pattern. We\u0026rsquo;ve used that pattern to implement an asynchronous call model for Kubernetes API requests. This call model has built-in support for timeouts, retries with exponential back-off, and lists that exceed the requested maximum size using the continuance functionality.\nUser-level thread pattern The user-level thread pattern is implemented by the classes in the oracle.kubernetes.operator.work package.\n Engine: The executor service and factory for Fibers. Fiber: The user-level thread. Fibers represent the execution of a single processing flow through a series of Steps. Fibers may be suspended and later resumed, and do not consume a Thread while suspended. Step: Individual CPU-bound activity in a processing flow. Packet: Context of the processing flow. NextAction: Used by a Step when it returns control to the Fiber to indicate what should happen next. Common \u0026lsquo;next actions\u0026rsquo; are to execute another Step or to suspend the Fiber. Component: Provider of SPI\u0026rsquo;s that may be useful to the processing flow. Container: Represents the containing environment and is a Component.  Each Step has a reference to the next Step in the processing flow; however, Steps are not required to indicate that the next Step be invoked by the Fiber when the Step returns a NextAction to the Fiber. This leads to common use cases where Fibers invoke a series of Steps that are linked by the \u0026lsquo;is-next\u0026rsquo; relationship, but just as commonly, use cases where the Fiber will invoke sets of Steps along a detour before returning to the normal flow.\nIn this sample, the caller creates an Engine, Fiber, linked set of Step instances, and Packet. The Fiber is then started. The Engine would typically be a singleton, since it\u0026rsquo;s backed by a ScheduledExecutorService. The Packet would also typically be pre-loaded with values that the Steps would use in their apply() methods.\nstatic class SomeClass { public static void main(String[] args) { Engine engine = new Engine(\u0026#34;worker-pool\u0026#34;); Fiber fiber = engine.createFiber(); Step step = new StepOne(new StepTwo(new StepThree(null))); Packet packet = new Packet(); fiber.start( step, packet, new CompletionCallback() { @Override public void onCompletion(Packet packet) { // Fiber has completed successfully  } @Override public void onThrowable(Packet packet, Throwable throwable) { // Fiber processing was terminated with an exception  } }); } } Steps must not invoke sleep or blocking calls from within apply(). This prevents the worker threads from serving other Fibers. Instead, use asynchronous calls and the Fiber suspend/resume pattern. Step provides a method, doDelay(), which creates a NextAction to drive Fiber suspend/resume that is a better option than sleep precisely because the worker thread can serve other Fibers during the delay. For asynchronous IO or similar patterns, suspend the Fiber. In the callback as the Fiber suspends, initiate the asynchronous call. Finally, when the call completes, resume the Fiber. The suspend/resume functionality handles the case where resumed before the suspending callback completes.\nIn this sample, the step uses asynchronous file IO and the suspend/resume Fiber pattern.\nstatic class StepTwo extends Step { public StepTwo(Step next) { super(next); } @Override public NextAction apply(Packet packet) { return doSuspend((fiber) -\u0026gt; { // The Fiber is now suspended  // Start the asynchronous call  try { Path path = Paths.get(URI.create(this.getClass().getResource(\u0026#34;/somefile.dat\u0026#34;).toString())); AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.READ); ByteBuffer buffer = ByteBuffer.allocate(1024); fileChannel.read(buffer, 0, buffer, new CompletionHandler\u0026lt;Integer, ByteBuffer\u0026gt;() { @Override void completed(Integer result, ByteBuffer attachment) { // Store data in Packet and resume Fiber  packet.put(\u0026#34;DATA_SIZE_READ\u0026#34;, result); packet.put(\u0026#34;DATA_FROM_SOMEFILE\u0026#34;, attachment); fiber.resume(packet); } @Override public void failed(Throwable exc, ByteBuffer attachment) { // log exc  completed(0, null); } }); } catch (IOException e) { // log exception  // If not resumed here, Fiber will never be resumed  } }); } } Call builder pattern The asynchronous call model is implemented by classes in the oracle.kubernetes.operator.helpers package, including CallBuilder and ResponseStep. The model is based on the Fiber suspend/resume pattern described above. CallBuilder provides many methods having names ending with \u0026ldquo;Async\u0026rdquo;, such as listPodAsync() or deleteServiceAsync(). These methods return a Step that can be returned as part of a NextAction. When creating these Steps, the developer must provide a ResponseStep. Only ResponseStep.onSuccess() must be implemented; however, it is often useful to override onFailure() as Kubernetes treats 404 (Not Found) as a failure.\nIn this sample, the developer is using the pattern to list pods from the default namespace that are labeled as part of cluster-1.\nstatic class StepOne extends Step { public StepOne(Step next) { super(next); } @Override public NextAction apply(Packet packet) { String namespace = \u0026#34;default\u0026#34;; Step step = CallBuilder.create().with($ -\u0026gt; { $.labelSelector = \u0026#34;weblogic.clusterName=cluster-1\u0026#34;; $.limit = 50; $.timeoutSeconds = 30; }).listPodAsync(namespace, new ResponseStep\u0026lt;V1PodList\u0026gt;(next) { @Override public NextAction onFailure(Packet packet, ApiException e, int statusCode, Map\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; responseHeaders) { if (statusCode == CallBuilder.NOT_FOUND) { return onSuccess(packet, null, statusCode, responseHeaders); } return super.onFailure(packet, e, statusCode, responseHeaders); } @Override NextAction onSuccess(Packet packet, V1PodList result, int statusCode, Map\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; responseHeaders) { // do something with the result Pod, if not null  return doNext(packet); } }); return doNext(step, packet); } } Notice that the required parameters, such as namespace, are method arguments, but optional parameters are designated using a simplified builder pattern using with() and a lambda.\nThe default behavior of onFailure() will retry with an exponential backoff the request on status codes 429 (TooManyRequests), 500 (InternalServerError), 503 (ServiceUnavailable), 504 (ServerTimeout) or a simple timeout with no response from the server.\nIf the server responds with status code 409 (Conflict), then this indicates an optimistic locking failure. Common use cases are that the code read a Kubernetes object in one asynchronous step, modified the object, and attempted to replace the object in another asynchronous step; however, another activity replaced that same object in the interim. In this case, retrying the request would give the same result. Therefore, developers may provide an \u0026ldquo;on conflict\u0026rdquo; step when calling super.onFailure(). The conflict step will be invoked after an exponential backoff delay. In this example, that conflict step should be the step that reads the existing Kubernetes object.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/",
	"title": "Manage WebLogic domains",
	"tags": [],
	"description": "Important considerations for WebLogic domains in Kubernetes.",
	"content": "Contents  Important considerations for WebLogic domains in Kubernetes Meet Kubernetes resource name restrictions Creating and managing WebLogic domains Modifying domain configurations About the Domain resource Managing lifecycle operations Scaling clusters About domain events Log files  Important considerations for WebLogic domains in Kubernetes Be aware of the following important considerations for WebLogic domains running in Kubernetes:\n  Domain Home Location: The WebLogic domain home location is determined by the Domain YAML file domainHome, if specified; otherwise, a default location is determined by the domainHomeSourceType setting.\n If the Domain domainHome field is not specified and domainHomeSourceType is Image (the default), then the operator will assume that the domain home is a directory under /u01/oracle/user_projects/domains/, and report an error if no domain is found or more than one domain is found. If the Domain domainHome field is not specified and domainHomeSourceType is PersistentVolume, then the operator will assume that the domain home is /shared/domains/DOMAIN_UID. Finally, if the Domain domainHome field is not specified and the domainHomeSourceType is FromModel, then the operator will assume that the domain home is /u01/domains/DOMAIN_UID.  Oracle strongly recommends storing an image containing a WebLogic domain home (domainHomeSourceType is Image) as private in the registry (for example, Oracle Cloud Infrastructure Registry, GitHub Container Registry, and such). A container image that contains a WebLogic domain has sensitive information including keys and credentials that are used to access external resources (for example, the data source password). For more information, see WebLogic domain in container image protection.\n   Log File Locations: The operator can automatically override WebLogic Server, domain, and introspector log locations. This occurs if the Domain logHomeEnabled field is explicitly set to true, or if logHomeEnabled isn\u0026rsquo;t set and domainHomeSourceType is set to PersistentVolume. When overriding, the log location will be the location specified by the logHome setting. For additional log file tuning information, see Log files.\n  Listen Address Overrides: The operator will automatically override all WebLogic domain default, SSL, admin, or custom channel listen addresses (using situational configuration overrides). These will become domainUID followed by a hyphen and then the server name, all lowercase, and underscores converted to hyphens. For example, if domainUID=domain1 and the WebLogic Server name is Admin_Server, then its listen address becomes domain1-admin-server.\n  Domain, Cluster, Server, and Network-Access-Point Names: WebLogic domain, cluster, server, and network-access-point (channel) names must contain only the characters A-Z, a-z, 0-9, -, or _, and must be kept to a reasonable length. This ensures that they can be safely used to form resource names that meet Kubernetes resource and DNS1123 naming requirements. For more details, see Meet Kubernetes resource name restrictions.\n  Node Ports: If you choose to expose any WebLogic channels outside the Kubernetes cluster using a NodePort, for example, the administration port or a T3 channel to allow WLST access, you need to ensure that you allocate each channel a unique port number across the entire Kubernetes cluster. If you expose the administration port in each WebLogic domain in the Kubernetes cluster, then each one must have a different port number. This is required because NodePorts are used to expose channels outside the Kubernetes cluster.\nExposing administrative, RMI, or T3 capable channels using a Kubernetes NodePort can create an insecure configuration. In general, only HTTP protocols should be made available externally and this exposure is usually accomplished by setting up an external load balancer that can access internal (non-NodePort) services. For more information, see T3 channels.\n   Host Path Persistent Volumes: If using a hostPath persistent volume, then it must be available on all worker nodes in the cluster and have read/write/many permissions for all container/pods in the WebLogic Server deployment. Be aware that many cloud provider\u0026rsquo;s volume providers may not support volumes across availability zones. You may want to use NFS or a clustered file system to work around this limitation.\n  Security Note: The USER_MEM_ARGS environment variable defaults to -Djava.security.egd=file:/dev/./urandom in all WebLogic Server pods and the WebLogic introspection job. It can be explicitly set to another value in your Domain YAML file using the env attribute under the serverPod configuration.\n  JVM Memory and Java Option Arguments: The following environment variables can be used to customize the JVM memory and Java options for both the WebLogic Server Managed Servers and Node Manager instances:\n JAVA_OPTIONS - Java options for starting WebLogic Server USER_MEM_ARGS - JVM memory arguments for starting WebLogic Server NODEMGR_JAVA_OPTIONS - Java options for starting a Node Manager instance NODEMGR_MEM_ARGS - JVM memory arguments for starting a Node Manager instance  For more information, see Domain resource.\n  The following features are not certified or supported in this release:\n Whole server migration Consensus leasing Node Manager (although it is used internally for the liveness probe and to start WebLogic Server instances) Multicast Multitenancy Production redeployment Mixed clusters (configured servers targeted to a dynamic cluster)  For up-to-date information about the features of WebLogic Server that are supported in Kubernetes environments, see My Oracle Support Doc ID 2349228.1.\nMeet Kubernetes resource name restrictions Kubernetes requires that the names of some resource types follow the DNS label standard as defined in DNS Label Names and RFC 1123. This requirement restricts the characters that are allowed in the names of these resources, and also limits the length of these names to no more than 63 characters.\nThe following is a list of such Kubernetes resources that the operator generates when a domain resource is deployed, including how their names are constructed.\n A domain introspector job named \u0026lt;domainUID\u0026gt;-\u0026lt;introspectorJobNameSuffix\u0026gt;. The default suffix is -introspector, which can be overridden using the operator\u0026rsquo;s Helm configuration introspectorJobNameSuffix (see WebLogic domain management). A ClusterIP type service and a pod for each WebLogic Server named \u0026lt;domainUID\u0026gt;-\u0026lt;serverName\u0026gt;. A ClusterIP type service for each WebLogic cluster named \u0026lt;domainUID\u0026gt;-cluster-\u0026lt;clusterName\u0026gt;. An optional NodePort type service, also known as an external service, for the WebLogic Administration Server named \u0026lt;domainUID\u0026gt;-\u0026lt;adminServerName\u0026gt;-\u0026lt;externalServiceNameSuffix\u0026gt;. The default suffix is -ext, which can be overridden using the operator\u0026rsquo;s Helm configuration externalServiceNameSuffix (see WebLogic domain management).  The operator puts in place certain validation checks and conversions to prevent these resources from violating Kubernetes restrictions.\n All the names previously described can contain only the characters A-Z, a-z, 0-9, -, or _, and must start and end with an alphanumeric character. Note that when generating pod and service names, the operator will convert configured names to lowercase and substitute a hyphen (-) for each underscore (_). A domainUID is required to be no more than 45 characters. WebLogic domain configuration names, such as the cluster names, Administration Server name, and Managed Server names must be kept to a legal length so that the resultant resource names do not exceed Kubernetes' limits.  When a domain resource or WebLogic domain configuration violates the limits, the domain startup will fail, and actual validation errors are reported in the domain resource\u0026rsquo;s status.\nCreating and managing WebLogic domains You can locate a WebLogic domain either in a persistent volume (PV) or in an image. For examples of each, see the WebLogic Kubernetes Operator samples.\nIf you want to create your own container images, for example, to choose a specific set of patches or to create a domain with a specific configuration or applications deployed, then you can create the domain custom resource manually to deploy your domain. This process is documented in this sample.\nModifying domain configurations You can modify the WebLogic domain configuration for Domain in PV, Domain in Image, and Model in Image before deploying a Domain YAML file:\nWhen the domain is in a persistent volume, you can use WLST or WDT to change the configuration.\nFor Domain in Image and Domain in PV you can use configuration overrides.\nConfiguration overrides allow changing a configuration without modifying its original config.xml or system resource XML files, and supports parameterizing overrides so that you can inject values into them from Kubernetes Secrets. For example, you can inject database user names, passwords, and URLs that are stored in a secret.\nFor Model in Image you use Runtime Updates.\nAbout the Domain resource For more information, see Domain resource.\nManaging lifecycle operations You can perform lifecycle operations on WebLogic Servers, clusters, or domains. See Starting and stopping and Restarting servers.\nScaling clusters The operator lets you initiate scaling of clusters in various ways:\n Using kubectl to edit the Domain resource Using the operator\u0026rsquo;s REST APIs Using WLDF policies Using a Prometheus action  About domain events The operator generates Kubernetes events at key points during domain processing. For more information, see Domain events.\nLog files The operator can automatically override WebLogic Server, domain, and introspector .log and .out locations. This occurs if the Domain logHomeEnabled field is explicitly set to true, or if logHomeEnabled isn\u0026rsquo;t set and domainHomeSourceType is set to PersistentVolume. When overriding, the log location will be the location specified by the logHome setting.\nIf you want to fine tune the .log and .out rotation behavior for WebLogic Servers and domains, then you can update the related Log MBean in your WebLogic configuration. Alternatively, for WebLogic Servers, you can set corresponding system properties in JAVA_OPTIONS:\n  Here are some WLST offline examples for creating and accessing commonly tuned Log MBeans:\n# domain log cd(\u0026#39;/\u0026#39;) create(dname,\u0026#39;Log\u0026#39;) cd(\u0026#39;/Log/\u0026#39; + dname); # configured server log for a server named \u0026#39;sname\u0026#39; cd(\u0026#39;/Servers/\u0026#39; + sname) create(sname, \u0026#39;Log\u0026#39;) cd(\u0026#39;/Servers/\u0026#39; + sname + \u0026#39;/Log/\u0026#39; + sname) # templated (dynamic) server log for a template named \u0026#39;tname\u0026#39; cd(\u0026#39;/ServerTemplates/\u0026#39; + tname) create(tname,\u0026#39;Log\u0026#39;) cd(\u0026#39;/ServerTemplates/\u0026#39; + tname + \u0026#39;/Log/\u0026#39; + tname)   Here is sample WLST offline code for commonly tuned Log MBean attributes:\n# minimum log file size before rotation in kilobytes set(\u0026#39;FileMinSize\u0026#39;, 1000) # maximum number of rotated files set(\u0026#39;FileCount\u0026#39;, 10) # set to true to rotate file every time on startup (instead of append) set(\u0026#39;RotateLogOnStartup\u0026#39;, \u0026#39;true\u0026#39;)   Here are the defaults for commonly tuned Log MBean attributes:\n   Log MBean Attribute Production Mode Default Development Mode Default     FileMinSize (in kilobytes) 5000 500   FileCount 100 7   RotateLogOnStartup false true      For WebLogic Server .log and .out files (including both dynamic and configured servers), you can alternatively set logging attributes using system properties that start with weblogic.log. and that end with the corresponding Log MBean attribute name.\nFor example, you can include -Dweblogic.log.FileMinSize=1000 -Dweblogic.log.FileCount=10 -Dweblogic.log.RotateLogOnStartup=true in domain.spec.serverPod.env.name.JAVA_OPTIONS to set the behavior for all WebLogic Servers in your domain. For information about setting JAVA_OPTIONS, see Domain resource.\n  Kubernetes stores pod logs on each of its nodes, and, depending on the Kubernetes implementation, extra steps may be necessary to limit their disk space usage. For more information, see Kubernetes Logging Architecture.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/quickstart/cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Remove the domain.   Remove the domain\u0026rsquo;s ingress by using helm:\n$ helm uninstall sample-domain1-ingress -n sample-domain1-ns   Remove the Kubernetes resources associated with the domain by using the sample delete-weblogic-domain-resources script:\n$ kubernetes/samples/scripts/delete-domain/delete-weblogic-domain-resources.sh -d sample-domain1   Use kubectl to confirm that the WebLogic Server instance Pods and Domain are gone:\n$ kubectl get pods -n sample-domain1-ns $ kubectl get domains -n sample-domain1-ns   Remove the domain namespace.   Configure the Traefik ingress controller to stop managing the ingresses in the domain namespace:\n$ helm upgrade traefik-operator traefik/traefik \\  --namespace traefik \\  --reuse-values \\  --set \u0026#34;kubernetes.namespaces={traefik}\u0026#34;   Delete the domain namespace:\n$ kubectl delete namespace sample-domain1-ns   Remove the operator.   Remove the operator:\n$ helm uninstall sample-weblogic-operator -n sample-weblogic-operator-ns   Remove the operator\u0026rsquo;s namespace:\n$ kubectl delete namespace sample-weblogic-operator-ns   Remove the ingress controller.   Remove the Traefik ingress controller:\n$ helm uninstall traefik-operator -n traefik   Remove the Traefik namespace:\n$ kubectl delete namespace traefik   Delete the generated image.   When no longer needed, delete the generated image by using the docker rmi command. Use the following command to delete an image tagged with domain-home-in-image:12.2.1.4:\n$ docker rmi domain-home-in-image:12.2.1.4   Delete the tools directory.   When no longer needed, delete the directory where WebLogic Deploy Tool and WebLogic Image Tool are installed.\n$ rm -rf /tmp/dhii-sample/tools/   "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/developerguide/",
	"title": "Developer Guide",
	"tags": [],
	"description": "",
	"content": "Developer Guide The Developer Guide provides information for developers who want to understand or contribute to the code.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/azure-kubernetes-service/",
	"title": "Azure Kubernetes Service",
	"tags": [],
	"description": "Sample for using the operator to set up a WLS cluster on the Azure Kubernetes Service.",
	"content": "Contents  Introduction  Azure Kubernetes Service cluster Domain home source types   Domain in PV: Running the WebLogic cluster on AKS with domain home on PV Model in Image: Running the WebLogic cluster on AKS with domain model in image Troubleshooting  Introduction This sample demonstrates how to use the WebLogic Kubernetes Operator (hereafter \u0026ldquo;the operator\u0026rdquo;) to set up a WebLogic Server (WLS) cluster on the Azure Kubernetes Service (AKS). After going through the steps, your WLS domain runs on an AKS cluster. You have several options for managing the cluster, depending on which domain home source type you choose. With Domain in PV, you can manage your WLS domain by accessing the WebLogic Server Administration Console or WLST. With Model in Image, you use the operator to perform WLS administrative operations.\nAzure Kubernetes Service cluster Azure Kubernetes Service makes it simple to deploy a managed Kubernetes cluster in Azure. AKS reduces the complexity and operational overhead of managing Kubernetes by offloading much of that responsibility to Azure. As a hosted Kubernetes service, Azure handles critical tasks like health monitoring and maintenance for you. The Kubernetes masters are managed by Azure. You manage and maintain only the agent nodes. As a managed Kubernetes service, AKS is free - you pay for only the agent nodes within your clusters, not for the masters.\nTo learn more, see What is Azure Kubernetes Service?\nDomain home source types This sample demonstrates running the WebLogic cluster on AKS using two domain home types. The instructions for each are self-contained and independent. This section lists the domain home source types recommended for use with AKS, along with some benefits of each. For complete details on domain home source types, see Choose a domain home source type.\n  Model in Image: running the WebLogic cluster on AKS with domain home in image offers the following benefits:\n Reuse image to create different domains with different domainUID and different configurations. Mutate the domain home configuration with additional model files supplied in a ConfigMap. Many such changes do not need to restart the entire domain for the change to take effect. The model file syntax is far simpler and less error prone than the configuration override syntax, and, unlike configuration overrides, allows you to directly add data sources and JMS modules.    Domain in PV: running the WebLogic cluster on AKS with domain home in PV offers the following benefits:\n Use standard Oracle-provided images with patches installed. No Docker environment required. You are able to run your business quickly without building knowledge of Docker. Mutate the live domain configuration with Administration Console from a browser or WLST.    Stop and Start an Azure Kubernetes Service (AKS) cluster using Azure CLI, as described in the azure docs. This allows you to optimize costs during your AKS cluster\u0026rsquo;s idle time. Don\u0026rsquo;t pay for running development clusters unless they are actively being used. You can pick up objects and cluster state right where you were left off.\n References For references to the relevant user documentation, see:\n Choose a domain home source type user documentation Model in Image user documentation  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/faq/oci-lb/",
	"title": "Using an OCI load balancer",
	"tags": [],
	"description": "If you are running your Kubernetes cluster on Oracle Container Engine for Kubernetes (OKE), then you can have OCI automatically provision load balancers for you by creating a `Service` of type `LoadBalancer` instead of (or in addition to) installing an ingress controller like Traefik or Voyager.",
	"content": "If you are running your Kubernetes cluster on Oracle Container Engine for Kubernetes (commonly known as OKE), then you can have OCI automatically provision load balancers for you by creating a Service of type LoadBalancer instead of (or in addition to) installing an ingress controller like Traefik or Voyager.\nOKE Kubernetes worker nodes typically do not have public IP addresses. This means that the NodePort services created by the operator are not usable, because they would expose ports on the worker node\u0026rsquo;s private IP addresses only, which are not reachable from outside the cluster.\nInstead, you can use an OCI load balancer to provide access to services running in OKE.\nIt is also possible, if desirable, to have an OCI load balancer route traffic to an ingress controller running inside the Kubernetes cluster and have that ingress controller in turn route traffic to services in the cluster.\n Requesting an OCI load balancer When your domain is created by the operator, a number of Kubernetes services are created by the operator, including one for the WebLogic Server Administration Server and one for each Managed Server and cluster.\nIn the example below, there is a domain called bobs-bookstore in the bob namespace. This domain has a cluster called cluster-1 which exposes traffic on port 31111.\nThe Kubernetes YAML file below defines a new Service in the same namespace. The selector targets all of the pods in this namespace which are part of the cluster cluster-1, using the annotations that are placed on those pods by the operator. It also defines the port and protocol.\nYou can include the optional oci-load-balancer-shape annotation (as shown) if you want to specify the shape of the load balancer. Otherwise the default shape (100Mbps) will be used.\napiVersion: v1 kind: Service metadata: name: bobs-bookstore-oci-lb-service namespace: bob annotations: service.beta.kubernetes.io/oci-load-balancer-shape: 400Mbps spec: ports: - name: http port: 31111 protocol: TCP targetPort: 31111 selector: weblogic.clusterName: cluster-1 weblogic.domainUID: bobs-bookstore sessionAffinity: None type: LoadBalancer When you apply this YAML file to your cluster, you will see the new service is created but initially the external IP is shown as \u0026lt;pending\u0026gt;.\n$ kubectl -n bob get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE bobs-bookstore-admin-server ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,7001/TCP,30101/TCP 9d bobs-bookstore-admin-server-ext NodePort 10.96.224.13 \u0026lt;none\u0026gt; 7001:32401/TCP 9d bobs-bookstore-cluster-cluster-1 ClusterIP 10.96.86.113 \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-oci-lb-service LoadBalancer 10.96.121.216 \u0026lt;pending\u0026gt; 31111:31671/TCP 9s After a short time (typically less than a minute), the OCI load balancer will be provisioned and the external IP address will be displayed:\n$ kubectl -n bob get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE bobs-bookstore-admin-server ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,7001/TCP,30101/TCP 9d bobs-bookstore-admin-server-ext NodePort 10.96.224.13 \u0026lt;none\u0026gt; 7001:32401/TCP 9d bobs-bookstore-cluster-cluster-1 ClusterIP 10.96.86.113 \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8888/TCP,8001/TCP,31111/TCP 9d bobs-bookstore-oci-lb-service LoadBalancer 10.96.121.216 132.145.235.215 31111:31671/TCP 55s You can now use the external IP address and port to access your pods. There are several options that can be used to configure more advanced load balancing behavior. For more information, including how to configure SSL support, supporting internal and external subnets, and so one, refer to the OCI documentation.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-fmw-domains/",
	"title": "Manage FMW Infrastructure domains",
	"tags": [],
	"description": "FMW Infrastructure domains contain the Java Required Files (JRF) feature and are the prerequisite for upper stack products like Oracle SOA Suite.",
	"content": "Contents  Limitations Obtaining the FMW Infrastructure image Creating an FMW Infrastructure image Configuring access to your database Running the Repository Creation Utility to set up your database schema Create a Kubernetes Secret with the RCU credentials Creating an FMW Infrastructure domain Patching the FMW Infrastructure image Additional considerations for Coherence Additional considerations for Model in Image  Starting with the 2.2.0 release, the operator supports FMW Infrastructure domains, that is, domains that are created with the FMW Infrastructure installer rather than the WebLogic Server installer. These domains contain the Java Required Files (JRF) feature and are the prerequisite for \u0026ldquo;upper stack\u0026rdquo; products like Oracle SOA Suite, for example. These domains also require a database and the use of the Repository Creation Utility (RCU).\nThis document provides details about the special considerations for running FMW Infrastructure domains with the operator. Other than those considerations listed here, FMW Infrastructure domains work in the same way as WebLogic Server domains. The remainder of the documentation in this site applies equally to FMW Infrastructure domains and WebLogic Server domains.\nFMW Infrastructure domains are supported using the Domain in PV, Domain in Image, or Model in Image domain home source types. If you plan to experiment with upper stack products, we strongly recommend using the domain on a persistent volume approach.\nFor more information about the deployment of Oracle Fusion Middleware products on Kubernetes, see https://oracle.github.io/fmw-kubernetes/.\nLimitations Compared to running a WebLogic Server domain in Kubernetes using the operator, the following limitations currently exist for FMW Infrastructure domains:\n The WebLogic Logging Exporter currently supports WebLogic Server logs only. Other logs will not be sent to Elasticsearch. Note, however, that you can use a sidecar with a log handling tool like Logstash or Fluentd to get logs. The WebLogic Monitoring Exporter currently supports the WebLogic MBean trees only. Support for JRF MBeans has not been added yet. Only configured clusters are supported. Dynamic clusters are not supported for FMW Infrastructure domains. Note that you can still use all of the scaling features, you just need to define the maximum size of your cluster at domain creation time. FMW Infrastructure domains are not supported with any version of the operator before version 2.2.0.  Obtaining the FMW Infrastructure image The WebLogic Kubernetes Operator requires patch 29135930. The standard pre-built FMW Infrastructure image, container-registry.oracle.com/middleware/fmw-infrastrucutre:12.2.1.3, already has this patch applied. For detailed instructions on how to log in to the Oracle Container Registry and accept license agreement, see this document. The FMW Infrastructure 12.2.1.4.0 images do not require patches.\nTo pull an image from the Oracle Container Registry, in a web browser, navigate to https://container-registry.oracle.com and log in using the Oracle Single Sign-On authentication service. If you do not already have SSO credentials, at the top of the page, click the Sign In link to create them.\nUse the web interface to accept the Oracle Standard Terms and Restrictions for the Oracle software images that you intend to deploy. Your acceptance of these terms is stored in a database that links the software images to your Oracle Single Sign-On login credentials.\nFirst, you will need to log in to the Oracle Container Registry:\n$ docker login container-registry.oracle.com Then, you can pull the image with this command:\n$ docker pull container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 If desired, you can:\n  Check the WLS version with docker run container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 sh -c 'source $ORACLE_HOME/wlserver/server/bin/setWLSEnv.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; java weblogic.version'\n  Check the WLS patches with docker run container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.4 sh -c '$ORACLE_HOME/OPatch/opatch lspatches'\n  Additional information about using this image is available in the Oracle Container Registry.\nCreating an FMW Infrastructure image You can also create an image containing the FMW Infrastructure binaries. We provide a sample in the Oracle GitHub account that demonstrates how to create an image to run the FMW Infrastructure. Please consult the README file associated with this sample for important prerequisite steps, such as building or pulling the Server JRE image and downloading the Fusion Middleware Infrastructure installer binary.\nAfter cloning the repository and downloading the installer from Oracle Technology Network or e-delivery, you create your image by running the provided script:\n$ cd docker-images/OracleFMWInfrastructure/dockerfiles $ ./buildDockerImage.sh -v 12.2.1.4 -s The image produced will be named oracle/fmw-infrastructure:12.2.1.4.\nYou must also install the required patch to use this image with the operator. We provide a sample that demonstrates how to create an image with the necessary patch installed.\nAfter downloading the patch from My Oracle Support, you create the patched image by running the provided script:\n$ cd docker-images/OracleFMWInfrastructure/samples/12213-patch-fmw-for-k8s $ ./build.sh This will produce an image named oracle/fmw-infrastructure:12213-update-k8s.\nAll samples and instructions reference the pre-built image, container-registry.oracle.com/middleware/fmw_infrastructure:12.2.1.4. Because these samples build an image based on WebLogic Server 12.2.1.3 and use the tag, oracle/fmw-infrastructure:12213-update-k8s, be sure to update your sample inputs to use this image value.\nThese samples allow you to create an image containing the FMW Infrastructure binaries and the necessary patch. You can use this image to run the Repository Creation Utility and to run your domain using the \u0026ldquo;domain on a persistent volume\u0026rdquo; model. If you want to use the \u0026ldquo;domain in an image\u0026rdquo; model, you will need to go one step further and add another layer with your domain in it. You can use WLST or WDT to create your domain.\nBefore creating a domain, you will need to set up the necessary schemas in your database.\nConfiguring access to your database FMW Infrastructure domains require a database with the necessary schemas installed in them. We provide a utility, called the Repository Creation Utility (RCU), which allows you to create those schemas. You must set up the database before you create your domain. There are no additional requirements added by running FMW Infrastructure in Kubernetes; the same existing requirements apply.\nFor testing and development, you may choose to run your database inside Kubernetes or outside of Kubernetes.\nThe Oracle Database images are only supported for non-production use. For more details, see My Oracle Support note: Oracle Support for Database Running on Docker (Doc ID 2216342.1)\n Running the database inside Kubernetes If you wish to run the database inside Kubernetes, you can use the official container image from the Oracle Container Registry. Please note that there is a Slim Variant (12.2.0.1-slim tag) of EE that has reduced disk space (4GB) requirements and a quicker container startup.\nRunning the database inside the Kubernetes cluster is possibly more relevant or desirable in test/development or CI/CD scenarios.\nHere is an example of a Kubernetes YAML file to define a deployment of the Oracle database:\napiVersion: apps/v1 kind: Deployment metadata: name: oracle-db namespace: default spec: replicas: 1 selector: matchLabels: app.kubernetes.io/instance: dev app.kubernetes.io/name: oracle-db strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: creationTimestamp: null labels: app.kubernetes.io/instance: dev app.kubernetes.io/name: oracle-db spec: containers: - env: - name: DB_SID value: devcdb - name: DB_PDB value: devpdb - name: DB_DOMAIN value: k8s image: container-registry.oracle.com/database/enterprise:12.2.0.1-slim imagePullPolicy: IfNotPresent name: oracle-db ports: - containerPort: 1521 name: tns protocol: TCP resources: limits: cpu: \u0026#34;1\u0026#34; memory: 2Gi requests: cpu: 200m terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 Notice that you can pass in environment variables to set the SID, the name of the PDB, and so on. The documentation describes the other variables that are available. The sys password defaults to Oradoc_db1. Follow the instructions in the documentation to reset this password.\nYou should also create a service to make the database available within the Kubernetes cluster with a well known name. Here is an example:\napiVersion: v1 kind: Service metadata: name: oracle-db namespace: default spec: ports: - name: tns port: 1521 protocol: TCP targetPort: 1521 selector: app.kubernetes.io/instance: dev app.kubernetes.io/name: oracle-db sessionAffinity: None type: ClusterIP In the example above, the database would be visible in the cluster using the address oracle-db.default.svc.cluster.local:1521/devpdb.k8s.\nWhen you run the database in the Kubernetes cluster, you will probably want to also run RCU from a pod inside your network, though this is not strictly necessary. You could create a NodePort to expose your database outside the Kubernetes cluster and run RCU on another machine with access to the cluster.\nRunning the database outside Kubernetes If you wish to run the database outside Kubernetes, you need to create a way for containers running in pods in Kubernetes to see the database. This can be done by defining a Kubernetes Service with no selector and associating it with an endpoint definition, as shown in the example below:\nkind: Service apiVersion: v1 metadata: name: database spec: type: ClusterIP ports: - port: 1521 targetPort: 1521 --- kind: Endpoints apiVersion: v1 metadata: name: database subsets: - addresses: - ip: 129.123.1.4 ports: - port: 1521 This creates a DNS name database in the current namespace, ordefault if no namespace is specified, as in the example above. In this example, the fully qualified name would be database.default.svc.cluster.local. The second part is the namespace. If you looked up the ClusterIP for such a service, it would have an IP address on the overlay network, that is the network inside the Kubernetes cluster. If you are using flannel, for example, the address might be something like 10.0.1.25. Note that this is usually a non-routed address.\nFrom a container in a pod running in Kubernetes, you can make a connection to that address and port 1521. Kubernetes will route the connection to the address provided in the endpoint definition, in this example, 129.123.1.4:1521. This IP address (or name) is resolved from the point of view of the Kubernetes Node\u0026rsquo;s IP stack, not the overlay network inside the Kubernetes cluster. Note that this is a \u0026ldquo;real\u0026rdquo; routed IP address.\nWhen you create your data sources, you would use the internal address, for example, database:1521/some.service.\nBecause your database is externally accessible, you can run RCU in the normal way, from any machine on your network.\nRunning the Repository Creation Utility to set up your database schema If you want to run RCU from a pod inside the Kubernetes cluster, you can use the container image that you built earlier as a \u0026ldquo;service\u0026rdquo; pod to run RCU. To do this, start up a pod using that image as follows:\n$ kubectl run rcu --generator=run-pod/v1 --image container-registry.oracle.com/middleware/fmw_infrastructure:12.2.1.4 -- sleep infinity This will create a Kubernetes Deployment called rcu containing a pod running a container created from the container-registry.oracle.com/middleware/fmw_infrastructure:12.2.1.4 image which will just run sleep infinity, which essentially creates a pod that we can \u0026ldquo;exec\u0026rdquo; into and use to run whatever commands we need to run.\nTo get inside this container and run commands, use this command:\n$ kubectl exec -ti rcu /bin/bash When you are finished with this pod, you can remove it with this command:\n$ kubectl delete pod rcu  You can use the same approach to get a temporary pod to run other utilities like WLST.\n Creating schemas Inside this pod, you can use the following command to run RCU in command-line (no GUI) mode to create your FMW schemas. You will need to provide the right prefix and connect string. You will be prompted to enter the password for the sys user, and then the password to use for the regular schema users:\n$ /u01/oracle/oracle_common/bin/rcu \\  -silent \\  -createRepository \\  -databaseType ORACLE \\  -connectString oracle-db.default:1521/devpdb.k8s \\  -dbUser sys \\  -dbRole sysdba \\  -useSamePasswordForAllSchemaUsers true \\  -selectDependentsForComponents true \\  -schemaPrefix FMW1 \\  -component MDS \\  -component IAU \\  -component IAU_APPEND \\  -component IAU_VIEWER \\  -component OPSS \\  -component WLS \\  -component STB You need to make sure that you maintain the association between the database schemas and the matching domain just like you did in a non-Kubernetes environment. There is no specific functionality provided to help with this. We recommend that you consider making the RCU prefix (value of the schemaPrefix argument) the same as your domainUID to help maintain this association.\nDropping schemas If you want to drop the schema, you can use a command like this:\n$ /u01/oracle/oracle_common/bin/rcu \\  -silent \\  -dropRepository \\  -databaseType ORACLE \\  -connectString oracle-db.default:1521/devpdb.k8s \\  -dbUser sys \\  -dbRole sysdba \\  -selectDependentsForComponents true \\  -schemaPrefix FMW1 \\  -component MDS \\  -component IAU \\  -component IAU_APPEND \\  -component IAU_VIEWER \\  -component OPSS \\  -component WLS \\  -component STB Again, you will need to set the right prefix and connection string, and you will be prompted to enter the sys user password.\nCreate a Kubernetes Secret with the RCU credentials You also need to create a Kubernetes Secret containing the credentials for the database schemas. When you create your domain using the sample provided below, it will obtain the RCU credentials from this secret.\nWe provide a sample that demonstrates how to create the secret. The schema owner user name required will be the schemaPrefix value followed by an underscore and a component name, such as FMW1_STB. The schema owner password will be the password you provided for regular schema users during RCU creation.\nCreating an FMW Infrastructure domain Now that you have your images and you have created your RCU schemas, you are ready to create your domain. We provide a sample that demonstrates how to create an FMW Infrastructure domain.\nPatching the FMW Infrastructure image There are two kinds of patches that can be applied to the FMW Infrastructure binaries:\n Patches which are eligible for Zero Downtime Patching (ZDP), meaning that they can be applied with a rolling restart. Non-ZDP eligible compliant patches, meaning that the domain must be shut down and restarted.  You can find out whether or not a patch is eligible for Zero Downtime Patching by consulting the README file that accompanies the patch.\nIf you wish to apply a ZDP compliant patch which can be applied with a rolling restart, after you have patched the FMW Infrastructure image as shown in this sample, you can edit the domain custom resource with the name of the new image and the operator will initiate a rolling restart of the domain.\nIf you wish to apply a non-ZDP compliant patch to the FMW Infrastructure binary image, you must shut down the entire domain before applying the patch. Please see the documentation on domain lifecycle operations for more information.\nAn example of a non-ZDP compliant patch is one that includes a schema change that can not be applied dynamically.\nAdditional considerations for Coherence If you are running a domain which contains Coherence, please refer to Coherence requirements for more information.\nAdditional considerations for Model in Image If you are using Model in Image, then see the Model in Image sample, which demonstrates a JRF model and its RCU schema setup, and see Model in Image requirements for JRF domain types.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/domains/lifecycle/",
	"title": "Domain lifecycle operations",
	"tags": [],
	"description": "Start and stop Managed Servers, clusters, and domains.",
	"content": "Domain lifecycle sample scripts Beginning in version 3.1.0, the operator provides sample scripts to start up or shut down a specific Managed Server or cluster in a deployed domain, or the entire deployed domain. Beginning in version 3.2.0, additional scripts are provided for scaling a WebLogic cluster, displaying the WebLogic cluster status, initiating rolling restart of a domain or a WebLogic cluster, and initiating explicit introspection of a domain.\nNote: Prior to running these scripts, you must have previously created and deployed the domain.\nThe scripts are located in the kubernetes/samples/scripts/domain-lifecycle directory. They are helpful when scripting the life cycle of a WebLogic Server domain. For more information, see the README.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/developerguide/domain-processing/",
	"title": "Domain processing",
	"tags": [],
	"description": "",
	"content": "When the operator starts, it lists all the existing Domains and then processes these Domains to create the necessary Kubernetes resources, such as Pods and Services, if they don\u0026rsquo;t already exist. This initialization also includes looking for any stranded resources that, while created by the operator, no longer correlate with a Domain.\nAfter this, the operator starts watches for changes to Domains and any changes to other resources created by the operator. When a watch event is received, the operator processes the modified Domain to again bring the runtime presence into alignment with the desired state.\nThe operator ensures that at most one Fiber is running for any given Domain. For instance, if the customer modifies a Domain to initiate a rolling restart, then the operator will create a Fiber to process this activity. However, if while the rolling restart is in process, the customer makes another change to the Domain, such as to increase the replicas field for a cluster, then the operator will cancel the in-flight Fiber and replace it with a new Fiber. This replacement processing must be able to handle taking over for the cancelled work regardless of where the earlier processing may have been in its flow. Therefore, domain processing always starts at the beginning of the \u0026ldquo;make right\u0026rdquo; flow without any state other than the current Domain resource.\nFinally, the operator periodically lists all Domains and rechecks them. This is a backstop against the possibility that a watch event is missed, such as because of a temporary network outage. Recheck activities will not interrupt already running processes for a given Domain.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/domain-lifecycle/",
	"title": "Domain life cycle",
	"tags": [],
	"description": "",
	"content": "Learn how to start, stop, restart, and scale the WebLogic Server instances in your domain.\n Startup and shutdown  There are fields on the Domain that specify which WebLogic Server instances should be running, started, or restarted. To start, stop, or restart servers, modify these fields on the Domain.\n Restarting  This document describes when WebLogic Server instances should and will be restarted in the Kubernetes environment.\n Scaling  The operator provides several ways to initiate scaling of WebLogic clusters.\n Domain introspection  This document describes domain introspection in the Oracle WebLogic Server in Kubernetes environment.\n Liveness and readiness probes customization  This document describes how to customize the liveness and readiness probes for WebLogic Server instance Pods.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/reference/",
	"title": "Reference",
	"tags": [],
	"description": "",
	"content": "See the following reference documentation.\n Swagger  Swagger REST API documentation.\n Domain resource  Use this document to set up and configure your own Domain YAML file.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/aks/",
	"title": "Azure Kubernetes Service (AKS)",
	"tags": [],
	"description": "Deploying WebLogic Server on Azure Kubernetes Service.",
	"content": "Contents  Introduction Basics Configure AKS cluster TLS/SSL configuration Networking DNS Configuration Database Review + create  Introduction Azure Kubernetes Service makes it simple to deploy a managed Kubernetes cluster in Azure. AKS reduces the complexity and operational overhead of managing Kubernetes by offloading much of that responsibility to Azure. As a hosted Kubernetes service, Azure handles critical tasks like health monitoring and maintenance for you. The Kubernetes masters are managed by Azure. You manage and maintain only the agent nodes. As a managed Kubernetes service, AKS is free - you pay for only the agent nodes within your clusters, not for the masters.\nTo learn more, see What is Azure Kubernetes Service?\nThis document describes the Azure Marketplace offer that makes it easy to get started with WebLogic Server on Azure. The offer handles all the initial setup, creating the AKS cluster, container registry, WebLogic Kubernetes Operator installation, and domain creation using the model-in-image domain home source type. For complete details on domain home source types, see Choose a domain home source type.\nIt is also possible to run the WebLogic Kubernetes Operator manually, without the aid of the Azure Marketplace offer. The steps for doing so are documented in the sample Azure Kubernetes Service.\nBasics Use the Basics blade to provide the basic configuration details for deploying an Oracle WebLogic Server configured cluster. To do this, enter the values for the fields listed in the following tables.\nProject details    Field Description     Subscription Select a subscription to use for the charges accrued by this offer. You must have a valid active subscription associated with the Azure account that is currently logged in. If you don’t have it already, follow the steps described in Associate or add an Azure subscription to your Azure Active Directory tenant.   Resource group A resource group is a container that holds related resources for an Azure solution. The resource group includes those resources that you want to manage as a group. You decide which resources belong in a resource group based on what makes the most sense for your organization. If you have an existing resource group into which you want to deploy this solution, you can enter its name here; however, the resource group must have no pre-existing resources in it. Alternatively, you can click the Create new, and enter the name so that Azure creates a new resource group before provisioning the resources. For more information about resource groups, see the Azure documentation.    Instance details    Field Description     Region Select an Azure region from the drop-down list.    Credentials for WebLogic    Field Description     Username for WebLogic Administrator Enter a user name to access the WebLogic Server Administration Console which is started automatically after the provisioning. For more information about the WebLogic Server Administration Console, see Overview of Administration Consoles in Understanding Oracle WebLogic Server.   Password for WebLogic Administrator Enter a password to access the WebLogic Server Administration Console.   Confirm password Re-enter the value of the preceding field.   Password for WebLogic Deploy Tooling runtime encrytion The deployment uses Weblogic Deploy Tooling, including the capability to encrypt the model. This password is used for that encrption. For more information, see Encrypt Model Tool and the WebLogic Deploy Tooling documentation.   Confirm password Re-enter the value of the preceding field.   User assigned managed identity The deployment requires a user-assigned managed identity with the Contributor or Owner role in the subscription referenced previously. For more information, please see Create, list, delete, or assign a role to a user-assigned managed identity using the Azure portal.    Optional Basic Configuration    Field Description     Accept defaults for optional configuration? If you want to retain the default values for the optional configuration, such as Name prefix for Managed Server, WebLogic Domain Name and others, set the toggle button to Yes, and click Next: Configure AKS cluster. If you want to specify different values for the optional configuration, set the toggle button to No, and enter the following details.   Name prefix for Managed Server Enter a prefix for the Managed Server name.   WebLogic Domain Name Enter the name of the domain that will be created by the offer.   Maximum dynamic cluster size The maximum size of the dynamic WebLogic cluster created.   Custom Java Options to start WebLogic Server Java VM arguments passed to the invocation of WebLogic Server. For more information, see the FAQ.   Enable T3 tunneling for Administration Server If selected, configure the necessary settings to enable T3 tunneling to the Administration Server. For more details, see External network access security.   Enable T3 tunneling for WebLogic cluster If selected, configure the necessary settings to enable T3 tunneling to the WebLogic Server cluster. For more details, see External network access security.    When you are satisfied with your selections, select Next : Configure AKS cluster.\nConfigure AKS cluster Use the Configure AKS Cluster blade to configure fundamental details of how Oracle WebLogic Server runs on AKS. To do this, enter the values for the fields listed in the following tables.\nAzure Kubernetes Service In this section, you can configure some options about the AKS which will run WebLogic Server.\n   Field Description     Create a new AKS cluster If set to Yes, the deployment will create a new AKS cluster resource in the specified resource group. If set to No, you have the opportunity to select an existing AKS cluster, into which the deployment is configured. Note: the offer assumes the existing AKS cluster has no WebLogic related deployments.   Node count The initial number of nodes in the AKS cluster. This value can be changed after deployment. For information, see Scaling.   Node size The default VM size is 2x Standard DSv2, 2 vcpus, 7 GB memory. If you want to select a different VM size, select Change Size, select the size from the list (for example, A3) on the Select a VM size page, and select Select. For more information about sizing the virtual machine, see the Azure documentation on Sizes.   Enable Container insights If selected, configure the necessary settings to integrate with Container insights. For more information, see Container insights overview.   Create Persistent Volume using Azure File share service If selected, configure the necessary settings to mount a persistent volume to the nodes of the AKS cluster. For more information, see Persistent storage.    Image selection In this section, you can configure the image that is deployed using the model-in-image domain home source type. There are several options for the WebLogic image and the application image deployed therein.\n   Field Description     Use a pre-existing WebLogic Server Docker image from Oracle Container Registry? If set to Yes, the subsequent options are constrained to allow only selecting from a set of pre-existing WebLogic Server Docker images stored in the Oracle Container Registry. Note: the images in the Oracle Container Registry are unpatched. If set to No, the user may refer to a pre-existing Azure Container Registry, and must specify the Docker tag of the WebLogic Server image within that registry that will be used to create the domain. The specified image is assumed to be compatible with the WebLogic Kubernetes Operator. This allows the use of custom images, such as with a specific set patches (PSUs). For more about WebLogic Server images see WebLogic Server images.   Create a new Azure Container Registry to store application images? If set to Yes, the offer will create a new Azure Container Registry (ACR) to hold the Docker images for use in the deployment. If set to No, you must specify an existing ACR. In this case, you must be sure the selected ACR has the admin account enabled. For details, please see Admin account.   Select existing ACR instance This option is shown only if Use a pre-existing WebLogic Server Docker image from Oracle Container Registry? is set to No. If visible, select an existing Acure Container Registry instance.   Please provide the image path This option is shown only if Use a pre-existing WebLogic Server Docker image from Oracle Container Registry? is set to No. If visible, the value must be a fully qualified Docker tag of an image within the specified ACR.   Username for Oracle Single Sign-On authentication The Oracle Single Sign-on account user name for which the Terms and Restrictions for the selected WebLogic Server image have been accepted.   Password for Oracle Single Sign-On authentication The password for that account.   Confirm password Re-enter the value of the preceding field.   Select WebLogic Server Docker tag Select one of the supported images.    Java EE Application In this section you can deploy a Java EE Application along with the WebLogic Server deployment.\n   Field Description     Deploy your application package? If set to Yes, you must specify a Java EE WAR, EAR, or JAR file suitable for deployment with the selected version of WebLogic Server. If set to No, no application is deployed.   Application package (.war,.ear,.jar) With the Browse button, you can select a file from a pre-existing Azure Storage Account and Storage Container within that account. To learn how to create a Storage Account and Container, see Create a storage account.   Fail deployment if application does not become ACTIVE. If selected, the deployment will wait for the deployed application to reach the ACTIVE state and fail the deployment if it does not. For more details, see the Oracle documentation.   Number of WebLogic Managed Server replicas The initial value of the replicas field of the Domain. For information, see Scaling.    When you are satisfied with your selections, select Next : TLS/SSL configuration.\nTLS/SSL configuration With the TLS/SSL configuration blade, you can configure Oracle WebLogic Server Administration Console on a secure HTTPS port, with your own SSL certificate provided by a Certifying Authority (CA).\nSelect Yes or No for the option Configure WebLogic Server Administration Console, Remote Console, cluster and custom T3 channel to use HTTPS (Secure) ports, with your own TLS/SSL certificate. based on your preference. If you select No, you don\u0026rsquo;t have to provide any details, and can proceed by selecting Next : Networking \u0026gt;. If you select Yes, you can choose to provide the required configuration details by either uploading existing keystores or by using keystores stored in Azure Key Vault.\nIf you want to upload existing keystores, select Upload existing KeyStores for the option How would you like to provide required configuration, and enter the values for the fields listed in the following table.\nTLS/SSL configuration settings    Field Description     Identity KeyStore Data file(.jks,.p12) Upload a custom identity keystore data file by doing the following: 1. Click on the file icon. 2. Navigate to the folder where the identity keystore file resides, and select the file. 3. Click Open.   Password Enter the passphrase for the custom identity keystore.   Confirm password Re-enter the value of the preceding field.   The Identity KeyStore type (JKS,PKCS12) Select the type of custom identity keystore. The supported values are JKS and PKCS12.   The alias of the server\u0026rsquo;s private key within the Identity KeyStore Enter the alias for the private key.   The passphrase for the server\u0026rsquo;s private key within the Identity KeyStore Enter the passphrase for the private key.   Confirm passphrase Re-enter the value of the preceding field.   Trust KeyStore Data file(.jks,.p12) Upload a custom trust keystore data file by doing the following: 1. Click on the file icon. 2. Navigate to the folder where the identity keystore file resides, and select the file. 3. Click Open.   Password Enter the password for the custom trust keystore.   Confirm password Re-enter the value of the preceding field.   The Identity KeyStore type (JKS,PKCS12) Select the type of custom trust keystore. The supported values are JKS and PKCS12.    When you are satisfied with your selections, select Next : Networking.\nNetworking Use this blade to configure options for load balancing and ingress controller.\nStandard Load Balancer service Selecting Yes here will cause the offer to provision the Azure Load Balancer as a Kubernetes load balancer service. Note, you must select Yes and provide further configuration when T3 tunneling is enabled on the Basics blade. For more information on the Standard Load Balancer see Use a public Standard Load Balancer in Azure Kubernetes Service (AKS). You can still deploy an Azure Application Gateway even if you select No here.\nIf you select Yes, you have the option of configuring the Load Balancer as an internal Load Balancer. For more information on Azure internal load balancers see Use an internal load balancer with Azure Kubernetes Service (AKS).\nIf you select Yes, you must fill in the following table to map the services to load balancer ports.\nService name prefix column:\nYou can fill in any valid value in this column.\nTarget and Port column:\nThe current offer has some restrictions on the T3 ports.\n For a Target value of admin-server-t3, you must use port 7005. For a Target value of cluster-1-t3, you must use port 8011.  For the non-T3 ports, the recommended values are the usual 7001 for the admin-server and 8001 for the cluster-1.\nApplication Gateway Ingress Controller In this section, you can create an Azure Application Gateway instance as the ingress controller of your WebLogic Server. This Application Gateway is pre-configured for end-to-end-SSL with TLS termination at the gateway using the provided SSL certificate and load balances across your cluster.\nSelect Yes or No for the option Connect to Azure Application Gateway? based on your preference. If you select No, you don\u0026rsquo;t have to provide any details, and can proceed by selecting Next : DNS Configuration \u0026gt;. If you select Yes, you must specify the details required for the Application Gateway integration by entering the values for the fields as described next.\nYou must select one of the following three options, each described in turn.\n Upload a TLS/SSL certificate: Upload the pre-signed certificate now. Identify an Azure Key Vault: The Key Vault must already contain the certificate and its password stored as secrets. Generate a self-signed front-end certificate: Generate a self-signed front-end certificate and apply it during deployment.  Upload a TLS/SSL certificate\n   Field Description     Frontend TLS/SSL certificate(.pfx) For information on how to create a certificate in PFX format, see Overview of TLS termination and end to end TLS with Application Gateway.   Password The password for the certificate   Confirm password Re-enter the value of the preceding field.   Trusted root certificate(.cer, .cert) A trusted root certificate is required to allow back-end instances in the application gateway. The root certificate is a Base-64 encoded X.509(.CER) format root certificate.   Service Principal A Base64 encoded JSON string of a service principal for the selected subscription. You can generate one with command `az ad sp create-for-rbac \u0026ndash;sdk-auth    Identify an Azure Key Vault\n   Field Description     Resource group name in current subscription containing the KeyVault Enter the name of the Resource Group containing the Key Vault that stores the application gateway SSL certificate and the data required for SSL termination.   Name of the Azure KeyVault containing secrets for the Certificate for SSL Termination Enter the name of the Azure Key Vault that stores the application gateway SSL certificate and the data required for SSL termination.   The name of the secret in the specified KeyVault whose value is the SSL Certificate Data Enter the name of the Azure Key Vault secret that holds the value of the SSL certificate data.   The name of the secret in the specified KeyVault whose value is the password for the SSL Certificate Enter the name of the Azure Key Vault secret that holds the value of the SSL certificate password.   Service Principal A Base64 encoded JSON string of a service principal for the selected subscription. You can generate one with command `az ad sp create-for-rbac \u0026ndash;sdk-auth    Generate a self-signed frontend certificate\n   Field Description     Trusted root certificate(.cer, .cert) A trusted root certificate is required to allow back-end instances in the application gateway. The root certificate is a Base-64 encoded X.509(.CER) format root certificate.   Service Principal A Base64 encoded JSON string of a service principal for the selected subscription. You can generate one with command `az ad sp create-for-rbac \u0026ndash;sdk-auth    Regardless of how you provide the certificates, there are several other options when configuring the Application Gateway, as described next.\n   Field Description     Enable cookie based affinity Select this box to enable cookie based affinity (sometimes called \u0026ldquo;sticky sessions\u0026rdquo;). For more information, see Enable Cookie based affinity with an Application Gateway.   Create ingress for Administration Console. Select Yes to create an ingress for the Administration Console with the path /console.   Create ingress for WebLogic Remote Console. Select Yes to create an ingress for the Remote Console with the path /remoteconsole.    When you are satisfied with your selections, select Next : DNS Configuration.\nDNS Configuration With the DNS Configuration blade, you can provision the Oracle WebLogic Server Administration Console using a custom DNS name.\nSelect Yes or No for the option Configure Custom DNS Alias? based on your preference. If you select No, you don\u0026rsquo;t have to provide any details, and can proceed by selecting Next : Database \u0026gt;. If you select Yes, you must choose either to configure a custom DNS alias based on an existing Azure DNS zone, or create an Azure DNS zone and a custom DNS alias. This can be done by selecting Yes or No for the option Use an existing Azure DNS Zone.\nFor more information about the DNS zones, see Overview of DNS zones and records.\n If you choose to configure a custom DNS alias based on an existing Azure DNS zone, by selecting Yes for the option Use an existing Azure DNS Zone, you must specify the DNS configuration details by entering the values for the fields listed in the following table.\n   Field Description     DNS Zone Name Enter the DNS zone name.   Name of the resource group contains the DNS Zone in current subscription Enter the name of the resource group that contains the DNS zone in the current subscription.   Label for Oracle WebLogic Server Administration Console Enter a label to generate a sub-domain of the Oracle WebLogic Server Administration Console. For example, if the domain is mycompany.com and the sub-domain is admin, then the WebLogic Server Administration Console URL will be admin.mycompany.com.   Label for WebLogic Cluster Specify a label to generate subdomain of WebLogic Cluster.    If you choose to create an Azure DNS zone and a custom DNS alias, by selecting No for the option Use an existing Azure DNS Zone, you must specify the values for the following fields:\n DNS Zone Name Label for Oracle WebLogic Server Administration Console Label for WebLogic Cluster  See the preceding table for the description of these fields.\nIn the case of creating an Azure DNS zone and a custom DNS alias, you must perform the DNS domain delegation at your DNS registry post deployment. See Delegation of DNS zones with Azure DNS.\n When you are satisfied with your selections, select Next : Database.\nDatabase Use the Database blade to configure Oracle WebLogic Server to connect to an existing database. Select Yes or No for the option Connect to Database? based on your preference. If you select No, you don\u0026rsquo;t have to provide any details, and can proceed by clicking Next : Review + create \u0026gt;. If you select Yes, you must specify the details of your database by entering the values for the fields listed in the following table.\n   Field Description     Choose database type Select an existing database that you want Oracle WebLogic Server to connect to, from the drop-down list. The available options are:\n• Azure Database for PostgreSQL • Oracle Database • Azure SQL • Other   JNDI Name Enter the JNDI name for your database JDBC connection.   DataSource Connection String Enter the JDBC connection string for your database. For information about obtaining the JDBC connection string, see Obtain the JDBC Connection String for Your Database.   Global transactions protocol Determines the transaction protocol (global transaction processing behavior) for the data source. For more information, see JDBC Data Source Transaction Options. IMPORTANT: The correct value for this parameter depends on the selected database type. For PostgreSQL, select EmulateTwoPhaseCommit.   Database Username Enter the user name of your database.   Database Password Enter the password for the database user.   Confirm password Re-enter the value of the preceding field.    If you select Other as the database type, there are some additional values you must provide. WebLogic Server provides support for application data access to any database using a JDBC-compliant driver. Refer to the documentation for driver requirements.\n   Field Description     DataSource driver (.jar) Use the Browse button to upload the JAR file for the JDBC driver to a storage container. To learn how to create a Storage Account and Container, see Create a storage account.   DataSource driver name The fully qualified Java class name of the JDBC driver.   Test table name The name of the database table to use when testing physical database connections. This value depends on the specified database. Some suggested values include the following. • For Oracle, use SQL ISVALID. • For PostgreSQL, SQL Server and MariaDB use SQL SELECT 1. • For Informix use SYSTABLES.    When you are satisfied with your selections, select Next : Review + create.\nReview + create In the Review + create blade, review the details you provided for deploying Oracle WebLogic Server on AKS. If you want to make changes to any of the fields, click \u0026lt; previous or click on the respective blade and update the details.\nIf you want to use this template to automate the deployment, download it by selecting Download a template for automation.\nClick Create to create this offer. This process may take 30 to 60 minutes.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/samples/tanzu-kubernetes-service/",
	"title": "Tanzu Kubernetes Service",
	"tags": [],
	"description": "Sample for using the operator to set up a WLS cluster on the Tanzu Kubernetes Service.",
	"content": "This sample demonstrates how to use the WebLogic Kubernetes Operator (hereafter “the operator”) to set up a WebLogic Server (WLS) cluster on the Tanzu Kubernetes Grid (TKG). After performing the sample steps, your WLS domain with a Model in Image domain source type runs on a TKG Kubernetes cluster instance. After the domain has been provisioned, you can monitor it using the WebLogic Server Administration console.\nTKG is a managed Kubernetes Service that lets you quickly deploy and manage Kubernetes clusters. To learn more, see the Tanzu Kubernetes Grid (TKG) overview page.\nContents  Prerequisites  Create a Tanzu Kubernetes cluster Oracle Container Registry   Install WebLogic Kubernetes Operator Create an image Create WebLogic domain Invoke the web application  Prerequisites This sample assumes the following prerequisite environment setup:\n WebLogic Kubernetes Operator: This document was tested with version v3.1.0. Operating System: GNU/Linux. Git; use git --version to test if git works. This document was tested with version 2.17.1. TKG CLI; use tkg version to test if TKG works. This document was tested with version v1.1.3. kubectl; use kubectl version to test if kubectl works. This document was tested with version v1.18.6. Helm version 3.1 or later; use helm version to check the helm version. This document was tested with version v3.2.1.  Create a Tanzu Kubernetes cluster Create the Kubernetes cluster using the TKG CLI. See the Tanzu documentation to set up your Kubernetes cluster. After your Kubernetes cluster is up and running, run the following commands to make sure kubectl can access the Kubernetes cluster:\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-cluster-101-control-plane-8nj7t NotReady master 2d20h v1.18.6+vmware.1 192.168.100.147 192.168.100.147 VMware Photon OS/Linux 4.19.132-1.ph3 containerd://1.3.4 k8s-cluster-101-md-0-577b7dc766-552hn Ready \u0026lt;none\u0026gt; 2d20h v1.18.6+vmware.1 192.168.100.148 192.168.100.148 VMware Photon OS/Linux 4.19.132-1.ph3 containerd://1.3.4 k8s-cluster-101-md-0-577b7dc766-m8wrc Ready \u0026lt;none\u0026gt; 2d20h v1.18.6+vmware.1 192.168.100.149 192.168.100.149 VMware Photon OS/Linux 4.19.132-1.ph3 containerd://1.3.4 k8s-cluster-101-md-0-577b7dc766-p2gkz Ready \u0026lt;none\u0026gt; 2d20h v1.18.6+vmware.1 192.168.100.150 192.168.100.150 VMware Photon OS/Linux 4.19.132-1.ph3 containerd://1.3.4 Oracle Container Registry You will need an Oracle Container Registry account. The following steps will direct you to accept the Oracle Standard Terms and Restrictions to pull the WebLogic Server images. Make note of your Oracle Account password and email. This sample pertains to 12.2.1.4, but other versions may work as well.\nInstall WebLogic Kubernetes Operator The WebLogic Kubernetes Operator is an adapter to integrate WebLogic Server and Kubernetes, allowing Kubernetes to serve as a container infrastructure hosting WLS instances. The operator runs as a Kubernetes Pod and stands ready to perform actions related to running WLS on Kubernetes.\nClone the WebLogic Kubernetes Operator repository to your machine. We will use several scripts in this repository to create a WebLogic domain. Kubernetes Operators use Helm to manage Kubernetes applications. The operator’s Helm chart is located in the kubernetes/charts/weblogic-operator directory. Install the operator by running the following commands.\nClone the repository.\n$ git clone --branch v3.3.2 https://github.com/oracle/weblogic-kubernetes-operator.git $ cd weblogic-kubernetes-operator Grant the Helm service account the cluster-admin role.\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: helm-user-cluster-admin-role roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: default namespace: kube-system EOF Create a namespace and service account for the operator.\n$ kubectl create namespace sample-weblogic-operator-ns namespace/sample-weblogic-operator-ns created $ kubectl create serviceaccount -n sample-weblogic-operator-ns sample-weblogic-operator-sa serviceaccount/sample-weblogic-operator-sa created Install the operator.\n$ helm install weblogic-operator kubernetes/charts/weblogic-operator \\  --namespace sample-weblogic-operator-ns \\  --set serviceAccount=sample-weblogic-operator-sa \\  --set \u0026#34;enableClusterRoleBinding=true\u0026#34; \\  --set \u0026#34;domainNamespaceSelectionStrategy=LabelSelector\u0026#34; \\  --set \u0026#34;domainNamespaceLabelSelector=weblogic-operator\\=enabled\u0026#34; \\  --wait NAME: weblogic-operator LAST DEPLOYED: Tue Nov 17 09:33:58 2020 NAMESPACE: sample-weblogic-operator-ns STATUS: deployed REVISION: 1 TEST SUITE: None Verify the operator with the following commands; the status will be running.\n$ helm list -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION sample-weblogic-operator sample-weblogic-operator-ns 1 2020-11-17 09:33:58.584239273 -0700 PDT deployed weblogic-operator-3.1 $ kubectl get pods -n sample-weblogic-operator-ns NAME READY STATUS RESTARTS AGE weblogic-operator-775b668c8f-nwwnn 1/1 Running 0 32s Create an image  Image creation prerequisites Image creation - Introduction Understanding your first archive Staging a ZIP file of the archive Staging model files Creating the image with WIT  Image creation prerequisites  The JAVA_HOME environment variable must be set and must reference a valid JDK 8 or 11 installation. Copy the sample to a new directory; for example, use the directory /tmp/mii-sample.  $ mkdir /tmp/mii-sample $ cp -r /root/weblogic-kubernetes-operator/kubernetes/samples/scripts/create-weblogic-domain/model-in-image/* /tmp/mii-sample Note: We will refer to this working copy of the sample as /tmp/mii-sample; however, you can use a different location.\nDownload the latest WebLogic Deploying Tooling (WDT) and WebLogic Image Tool (WIT) installer ZIP files to your /tmp/mii-sample/model-images directory. Both WDT and WIT are required to create your Model in Image container images.\n$ cd /tmp/mii-sample/model-images $ curl -m 120 -fL https://github.com/oracle/weblogic-deploy-tooling/releases/latest/download/weblogic-deploy.zip \\  -o /tmp/mii-sample/model-images/weblogic-deploy.zip $ curl -m 120 -fL https://github.com/oracle/weblogic-image-tool/releases/latest/download/imagetool.zip \\  -o /tmp/mii-sample/model-images/imagetool.zip To set up the WebLogic Image Tool, run the following commands:\n$ cd /tmp/mii-sample/model-images $ unzip imagetool.zip $ ./imagetool/bin/imagetool.sh cache addInstaller \\  --type wdt \\  --version latest \\  --path /tmp/mii-sample/model-images/weblogic-deploy.zip These steps will install WIT to the /tmp/mii-sample/model-images/imagetool directory, plus put a wdt_latest entry in the tool’s cache which points to the WDT ZIP file installer. You will use WIT later in the sample for creating model images.\nImage creation - Introduction The goal of image creation is to demonstrate using the WebLogic Image Tool to create an image named model-in-image:WLS-v1 from files that you will stage to /tmp/mii-sample/model-images/model-in-image:WLS-v1/. The staged files will contain a web application in a WDT archive, and WDT model configuration for a WebLogic Administration Server called admin-server and a WebLogic cluster called cluster-1.\nOverall, a Model in Image image must contain a WebLogic Server installation and a WebLogic Deploy Tooling installation in its /u01/wdt/weblogic-deploy directory. In addition, if you have WDT model archive files, then the image must also contain these files in its /u01/wdt/models directory. Finally, an image optionally may also contain your WDT model YAML file and properties files in the same /u01/wdt/models directory. If you do not specify a WDT model YAML file in your /u01/wdt/models directory, then the model YAML file must be supplied dynamically using a Kubernetes ConfigMap that is referenced by your Domain spec.model.configMap field. We provide an example of using a model ConfigMap later in this sample.\nThe following sections contain the steps for creating the image model-in-image:WLS-v1.\nUnderstanding your first archive The sample includes a predefined archive directory in /tmp/mii-sample/archives/archive-v1 that you will use to create an archive ZIP file for the image.\nThe archive top directory, named wlsdeploy, contains a directory named applications, which includes an ‘exploded’ sample JSP web application in the directory, myapp-v1. Three useful aspects to remember about WDT archives are:\n A model image can contain multiple WDT archives. WDT archives can contain multiple applications, libraries, and other components. WDT archives have a well defined directory structure, which always has wlsdeploy as the top directory.  The application displays important details about the WebLogic Server instance that it’s running on: namely its domain name, cluster name, and server name, as well as the names of any data sources that are targeted to the server.\nStaging a ZIP file of the archive When you create the image, you will use the files in the staging directory, /tmp/mii-sample/model-in-image__WLS-v1. In preparation, you need it to contain a ZIP file of the WDT application archive.\nRun the following commands to create your application archive ZIP file and put it in the expected directory:\n# Delete existing archive.zip in case we have an old leftover version $ rm -f /tmp/mii-sample/model-images/model-in-image__WLS-v1/archive.zip # Move to the directory which contains the source files for our archive $ cd /tmp/mii-sample/archives/archive-v1 # Zip the archive to the location will later use when we run the WebLogic Image Tool $ zip -r /tmp/mii-sample/model-images/model-in-image__WLS-v1/archive.zip wlsdeploy Staging model files In this step, you explore the staged WDT model YAML file and properties in the /tmp/mii-sample/model-in-image__WLS-v1 directory. The model in this directory references the web application in your archive, configures a WebLogic Server Administration Server, and configures a WebLogic cluster. It consists of only two files, model.10.properties, a file with a single property, and, model.10.yaml, a YAML file with your WebLogic configuration model.10.yaml.\nCLUSTER_SIZE=5 Here is the WLS model.10.yaml:\ndomainInfo: AdminUserName: \u0026#39;@@SECRET:__weblogic-credentials__:username@@\u0026#39; AdminPassword: \u0026#39;@@SECRET:__weblogic-credentials__:password@@\u0026#39; ServerStartMode: \u0026#39;prod\u0026#39; topology: Name: \u0026#39;@@ENV:CUSTOM_DOMAIN_NAME@@\u0026#39; AdminServerName: \u0026#39;admin-server\u0026#39; Cluster: \u0026#39;cluster-1\u0026#39;: DynamicServers: ServerTemplate: \u0026#39;cluster-1-template\u0026#39; ServerNamePrefix: \u0026#39;managed-server\u0026#39; DynamicClusterSize: \u0026#39;@@PROP:CLUSTER_SIZE@@\u0026#39; MaxDynamicClusterSize: \u0026#39;@@PROP:CLUSTER_SIZE@@\u0026#39; MinDynamicClusterSize: \u0026#39;0\u0026#39; CalculatedListenPorts: false Server: \u0026#39;admin-server\u0026#39;: ListenPort: 7001 ServerTemplate: \u0026#39;cluster-1-template\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 appDeployments: Application: myapp: SourcePath: \u0026#39;wlsdeploy/applications/myapp-v1\u0026#39; ModuleType: ear Target: \u0026#39;cluster-1\u0026#39; The model files:\n  Define a WebLogic domain with:\n Cluster cluster-1 Administration Server admin-server A cluster-1 targeted EAR application that’s located in the WDT archive ZIP file at wlsdeploy/applications/myapp-v1    Leverage macros to inject external values:\n The property file CLUSTER_SIZE property is referenced in the model YAML file DynamicClusterSize and MaxDynamicClusterSize fields using a PROP macro. The model file domain name is injected using a custom environment variable named CUSTOM_DOMAIN_NAME using an ENV macro.  You set this environment variable later in this sample using an env field in its Domain. This conveniently provides a simple way to deploy multiple differently named domains using the same model image.   The model file administrator user name and password are set using a weblogic-credentials secret macro reference to the WebLogic credential secret.  This secret is in turn referenced using the webLogicCredentialsSecret field in the Domain. The weblogic-credentials is a reserved name that always dereferences to the owning Domain actual WebLogic credentials secret name.      A Model in Image image can contain multiple properties files, archive ZIP files, and YAML files but in this sample you use just one of each. For a complete discussion of Model in Images model file naming conventions, file loading order, and macro syntax, see Model files files in the Model in Image user documentation.\nCreating the image with WIT At this point, you have staged all of the files needed for the image model-in-image:WLS-v1; they include:\n /tmp/mii-sample/model-images/weblogic-deploy.zip /tmp/mii-sample/model-images/model-in-image__WLS-v1/model.10.yaml /tmp/mii-sample/model-images/model-in-image__WLS-v1/model.10.properties /tmp/mii-sample/model-images/model-in-image__WLS-v1/archive.zip  If you don’t see the weblogic-deploy.zip file, then you missed a step in the prerequisites.\nNow, you use the Image Tool to create an image named model-in-image:WLS-v1 that’s layered on a base WebLogic image. You’ve already set up this tool during the prerequisite steps.\nRun the following commands to create the model image and verify that it worked:\n$ cd /tmp/mii-sample/model-images $ ./imagetool/bin/imagetool.sh update \\  --tag model-in-image:WLS-v1 \\  --fromImage container-registry.oracle.com/middleware/weblogic:12.2.1.4 \\  --wdtModel ./model-in-image__WLS-v1/model.10.yaml \\  --wdtVariables ./model-in-image__WLS-v1/model.10.properties \\  --wdtArchive ./model-in-image__WLS-v1/archive.zip \\  --wdtModelOnly \\  --wdtDomainType WLS \\  --chown oracle:root If you don’t see the imagetool directory, then you missed a step in the prerequisites.\nThis command runs the WebLogic Image Tool in its Model in Image mode, and does the following:\n Builds the final image as a layer on the container-registry.oracle.com/middleware/weblogic:12.2.1.4 base image. Copies the WDT ZIP file that’s referenced in the WIT cache into the image.  Note that you cached WDT in WIT using the keyword latest when you set up the cache during the sample prerequisites steps. This lets WIT implicitly assume it’s the desired WDT version and removes the need to pass a -wdtVersion flag.   Copies the specified WDT model, properties, and application archives to image location /u01/wdt/models.  When the command succeeds, it should end with output like the following:\n[INFO ] Build successful. Build time=36s. Image tag=model-in-image:WLS-v1 Also, if you run the docker images command, then you will see an image named model-in-image:WLS-v1.\n Note: If you have Kubernetes cluster worker nodes that are remote to your local machine, then you need to put the image in a location that these nodes can access. See Ensuring your Kubernetes cluster can access images.\n Create WebLogic domain In this section, you will deploy the new image to namespace sample-domain1-ns, including the following steps:\n Create a namespace for the WebLogic domain. Upgrade the operator to manage the WebLogic domain namespace. Create a Secret containing your WebLogic administrator user name and password. Create a Secret containing your Model in Image runtime encryption password:  All Model in Image domains must supply a runtime encryption Secret with a password value. It is used to encrypt configuration that is passed around internally by the operator. The value must be kept private but can be arbitrary; you can optionally supply a different secret value every time you restart the domain.   Deploy a Domain YAML file that references the new image. Wait for the domain’s Pods to start and reach their ready state.  Namespace Create a namespace that can host one or more domains:\n$ kubectl create namespace sample-domain1-ns ## label the domain namespace so that the operator can autodetect and create WebLogic Server pods. $ kubectl label namespace sample-domain1-ns weblogic-operator=enabled Secrets First, create the secrets needed by the WLS type model domain. In this case, you have two secrets.\nRun the following kubectl commands to deploy the required secrets:\n$ kubectl -n sample-domain1-ns create secret generic \\  sample-domain1-weblogic-credentials \\  --from-literal=username=weblogic --from-literal=password=welcome1 $ kubectl -n sample-domain1-ns label secret \\  sample-domain1-weblogic-credentials \\  weblogic.domainUID=sample-domain1 $ kubectl -n sample-domain1-ns create secret generic \\  sample-domain1-runtime-encryption-secret \\  --from-literal=password=my_runtime_password $ kubectl -n sample-domain1-ns label secret \\  sample-domain1-runtime-encryption-secret \\  weblogic.domainUID=sample-domain1 Some important details about these secrets:\n  The WebLogic credentials secret:\n It is required and must contain username and password fields. It must be referenced by the spec.webLogicCredentialsSecret field in your Domain. It also must be referenced by macros in the domainInfo.AdminUserName and domainInfo.AdminPassWord fields in your model YAML file.    The Model WDT runtime secret:\n This is a special secret required by Model in Image. It must contain a password field. It must be referenced using the spec.model.runtimeEncryptionSecret field in its Domain. It must remain the same for as long as the domain is deployed to Kubernetes but can be changed between deployments. It is used to encrypt data as it\u0026rsquo;s internally passed using log files from the domain\u0026rsquo;s introspector job and on to its WebLogic Server pods.    Deleting and recreating the secrets:\n You delete a secret before creating it, otherwise the create command will fail if the secret already exists. This allows you to change the secret when using the kubectl create secret command.    You name and label secrets using their associated domain UID for two reasons:\n To make it obvious which secrets belong to which domains. To make it easier to clean up a domain. Typical cleanup scripts use the weblogic.domainUID label as a convenience for finding all resources associated with a domain.    Domain resource Now, you create a Domain YAML file. A Domain is the key resource that tells the operator how to deploy a WebLogic domain.\nCopy the following to a file called /tmp/mii-sample/mii-initial.yaml or similar, or use the file /tmp/mii-sample/domain-resources/WLS/mii-initial-d1-WLS-v1.yaml that is included in the sample source.\n  Click here to view the WLS Domain YAML file.   # # This is an example of how to define a Domain resource. # apiVersion: \u0026#34;weblogic.oracle/v8\u0026#34; kind: Domain metadata: name: sample-domain1 namespace: sample-domain1-ns labels: weblogic.domainUID: sample-domain1 spec: # Set to \u0026#39;FromModel\u0026#39; to indicate \u0026#39;Model in Image\u0026#39;. domainHomeSourceType: FromModel # The WebLogic Domain Home, this must be a location within # the image for \u0026#39;Model in Image\u0026#39; domains. domainHome: /u01/domains/sample-domain1 # The WebLogic Server image that the Operator uses to start the domain image: \u0026#34;model-in-image:WLS-v1\u0026#34; # Defaults to \u0026#34;Always\u0026#34; if image tag (version) is \u0026#39;:latest\u0026#39; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: regsecret # Identify which Secret contains the WebLogic Admin credentials, # the secret must contain \u0026#39;username\u0026#39; and \u0026#39;password\u0026#39; fields. webLogicCredentialsSecret: name: sample-domain1-weblogic-credentials # Whether to include the WebLogic Server stdout in the pod\u0026#39;s stdout, default is true includeServerOutInPodLog: true # Whether to enable overriding your log file location, see also \u0026#39;logHome\u0026#39; #logHomeEnabled: false # The location for domain log, server logs, server out, introspector out, and Node Manager log files # see also \u0026#39;logHomeEnabled\u0026#39;, \u0026#39;volumes\u0026#39;, and \u0026#39;volumeMounts\u0026#39;. #logHome: /shared/logs/sample-domain1 # Set which WebLogic Servers the Operator will start # - \u0026#34;NEVER\u0026#34; will not start any server in the domain # - \u0026#34;ADMIN_ONLY\u0026#34; will start up only the administration server (no managed servers will be started) # - \u0026#34;IF_NEEDED\u0026#34; will start all non-clustered servers, including the administration server, and clustered servers up to their replica count. serverStartPolicy: \u0026#34;IF_NEEDED\u0026#34; # Settings for all server pods in the domain including the introspector job pod serverPod: # Optional new or overridden environment variables for the domain\u0026#39;s pods # - This sample uses CUSTOM_DOMAIN_NAME in its image model file # to set the WebLogic domain name env: - name: CUSTOM_DOMAIN_NAME value: \u0026#34;domain1\u0026#34; - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false\u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-XX:+UseContainerSupport -Djava.security.egd=file:/dev/./urandom \u0026#34; # Optional volumes and mounts for the domain\u0026#39;s pods. See also \u0026#39;logHome\u0026#39;. #volumes: #- name: weblogic-domain-storage-volume # persistentVolumeClaim: # claimName: sample-domain1-weblogic-sample-pvc #volumeMounts: #- mountPath: /shared # name: weblogic-domain-storage-volume # The desired behavior for starting the domain\u0026#39;s administration server. adminServer: # The serverStartState legal values are \u0026#34;RUNNING\u0026#34; or \u0026#34;ADMIN\u0026#34; # \u0026#34;RUNNING\u0026#34; means the listed server will be started up to \u0026#34;RUNNING\u0026#34; mode # \u0026#34;ADMIN\u0026#34; means the listed server will be start up to \u0026#34;ADMIN\u0026#34; mode serverStartState: \u0026#34;RUNNING\u0026#34; # Setup a Kubernetes node port for the administration server default channel #adminService: # channels: # - channelName: default # nodePort: 30701 # The number of managed servers to start for unlisted clusters replicas: 1 # The desired behavior for starting a specific cluster\u0026#39;s member servers clusters: - clusterName: cluster-1 serverStartState: \u0026#34;RUNNING\u0026#34; replicas: 2 # Change the `restartVersion` to force the introspector job to rerun # and apply any new model configuration, to also force a subsequent # roll of your domain\u0026#39;s WebLogic Server pods. restartVersion: \u0026#39;1\u0026#39; configuration: # Settings for domainHomeSourceType \u0026#39;FromModel\u0026#39; model: # Valid model domain types are \u0026#39;WLS\u0026#39;, \u0026#39;JRF\u0026#39;, and \u0026#39;RestrictedJRF\u0026#39;, default is \u0026#39;WLS\u0026#39; domainType: \u0026#34;WLS\u0026#34; # Optional configmap for additional models and variable files #configMap: sample-domain1-wdt-config-map # All \u0026#39;FromModel\u0026#39; domains require a runtimeEncryptionSecret with a \u0026#39;password\u0026#39; field runtimeEncryptionSecret: sample-domain1-runtime-encryption-secret # Secrets that are referenced by model yaml macros # (the model yaml in the optional configMap or in the image) #secrets: #- sample-domain1-datasource-secret     Note: Before you deploy the domain custom resource, determine if you have Kubernetes cluster worker nodes that are remote to your local machine. If so, you need to put the Domain\u0026rsquo;s image in a location that these nodes can access and you may also need to modify your Domain YAML file to reference the new location. See Ensuring your Kubernetes cluster can access images.\n Run the following command to create the domain custom resource:\n$ kubectl apply -f /tmp/mii-sample/domain-resources/WLS/mii-initial-d1-WLS-v1.yaml  Note: If you are choosing not to use the predefined Domain YAML file and instead created your own Domain YAML file earlier, then substitute your custom file name in the above command. Previously, we suggested naming it /tmp/mii-sample/mii-initial.yaml.\n Verify the WebLogic Server pods are all running:\n$ kubectl get all -n sample-domain1-ns NAME READY STATUS RESTARTS AGE pod/sample-domain1-admin-server 1/1 Running 0 41m pod/sample-domain1-managed-server1 1/1 Running 0 40m pod/sample-domain1-managed-server2 1/1 Running 0 40m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/sample-domain1-admin-server ClusterIP None \u0026lt;none\u0026gt; 7001/TCP 41m service/sample-domain1-cluster-cluster-1 ClusterIP 100.66.99.27 \u0026lt;none\u0026gt; 8001/TCP 40m service/sample-domain1-managed-server1 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 40m service/sample-domain1-managed-server2 ClusterIP None \u0026lt;none\u0026gt; 8001/TCP 40m Invoke the web application Create a load balancer to access the WebLogic Server Administration Console and applications deployed in the cluster. Tanzu supports the MetalLB load balancer and NGINX ingress for routing.\nInstall the MetalLB load balancer by running following commands:\n## create namespace metallb-system $ kubectl create ns metallb-system ## deploy MetalLB load balancer $ kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.9.2/manifests/metallb.yaml -n metallb-system ## create secret $ kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=\u0026#34;$(openssl rand -base64 128)\u0026#34; $ cat metallb-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: |address-pools: - name: default protocol: layer2 addresses: - 192.168.100.50-192.168.100.65 $ kubectl apply -f metallb-configmap.yaml configmap/config created $ kubectl get all -n metallb-system NAME READY STATUS RESTARTS AGE pod/controller-684f5d9b49-jkzfk 1/1 Running 0 2m14s pod/speaker-b457r 1/1 Running 0 2m14s pod/speaker-bzmmj 1/1 Running 0 2m14s pod/speaker-gphh5 1/1 Running 0 2m14s pod/speaker-lktgc 1/1 Running 0 2m14s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/speaker 4 4 4 4 4 beta.kubernetes.io/os=linux 2m14s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/controller 1/1 1 1 2m14s NAME DESIRED CURRENT READY AGE replicaset.apps/controller-684f5d9b49 1 1 1 2m14s Install NGINX.\n$ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx --force-update $ helm install ingress-nginx ingress-nginx/ingress-nginx Create ingress for accessing the application deployed in the cluster and to access the Administration console.\n$ cat ingress.yaml apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: sample-nginx-ingress-pathrouting namespace: sample-domain1-ns annotations: kubernetes.io/ingress.class: nginx spec: rules: - host: http: paths: - path: / backend: serviceName: sample-domain1-cluster-cluster-1 servicePort: 8001 - path: /console backend: serviceName: sample-domain1-admin-server servicePort: 7001 $ kubectl apply -f ingress.yaml Verify ingress is running.\n$ kubectl get ingresses -n sample-domain1-ns NAME CLASS HOSTS ADDRESS PORTS AGE sample-nginx-ingress-pathrouting \u0026lt;none\u0026gt; * 192.168.100.50 80 7m18s Access the Administration Console using the load balancer IP address, http://192.168.100.50/console\nAccess the sample application.\n# Access the sample application using the load balancer IP (192.168.100.50) $ curl http://192.168.100.50/myapp_war/index.jsp \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v1\u0026#39; of the mii-sample JSP web-app. Welcome to WebLogic Server \u0026#39;managed-server1\u0026#39;! domain UID = \u0026#39;sample-domain1\u0026#39; domain name = \u0026#39;domain1\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found 0 local data sources: ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; $ curl http://192.168.100.50/myapp_war/index.jsp \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;pre\u0026gt; ***************************************************************** Hello World! This is version \u0026#39;v1\u0026#39; of the mii-sample JSP web-app. Welcome to WebLogic Server \u0026#39;managed-server2\u0026#39;! domain UID = \u0026#39;sample-domain1\u0026#39; domain name = \u0026#39;domain1\u0026#39; Found 1 local cluster runtime: Cluster \u0026#39;cluster-1\u0026#39; Found 0 local data sources: ***************************************************************** \u0026lt;/pre\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/faq/volumes/",
	"title": "Providing access to a PersistentVolumeClaim",
	"tags": [],
	"description": "I need to provide an instance with access to a PersistentVolumeClaim.",
	"content": " I need to provide an instance with access to a PersistentVolumeClaim.\n Some applications need access to a file, either to read data or to provide additional logging beyond what is built into the operator. One common way of doing that within Kubernetes is to create a PersistentVolumeClaim (PVC) and map it to a file. The domain configuration can then be used to provide access to the claim across the domain, within a single cluster, or for a single server. In each case, the access is configured within the serverPod element of the configuration of the desired scope.\nFor example, here is a read-only PersistentVolumeClaim specification. Note that its name is myclaim.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: accessModes: - ReadOnlyMany volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: slow To provide access to this claim to all Managed Servers in the cluster-1 cluster, specify the following in your Domain:\nclusters: - clusterName: cluster-1 serverPod: volumes: - name: my-volume-1 persistentVolumeClaim: claimName: myclaim volumeMounts: - name: my-volume-1 mountPath: /weblogic-operator/my/volume1 Note the use of the claim name in the claimName field of the volume entry. Both a volume and a volumeMount entry are required, and must have the same name. The volume entry associates that name with the claim, while the volumeMount entry defines the path to it that the application can use to access the file.\nNOTE: If the PVC is mapped either across the domain or to a cluster, its access mode must be either ReadOnlyMany or ReadWriteMany.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/developerguide/documentation/",
	"title": "Documentation",
	"tags": [],
	"description": "",
	"content": "This documentation is produced using Hugo. To make an update to the documentation, follow this process:\n  If you have not already done so, clone the repository.\n$ git clone https://github.com/oracle/weblogic-kubernetes-operator   Create a new branch.\n$ git checkout -b your-branch   Make your documentation updates by editing the source files in documentation/staging/content. Make sure you check in the changes from the documentation/staging/content area only; do not build the site and check in the static files.\n   If you wish to view your changes, you can run the site locally using these commands. The site will be available on the URL shown here:\n$ cd documentation/staging $ hugo server -b http://localhost:1313/weblogic-kubernetes-operator   When you are ready to submit your changes, push your branch to origin and submit a pull request. Remember to follow the guidelines in the Contribute to the operator document.\n  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": "  Certificates  Operator SSL/TLS certificate handling\n Domain security  WebLogic domain security and the operator\n Encryption  WebLogic domain encryption and the operator\n Service accounts  Kubernetes ServiceAccounts for the operator\n RBAC  Operator role-based authorization\n Secrets  Kubernetes Secrets for the operator\n OpenShift  OpenShift information for the operator\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/developerguide/backwards-compatibility/",
	"title": "Backward compatibility",
	"tags": [],
	"description": "",
	"content": "Starting with the 2.0.1 release, operator releases must be backward compatible with respect to the Domain schema, operator Helm chart input values, configuration overrides template, Kubernetes resources created by the operator Helm chart, Kubernetes resources created by the operator, and the operator REST interface. We will maintain compatibility for three releases, except in the case of a clearly communicated deprecated feature, which will be maintained for one release after a replacement is available.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/domain-events/",
	"title": "Domain events",
	"tags": [],
	"description": "",
	"content": "Contents  Overview Operator-generated event types Operator-generated event details How to access the events Examples of generated events  Overview This document describes Kubernetes events that the operator generates about resources that it manages, during key points of its processing workflow. These events provide an additional way of monitoring your domain resources. Note that the Kubernetes server also generates events for standard Kubernetes resources, such as pods, services, and jobs that the operator generates on behalf of deployed domain custom resources.\nOperator-generated event types The operator generates these event types in a domain namespace, which indicate the following:\n DomainCreated: A new domain is created. DomainChanged: A change has been made to an existing domain. DomainDeleted: An existing domain has been deleted. DomainProcessingStarting: The operator has started to process a new domain or to update an existing domain. This event may be a result of a DomainCreate, DomainChanged, or DomainDeleted event, or a result of a retry after a failed attempt. DomainProcessingFailed: The operator has encountered a problem while it was processing the domain resource. The failure either could be a configuration error or a Kubernetes API error. DomainProcessingRetrying: The operator is going to retry the processing of a domain after it encountered a failure. DomainProcessingCompleted: The operator successfully completed the processing of a domain resource. DomainProcessingAborted: The operator stopped processing a domain when the operator encountered a fatal error or a failure that persisted after the specified maximum number of retries. DomainRollStarting: The operator has detected domain resource or Model in Image model updates that require it to perform a rolling restart of the domain. If the domain roll is due to a change to domain resource fields image, imagePullPolicy, livenessProbe, readinessProbe, restartVersion, domainHome, includeServerOutInPodLog, or logHome, then the event message reports the field name plus its old and new values. If the domain roll is due to other domain resource changes that cause servers to be restarted (see full list of fields that cause servers to be restarted), then the event message simply reports that the domain resource has changed. If the domain roll is due to a Model in Image model update, then the event message reports there has been a change in the WebLogic domain configuration without the details. DomainRollCompleted: The operator has successfully completed a rolling restart of a domain. PodCycleStarting: The operator has started to replace a server pod after it detects that the current pod does not conform to the current domain resource or WebLogic domain configuration. DomainValidationError: A validation error or warning is found in a domain resource. Please refer to the event message for details. NamespaceWatchingStarted: The operator has started watching for domains in a namespace. NamespaceWatchingStopped: The operator has stopped watching for domains in a namespace. Note that the creation of this event in a domain namespace is the operator\u0026rsquo;s best effort only; the event will not be generated if the required Kubernetes privilege is removed when a namespace is no longer managed by the operator.  The operator also generates these event types in the operator\u0026rsquo;s namespace, which indicate the following:\n StartManagingNamespace: The operator has started managing domains in a namespace. StopManagingNamespace: The operator has stopped managing domains in a namespace. StartManagingNamespaceFailed: The operator failed to start managing domains in a namespace because it does not have the required privileges.  Operator-generated event details Each operator-generated event contains the following fields:\n metadata  namespace: Namespace in which the event is generated. labels: weblogic.createdByOperator=true and, for a domain event, weblogic.domainUID=\u0026lt;domainUID\u0026gt;.   type: String that describes the type of the event. Possible values are Normal or Warning. count: Integer that indicates the number of occurrences of the event. Note that the events are matched by the combination of the reason, involvedObject, and message fields. reportingComponent: String that describes the component that reports the event. The value is weblogic.operator for all operator-generated events. reportingInstance: String that describes the instance that reports the event. The value is the Kubernetes pod name of the operator instance that generates the event. firstTimestamp: DateTime field that presents the timestamp of the first occurrence of this event. lastTimestamp: DateTime field that presents the timestamp of the last occurrence of this event. reason: Short, machine understandable string that gives the reason for the transition to the object\u0026rsquo;s current status. message: String that describes the details of the event. involvedObject: V1ObjectReference object that describes the Kubernetes resources with which this event is associated.  name: String that describes the name of the resource with which the event is associated. It may be the domainUID, the name of the namespace that the operator watches, or the name of the operator pod. namespace: String that describes the namespace of the event, which is either the namespace of the domain resource or the namespace of the operator. kind: String that describes the kind of resource this object represents. The value is Domain for a domain event, Namespace for a namespace event in the domain namespace, or Pod for the operator pod. apiVersion: String that describes the apiVersion of the involved object, which is the apiVersion of the domain resource, for example, weblogic.oracle/v8, for a domain event or unset for a namespace event. UID: String that describes the unique identifier of the object that is generated by the Kubernetes server.    How to access the events To access the events that are associated with all resources in a particular namespace, run:\n$ kubectl get events -n [namespace] To get the events and sort them by their last timestamp, run:\n$ kubectl get events -n [namespace] --sort-by=lastTimestamp To get all the events that are generated by the operator, run:\n$ kubectl get events -n [namespace] --selector=weblogic.createdByOperator=true To get all the events that are generated by the operator for a particular domain resource, for example sample-domain1, run:\n$ kubectl get events -n [namespace] --selector=weblogic.domainUID=sample-domain1,weblogic.createdByOperator=true --sort-by=lastTimestamp Examples of generated events Here are some examples of operator-generated events from the output of the kubectl describe event or kubectl get events commands for sample-domain1 in namespace sample-domain1-ns.\nExample of a DomainProcessingStarting event:\nName: sample-domain1.DomainProcessingStarting.1c415c9cf54c0f2 Namespace: sample-domain1-ns Labels: weblogic.createdByOperator=true weblogic.domainUID=sample-domain1 Annotations: \u0026lt;none\u0026gt; API Version: v1 Count: 4 Event Time: \u0026lt;nil\u0026gt; First Timestamp: 2021-01-19T20:06:21Z Involved Object: API Version: weblogic.oracle/v8 Kind: Domain Name: sample-domain1 Namespace: sample-domain1-ns UID: 9dc647fb-b9d2-43f8-bac7-69258560a99a Kind: Event Last Timestamp: 2021-01-19T20:12:29Z Message: Creating or updating Kubernetes presence for WebLogic Domain with UID sample-domain1 Metadata: Creation Timestamp: 2021-01-19T20:06:21Z Resource Version: 2635264 Self Link: /api/v1/namespaces/sample-domain1-ns/events/sample-domain1.DomainProcessingStarting.1c415c9cf54c0f2 UID: 093383eb-6fc6-46c7-aaa4-c4ca8399bfab Reason: DomainProcessingStarting Reporting Component: weblogic.operator Reporting Instance: weblogic-operator-67c9999d99-rff62 Source: Type: Normal Events: \u0026lt;none\u0026gt; Example of a DomainProcessingFailed event:\nName: sample-domain1.DomainProcessingFailed.1c416683eb212c63 Namespace: sample-domain1-ns Labels: weblogic.createdByOperator=true weblogic.domainUID=sample-domain1 Annotations: \u0026lt;none\u0026gt; API Version: v1 Count: 12 Event Time: \u0026lt;nil\u0026gt; First Timestamp: 2021-01-19T20:06:24Z Involved Object: API Version: weblogic.oracle/v8 Kind: Domain Name: sample-domain1 Namespace: sample-domain1-ns UID: 9dc647fb-b9d2-43f8-bac7-69258560a99a Kind: Event Last Timestamp: 2021-01-19T20:12:11Z Message: Failed to complete processing domain resource sample-domain1 due to: rpc error: code = Unknown desc = pull access denied for domain-home-in-image, repository does not exist or may require 'docker login', the processing will be retried if needed Metadata: Creation Timestamp: 2021-01-19T20:06:24Z Resource Version: 2635213 Self Link: /api/v1/namespaces/sample-domain1-ns/events/sample-domain1.DomainProcessingFailed.1c416683eb212c63 UID: f3e017ea-1b38-4ba3-bd58-0ee731f3ae4e Reason: DomainProcessingFailed Reporting Component: weblogic.operator Reporting Instance: weblogic-operator-67c9999d99-rff62 Source: Type: Warning Events: \u0026lt;none\u0026gt; Example of a DomainProcessingCompleted event:\nName: sample-domain1.DomainProcessingCompleted.1c478d91fdada118 Namespace: sample-domain1-ns Labels: weblogic.createdByOperator=true weblogic.domainUID=sample-domain1 Annotations: \u0026lt;none\u0026gt; API Version: v1 Count: 1 Event Time: \u0026lt;nil\u0026gt; First Timestamp: 2021-01-19T20:13:07Z Involved Object: API Version: weblogic.oracle/v8 Kind: Domain Name: sample-domain1 Namespace: sample-domain1-ns UID: 9dc647fb-b9d2-43f8-bac7-69258560a99a Kind: Event Last Timestamp: 2021-01-19T20:13:07Z Message: Successfully completed processing domain resource sample-domain1 Metadata: Creation Timestamp: 2021-01-19T20:13:07Z Resource Version: 2635401 Self Link: /api/v1/namespaces/sample-domain1-ns/events/sample-domain1.DomainProcessingCompleted.1c478d91fdada118 UID: ea7734af-31bc-4f8e-b02b-5ef6b240749e Reason: DomainProcessingCompleted Reporting Component: weblogic.operator Reporting Instance: weblogic-operator-67c9999d99-rff62 Source: Type: Normal Events: \u0026lt;none\u0026gt; Example of domain processing completed after failure and retries:\nThe scenario is that the operator initially failed to process the domain resource because the specified image was missing, and then completed the processing during a retry after the image was recreated. Note that this is not a full list of events; some of the events that are generated by the Kubernetes server have been removed to make the list less cluttered.\nThe output of command kubectl get events -n sample-domain1-ns --sort-by=lastTimestamp\nLAST SEEN TYPE REASON OBJECT MESSAGE 7m54s Normal NamespaceWatchingStarted namespace/sample-domain1-ns Started watching namespace sample-domain1-ns 7m44s Normal DomainCreated domain/sample-domain1 Domain resource sample-domain1 was created 7m43s Normal SuccessfulCreate job/sample-domain1-introspector Created pod: sample-domain1-introspector-d42rf 7m43s Normal Scheduled pod/sample-domain1-introspector-d42rf Successfully assigned sample-domain1-ns/sample-domain1-introspector-d42rf to doxiao-1 7m4s Normal Pulling pod/sample-domain1-introspector-d42rf Pulling image \u0026quot;domain-home-in-image:12.2.1.4\u0026quot; 7m2s Warning Failed pod/sample-domain1-introspector-d42rf Error: ErrImagePull 7m2s Warning Failed pod/sample-domain1-introspector-d42rf Failed to pull image \u0026quot;domain-home-in-image:12.2.1.4\u0026quot;: rpc error: code = Unknown desc = pull access denied for domain-home-in-image, repository does not exist or may require 'docker login' 6m38s Warning Failed pod/sample-domain1-introspector-d42rf Error: ImagePullBackOff 6m38s Normal BackOff pod/sample-domain1-introspector-d42rf Back-off pulling image \u0026quot;domain-home-in-image:12.2.1.4\u0026quot; 5m43s Warning DeadlineExceeded job/sample-domain1-introspector Job was active longer than specified deadline 5m43s Warning DomainProcessingFailed domain/sample-domain1 Failed to complete processing domain resource sample-domain1 due to: Job sample-domain1-introspector failed due to reason: DeadlineExceeded. ActiveDeadlineSeconds of the job is configured with 120 seconds. The job was started 120 seconds ago. Ensure all domain dependencies have been deployed (any secrets, config-maps, PVs, and PVCs that the domain resource references). Use kubectl describe for the job and its pod for more job failure information. The job may be retried by the operator up to 5 times with longer ActiveDeadlineSeconds value in each subsequent retry. Use tuning parameter domainPresenceFailureRetryMaxCount to configure max retries., the processing will be retried if needed 5m43s Normal SuccessfulDelete job/sample-domain1-introspector Deleted pod: sample-domain1-introspector-d42rf 5m32s Normal Scheduled pod/sample-domain1-introspector-cmxjs Successfully assigned sample-domain1-ns/sample-domain1-introspector-cmxjs to doxiao-1 5m32s Normal SuccessfulCreate job/sample-domain1-introspector Created pod: sample-domain1-introspector-cmxjs 4m52s Normal Pulling pod/sample-domain1-introspector-cmxjs Pulling image \u0026quot;domain-home-in-image:12.2.1.4\u0026quot; 4m50s Warning Failed pod/sample-domain1-introspector-cmxjs Failed to pull image \u0026quot;domain-home-in-image:12.2.1.4\u0026quot;: rpc error: code = Unknown desc = pull access denied for domain-home-in-image, repository does not exist or may require 'docker login' 4m50s Warning Failed pod/sample-domain1-introspector-cmxjs Error: ErrImagePull 4m27s Warning Failed pod/sample-domain1-introspector-cmxjs Error: ImagePullBackOff 4m27s Normal BackOff pod/sample-domain1-introspector-cmxjs Back-off pulling image \u0026quot;domain-home-in-image:12.2.1.4\u0026quot; 2m32s Warning DomainProcessingFailed domain/sample-domain1 Failed to complete processing domain resource sample-domain1 due to: Job sample-domain1-introspector failed due to reason: DeadlineExceeded. ActiveDeadlineSeconds of the job is configured with 180 seconds. The job was started 180 seconds ago. Ensure all domain dependencies have been deployed (any secrets, config-maps, PVs, and PVCs that the domain resource references). Use kubectl describe for the job and its pod for more job failure information. The job may be retried by the operator up to 5 times with longer ActiveDeadlineSeconds value in each subsequent retry. Use tuning parameter domainPresenceFailureRetryMaxCount to configure max retries., the processing will be retried if needed 2m32s Normal SuccessfulDelete job/sample-domain1-introspector Deleted pod: sample-domain1-introspector-cmxjs 2m32s Warning DeadlineExceeded job/sample-domain1-introspector Job was active longer than specified deadline 2m22s Normal DomainProcessingRetrying domain/sample-domain1 Retrying the processing of domain resource sample-domain1 after one or more failed attempts 2m20s Normal SuccessfulCreate job/sample-domain1-introspector Created pod: sample-domain1-introspector-ght6p 2m20s Normal Scheduled pod/sample-domain1-introspector-ght6p Successfully assigned sample-domain1-ns/sample-domain1-introspector-ght6p to doxiao-1 2m17s Normal BackOff pod/sample-domain1-introspector-ght6p Back-off pulling image \u0026quot;domain-home-in-image:12.2.1.4\u0026quot; 2m17s Warning Failed pod/sample-domain1-introspector-ght6p Error: ImagePullBackOff 2m7s Warning DomainProcessingFailed domain/sample-domain1 Failed to complete processing domain resource sample-domain1 due to: Back-off pulling image \u0026quot;domain-home-in-image:12.2.1.4\u0026quot;, the processing will be retried if needed 2m7s Normal Pulling pod/sample-domain1-introspector-ght6p Pulling image \u0026quot;domain-home-in-image:12.2.1.4\u0026quot; 2m5s Warning Failed pod/sample-domain1-introspector-ght6p Error: ErrImagePull 2m5s Warning Failed pod/sample-domain1-introspector-ght6p Failed to pull image \u0026quot;domain-home-in-image:12.2.1.4\u0026quot;: rpc error: code = Unknown desc = pull access denied for domain-home-in-image, repository does not exist or may require 'docker login' 114s Normal Created pod/sample-domain1-introspector-ght6p Created container sample-domain1-introspector 114s Normal Pulled pod/sample-domain1-introspector-ght6p Container image \u0026quot;domain-home-in-image:12.2.1.4\u0026quot; already present on machine 114s Normal Started pod/sample-domain1-introspector-ght6p Started container sample-domain1-introspector 114s Warning DomainProcessingFailed domain/sample-domain1 Failed to complete processing domain resource sample-domain1 due to: rpc error: code = Unknown desc = pull access denied for domain-home-in-image, repository does not exist or may require 'docker login', the processing will be retried if needed 98s Normal Completed job/sample-domain1-introspector Job completed 97s Normal Killing pod/sample-domain1-introspector-ght6p Stopping container sample-domain1-introspector 96s Normal Scheduled pod/sample-domain1-admin-server Successfully assigned sample-domain1-ns/sample-domain1-admin-server to doxiao-1 96s Normal DomainProcessingStarting domain/sample-domain1 Creating or updating Kubernetes presence for WebLogic Domain with UID sample-domain1 95s Normal Pulled pod/sample-domain1-admin-server Container image \u0026quot;domain-home-in-image:12.2.1.4\u0026quot; already present on machine 95s Normal Started pod/sample-domain1-admin-server Started container weblogic-server 95s Normal Created pod/sample-domain1-admin-server Created container weblogic-server 59s Normal Scheduled pod/sample-domain1-managed-server1 Successfully assigned sample-domain1-ns/sample-domain1-managed-server1 to doxiao-1 58s Normal Scheduled pod/sample-domain1-managed-server2 Successfully assigned sample-domain1-ns/sample-domain1-managed-server2 to doxiao-1 58s Normal DomainProcessingCompleted domain/sample-domain1 Successfully completed processing domain resource sample-domain1 57s Normal Pulled pod/sample-domain1-managed-server1 Container image \u0026quot;domain-home-in-image:12.2.1.4\u0026quot; already present on machine 57s Normal Started pod/sample-domain1-managed-server1 Started container weblogic-server 57s Normal Created pod/sample-domain1-managed-server1 Created container weblogic-server 57s Normal Pulled pod/sample-domain1-managed-server2 Container image \u0026quot;domain-home-in-image:12.2.1.4\u0026quot; already present on machine 57s Normal Created pod/sample-domain1-managed-server2 Created container weblogic-server 57s Normal Started pod/sample-domain1-managed-server2 Started container weblogic-server Example of a StartManagingNamespace event in the operator\u0026rsquo;s namespace:\nName: weblogic-operator-67c9999d99-clgpw.StartManagingNamespace.sample-domain1-ns.5f68d728281fcbaf Namespace: sample-weblogic-operator-ns Labels: weblogic.createdByOperator=true Annotations: \u0026lt;none\u0026gt; API Version: v1 Count: 1 Event Time: \u0026lt;nil\u0026gt; First Timestamp: 2021-02-01T21:04:02Z Involved Object: Kind: Pod Name: weblogic-operator-67c9999d99-clgpw Namespace: sample-weblogic-operator-ns Kind: Event Last Timestamp: 2021-02-01T21:04:02Z Message: Start managing namespace sample-domain1-ns Metadata: Creation Timestamp: 2021-02-01T21:04:02Z Resource Version: 5119747 Self Link: /api/v1/namespaces/sample-weblogic-operator-ns/events/weblogic-operator-67c9999d99-clgpw.StartManagingNamespace.sample-domain1-ns.5f68d728281fcbaf UID: 6d01f382-d36e-47fa-9222-6f26e90c5be2 Reason: StartManagingNamespace Reporting Component: weblogic.operator Reporting Instance: weblogic-operator-67c9999d99-clgpw Source: Type: Normal Events: \u0026lt;none\u0026gt; Example of the sequence of operator generated events in a domain rolling restart after the domain resource\u0026rsquo;s image and logHomeEnabled changed, which is the output of the command `kubectl get events -n sample-domain1-ns \u0026ndash;selector=weblogic.domainUID=sample-domain1,weblogic.createdByOperator=true \u0026ndash;sort-by=lastTimestamp'.\nLAST SEEN TYPE REASON OBJECT MESSAGE 2m58s Normal DomainChanged domain/sample-domain1 Domain resource sample-domain1 was changed 2m58s Normal DomainProcessingStarting domain/sample-domain1 Creating or updating Kubernetes presence for WebLogic Domain with UID sample-domain1 2m58s Normal DomainRollStarting domain/sample-domain1 Rolling restart WebLogic server pods in domain sample-domain1 because: 'image' changed from 'oracle/weblogic' to 'oracle/weblogic:14.1.1.0', 'logHome' changed from 'null' to '/shared/logs/sample-domain1' 2m58s Normal PodCycleStarting domain/sample-domain1 Replacing pod sample-domain1-adminserver because: In container 'weblogic-server': 'image' changed from 'oracle/weblogic' to 'oracle/weblogic:14.1.1.0', env 'LOG_HOME' changed from 'null' to '/shared/logs/sample-domain1' 2m7s Normal PodCycleStarting domain/sample-domain1 Replacing pod sample-domain1-managed-server1 because: In container 'weblogic-server': 'image' changed from 'oracle/weblogic' to 'oracle/weblogic:14.1.1.0', env 'LOG_HOME' changed from 'null' to '/shared/logs/sample-domain1' 71s Normal PodCycleStarting domain/sample-domain1 Replacing pod sample-domain1-managed-server2 because: In container 'weblogic-server': 'image' changed from 'oracle/weblogic' to 'oracle/weblogic:14.1.1.0', env 'LOG_HOME' changed from 'null' to '/shared/logs/sample-domain1' 19s Normal DomainRollCompleted domain/sample-domain1 Rolling restart of domain sample-domain1 completed 19s Normal DomainProcessingCompleted domain/sample-domain1 Successfully completed processing domain resource sample-domain1 Example of a DomainRollStarting event:\nName: sample-domain1.DomainRollStarting.7d33e9b787e9c318 Namespace: sample-domain1-ns Labels: weblogic.createdByOperator=true weblogic.domainUID=sample-domain1 Annotations: \u0026lt;none\u0026gt; API Version: v1 Count: 1 Event Time: \u0026lt;nil\u0026gt; First Timestamp: 2021-05-18T02:00:24Z Involved Object: API Version: weblogic.oracle/v8 Kind: Domain Name: sample-domain1 Namespace: sample-domain1-ns UID: 5df7dcda-d606-4509-9a06-32f25e16e166 Kind: Event Last Timestamp: 2021-05-18T02:00:24Z Message: Rolling restart WebLogic server pods in domain sample-domain1 because: 'image' changed from 'oracle/weblogic' to 'oracle/weblogic:14.1.1.0', 'logHome' changed from 'null' to '/shared/logs/sample-domain1' Metadata: Creation Timestamp: 2021-05-18T02:00:24Z Resource Version: 12842363 Self Link: /api/v1/namespaces/sample-domain1-ns/events/sample-domain1.DomainRollStarting.7d33e9b787e9c318 UID: 6ec92655-9d06-43b1-8b26-c01ebccadecf Reason: DomainRollStarting Reporting Component: weblogic.operator Reporting Instance: weblogic-operator-fc4ccc8b5-rh4v6 Source: Type: Normal Events: \u0026lt;none\u0026gt; Example of a PodCycleStarting event:\nName: sample-domain1.PodCycleStarting.7d34bc3232231f49 Namespace: sample-domain1-ns Labels: weblogic.createdByOperator=true weblogic.domainUID=sample-domain1 Annotations: \u0026lt;none\u0026gt; API Version: v1 Count: 1 Event Time: \u0026lt;nil\u0026gt; First Timestamp: 2021-05-18T02:01:18Z Involved Object: API Version: weblogic.oracle/v8 Kind: Domain Name: sample-domain1 Namespace: sample-domain1-ns UID: 5df7dcda-d606-4509-9a06-32f25e16e166 Kind: Event Last Timestamp: 2021-05-18T02:01:18Z Message: Replacing pod sample-domain1-managed-server1 because: In container 'weblogic-server': 'image' changed from 'oracle/weblogic' to 'oracle/weblogic:14.1.1.0', env 'LOG_HOME' changed from 'null' to '/shared/logs/sample-domain1' Metadata: Creation Timestamp: 2021-05-18T02:01:18Z Resource Version: 12842530 Self Link: /api/v1/namespaces/sample-domain1-ns/events/sample-domain1.PodCycleStarting.7d34bc3232231f49 UID: 4c6a203e-9b93-4b46-b9e3-1a448b52c7ca Reason: PodCycleStarting Reporting Component: weblogic.operator Reporting Instance: weblogic-operator-fc4ccc8b5-rh4v6 Source: Type: Normal Events: \u0026lt;none\u0026gt; "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/model-in-image/overview/",
	"title": "Overview",
	"tags": [],
	"description": "Introduction to Model in Image, description of its runtime behavior, and references.",
	"content": "Contents  Introduction WebLogic Deploy Tool models Runtime behavior Runtime updates Continuous integration and delivery (CI/CD) References  Introduction Model in Image is an alternative to the operator\u0026rsquo;s Domain in Image and Domain in PV domain types. See Choose a domain home source type for a comparison of operator domain types.\nUnlike Domain in PV and Domain in Image, Model in Image eliminates the need to pre-create your WebLogic domain home prior to deploying your Domain YAML file.\nIt enables:\n Defining a WebLogic domain home configuration using WebLogic Deploy Tool (WDT) model files and application archives. Embedding these files in a single image that also contains a WebLogic installation, and using the WebLogic Image Tool (WIT) to generate this image. Or, alternatively, embedding the files in one or more application specific images. Optionally, supplying additional model files using a Kubernetes ConfigMap. Supplying Kubernetes Secrets that resolve macro references within the models. For example, a secret can be used to supply a database credential. Updating WDT model files at runtime. For example, you can add a data source to a running domain. See Runtime updates for details.  This feature is supported for standard WLS domains, Restricted JRF domains, and JRF domains.\nFor JRF domains, Model in Image provides additional support for initializing the infrastructure database for a domain when a domain is started for the first time, supplying an database password, and obtaining an database wallet for re-use in subsequent restarts of the same domain. See Requirements for JRF domain types.\nWebLogic Deploy Tool models WDT models are a convenient and simple alternative to WebLogic Scripting Tool (WLST) configuration scripts and templates. They compactly define a WebLogic domain using YAML files and support including application archives in a ZIP file. For a description of the model format and its integration with Model in Image, see Usage and Model files. The WDT model format is fully described in the open source, WebLogic Deploy Tool GitHub project.\nRuntime behavior When you deploy a Model in Image domain resource YAML file:\n  The operator will run a Kubernetes Job called the \u0026lsquo;introspector job\u0026rsquo; that:\n Merges your WDT artifacts. Runs WDT tooling to generate a domain home. Packages the domain home and passes it to the operator.    After the introspector job completes:\n The operator creates a ConfigMap named DOMAIN_UID-weblogic-domain-introspect-cm (possibly with some additional maps distinguished by serial names) and puts the packaged domain home in it. The operator subsequently boots your domain\u0026rsquo;s WebLogic Server pods. The pods will obtain their domain home from the ConfigMap.    Runtime updates Model updates can be applied at runtime by changing an image, secrets, a domain resource, or a WDT model ConfigMap after initial deployment.\nSome updates may be applied to a running domain without requiring any WebLogic pod restarts (an online update), but others may require rolling the pods in order to propagate the update\u0026rsquo;s changes (an offline update), and still others may require shutting down the entire domain before applying the update (a full domain restart update). It is the administrator\u0026rsquo;s responsibility to make the necessary changes to a domain resource in order to initiate the correct type of update.\nSee Runtime updates.\nContinuous integration and delivery (CI/CD) To understand how Model in Image works with CI/CD, see CI/CD considerations.\nReferences  Model in Image sample WebLogic Deploy Tool (WDT) WebLogic Image Tool (WIT) Domain schema, documentation HTTP load balancers: Ingress documentation, sample CI/CD considerations  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/faq/configmaps/",
	"title": "Providing access to a ConfigMap",
	"tags": [],
	"description": "I need to provide an instance with access to a ConfigMap.",
	"content": " I need to provide an instance with access to a ConfigMap.\n Configuration files can be supplied to Kubernetes Pods and Jobs by a ConfigMap, which consists of a set of key-value pairs. Each entry may be accessed by one or more operator-managed nodes as a read-only text file. Access can be provided across the domain, within a single cluster, or for a single server. In each case, the access is configured within the serverPod element of the desired scope.\nFor example, given a ConfigMap named my-map with entries key-1 and key-2, you can provide access to both values as separate files in the same directory within the cluster-1 cluster with the following in your Domain:\nclusters: - clusterName: cluster-1 serverPod: volumes: - name: my-volume-1 configMap: name: my-map items: - key: key-1 path: first - key: key-2 path: second volumeMounts: - name: my-volume-1 mountPath: /weblogic-operator/my This provides access to two files, found at paths /weblogic-operator/my/first and /weblogic-operator/my/second. Both a volume and a volumeMount entry are required, and must have the same name. The name of the ConfigMap is specified in the name field under the configMap entry. The items entry is an array, in which each entry maps a ConfigMap key to a file name under the directory specified as mountPath under a volumeMount.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/cicd/",
	"title": "CI/CD considerations",
	"tags": [],
	"description": "Learn about managing domain images with continuous integration and continuous delivery (CI/CD).",
	"content": "Overview In this section, we will discuss the recommended techniques for managing the evolution and mutation of container images to run WebLogic Server in Kubernetes. There are several approaches and techniques available, and the choice of which to use depends very much on your particular requirements. We will start with a review of the \u0026ldquo;problem space,\u0026rdquo; and then talk about the considerations that would lead us to choose various approaches. We will provide details about several approaches to implementing CI/CD and links to examples.\nReview of the problem space Kubernetes makes a fundamental assumption that images are immutable, that they contain no state, and that updating them is as simple as throwing away a pod/container and replacing it with a new one that uses a newer version of the image. These assumptions work very well for microservices applications, but for more traditional workloads, we need to do some extra thinking and some extra work to get the behavior we want.\nCI/CD is an area where the standard assumptions aren\u0026rsquo;t always suitable. In the microservices architecture, you typically minimize dependencies and build images from scratch with all of the dependencies in them. You also typically keep all of the configuration outside of the image, for example, in Kubernetes config maps or secrets, and all of the state outside of the image too. This makes it very easy to update running pods with a new image.\nLet\u0026rsquo;s consider how a WebLogic image is different. There will, of course, be a base layer with the operating system; let\u0026rsquo;s assume it is Oracle Linux \u0026ldquo;slim\u0026rdquo;. Then you need a JDK and this is very commonly in another layer. Many people will use the officially supported JDK images from the Docker Store, like the Server JRE image, for example. On top of this, you need the WebLogic Server binaries (the \u0026ldquo;Oracle Home\u0026rdquo;). On top of that, you may wish to have some patches or updates installed. And then you need your domain, that is the configuration.\nThere is also other information associated with a domain that needs to live somewhere, for example leasing tables, message and transaction stores, and so on. We recommend that these be kept in a database to take advantage of built-in database server HA, and the fact that disaster recovery of sites across all but the shortest distances, almost always requires using a single database server to consolidate and replicate data (DataGuard).\nThere are three common approaches on how to structure these components:\n The first, \u0026ldquo;domain on a persistent volume\u0026rdquo; or Domain in PV, places the JDK and WebLogic binaries in the image, but the domain home is kept on a separate persistent storage outside of the image. The second, Domain in Image, puts the JDK, WebLogic Server binaries, and the domain home all in the image. The third approach, Model in Image, puts the JDK, WebLogic Server binaries, and a domain model in the image, and generates the domain home at runtime from the domain model.  All of these approaches are perfectly valid (and fully supported) and they have various advantages and disadvantages. We have listed the relative advantages of these approaches here.\nOne of the key differences between these approaches is how many images you have, and therefore, how you build and maintain them - your image CI/CD process. Let\u0026rsquo;s take a short detour and talk about image layering.\n Container image layering  Learn about container image layering and why it is important.\n Why layering matters  Learn why container image layering affects CI/CD processes.\n Choose an approach  How to choose an approach.\n Mutate the domain layer  How to mutate the domain layer.\n Copy domains  How to copy domains.\n Tools  Tools that are available to build CI/CD pipelines.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/faq/",
	"title": "Frequently asked questions",
	"tags": [],
	"description": "",
	"content": "This section provides answers to frequently asked questions.\n Answers for newcomers  Answers to commonly asked newcomer questions.\n Managing domain namespaces  Considerations for managing namespaces while the operator is running.\n Cannot pull image  My domain will not start and I see errors like `ImagePullBackoff` or `Cannot pull image`.\n Boot identity not valid  One or more WebLogic Server instances in my domain will not start and I see errors in the server log like this: Boot identity not valid.\n Domain secret mismatch  One or more WebLogic Server instances in my domain will not start and the domain resource `status` or the pod log reports errors like this: Domain secret mismatch.\n Node heating problem  The operator creates a Pod for each WebLogic Server instance that is started. The Kubernetes Scheduler then selects a Node for each Pod. Because the default scheduling algorithm gives substantial weight to selecting a Node where the necessary container images have already been pulled, this often results in Kubernetes running many of the Pods for WebLogic Server instances on the same Node while other Nodes are not fairly utilized. This is commonly known as the Node heating problem.\n Disabling Fast Application Notifications  To support Fast Application Notifications (FAN), Oracle databases configure GRID (Oracle Grid Infrastructure). GRID is typically associated with (and required by) Oracle RAC databases but can also be used in other configurations. Oracle Autonomous Database-Serverless (ATP-S) does not provide GRID.\n Using OCI File Storage (FSS) for persistent volumes  If you are running your Kubernetes cluster on Oracle Container Engine for Kubernetes (OKE), and you use OCI File Storage (FSS) for persistent volumes to store the WebLogic domain home, then the file system handling, as demonstrated in the operator persistent volume sample, will require an update to properly initialize the file ownership on the persistent volume when the domain is initially created.\n Using an OCI load balancer  If you are running your Kubernetes cluster on Oracle Container Engine for Kubernetes (OKE), then you can have OCI automatically provision load balancers for you by creating a `Service` of type `LoadBalancer` instead of (or in addition to) installing an ingress controller like Traefik or Voyager.\n Providing access to a PersistentVolumeClaim  I need to provide an instance with access to a PersistentVolumeClaim.\n Providing access to a ConfigMap  I need to provide an instance with access to a ConfigMap.\n External WebLogic clients  This FAQ describes approaches for giving WebLogic applications access to WebLogic JMS or EJB resources when either the applications or their resources are located in Kubernetes. This includes between different Kubernetes namespaces within the same Kubernetes cluster, between different Kubernetes clusters, and between a non-Kubernetes location and Kubernetes.\n Coherence requirements  If you are running Coherence on Kubernetes, either inside a WebLogic domain or standalone, then there are some additional requirements to make sure that Coherence can form clusters.\n Pod memory and CPU resources  Tune container memory and CPU usage by configuring Kubernetes resource requests and limits, and tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your Domain YAML file.\n Handling security validations  Why am I seeing these security warnings?\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/faq/external-clients/",
	"title": "External WebLogic clients",
	"tags": [],
	"description": "This FAQ describes approaches for giving WebLogic applications access to WebLogic JMS or EJB resources when either the applications or their resources are located in Kubernetes. This includes between different Kubernetes namespaces within the same Kubernetes cluster, between different Kubernetes clusters, and between a non-Kubernetes location and Kubernetes.",
	"content": "Contents  Overview Load balancer tunneling Adding a WebLogic custom channel  When is a WebLogic custom channel needed? Configuring a WebLogic custom channel WebLogic custom channel notes   Kubernetes NodePorts  NodePort overview NodePort warnings NodePort steps Setting up a NodePort Sample NodePort resource Table of NodePort attributes   Enabling unknown host access  When is it necessary to enable unknown host access? How to enable unknown host access   Configuring WebLogic Server affinity load balancing algorithms Configuring external listen addresses for WebLogic default channels Security notes Optional reading  Overview If a WebLogic EJB or JMS resource is located in the same Kubernetes namespace as an application that calls the resource, then:\n  No additional configuration steps are needed.\n  The URL that the application specifies depends on both the location of the application and the location of its target resource.\n  If the application is running in a WebLogic Server JVM, and the JVM hosts the target resource or the JVM is located within the same WebLogic cluster as the target resource, then the application can simply specify the JNDI name of the resource and it should not specify a URL.\n  If the application is running outside of one of the above locations, but is still in the same Kubernetes namespace as the target resource, then, in addition to a JNDI name, the application must also specify a t3 or t3s URL that includes the DNS name of the target resource\u0026rsquo;s Kubernetes ClusterIP service. To see the service DNS names for a deployed domain, run kubectl -n MYNS get services.\nExample URLs:\n  For a target resource that\u0026rsquo;s hosted anywhere in WebLogic cluster mycluster with domain UID myuid where the cluster is listening on port 7001:\nt3://myuid-cluster-mycluster:7001\n  For a target resource that\u0026rsquo;s hosted in a non-clustered WebLogic Server myserver that is part of a domain with a domain UID myuid where the server is listening on port 7001:\nt3://myuid-myserver:7001\n      If a WebLogic EJB or JMS resource is located in the same Kubernetes cluster as an application that calls the resource, but the application and resource are in different Kubernetes namespaces, then:\n  The URL used by the application can begin with t3 or t3s, and the DNS address in the URL must be fully specified to differentiate the namespace. For example:\n If the target resource is in WebLogic cluster mycluster with domain UID myuid in namespace myns, and the cluster is listening on port 8001, then the cluster service name that is generated by the operator will be myuid-cluster-mycluster and the full URL will be t3://myuid-cluster-mycluster.myns:8001. If the target resource is a WebLogic Server myserver with domain UID myuid in namespace myns, and the server is listening on port 7001, then the server service name that is generated by the operator will be myuid-myserver and the full URL will be t3://myuid-myserver.myns:7001.    The applications should access their target WebLogic Server or cluster using a WebLogic custom T3 or T3S channel that is configured on the target with the following configuration:\n Specify a public address that matches the same fully decorated DNS address that the applications will use. For example:  If the target resource is in WebLogic cluster mycluster that is part of domain with a domain UID myuid running in domain myns, then the cluster service name will be myuid-cluster-mycluster and the fully decorated name will be myuid-cluster-mycluster.myns. If the target resource is in WebLogic Server myserver that is part of domain with a domain UID myuid running in domain myns, then the server service name will be myuid-myserver and the fully decorated name will be myuid-myserver.myns.   Do not specify a public port. It should be the same as the the channel\u0026rsquo;s listen port, which is the default. This is because there is no port mapping between namespaces. Do not enable outbound enabled. There is no need to enable tunneling, and you can continue to use URLs that begin with t3 or t3s. Application URLs must specify the channel\u0026rsquo;s port.    If the applications run within a WebLogic Server JVM, then the WebLogic Server instances that host the target EJB or JMS resources may need to be configured to enable unknown host access. This should not be needed when the public address on the network channel is configured, as directed in the previous bulleted item.\n  If JTA transactions span between domains in different namespaces, then additional configuration is required to ensure that JTA transaction managers can communicate with each-other between the domains: configure each server\u0026rsquo;s default channel External Listen Address to a service name that is decorated with the server\u0026rsquo;s namespace. For more information, see Configuring external listen addresses for WebLogic default channels.\n  If a WebLogic EJB or JMS resource is hosted inside a Kubernetes cluster, but an application that calls the resource is hosted outside of the Kubernetes cluster, then:\n  There are two supported approaches for exposing an external address and port that the applications can use:\n Load balancer tunneling (preferred) Kubernetes NodePorts    The applications must specify a URL that resolves to the external address. If tunneling, then this URL must begin with http or https instead of t3 or t3s. For example, http://my-lb-address:my-lb-port.\n  You may need to enable unknown host access on the WebLogic Server instances that host the EJB or JMS resources.\n  If tunneling, then you may need to configure clusters that host EJB or JMS resources with a \u0026lsquo;server affinity\u0026rsquo; default load balancer algorithm. This can significantly speedup connection creation for EJB and JMS clients. See Configuring WebLogic Server affinity load balancing algorithms.\n  JTA NOTE: The operator does not directly support external WebLogic JTA access to a Kubernetes hosted WebLogic cluster. This is because external JTA access requires each server in a cluster to be individually addressable, but this conflicts with the current operator requirement that a network channel in a cluster have the same port across all servers in the cluster. If this is a requirement, then contact the WebLogic Kubernetes Operator team for potential solutions.\n  If a WebLogic EJB or JMS resource is hosted outside of a Kubernetes cluster, and the EJB or JMS applications that call the resource are located within the cluster, then:\n  You may need to enable unknown host access on the external WebLogic Server instance or instances.\n  Plus, if the target server or servers can be accessed only by tunneling through a load balancer using HTTP:\n Set up an HTTP tunneling-enabled custom channel on the external WebLogic Server instances. Specify URLs on the source server that resolve to the load balancer\u0026rsquo;s address and that start with http instead of t3. Ensure the load balancer configures the HTTP flow to be \u0026lsquo;sticky\u0026rsquo;. You may need to configure clusters that host EJB or JMS resources with a \u0026lsquo;server affinity\u0026rsquo; default load balancer algorithm. This can significantly speedup tunneling connection creation for EJB and JMS clients. See Configuring WebLogic Server affinity load balancing algorithms.    JTA NOTE: WebLogic does not directly support JTA access to a cluster that is exposed using a single port. This is because external JTA requires each server in a cluster to be individually addressable. If this is a requirement, then contact the WebLogic Kubernetes Operator team for potential solutions.\n  All DNS addresses must be \u0026lsquo;DNS-1123\u0026rsquo; compliant; this means that any DNS names created using the name of a service, pod, WebLogic Server, WebLogic cluster, and such, must be lowercase with underscores converted to dashes (hyphens). For example, if a WebLogic Server instance is named My_Server and the domain UID is MyUid, then its listen address within the namespace will be myuid-my-server. For more information on Kubernetes resource compliant naming, please see Meet Kubernetes resource name restrictions.\n Load balancer tunneling Load balancer tunneling is the preferred approach for giving applications that are hosted outside of a Kubernetes cluster access to EJB and JMS resources that are hosted within the cluster. This approach involves configuring a network channel on the resource\u0026rsquo;s WebLogic cluster, ensuring the network channel accepts T3 protocol traffic that\u0026rsquo;s tunneled over HTTP, deploying a load balancer that redirects external HTTP network traffic to the network channel, and ensuring that the applications specify a URL that resolves the load balancer\u0026rsquo;s network address where the URL begins with http or https.\nHere are the steps:\n  In WebLogic, configure a custom channel for the T3 protocol that enables HTTP tunneling, and specifies an external address and port that correspond to the address and port that remote applications will use to access the load balancer. See Adding a WebLogic custom channel for samples and details.\n  Set up a load balancer that redirects HTTP traffic to the custom channel. For more information on load balancers, see Ingress. If you\u0026rsquo;re using OKE/OCI to host your Kubernetes cluster, also see Using an OCI Load Balancer.\n  Important: Ensure that the load balancer configures the HTTP flow to be \u0026lsquo;sticky\u0026rsquo; - for example, a Traefik load balancer has a sticky sessions option. This ensures that all of the packets of a tunneling client connection flow to the same pod, otherwise the connection will stall when its packets are load balanced to a different pod.\n  Important: For EJB and JMS resources that are targeted to a cluster, we recommend that you configure a default load balancer algorithm that provides server affinity (round-robin-affinity, weight-based-affinity, random-affinity) to speedup connection creation for EJB and JMS clients. See Configuring WebLogic Server affinity load balancing algorithms.\n  If you are adding access for applications that are hosted on remote WebLogic Server instances, then the Kubernetes hosted servers may need to enable unknown host access.\n  Remote applications can then access the custom channel using an http:// URL instead of a t3:// URL.\n  Review the Security notes.\n  Adding a WebLogic custom channel The following sections describe WebLogic custom channel considerations and configuration.\nWhen is a WebLogic custom channel needed? WebLogic implicitly creates a multi-protocol default channel that spans the Listen Address and Port fields specified on each server in the cluster, but this channel is usually unsuitable for external network traffic from EJB and JMS applications. Instead, you may need to configure an additional, dedicated WebLogic custom channel to handle remote EJB or JMS application network traffic.\nA custom channel provides a way to configure an external listen address and port for use by external applications, unlike a default channel. External listen address or port configuration is needed when a channel\u0026rsquo;s configured listen address or port would not work if used to form a URL in the remote application. This is because remote EJB and JMS applications internally use their application\u0026rsquo;s channel\u0026rsquo;s configured network information to reconnect to WebLogic when needed. (The EJB and JMS applications do not always use the initial URL specified in the application\u0026rsquo;s JNDI context.)\nA custom channel can be locked down using two-way SSL as a way to prevent access by unauthorized external JMS and EJB applications, only accepts protocols that are explicitly enabled for the channel, and can be configured to be the only channel that accepts EJB/JMS applications that tunnel over HTTP. A default channel may often be deliberately unencrypted for convenient internal use, or, if used externally, is used for web traffic (not tunneling traffic) only. In addition, a default channel supports several protocols but it\u0026rsquo;s a best practice to limit the protocols that can be accessed by external applications. Finally, external applications may require access using HTTP tunneling in order to make connections, but it\u0026rsquo;s often inadvisable to enable tunneling for an unsecured default channel that\u0026rsquo;s already servicing external HTTP traffic. This is because enabling HTTP tunneling would potentially allow unauthorized external JMS and EJB applications unsecured access to the WebLogic cluster through the same HTTP path.\nConfiguring a WebLogic custom channel The basic requirements for configuring a custom channel for remote EJB and JMS access are:\n  Configure a T3 protocol network access point (NAP) with the same name and port on each server (the operator will set the listen address for you).\n  Configure the external listen address and port on each NAP to match the address and port component of a URL your applications can use. For example, if you are providing access to remote applications using a load balancer, then these should match the address and port of the load balancer.\n  If you want WebLogic T3 applications to tunnel through HTTP, then enable HTTP tunneling on each NAP. This is often necessary for load balancers.\n  Do NOT set outbound-enabled to true on the network access point (the default is false), because this may cause internal network traffic to stall in an attempt to route through the network access point.\n  For operator controlled WebLogic clusters, ensure you haven\u0026rsquo;t enabled calculated-listen-ports for WebLogic dynamic cluster servers. The operator requires that a channel have the same port on each server in a cluster, but calculated-listen-ports causes the port to be different on each server.\n  For clusters that are not operator controlled, minimally ensure that the server\u0026rsquo;s default channel ListenAddress is configured. Oracle strongly recommends configuring a ListenAddress on all WebLogic Server instances. Note that if a NAP\u0026rsquo;s ListenAddress is left blank, then it will use the default channel\u0026rsquo;s ListenAddress. (This is not a concern for operator controlled clusters as the operator sets the listen addresses on every WebLogic Server instance.)\n  For example, here is a snippet of a WebLogic domain config.xml file for channel MyChannel defined for an operator controlled WebLogic dynamic cluster named cluster-1:\n\u0026lt;server-template\u0026gt; \u0026lt;name\u0026gt;cluster-1-template\u0026lt;/name\u0026gt; \u0026lt;listen-port\u0026gt;8001\u0026lt;/listen-port\u0026gt; \u0026lt;cluster\u0026gt;cluster-1\u0026lt;/cluster\u0026gt; \u0026lt;network-access-point\u0026gt; \u0026lt;name\u0026gt;MyChannel\u0026lt;/name\u0026gt; \u0026lt;protocol\u0026gt;t3\u0026lt;/protocol\u0026gt; \u0026lt;public-address\u0026gt;some.public.address.com\u0026lt;/public-address\u0026gt; \u0026lt;listen-port\u0026gt;7999\u0026lt;/listen-port\u0026gt; \u0026lt;public-port\u0026gt;30999\u0026lt;/public-port\u0026gt; \u0026lt;http-enabled-for-this-protocol\u0026gt;true\u0026lt;/http-enabled-for-this-protocol\u0026gt; \u0026lt;tunneling-enabled\u0026gt;true\u0026lt;/tunneling-enabled\u0026gt; \u0026lt;outbound-enabled\u0026gt;false\u0026lt;/outbound-enabled\u0026gt; \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; \u0026lt;two-way-ssl-enabled\u0026gt;false\u0026lt;/two-way-ssl-enabled\u0026gt; \u0026lt;client-certificate-enforced\u0026gt;false\u0026lt;/client-certificate-enforced\u0026gt; \u0026lt;/network-access-point\u0026gt; \u0026lt;/server-template\u0026gt; \u0026lt;cluster\u0026gt; \u0026lt;name\u0026gt;cluster-1\u0026lt;/name\u0026gt; \u0026lt;cluster-messaging-mode\u0026gt;unicast\u0026lt;/cluster-messaging-mode\u0026gt; \u0026lt;dynamic-servers\u0026gt; \u0026lt;name\u0026gt;cluster-1\u0026lt;/name\u0026gt; \u0026lt;server-template\u0026gt;cluster-1-template\u0026lt;/server-template\u0026gt; \u0026lt;maximum-dynamic-server-count\u0026gt;5\u0026lt;/maximum-dynamic-server-count\u0026gt; \u0026lt;calculated-listen-ports\u0026gt;false\u0026lt;/calculated-listen-ports\u0026gt; \u0026lt;server-name-prefix\u0026gt;managed-server\u0026lt;/server-name-prefix\u0026gt; \u0026lt;dynamic-cluster-size\u0026gt;5\u0026lt;/dynamic-cluster-size\u0026gt; \u0026lt;max-dynamic-cluster-size\u0026gt;5\u0026lt;/max-dynamic-cluster-size\u0026gt; \u0026lt;/dynamic-servers\u0026gt; \u0026lt;/cluster\u0026gt; Here is a snippet of offline WLST code that corresponds to the above config.xml file snippet:\ntemplateName = \u0026#34;cluster-1-template\u0026#34; cd(\u0026#39;/ServerTemplates/%s\u0026#39; % templateName) templateChannelName = \u0026#34;MyChannel\u0026#34; create(templateChannelName, \u0026#39;NetworkAccessPoint\u0026#39;) cd(\u0026#39;NetworkAccessPoints/%s\u0026#39; % templateChannelName) set(\u0026#39;Protocol\u0026#39;, \u0026#39;t3\u0026#39;) set(\u0026#39;ListenPort\u0026#39;, 7999) set(\u0026#39;PublicPort\u0026#39;, 30999) set(\u0026#39;PublicAddress\u0026#39;, \u0026#39;some.public.address.com\u0026#39;) set(\u0026#39;HttpEnabledForThisProtocol\u0026#39;, true) set(\u0026#39;TunnelingEnabled\u0026#39;, true) set(\u0026#39;OutboundEnabled\u0026#39;, false) set(\u0026#39;Enabled\u0026#39;, true) set(\u0026#39;TwoWaySslEnabled\u0026#39;, false) set(\u0026#39;ClientCertificateEnforced\u0026#39;, false) Here is a snippet of WDT model YAML file configuration that corresponds to the above snippets:\ntopology: Cluster: \u0026#39;cluster-1\u0026#39;: DynamicServers: ServerTemplate: \u0026#39;cluster-1-template\u0026#39; ServerNamePrefix: \u0026#39;managed-server\u0026#39; DynamicClusterSize: \u0026#39;5\u0026#39; MaxDynamicClusterSize: \u0026#39;5\u0026#39; MinDynamicClusterSize: \u0026#39;0\u0026#39; CalculatedListenPorts: false ServerTemplate: \u0026#39;cluster-1-template\u0026#39;: Cluster: \u0026#39;cluster-1\u0026#39; ListenPort: 8001 NetworkAccessPoint: MyT3Channel: Protocol: \u0026#39;t3\u0026#39; ListenPort: 7999 PublicPort: 30999 PublicAddress: \u0026#39;some.public.address.com\u0026#39; HttpEnabledForThisProtocol: true TunnelingEnabled: true OutboundEnabled: false Enabled: true TwoWaySslEnabled: false ClientCertificateEnforced: false In this example:\n  WebLogic binds the custom network channel to port 7999 and the default network channel to 8001.\n  The operator will automatically create a Kubernetes Service named DOMAIN_UID-cluster-cluster-1 for both the custom and default channel.\n  The operator will automatically set the ListenAddress on each WebLogic Server instance for each of its channels.\n  Internal applications running in the same Kubernetes cluster as the channel can access the cluster using t3://DOMAIN_UID-cluster-cluster-1:8001.\n  External applications would be expected to access the cluster using the custom channel with URLs like t3://some.public.address.com:30999 or, if using tunneling, http://some.public.address.com:30999.\n  WebLogic custom channel notes   Channel configuration for a configured cluster requires configuring the same network access point on each server. The operator currently doesn\u0026rsquo;t test or support network channels that have a different configuration on each server in the cluster.\n  Additional steps are required for external applications beyond configuring the custom channel - see Overview.\n  Kubernetes NodePorts The following sections provide detailed information about Kubernetes NodePorts.\nNodePort overview Kubernetes NodePorts provide an alternative approach for giving external WebLogic EJB or JMS applications access to a Kubernetes hosted WebLogic cluster. This approach involves configuring a network channel on the desired WebLogic cluster that accepts T3 protocol traffic, and exposing a Kubernetes NodePort that redirects external network traffic on the Kubernetes Nodes to the network channel.\nNodePort warnings Although Kubernetes NodePorts are good for use in demos and getting-started guides, they are usually not suited for production systems for multiple reasons, including:\n They can directly expose internal applications to the outside world. They bypass almost all network security in Kubernetes. They allow all protocols (load balancers can limit to the HTTP protocol). They cannot expose standard, low-numbered ports like 80 and 443 (or even 8080 and 8443). Some Kubernetes cloud environments cannot expose usable NodePorts because their Kubernetes clusters run on a private network that cannot be reached by external clients.  Load balancer tunneling is the preferred approach over Kubernetes NodePorts.\nNodePort steps Here are the high level steps:\n  Review NodePort warnings.\n  In WebLogic, configure a custom channel for the T3 protocol that specifies an external address and port that are suitable for remote application use. See Adding a WebLogic custom channel.\n  Define a Kubernetes NodePort to publicly expose the WebLogic ports. See Setting up a NodePort.\n  If you are adding access for remote WebLogic Server instances, then the Kubernetes hosted servers may need to enable unknown host access.\n  Review the Security notes.\n  Setting up a NodePort A Kubernetes NodePort exposes a port on each worker node in the Kubernetes cluster (they are not typically exposed on masters), where the port is accessible from outside of a Kubernetes cluster. This port redirects network traffic to pods within the Kubernetes cluster. Setting up a Kubernetes NodePort is one approach for giving external WebLogic applications access to JMS or EJBs.\nIf an EJB or JMS service is running on an Administration Server, then you can skip the rest of this section and use the spec.adminServer.adminService.channels Domain field to have the operator create a NodePort for you. See Reference - Domain. Otherwise, if the EJB or JMS service is running in a WebLogic cluster or standalone WebLogic Server Managed Server, and you desire to provide access to the service using a NodePort, then the NodePort must be exposed \u0026lsquo;manually\u0026rsquo; - see the following sample and table.\nSetting up a NodePort usually also requires setting up a custom network channel. See Adding a WebLogic custom channel.\n Sample NodePort resource The following NodePort YAML file exposes an external node port of 30999 and internal port 7999 for a domain UID of DOMAIN_UID, a domain name of DOMAIN_NAME, and a cluster name of CLUSTER_NAME. It assumes that 7999 corresponds to a T3 protocol port of a channel that\u0026rsquo;s configured on your WebLogic cluster.\napiVersion: v1 kind: Service metadata: namespace: default name: DOMAIN_UID-cluster-CLUSTER_NAME-ext labels: weblogic.domainUID: DOMAIN_UID spec: type: NodePort externalTrafficPolicy: Cluster sessionAffinity: ClientIP selector: weblogic.domainUID: DOMAIN_UID weblogic.clusterName: CLUSTER_NAME ports: - name: myclustert3channel nodePort: 30999 port: 7999 protocol: TCP targetPort: 7999 Table of NodePort attributes    Attribute Description     metadata.name For this particular use case, the NodePort name can be arbitrary as long as it is DNS compatible. But, as a convention, it\u0026rsquo;s recommended to use DOMAIN_UID-cluster-CLUSTER_NAME-ext. To ensure the name is DNS compatible, use all lowercase and convert any underscores (_) to dashes (-).   metadata.namespace Must match the namespace of your WebLogic cluster.   metadata.labels Optional. It\u0026rsquo;s helpful to set a weblogic.domainUid label so that cleanup scripts can locate all Kubernetes resources associated with a particular domain UID.   spec.type Must be NodePort.   spec.externalTrafficPolicy Set to Cluster for most use cases. This may lower performance, but ensures that a client that attaches to a node without any pods that match the spec.selector will be rerouted to a node with pods that do match. If set to Local, then connections to a particular node will route only to that node\u0026rsquo;s pods and will fail if the node doesn\u0026rsquo;t host any pods with the given spec.selector. It\u0026rsquo;s recommended for applications of a spec.externalTrafficPolicy: Local NodePort to use a URL that resolves to a list of all nodes, such as t3://mynode1,mynode2:30999, so that a client connect attempt will implicitly try mynode2 if mynode1 fails (alternatively, use a round-robin DNS address in place of mynode1,mynode2).   spec.sessionAffinity Set to ClientIP to ensure an HTTP tunneling connection always routes to the same pod, otherwise the connection may hang and fail.   spec.selector Specifies a weblogic.domainUID and weblogic.clusterName to associate the NodePort resource with your cluster\u0026rsquo;s pods. The operator automatically sets these labels on the WebLogic cluster pods that it deploys for you.   spec.ports.name This name is arbitrary.   spec.ports.nodePort The external port that applications will use. This must match the external port that\u0026rsquo;s configured on the WebLogic configured channels/network access points. By default, Kubernetes requires that this value range from 30000 to 32767.   spec.ports.port and spec.targetPort These must match the port that\u0026rsquo;s configured on the WebLogic configured channel/network access points.    Enabling unknown host access The following sections describe when and how to enable unknown host access.\nWhen is it necessary to enable unknown host access? If a source WebLogic Server attempts to initiate an EJB, JMS, or JTA connection with a target WebLogic Server, then the target WebLogic Server will reject the connection by default, if it cannot find the source server\u0026rsquo;s listen address in its DNS. Such a failed connection attempt can yield log messages or exceptions like \u0026quot;...RJVM has already been shutdown...\u0026quot; or \u0026quot;...address was valid earlier, but now we get...\u0026quot;.\nThis means that it\u0026rsquo;s usually necessary to enable unknown host access on an external WebLogic Server instance so that it can support EJB, JMS, or JTA communication that is initiated by an operator hosted WebLogic Server. For example, if an operator hosted WebLogic Server with service address mydomainuid-myservername initiates a JMS connection to a remote WebLogic Server, then the remote server will implicitly attempt to look up mydomainuid-myservername in its DNS as part of the connection setup, and this lookup will typically fail.\nSimilarly, this also means that it\u0026rsquo;s necessary to enable unknown host access on an operator hosted WebLogic Server that accepts EJB or JMS connection requests from external WebLogic Server instances when the external WebLogic Server\u0026rsquo;s listen addresses cannot be resolved by the DNS running in the Kuberneters cluster.\nHow to enable unknown host access To enable an \u0026lsquo;unknown host\u0026rsquo; source WebLogic Server to initiate EJB, JMS, or JTA communication with a target WebLogic Server:\n Set the weblogic.rjvm.allowUnknownHost Java system property to true on each target WebLogic Server instance.  For operator hosted WebLogic Server instances, you can set this property by including -Dweblogic.rjvm.allowUnknownHost=true in the JAVA_OPTIONS Domain environment variable defined in the domain resource\u0026rsquo;s spec.serverPod.env attribute.   Also apply patch 30656708 on each target WebLogic Server instance for versions 12.2.1.4 (PS4) or earlier.  Configuring WebLogic Server affinity load balancing algorithms When providing external clients access to EJB and JMS resources using load balancer tunneling, we recommend that you configure a server affinity load balancing algorithm for the cluster in which the resources are targeted. Using a server affinity based algorithm reduces the amount of time it takes for EJB and JMS standalone or server hosted clients to establish a connection through a tunneling port. It does this by ensuring that the clients prefer communicating with a target server instance which already has an established connection to the client, where this server instance is implicitly picked when such clients create their JNDI context. See Load Balancing for EJBs and RMI Objects in Load Balancing in a Cluster.\nHere is a snippet of offline WLST code for enabling server affinity in a cluster \u0026lsquo;cluster-1\u0026rsquo;:\nclusterName = \u0026#34;cluster-1\u0026#34; cd(\u0026#39;/Clusters/%s\u0026#39; % clusterName) set(\u0026#39;DefaultLoadAlgorithm\u0026#39;, \u0026#39;round-robin-affinity\u0026#39;) Here is a snippet of WDT model YAML file configuration for enabling server affinity in a cluster \u0026lsquo;cluster-1\u0026rsquo;:\ntopology: Cluster: \u0026#39;cluster-1\u0026#39;: DynamicServers: ServerTemplate: \u0026#39;cluster-1-template\u0026#39; ServerNamePrefix: \u0026#39;managed-server\u0026#39; DynamicClusterSize: \u0026#39;5\u0026#39; MaxDynamicClusterSize: \u0026#39;5\u0026#39; MinDynamicClusterSize: \u0026#39;0\u0026#39; CalculatedListenPorts: false DefaultLoadAlgorithm: \u0026#39;round-robin-affinity\u0026#39; Configuring external listen addresses for WebLogic default channels WebLogic Server provides an external listen address/DNS name feature to enable external JMS and EJB clients to properly reestablish and/or load balance connections to servers with listen addresses that aren\u0026rsquo;t directly available in the client\u0026rsquo;s DNS (such as through a firewall or a load balancer, or between different namespaces in Kubernetes). For more information on external listen addresses, see the ExternalDNSName attribute in the ServerMBean  reference.\nHere is a snippet of offline WLST code for setting the external listen address for a standalone server named AdminServer in namespace weblogic-domain for domain-uid domain1:\nserverName = \u0026#34;AdminServer\u0026#34; domainUid = \u0026#34;domain1\u0026#34; nameSpace = \u0026#34;weblogic-domain\u0026#34; address = domainUid + \u0026#39;_\u0026#39; + serverName + \u0026#39;_\u0026#39; + nameSpace externalDNSName = address.lower().replace(\u0026#39;_\u0026#39;,\u0026#39;-\u0026#39;) # DNS names must be DNS-1123 compliant cd(\u0026#39;/Servers/%s\u0026#39; % serverName) set(\u0026#39;ExternalDNSName\u0026#39;, externalDNSName) Here is a snippet of offline WLST code for setting the external listen address for a server template named cluster-1-template in namespace weblogic-domain for domain-uid domain1:\ntemplateName = \u0026#34;cluster-1-template\u0026#34; serverName = \u0026#34;managed-server${id}\u0026#34; domainUid = \u0026#34;domain1\u0026#34; nameSpace = \u0026#34;weblogic-domain\u0026#34; address = domainUid + \u0026#39;_\u0026#39; + serverName + \u0026#39;_\u0026#39; + nameSpace externalDNSName = address.lower().replace(\u0026#39;_\u0026#39;,\u0026#39;-\u0026#39;) # DNS names must be DNS-1123 compliant cd(\u0026#39;/ServerTemplates/%s\u0026#39; % templateName) set(\u0026#39;ExternalDNSName\u0026#39;, externalDNSName)  The server name contains the server template macro \u0026ldquo;${id}\u0026rdquo; in which the correct instance ID will be substituted by the WebLogic Server runtime. See Using Macros in Server Templates.\n Here is a snippet of WDT model YAML file configuration to set the ExternalDNSName attribute for both a standalone server and for a server template:\ntopology: Cluster: \u0026#34;cluster-1\u0026#34;: DynamicServers: ServerTemplate: \u0026#34;cluster-1-template\u0026#34; ServerNamePrefix: \u0026#34;managed-server\u0026#34; DynamicClusterSize: 5 MaxDynamicClusterSize: 5 CalculatedListenPorts: false Server: \u0026#34;admin-server\u0026#34;: ListenPort: 7001 ExternalDNSName: \u0026#39;@@ENV:DOMAIN_UID@@-admin-server.@@ENV:NAMESPACE@@\u0026#39; ServerTemplate: \u0026#34;cluster-1-template\u0026#34;: Cluster: \u0026#34;cluster-1\u0026#34; ListenPort : 8001 ExternalDNSName: \u0026#39;@@ENV:DOMAIN_UID@@-managed-server${id}.@@ENV:NAMESPACE@@\u0026#39;  In the previous example, DOMAIN_UID and NAMESPACE are assumed to already be \u0026lsquo;DNS-1123\u0026rsquo; compliant. Alternatively, you can substitute the macros with DNS-1123 acceptable values (changed to lowercase and underscores converted to dashes). For more information on Kubernetes resource compliant naming, please see Meet Kubernetes resource name restrictions.\n Security notes   With some cloud providers, a load balancer or NodePort may implicitly expose a port to the public Internet. See also NodePort warnings.\n  If an externally available port supports a protocol suitable for WebLogic applications, note that WebLogic allows access to JNDI entries, EJB/RMI applications, and JMS by anonymous users by default.\n  You can configure a custom channel with a secure protocol and two-way SSL to help prevent external access by unwanted applications. See When is a WebLogic custom channel needed?.\n  For a detailed description of external network access security, see External network access security.\n  Optional reading   For a description of the WebLogic URL syntax for JMS, EJB, and JNDI applications see Understanding WebLogic URLs.\n  If JMS applications need to specify a URL and the applications are hosted on a WebLogic Server or cluster, then, as a best practice, the application should simply be coded to use local JNDI names for its JMS Connection Factories and JMS Destinations, and these local JNDI names should be configured in WebLogic to map to the remote location using a Foreign JMS Server. A Foreign JMS Server maps the local JNDI name of a JMS Connection Factory or JMS Destination to a remote JNDI name located at a different URL, where the local JNDI name, remote JNDI name, remote user name, remote password, and remote URL are all configurable. See Integrating Remote JMS Destinations.\n  For a detailed description of using T3 in combination with port mapping, see T3 RMI Communication for WebLogic Server Running on Kubernetes.\n  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/istio/istio/",
	"title": "Istio support",
	"tags": [],
	"description": "Lets you run the operator, and WebLogic domains managed by the operator, with Istio sidecar injection enabled. You can use Istio gateways and virtual services to access applications deployed in these domains.",
	"content": "Overview WebLogic Kubernetes Operator version 2.6 and later, includes support for Istio 1.4.2 and later. This support lets you run the operator, and WebLogic domains managed by the operator, with Istio sidecar injection enabled. You can use Istio gateways and virtual services to access applications deployed in these domains. If your applications have suitable tracing code in them, then you will also be able to use distributed tracing, such as Jaeger, to trace requests across domains and to other components and services that have tracing enabled.\nLimitations The current support for Istio has these limitations:\n  It is tested with Istio 1.4.2 and later (up to 1.7.x); it is tested with both single and multicluster installations of Istio.\nNOTE: The WebLogic Kubernetes Operator creates Kubernetes headless Services for the domain; Istio 1.6.x does not work with headless Services. See Headless service broken in 1.6.0. Instead, use Istio version 1.7 and higher.\n  You cannot expose any of the default channels; any attempt will result in an error when deploying the domain.\n  If the istio-ingressgateway service in your environment does not have an EXTERNAL-IP defined, in order to use WLST commands, define a network access point (NAP) in your WebLogic domain and expose it as a NodePort in your Domain YAML file and access it through the NodePort instead of accessing the channel through the Istio mesh network.\n  To learn more about service mesh, see Istio.\nUsing the operator with Istio support These instructions assume that you are using a Kubernetes cluster with Istio installed and configured already. The operator will not install Istio for you.\n You can deploy the operator into a namespace which has Istio automatic sidecar injection enabled. Before installing the operator, create the namespace in which you want to run the domain and label it.\n$ kubectl create namespace weblogic-operator $ kubectl label namespace weblogic-operator istio-injection=enabled After the namespace is labeled, you can install the operator.\nWhen the operator pod starts, you will notice that Istio automatically injects an initContainer called istio-init and the envoy container istio-proxy.\nYou can validate this using the following commands:\n$ kubectl --namespace weblogic-operator get pods $ kubectl --namespace weblogic-operator get pod weblogic-operator-xxx-xxx -o yaml In the second command, change weblogic-operator-xxx-xxx to the name of your pod.\nCreating a domain with Istio support You can configure your domains to run with Istio automatic sidecar injection enabled. Before creating your domain, create the namespace in which you want to run the operator and label it for automatic injection.\n$ kubectl create namespace domain1 $ kubectl label namespace domain1 istio-injection=enabled To enable Istio support for a domain, you need to add the configuration section to your domain custom resource YAML file, as shown in the following example:\napiVersion: \u0026#34;weblogic.oracle/v8\u0026#34; kind: Domain metadata: name: domain2 namespace: domain1 labels: weblogic.domainUID: domain2 spec: ... other content ... configuration: istio: enabled: true readinessPort: 8888 To enable Istio support, you must include the istio section and set enabled: true as shown. The readinessPort is optional and defaults to 8888 if not provided; it is used for a readiness health check.\nHow Istio-enabled domains differ from regular domains Istio enforces a number of requirements on Pods. When you enable Istio support in the Domain YAML file, the introspector job automatically creates configuration overrides with the necessary channels for the domain to satisfy Istio\u0026rsquo;s requirements, including:\nWhen deploying a domain with Istio sidecar injection enabled, the operator automatically adds the following network channels using configuration overrides.\nhttps://istio.io/latest/docs/ops/configuration/traffic-management/protocol-selection/\nFor non-SSL traffic:\n   Name Port Protocol Exposed as a container port     http-probe From configuration Istio readinessPort http No   tcp-default Server listening port t3 Yes   http-default Server listening port http Yes   tcp-snmp Server listening port snmp Yes   tcp-cbt server listening port CLUSTER-BROADCAST No   tcp-iiop Server listening port http No    For SSL traffic, if SSL is enabled on the server:\n   Name Port Protocol Exposed as a container port     tls-default Server SSL listening port t3s Yes   https-secure Server SSL listening port https Yes   tls-iiops Server SSL listening port iiops No   tls-ldaps Server SSL listening port ldaps No   tls-cbts Server listening port CLUSTER-BROADCAST-SECURE No    If the WebLogic administration port is enabled on the Administration Server:\n   Name Port Protocol Exposed in the container port     https-admin WebLogic administration port https Yes    Additionally, when Istio support is enabled for a domain, the operator ensures that the Istio sidecar is not injected into the introspector job\u0026rsquo;s pods.\nSupport for network changes in Istio v1.10 and later Starting with Istio 1.10, the networking behavior was changed in that the proxy no longer redirects the traffic to the localhost interface, but instead forwards it to the network interface associated with the pod\u0026rsquo;s IP.\nTo learn more about changes to Istio networking beginning with Istio 1.10, see Upcoming networking changes in Istio 1.10.\nIn order to support Istio v1.10 and later, as well as previous releases, the operator will:\n Add an additional WebLogic HTTP protocol network channel for the readiness probe that is bound to the localhost network interface. Add additional WebLogic network channels, bound to the localhost network interface, for each defined custom network channel. Continue to automatically add the network channels described above in How Istio-enabled domains differ from regular domains  When adding additional WebLogic network channels for the readiness probe and any defined custom channels, the name of the additional channel will be appended with \u0026lsquo;-lhNN\u0026rsquo;, where NN represents a two digit value for uniqueness.\nFor example, the additional WebLogic HTTP protocol network channel for the readiness probe would be defined as follows:\n   Name Port Listening address Protocol Exposed as a container port     http-probe-lh01 From configuration Istio readinessPort 127.0.0.1 http No    As another example, for a custom WebLogic network channel defined as T3Channel with port 5556 and protocol t3, the additional channel would be defined as follows:\n   Name Port Listening address Protocol Exposed as a container port     T3Channel-lh01 5556 127.0.0.1 t3 Yes    Apply the Domain YAML file After the Domain YAML file is modified, apply it by:\n$ kubectl apply -f domain.yaml After all the servers are up, you will see output like this:\n$ kubectl -n sample-domain1-ns get pods NAME READY STATUS RESTARTS AGE sample-domain1-admin-server 2/2 Running 0 154m sample-domain1-managed-server1 2/2 Running 0 153m sample-domain1-managed-server2 2/2 Running 0 153m If you use istioctl proxy-status, you will see the mesh status:\nistioctl proxy-status NAME CDS LDS EDS RDS PILOT VERSION istio-ingressgateway-5c7d8d7b5d-tjgtd.istio-system SYNCED SYNCED SYNCED NOT SENT istio-pilot-6cfcdb75dd-87lqm 1.5.4 sample-domain1-admin-server.sample-domain1-ns SYNCED SYNCED SYNCED SYNCED istio-pilot-6cfcdb75dd-87lqm 1.5.4 sample-domain1-managed-server1.sample-domain1-ns SYNCED SYNCED SYNCED SYNCED istio-pilot-6cfcdb75dd-87lqm 1.5.4 sample-domain1-managed-server2.sample-domain1-ns SYNCED SYNCED SYNCED SYNCED istio-pilot-6cfcdb75dd-87lqm 1.5.4 weblogic-operator-7d86fffbdd-5dxzt.sample-weblogic-operator-ns SYNCED SYNCED SYNCED SYNCED istio-pilot-6cfcdb75dd-87lqm 1.5.4 Exposing applications in Istio-enabled domains When a domain is running with Istio support, you should use the Istio ingress gateway to provide external access to applications, instead of using an ingress controller like Traefik. Using the Istio ingress gateway, you can also view the traffic in Kiali and use distributed tracing from the entry point to the cluster.\nTo configure external access to your domain, you need to create an Istio Gateway and VirtualService, as shown in the example below:\n--- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: domain1-gateway namespace: domain1 spec: selector: istio: ingressgateway servers: - hosts: - \u0026#39;*\u0026#39; port: name: http number: 80 protocol: HTTP --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: domain1-virtualservice namespace: domain1 spec: gateways: - domain1-gateway hosts: - \u0026#39;*\u0026#39; http: - match: - uri: prefix: /console - port: 7001 route: - destination: host: sample-domain1-admin-server.sample-domain1-ns.svc.cluster.local port: number: 7001 - match: - uri: prefix: /testwebapp - port: 8001 route: - destination: host: sample-domain1-cluster-cluster-1.domain1.svc.cluster.local port: number: 8001 This example creates a gateway that will accept requests with any host name using HTTP on port 80, and a virtual service that will route all of those requests to the cluster service for cluster-1 in domain1 in the namespace domain1. Note: In a production environment, hosts should be limited to the proper DNS name.\nAfter the gateway and virtual service has been set up, you can access it through your ingress host and port. Refer to Determining the ingress IP and ports.\nFor more information about providing ingress using Istio, see the Istio documentation.\nTraffic management Istio provides traffic management capabilities, including the ability to visualize traffic in Kiali. You do not need to change your applications to use this feature. The Istio proxy (envoy) sidecar that is injected into your pods provides it. The image below shows an example with traffic flowing: In from the Istio gateway on the left, to a domain called domain1.\nIn this example, you can see how the traffic flows to the cluster services and then to the individual Managed Servers.\nTo learn more, see Istio traffic management.\nDistributed tracing Istio provides distributed tracing capabilities, including the ability to view traces in Jaeger. In order to use distributed tracing though, first you will need to instrument your WebLogic application, for example, using the Jaeger Java client. The image below shows an example of a distributed trace that shows a transaction following the same path through the system as shown in the image above.\nTo learn more, see distrubting tracing in Istio.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/faq/coherence-requirements/",
	"title": "Coherence requirements",
	"tags": [],
	"description": "If you are running Coherence on Kubernetes, either inside a WebLogic domain or standalone, then there are some additional requirements to make sure that Coherence can form clusters.",
	"content": "If you are running Coherence on Kubernetes, either inside a WebLogic domain or standalone, then there are some additional requirements to make sure that Coherence can form clusters.\nNote that some Fusion Middleware products, like SOA Suite, use Coherence and so these requirements apply to them.\nUnicast and Well Known Address When the first Coherence process starts, it will form a cluster. The next Coherence process to start (for example, in a different pod), will use UDP to try to contact the senior member.\nIf you create a WebLogic domain which contains a Coherence cluster using the samples provided in this project, then that cluster will be configured correctly so that it is able to form; you do not need to do any additional manual configuration.\nIf you are running Coherence standalone (outside a WebLogic domain), then you should configure Coherence to use unicast and provide a \u0026ldquo;well known address (WKA)\u0026rdquo; so that all members can find the senior member. Most Kubernetes overlay network providers do not support multicast.\nThis is done by specifying Coherence well known addresses in a variable named coherence.wka as shown in the following example:\n-Dcoherence.wka=my-cluster-service In this example my-cluster-service should be the name of the Kubernetes service that is pointing to all of the members of that Coherence cluster.\nFor more information about running Coherence in Kubernetes outside of a WebLogic domain, refer to the Coherence operator documentation.\nOperating system library requirements In order for Coherence clusters to form correctly, the conntrack library must be installed. Most Kubernetes distributions will do this for you. If you have issues with clusters not forming, then you should check that conntrack is installed using this command (or equivalent):\n$ rpm -qa | grep conntrack libnetfilter_conntrack-1.0.6-1.el7_3.x86_64 conntrack-tools-1.4.4-4.el7.x86_64 You should see output similar to that shown above. If you do not, then you should install conntrack using your operating system tools.\nFirewall (iptables) requirements Some Kubernetes distributions create iptables rules that block some types of traffic that Coherence requires to form clusters. If you are not able to form clusters, then you can check for this issue using the following command:\n$ iptables -t nat -v -L POST_public_allow -n Chain POST_public_allow (1 references) pkts bytes target prot opt in out source destination 164K 11M MASQUERADE all -- * !lo 0.0.0.0/0 0.0.0.0/0 0 0 MASQUERADE all -- * !lo 0.0.0.0/0 0.0.0.0/0 If you see output similar to the example above, for example, if you see any entries in this chain, then you need to remove them. You can remove the entries using this command:\n$ iptables -t nat -v -D POST_public_allow 1 Note that you will need to run that command for each line. So in the example above, you would need to run it twice.\nAfter you are done, you can run the previous command again and verify that the output is now an empty list.\nAfter making this change, restart your domains and the Coherence cluster should now form correctly.\nMake iptables updates permanent across reboots The recommended way to make iptables updates permanent across reboots is to create a systemd service that applies the necessary updates during the startup process.\nHere is an example; you may need to adjust this to suit your own environment:\n Create a systemd service:  $ echo \u0026#39;Set up systemd service to fix iptables nat chain at each reboot (so Coherence will work)...\u0026#39; $ mkdir -p /etc/systemd/system/ $ cat \u0026gt; /etc/systemd/system/fix-iptables.service \u0026lt;\u0026lt; EOF [Unit] Description=Fix iptables After=firewalld.service After=docker.service [Service] ExecStart=/sbin/fix-iptables.sh [Install] WantedBy=multi-user.target EOF  Create the script to update iptables:  $ cat \u0026gt; /sbin/fix-iptables.sh \u0026lt;\u0026lt; EOF #!/bin/bash echo \u0026#39;Fixing iptables rules for Coherence issue...\u0026#39; TIMES=$((`iptables -t nat -v -L POST_public_allow -n --line-number | wc -l` - 2)) COUNTER=1 while [ $COUNTER -le $TIMES ]; do iptables -t nat -v -D POST_public_allow 1 ((COUNTER++)) done EOF  Start the service (or just reboot):  $ echo \u0026#39;Start the systemd service to fix iptables nat chain...\u0026#39; $ systemctl enable --now fix-iptables "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/kubernetes/k8s-setup/",
	"title": "Set up Kubernetes",
	"tags": [],
	"description": "Get help for setting up a Kubernetes environment",
	"content": "Contents  Cheat sheet for setting up Kubernetes Set up Kubernetes on bare compute resources in a cloud  Prerequisites Quick start   Install Kubernetes on your own compute resources Install Docker for Mac with Kubernetes  Cheat sheet for setting up Kubernetes If you need some help setting up a Kubernetes environment to experiment with the operator, please read on! The supported environments are either an on-premises installation of Kubernetes, for example, on bare metal, or on a cloud provider like Oracle Cloud, Microsoft Azure, Google, or Amazon. Cloud providers allow you to provision a managed Kubernetes environment from their management consoles. You could also set up Kubernetes manually using compute resources on a cloud. There are also a number of ways to run a Kubernetes single-node cluster that are suitable for development or testing purposes. Your options include:\n\u0026ldquo;Production\u0026rdquo; options:\n Set up your own Kubernetes environment on bare compute resources on a cloud. Use your cloud provider\u0026rsquo;s management console to provision a managed Kubernetes environment. Install Kubernetes on your own compute resources (for example, \u0026ldquo;real\u0026rdquo; computers, outside a cloud).  \u0026ldquo;Development/test\u0026rdquo; options:\n Install Docker for Mac or Docker for Windows and enable its embedded Kubernetes cluster. We do not recommend or support other development/test options like Minikube, Minishift, kind, and so on.  We have provided our hints and tips for several of these options in the sections below.\nSet up Kubernetes on bare compute resources in a cloud Follow the basic steps from the Terraform Kubernetes installer for Oracle Cloud Infrastructure.\nPrerequisites  Download and install Terraform (v0.10.3 or later). Download and install the OCI Terraform Provider (v2.0.0 or later). Create an Terraform configuration file at ~/.terraformrc that specifies the path to the OCI provider: providers { oci = \u0026quot;\u0026lt;path_to_provider_binary\u0026gt;/terraform-provider-oci\u0026quot; }  Ensure that you have kubectl installed if you plan to interact with the cluster locally.  Quick start   Do a git clone of the Terraform Kubernetes installer project:\n$ git clone https://github.com/oracle/terraform-kubernetes-installer.git   Initialize your project:\n$ cd terraform-kubernetes-installer $ terraform init   Copy the example terraform.tvfars:\n$ cp terraform.example.tfvars terraform.tfvars   Edit the terraform.tvfars file to include values for your tenancy, user, and compartment. Optionally, edit the variables to change the Shape of the VMs for your Kubernetes master and workers, and your etcd cluster. For example:\n#give a label to your cluster to help identify it if you have multiple label_prefix=\u0026quot;weblogic-operator-1-\u0026quot; #identification/authorization info tenancy_ocid = \u0026quot;ocid1.tenancy....\u0026quot; compartment_ocid = \u0026quot;ocid1.compartment....\u0026quot; fingerprint = \u0026quot;...\u0026quot; private_key_path = \u0026quot;/Users/username/.oci/oci_api_key.pem\u0026quot; user_ocid = \u0026quot;ocid1.user...\u0026quot; #shapes for your VMs etcdShape = \u0026quot;VM.Standard1.2\u0026quot; k8sMasterShape = \u0026quot;VM.Standard1.8\u0026quot; k8sWorkerShape = \u0026quot;VM.Standard1.8\u0026quot; k8sMasterAd1Count = \u0026quot;1\u0026quot; k8sWorkerAd1Count = \u0026quot;2\u0026quot; #this ingress is set to wide-open for testing **not secure** etcd_ssh_ingress = \u0026quot;0.0.0.0/0\u0026quot; master_ssh_ingress = \u0026quot;0.0.0.0/0\u0026quot; worker_ssh_ingress = \u0026quot;0.0.0.0/0\u0026quot; master_https_ingress = \u0026quot;0.0.0.0/0\u0026quot; worker_nodeport_ingress = \u0026quot;0.0.0.0/0\u0026quot; #create iscsi volumes to store your etcd and /var/lib/docker info worker_iscsi_volume_create = true worker_iscsi_volume_size = 100 etcd_iscsi_volume_create = true etcd_iscsi_volume_size = 50   Test and apply your changes:\n$ terraform plan $ terraform apply   Test your cluster using the built-in script scripts/cluster-check.sh:\n$ scripts/cluster-check.sh   Output the SSH private key:\n# output the ssh private key for use later $ rm -f generated/instances_id_rsa \u0026amp;\u0026amp; terraform output ssh_private_key \u0026gt; generated/instances_id_rsa \u0026amp;\u0026amp; chmod 600 generated/instances_id_rsa   If you need shared storage between your Kubernetes worker nodes, enable and configure NFS:\n  In the current GA version, the OCI Container Engine for Kubernetes supports network block storage that can be shared across nodes with access permission RWOnce (meaning that only one can write, others can read only). If you choose to place your domain in a persistent volume, you must use a shared file system to store the WebLogic domain configuration, which MUST be accessible from all the pods across the nodes. Oracle recommends that you use the Oracle Cloud Infrastructure File Storage Service (or equivalent on other cloud providers). Alternatively, you may install an NFS server on one node and share the file system across all the nodes.\nCurrently, we recommend that you use NFS version 3.0 for running WebLogic Server on OCI Container Engine for Kubernetes. During certification, we found that when using NFS 4.0, the servers in the WebLogic domain went into a failed state intermittently. Because multiple threads use NFS (default store, diagnostics store, Node Manager, logging, and domain_home), there are issues when accessing the file store. These issues are removed by changing the NFS to version 3.0.\n $ terraform output worker_public_ips IP1, IP2 $ terraform output worker_private_ips PRIVATE_IP1, PRIVATE_IP2 $ ssh -i `pwd`/generated/instances_id_rsa opc@IP1 worker-1$ sudo su - worker-1# yum install -y nfs-utils worker-1# mkdir /scratch worker-1# echo \u0026quot;/scratch PRIVATE_IP2(rw)\u0026quot; \u0026gt;\u0026gt; /etc/exports worker-1# systemctl restart nfs worker-1# exit worker-1$ exit # configure worker-2 to mount the share from worker-1 $ ssh -i `pwd`/generated/instances_id_rsa opc@IP2 worker-2$ sudo su - worker-2# yum install -y nfs-utils worker-2# mkdir /scratch worker-2# echo \u0026quot;PRIVATE_IP1:/scratch /scratch nfs nfsvers=3 0 0\u0026quot; \u0026gt;\u0026gt; /etc/fstab worker-2# mount /scratch worker-2# exit worker-2$ exit Install Kubernetes on your own compute resources For example, on Oracle Linux servers outside a cloud.\nThese instructions are for Oracle Linux 7u2+. If you are using a different flavor of Linux, you will need to adjust them accordingly.\nThese steps must be run with the root user, until specified otherwise! Any time you see YOUR_USERID in a command, you should replace it with your actual userid.\n   Choose the directories where your Docker and Kubernetes files will be stored. The Docker directory should be on a disk with a lot of free space (more than 100GB) because it will be used for the /var/lib/docker file system, which contains all of your images and containers. The Kubernetes directory will be used for the /var/lib/kubelet file system and persistent volume storage.\n$ export docker_dir=/scratch/docker $ export k8s_dir=/scratch/k8s_dir   Create a shell script that sets up the necessary environment variables. You should probably just append this to the user\u0026rsquo;s .bashrc so that it will get executed at login. You will also need to configure your proxy settings here if you are behind an HTTP proxy:\n#!/bin/bash export PATH=$PATH:/sbin:/usr/sbin pod_network_cidr=\u0026#34;10.244.0.0/16\u0026#34; k8s_dir=$k8s_dir ## grab my IP address to pass into kubeadm init, and to add to no_proxy vars # assume ipv4 and eth0 ip_addr=`ip -f inet addr show eth0 | egrep inet | awk \u0026#39;{print $2}\u0026#39; | awk -F/ \u0026#39;{print $1}\u0026#39;\\` export HTTPS_PROXY=http://proxy:80 export https_proxy=http://proxy:80 export NO_PROXY=localhost,127.0.0.1,.my.domain.com,/var/run/docker.sock,$pod_network_cidr,$ip_addr export no_proxy=localhost,127.0.0.1,.my.domain.com,/var/run/docker.sock,$pod_network_cidr,$ip_addr export HTTP_PROXY=http://proxy:80 export http_proxy=http://proxy:80 export KUBECONFIG=$k8s_dir/admin.conf Source that script to set up your environment variables:\n$ . ~/.bashrc If you want command completion, you can add the following to the script:\n[ -f /usr/share/bash-completion/bash_completion ] \u0026amp;\u0026amp; . /usr/share/bash-completion/bash_completion source \u0026lt;(kubectl completion bash)   Create the directories you need:\n$ mkdir -p $docker_dir $k8s_dir/kubelet $ ln -s $k8s_dir/kubelet /var/lib/kubelet   Set an environment variable with the Docker version you want to install:\n$ docker_version=\u0026#34;18.09.1.ol\u0026#34;   Install Docker, removing any previously installed version:\n### install docker and curl-devel (for git if needed) $ yum-config-manager --enable ol7_addons ol7_latest # we are going to just uninstall any docker-engine that is installed $ yum -y erase docker-engine docker-engine-selinux # now install the docker-engine at our specified version $ yum -y install docker-engine-$docker_version curl-devel   Update the Docker options:\n# edit /etc/sysconfig/docker to add custom OPTIONS $ cat /etc/sysconfig/docker | sed \u0026#34;s#^OPTIONS=.*#OPTIONS=\u0026#39;--selinux-enabled --group=docker -g $docker_dir\u0026#39;#g\u0026#34; \u0026gt; /tmp/docker.out $ diff /etc/sysconfig/docker /tmp/docker.out $ mv /tmp/docker.out /etc/sysconfig/docker   Set up the Docker network, including the HTTP proxy configuration, if you need it:\n# generate a custom /setc/sysconfig/docker-network $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysconfig/docker-network # /etc/sysconfig/docker-network DOCKER_NETWORK_OPTIONS=\u0026#34;-H tcp://0.0.0.0:4243 -H unix:///var/run/docker.sock\u0026#34; HTTP_PROXY=\u0026#34;http://proxy:80\u0026#34; HTTPS_PROXY=\u0026#34;http://proxy:80\u0026#34; NO_PROXY=\u0026#34;localhost,127.0.0.0/8,.my.domain.com,/var/run/docker.sock\u0026#34; EOF   Add your user to the docker group:\n$ usermod -aG docker YOUR_USERID   Enable and start the Docker service that you just installed and configured:\n$ systemctl enable docker \u0026amp;\u0026amp; systemctl start docker   Install the Kubernetes packages:\n#!/bin/bash # generate the yum repo config cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF setenforce 0 # install kube* packages v=${1:-1.17.0-0} old_ver=`echo $v | egrep \u0026#34;^1.7\u0026#34;` yum install -y kubelet-$v kubeadm-$v kubectl-$v kubernetes-cni # change the cgroup-driver to match what docker is using cgroup=`docker info 2\u0026gt;\u0026amp;1 | egrep Cgroup | awk \u0026#39;{print $NF}\u0026#39;` [ \u0026#34;$cgroup\u0026#34; == \u0026#34;\u0026#34; ] \u0026amp;\u0026amp; echo \u0026#34;cgroup not detected!\u0026#34; \u0026amp;\u0026amp; exit 1 cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf | sed \u0026#34;s#KUBELET_CGROUP_ARGS=--cgroup-driver=.*#KUBELET_CGROUP_ARGS=--cgroup-driver=$cgroup\\\u0026#34;#\u0026#34;\u0026gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.out diff /etc/systemd/system/kubelet.service.d/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.out mv /etc/systemd/system/kubelet.service.d/10-kubeadm.conf.out /etc/systemd/system/kubelet.service.d/10-kubeadm.conf if [ \u0026#34;$old_ver\u0026#34; = \u0026#34;\u0026#34; ] ; then # run with swap if not in version 1.7* (starting in 1.8, kubelet # fails to start with swap enabled) # cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/systemd/system/kubelet.service.d/90-local-extras.conf [Service] Environment=\u0026#34;KUBELET_EXTRA_ARGS=--fail-swap-on=false\u0026#34; EOF fi   Enable and start the Kubernetes Service:\n$ systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet   Install and use Flannel for CNI:\n#!/bin/bash # run kubeadm init as root echo Running kubeadm init --skip-preflight-checks --apiserver-advertise-address=$ip_addr --pod-network-cidr=$pod_network_cidr echo \u0026#34; see /tmp/kubeadm-init.out for output\u0026#34; kubeadm init --skip-preflight-checks --apiserver-advertise-address=$ip_addr --pod-network-cidr=$pod_network_cidr \u0026gt; /tmp/kubeadm-init.out 2\u0026gt;\u0026amp;1 if [ $? -ne 0 ] ; then echo \u0026#34;ERROR: kubeadm init returned non 0\u0026#34; chmod a+r /tmp/kubeadm-init.out exit 1 else echo; echo \u0026#34;kubeadm init complete\u0026#34; ; echo # tail the log to get the \u0026#34;join\u0026#34; token tail -6 /tmp/kubeadm-init.out fi cp /etc/kubernetes/admin.conf $KUBECONFIG chown YOUR_USERID:YOUR_GROUP $KUBECONFIG chmod 644 $KUBECONFIG  The following steps should be run with your normal (non-root) user.\n   Configure CNI:\n$ sudo -u YOUR_USERID kubectl create clusterrolebinding permissive-binding --clusterrole=cluster-admin --user=admin --user=kubelet --group=system:serviceaccounts $ sudo -u YOUR_USERID kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml Wait for kubectl get nodes to show Ready for this host:\n#!/bin/bash host=`hostname | awk -F. \u0026#39;{print $1}\u0026#39;` status=\u0026#34;NotReady\u0026#34; max=10 count=1 while [ ${status:=Error} != \u0026#34;Ready\u0026#34; -a $count -lt $max ] ; do sleep 30 status=`sudo -u YOUR_USERID kubectl get nodes | egrep $host | awk \u0026#39;{print $2}\u0026#39;` echo \u0026#34;kubectl status is ${status:=Error}, iteration $countof $max\u0026#34; count=`expr $count + 1` done status=`sudo -u YOUR_USERID kubectl get nodes | egrep $host | awk \u0026#39;{print $2}\u0026#39;` if [ ${status:=Error} != \u0026#34;Ready\u0026#34; ] ; then echo \u0026#34;ERROR: kubectl get nodes reports status=${status:=Error}after configuration, exiting!\u0026#34; exit 1 fi   Taint the nodes:\n$ sudo -u YOUR_USERID kubectl taint nodes --all node-role.kubernetes.io/master- $ sudo -u YOUR_USERID kubectl get nodes $ sudo -u YOUR_USERID kubeadm version Congratulations! Docker and Kubernetes are installed and configured!\n  Install Docker for Mac with Kubernetes Docker for Mac 18+ provides an embedded Kubernetes environment that is a quick and easy way to get a simple test environment set up on your Mac. To set it up, follow these instructions:\n  Install \u0026ldquo;Docker for Mac\u0026rdquo; https://download.docker.com/mac/edge/Docker.dmg. Then start up the Docker application (press Command-Space bar, type in Docker and run it). After it is running you will see the Docker icon appear in your status bar:\n  Click the Docker icon and select \u0026ldquo;Preferences\u0026hellip;\u0026rdquo; from the drop down menu. Go to the \u0026ldquo;Advanced\u0026rdquo; tab and give Docker a bit more memory if you have enough to spare:\n  Go to the \u0026ldquo;Kubernetes\u0026rdquo; tab and click on the option to enable Kubernetes:\nIf you are behind an HTTP proxy, then you should also go to the \u0026ldquo;Proxies\u0026rdquo; tab and enter your proxy details.\n Docker will download the Kubernetes components and start them up for you. When it is done, you will see the Kubernetes status go to green/running in the menu:\n  Ensure that kubectl on your Mac, is pointing to the correct cluster and context.\n$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE docker-for-desktop docker-for-desktop-cluster docker-for-desktop kubernetes-admin@kubernetes kubernetes kubernetes-admin $ kubectl config use-context docker-for-desktop Switched to context \u0026quot;docker-for-desktop\u0026quot;. $ kubectl config get-clusters NAME kubernetes docker-for-desktop-cluster $ kubectl config set-cluster docker-for-desktop-cluster Cluster \u0026quot;docker-for-desktop-cluster\u0026quot; set.   You should add docker-for-desktop to your /etc/hosts file entry for 127.0.0.1, as shown in this example, and you must be an admin user to edit this file:\n## # Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 127.0.0.1\tlocalhost docker-for-desktop 255.255.255.255\tbroadcasthost ::1 localhost   You may also have to tell kubectl to ignore the certificate by entering this command:\n$ kubectl config set-cluster docker-for-desktop --insecure-skip-tls-verify=true   Then validate you are talking to the Kubernetes in Docker by entering these commands:\n$ kubectl cluster-info Kubernetes master is running at https://docker-for-desktop:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Important note about persistent volumes   Docker for Mac has some restrictions on where you can place a directory that can be used as a HostPath for a persistent volume. To keep it simple, place your directory somewhere under /Users.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/faq/resource-settings/",
	"title": "Pod memory and CPU resources",
	"tags": [],
	"description": "Tune container memory and CPU usage by configuring Kubernetes resource requests and limits, and tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your Domain YAML file.",
	"content": "Contents  Introduction Setting resource requests and limits in a Domain YAML file Determining Pod Quality Of Service Java heap size and memory resource considerations  Importance of setting heap size and memory resources Default heap sizes Configuring heap size   CPU resource considerations Operator sample heap and resource configuration Configuring CPU affinity Measuring JVM heap, Pod CPU, and Pod memory References  Introduction The operator creates a container in its own Pod for each WebLogic Server instance. You can tune container memory and CPU usage by configuring Kubernetes resource requests and limits, and you can tune a WebLogic JVM heap usage using the USER_MEM_ARGS environment variable in your Domain YAML file. A resource request sets the minimum amount of a resource that a container requires. A resource limit is the maximum amount of a resource a container is given and prevents a container from using more than its share of a resource. Additionally, resource requests and limits determine a Pod\u0026rsquo;s quality of service.\nThis FAQ discusses tuning these parameters so WebLogic Server instances run efficiently.\nSetting resource requests and limits in a Domain You can set Kubernetes memory and CPU requests and limits in a Domain YAML file using its spec.serverPod.resources stanza, and you can override the setting for individual WebLogic Server instances or clusters using the serverPod.resources element in spec.adminServer, spec.clusters, or spec.managedServers. For example:\nspec: serverPod: resources: requests: cpu: \u0026#34;250m\u0026#34; memory: \u0026#34;768Mi\u0026#34; limits: cpu: \u0026#34;2\u0026#34; memory: \u0026#34;2Gi\u0026#34; Limits and requests for CPU resources are measured in CPU units. One CPU, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An m suffix in a CPU attribute indicates \u0026lsquo;milli-CPU\u0026rsquo;, so 250m is 25% of a CPU.\nMemory can be expressed in various units, where one Mi is one IEC unit mega-byte (1024^2), and one Gi is one IEC unit giga-byte (1024^3).\nSee also Managing Resources for Containers, Assign Memory Resources to Containers and Pods and Assign CPU Resources to Containers and Pods in the Kubernetes documentation.\nDetermining Pod Quality Of Service A Pod\u0026rsquo;s Quality of Service (QoS) is based on whether it\u0026rsquo;s configured with resource requests and limits:\n  Best Effort QoS (lowest priority): If you don\u0026rsquo;t configure requests and limits for a Pod, then the Pod is given a best-effort QoS. In cases where a Node runs out of non-shareable resources, the default out-of-resource eviction policy evicts running Pods with the best-effort QoS first.\n  Burstable QoS (medium priority): If you configure both resource requests and limits for a Pod, and set the requests to be less than their respective limits, then the Pod will be given a burstable QoS. Similarly, if you only configure resource requests (without limits) for a Pod, then the Pod QoS is also burstable. If a Node runs out of non-shareable resources, the Node\u0026rsquo;s kubelet will evict burstable Pods only when there are no more running best-effort Pods.\n  Guaranteed QoS (highest priority): If you set a Pod\u0026rsquo;s requests and the limits to equal values, then the Pod will have a guaranteed QoS. These settings indicate that your Pod will consume a fixed amount of memory and CPU. With this configuration, if a Node runs out of shareable resources, then the Node\u0026rsquo;s kubelet will evict best-effort and burstable QoS Pods before terminating guaranteed QoS Pods.\n  For most use cases, Oracle recommends configuring WebLogic Pods with memory and CPU requests and limits, and furthermore, setting requests equal to their respective limits in order to ensure a guaranteed QoS.\n In later versions of Kubernetes, it is possible to fine tune scheduling and eviction policies using Pod Priority Preemption in combination with the serverPod.priorityClassName Domain field. Note that Kubernetes already ships with two PriorityClasses: system-cluster-critical and system-node-critical. These are common classes and are used to ensure that critical components are always scheduled first.\n Java heap size and memory resource considerations Oracle recommends configuring Java heap sizes for WebLogic JVMs instead of relying on the defaults.\n Importance of setting heap size and memory resources It\u0026rsquo;s extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods.\nA WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\nA Pod memory limit must be sufficiently sized to accommodate the configured heap and native memory requirements, but should not be sized too large so as not to waste memory resources. If a JVM\u0026rsquo;s memory usage (sum of heap and native memory) exceeds its Pod\u0026rsquo;s limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.\nOracle recommends setting minimum and maximum heap (or heap percentages) and at least a container memory request.\nIf resource requests and resource limits are set too high, then your Pods may not be scheduled due to a lack of Node resources. It will unnecessarily use up CPU shared resources that could be used by other Pods, or may prevent other Pods from running.\n Default heap sizes With the latest Java versions, Java 8 update 191 and later, or Java 11, if you don\u0026rsquo;t configure a heap size (no -Xms or -Xms), the default heap size is dynamically determined:\n  If you configure the memory limit for a container, then the JVM default maximum heap size will be 25% (1/4th) of container memory limit and the default minimum heap size will be 1.56% (1/64th) of the limit value.\nIn this case, the default JVM heap settings are often too conservative because the WebLogic JVM is the only major process running in the container.\n  If no memory limit is configured, then the JVM default maximum heap size will be 25% (1/4th) of its Node\u0026rsquo;s machine RAM and the default minimum heap size will be 1.56% (1/64th) of the RAM.\nIn this case, the default JVM heap settings can have undesirable behavior, including using unnecessary amounts of memory to the point where it might affect other Pods that run on the same Node.\n  Configuring heap size If you specify Pod memory limits, Oracle recommends configuring WebLogic Server heap sizes as a percentage. The JVM will interpret the percentage as a fraction of the limit. This is done using the JVM -XX:MinRAMPercentage and -XX:MaxRAMPercentage options in the USER_MEM_ARGS Domain environment variable. For example:\nspec: resources: env: - name: USER_MEM_ARGS value: \u0026#34;-XX:MinRAMPercentage=25.0 -XX:MaxRAMPercentage=50.0 -Djava.security.egd=file:/dev/./urandom\u0026#34; Additionally, there\u0026rsquo;s a node-manager process that\u0026rsquo;s running in the same container as the WebLogic Server, which has its own heap and native memory requirements. Its heap is tuned by using -Xms and -Xmx in the NODEMGR_MEM_ARGS environment variable. Oracle recommends setting the Node Manager heap memory to fixed sizes, instead of percentages, where the default tuning is usually sufficient.\nNotice that the NODEMGR_MEM_ARGS, USER_MEM_ARGS, and WLST_EXTRA_PROPERTIES environment variables all include -Djava.security.egd=file:/dev/./urandom by default. This helps to speed up the Node Manager and WebLogic Server startup on systems with low entropy, plus similarly helps to speed up introspection job usage of the WLST encrypt command. We have included this property in the above example for specifying a custom USER_MEM_ARGS value in order to preserve this speedup. See the environment variable defaults documentation for more information.\n In some cases, you might only want to configure memory resource requests but not configure memory resource limits. In such scenarios, you can use the traditional fixed heap size settings (-Xms and -Xmx) in your WebLogic Server USER_MEM_ARGS instead of the percentage settings (-XX:MinRAMPercentage and -XX:MaxRAMPercentage).\nCPU resource considerations It\u0026rsquo;s important to set both a CPU request and a limit for WebLogic Server Pods. This ensures that all WebLogic Server Pods have enough CPU resources, and, as discussed earlier, if the request and limit are set to the same value, then they get a guaranteed QoS. A guaranteed QoS ensures that the Pods are handled with a higher priority during scheduling and as such, are the least likely to be evicted.\nIf a CPU request and limit are not configured for a WebLogic Server Pod:\n  The Pod can end up using all the CPU resources available on its Node and starve other containers from using shareable CPU cycles.\n  The WebLogic Server JVM may choose an unsuitable garbage collection (GC) strategy.\n  A WebLogic Server self-tuning work-manager may incorrectly optimize the number of threads it allocates for the default thread pool.\n  It\u0026rsquo;s also important to keep in mind that if you set a value of CPU core count that\u0026rsquo;s larger than the core count of your biggest Node, then the Pod will never be scheduled. Let\u0026rsquo;s say you have a Pod that needs 4 cores but you have a Kubernetes cluster that\u0026rsquo;s comprised of 2 core VMs. In this case, your Pod will never be scheduled and will have Pending status. For example:\n$ kubectl get pod sample-domain1-managed-server1 -n sample-domain1-ns NAME READY STATUS RESTARTS AGE sample-domain1-managed-server1 0/1 Pending 0 65s $ kubectl describe pod sample-domain1-managed-server1 -n sample-domain1-ns Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 16s (x3 over 26s) default-scheduler 0/2 nodes are available: 2 Insufficient cpu. Operator sample heap and resource configuration The operator samples configure non-default minimum and maximum heap sizes for WebLogic Server JVMs of at least 256MB and 512MB respectively. You can edit a sample\u0026rsquo;s template or Domain YAML file resources.env USER_MEM_ARGS to have different values. See Configuring heap size.\nSimilarly, the operator samples configure CPU and memory resource requests to at least 250m and 768Mi respectively.\nThere\u0026rsquo;s no memory or CPU limit configured by default in samples and so the default QoS for sample WebLogic Server Pod\u0026rsquo;s is burstable.\nIf you wish to set resource requests or limits differently on a sample Domain YAML file or template, see Setting resource requests and limits in a Domain resource. Or, for samples that generate their Domain resource using an \u0026lsquo;inputs\u0026rsquo; file, see the serverPodMemoryRequest, serverPodMemoryLimit, serverPodCpuRequest, and serverPodCpuLimit parameters in the sample\u0026rsquo;s create-domain.sh input file.\nConfiguring CPU affinity A Kubernetes hosted WebLogic Server may exhibit high lock contention in comparison to an on-premises deployment. This lock contention may be due to a lack of CPU cache affinity or scheduling latency when workloads move between different CPU cores.\nIn an on-premises deployment, CPU cache affinity, and therefore reduced lock contention, can be achieved by binding WLS Java process to a particular CPU core(s) (using the taskset command).\nIn a Kubernetes deployment, similar cache affinity can be achieved by doing the following:\n Ensuring a Pod\u0026rsquo;s CPU resource request and limit are set and equal (to ensure a guaranteed QoS). Configuring the kubelet CPU manager policy to be static (the default is none). See Control CPU Management Policies on the Node. Note that some Kubernetes environments may not allow changing the CPU management policy.  Measuring JVM heap, Pod CPU, and Pod memory You can monitor JVM heap, Pod CPU, and Pod memory using Prometheus and Grafana. Also, see Tools for Monitoring Resources in the Kubernetes documentation.\nReferences  Managing Resources for Containers in the Kubernetes documentation. Assign Memory Resources to Containers and Pods in the Kubernetes documentation. Assign CPU Resources to Containers and Pods in the Kubernetes documentation. Pod Priority Preemption in the Kubernetes documentation. GCP Kubernetes best practices: Resource requests and limits Tools for Monitoring Resources in the Kubernetes documentation. Blog \u0026ndash; Docker support in Java 8. (Discusses Java container support in general.) Blog \u0026ndash; Kubernetes Patterns : Capacity Planning  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/faq/security-validation/",
	"title": "Handling security validations",
	"tags": [],
	"description": "Why am I seeing these security warnings?",
	"content": " After applying the July2021 PSU, I\u0026rsquo;m now seeing security warnings, such as:\nDescription: Production Mode is enabled but user lockout settings are not secure in realm: myrealm, i.e. LockoutThreshold should not be greater than 5, LockoutDuration should not be less than 30.\nSOLUTION: Update the user lockout settings (LockoutThreshold, LockoutDuration) to be secure.\n WebLogic Server has a new, important feature to ensure and help you secure your WLS domains when running in production. With the July 2021 PSU applied, WebLogic Server regularly validates your domain configuration settings against a set of security configuration guidelines to determine whether the domain meets key security guidelines recommended by Oracle. For more information and additional details, see MOS Doc 2788605.1 \u0026ldquo;WebLogic Server Security Warnings Displayed Through the Admin Console\u0026rdquo; and Review Potential Security Issues in Securing a Production Environment for Oracle WebLogic Server.\nWarnings may be at the level of the JDK, or that SSL is not enabled. Some warnings may recommend updating your WebLogic configuration. You can make the recommended configuration changes using an approach that depends on your domain home source type:\n  For Domain in PV, use the WebLogic Scripting Tool (WLST), WebLogic Server Administration Console, WebLogic Deploy Tooling (WDT), or configuration overrides.\n  For Domain in Image, create a new image with the recommended changes or use configuration overrides.\n  For Model in Image, supply model files with the recommended changes in its image\u0026rsquo;s modelHome directory or use runtime updates.\n  For information about handling file permission warnings on the OpenShift Kubernetes Platform, see the OpenShift chapter in the Security section.\n "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/model-in-image/usage/",
	"title": "Usage",
	"tags": [],
	"description": "Steps for creating and deploying Model in Image images and their associated Domain YAML files.",
	"content": "This document describes what\u0026rsquo;s needed to create and deploy a typical Model in Image domain.\nContents  WebLogic Kubernetes Operator WebLogic Server image Directory structure Supplying initial model files and WDT Optional WDT model ConfigMap Required runtime encryption secret Secrets for model macros Domain fields Always use external state Requirements for JRF domain types  WebLogic Kubernetes Operator Deploy the operator and ensure that it is monitoring the desired namespace for your Model in Image domain. See Manage operators and Quick Start.\nWebLogic Server image Model in Image requires an image with a WebLogic Server installation.\n  You can start with a WebLogic Server 12.2.1.3 or later Oracle Container Registry pre-built base image such as container-registry.oracle.com/middleware/weblogic:12.2.1.3 for WLS domains or container-registry.oracle.com/middleware/fmw-infrastructure:12.2.1.3 for JRF domains. For an example of this approach for both WLS and JRF domains, see the Model in Image sample. For detailed instructions on how to log in to the Oracle Container Registry and accept the license agreement for an image (required to allow pulling an Oracle Container Registry image), see this document.\n  Or, you can manually build your own base image, as described in Preparing a Base Image. This is useful if you want your base images to include additional patches. Note that any 12.2.1.3 image must also include patch 29135930 (the pre-built images already contain this patch).\n  Directory structure Model in Image requires the following directory structure in its pods for its (optional) WDT model artifacts and (required) WDT binaries:\n   Domain resource attribute Default directory Contents     domain.spec.configuration.model.modelHome /u01/wdt/models Zero or more model .yaml, .properties, and/or archive .zip files.   domain.spec.configuration.model.wdtInstallHome /u01/wdt/weblogic-deploy Unzipped WDT installation binaries (required).    Supplying initial model files and WDT Model in Image minimally requires an image with a WebLogic installation (see WebLogic Server image), plus access to:\n A WDT installation in domain.spec.configuration.model.wdtInstallHome. One or more WDT model .yaml files that configure your domain in the domain.spec.configuration.model.modelHome directory or in the optional WDT model ConfigMap. Zero or more WDT model .properties files in the domain.spec.configuration.model.modelHome directory or in the optional WDT model ConfigMap. Zero or more WDT model application .zip archives in the domain.spec.configuration.model.modelHome directory. Archives must be supplied in the model home because application archives are not supported in the optional WDT model ConfigMap.  There are multiple methods for supplying Model in Image WDT artifacts:\n  Include in main image: You can include the artifacts in your domain resource domain.spec.image in its domain.spec.configuration.model.modelHome and domain.spec.configuration.model.wdtInstallHome directories as a layer on top of your base image (where the base image includes your WebLogic installation).\nUse either of the following methods.\n Manual image creation uses Docker commands to layer the WDT artifacts, described in the previous table, on top of your base image into a new image. The WebLogic Image Tool (WIT) has built-in options for layering WDT model files, WDT binaries, WebLogic Server binaries, and WebLogic Server patches in an image. The Model in Image sample uses the WIT approach.    Use auxiliary images: Use auxiliary images to create one or more small images that contain the desired files. This automatically copies files from each of the small images into each pod\u0026rsquo;s file system\u0026rsquo;s configuration.model.modelHome or configuration.model.wdtInstallHome location.\n  Use a Persistent Volume Claim (PVC): This method is for advanced use cases only. Supply WDT model YAML, variable, or archive files in a Persistent Volume Claim and modify configuration.model.modelHome and configuration.model.wdtInstallHome to the corresponding directory within the PVC\u0026rsquo;s mount location.\n  Use a WDT model ConfigMap: Use the Optional WDT model ConfigMap for WDT model YAML and .properties files. This can be combined with any of the previously mentioned methods and is most often used to facilitate runtime updates to models supplied by one of these methods.\n  For more information about model file syntax, see Model files.\nOptional WDT model ConfigMap You can create a WDT model ConfigMap that defines additional model .yaml and .properties files beyond what you\u0026rsquo;ve already supplied in your image, and then reference this ConfigMap using your Domain YAML file\u0026rsquo;s configuration.model.configMap attribute. This is optional if the supplied image already fully defines your model.\nWDT model ConfigMap files will be merged with the WDT files defined in your image at runtime before your domain home is created. The ConfigMap files can add to, remove from, or alter the model configuration that you supplied within your image.\nFor example, place additional .yaml and .properties files in a directory called /home/acmeuser/wdtoverride and run the following commands:\n$ kubectl -n MY-DOMAIN-NAMESPACE \\  create configmap MY-DOMAINUID-my-wdt-config-map \\  --from-file /home/acmeuser/wdtoverride $ kubectl -n MY-DOMAIN-NAMESPACE \\  label configmap MY-DOMAINUID-my-wdt-config-map \\  weblogic.domainUID=MY-DOMAINUID See Model files for a discussion of model file syntax and loading order, and see Runtime updates for a discussion of using WDT model ConfigMaps to update the model configuration of a running domain.\nRequired runtime encryption secret Model in Image requires a runtime encryption secret with a secure password key. This secret is used by the operator to encrypt model and domain home artifacts before it adds them to a runtime ConfigMap or log. You can safely change the password, at any time after you\u0026rsquo;ve fully shut down a domain, but it must remain the same for the life of a running domain. The runtime encryption secret that you create can be named anything, but note that it is a best practice to name and label secrets with their domain UID to help ensure that cleanup scripts can find and delete them.\nNOTE: Because the runtime encryption password does not need to be shared and needs to exist only for the life of a domain, you may want to use a password generator.\nExample:\n$ kubectl -n MY-DOMAIN-NAMESPACE \\  create secret generic MY-DOMAINUID-runtime-encrypt-secret \\  --from-literal=password=welcome1 $ kubectl -n MY-DOMAIN-NAMESPACE \\  label secret MY-DOMAINUID-runtime-encrypt-secret \\  weblogic.domainUID=MY-DOMAINUID Corresponding Domain YAML file snippet:\nconfiguration: model: runtimeEncryptionSecret: MY-DOMAINUID-runtime-encrypt-secret Secrets for model macros Create additional secrets as needed by macros in your model files. For example, these can store database URLs and credentials that are accessed using @@SECRET macros in your model that reference the secrets. For a discussion of model macros, see Model files.\nDomain fields The following Domain fields are specific to Model in Image domains.\n   Domain Resource Attribute Notes     domainHomeSourceType Required. Set to FromModel.   domainHome Must reference an empty or non-existent directory within your image. Do not include the mount path of any persistent volume. Note that Model in Image recreates the domain home for a WebLogic Server pod every time the pod restarts.   configuration.model.configMap Optional. Set if you have stored additional models in a ConfigMap as per Optional WDT model ConfigMap.   configuration.secrets Optional. Set this array if your image or ConfigMap models contain macros that reference custom Kubernetes Secrets. For example, if your macros depend on secrets my-secret and my-other-secret, then set to [my-secret, my-other-secret].   configuration.model.runtimeEncryptionSecret Required. All Model in Image domains must specify a runtime encryption secret. See Required runtime encryption secret.   configuration.model.domainType Set the type of domain. Valid values are WLS, JRF, and RestrictedJRF, where WLS is the default. See WDT Domain Types.   configuration.model.runtimeEncryptionSecret Required. All Model in Image domains must specify a runtime encryption secret. See Required runtime encryption secret.   configuration.model.modelHome Optional. Location of the WDT model home, which can include model YAML files, .properties files, and application .zip archives. Defaults to /u01/wdt/models.   configuration.model.wdtInstallHome Optional. Location of the WDT install. Defaults to /u01/wdt/weblogic-deploy.    Notes:\n  There are additional attributes that are common to all domain home source types, such as the image field. See the Domain Resource schema and documentation for a full list of Domain fields.\n  There are also additional fields that are specific to JRF domain types. For more information, see Requirements for JRF domain types.\n  For fully specified Model in Image Domain YAML file examples, see the operator source directory kubernetes/samples/scripts/create-weblogic-domain/model-in-image/domain-resources for the Model in Image sample. The WLS and JRF subdirectories in this directory correspond to the configuration.model.domainType.\n  Always use external state Regardless of the domain home source type, we recommend that you always keep state outside the image. This includes cluster database leasing tables, JMS and transaction stores, EJB timers, and so on. This ensures that data will not be lost when a container is destroyed.\nWe recommend that state be kept in a database to take advantage of built-in database server high availability features, and the fact that disaster recovery of sites across all but the shortest distances, almost always requires using a single database server to consolidate and replicate data (DataGuard).\nFor more information see:\n Tuning JDBC Stores in Tuning Performance of Oracle WebLogic Server. Using a JDBC Store in Administering the WebLogic Persistent Store. High Availability Best Practices in Administering JMS Resources for Oracle WebLogic Server. Leasing in Administering Clusters for Oracle WebLogic Server.  Requirements for JRF domain types This section applies only for a JRF domain type. Skip it if your domain type is WLS or RestrictedJRF.\n A JRF domain requires an infrastructure database, initializing this database using RCU, and configuring your domain to access this database. All of these steps must occur before you first deploy your domain. When you first deploy your domain, the introspector job will initialize it\u0026rsquo;s OPSS schema tables in the database - a process that can take several minutes.\nFurthermore, if you want to safely ensure that a restarted JRF domain can access updates to the infrastructure database that the domain made at an earlier time, the original domain\u0026rsquo;s wallet file must be safely saved as soon as practical, and the restarted domain must be supplied a wallet file that was obtained from a previous run of the domain.\nJRF Domain YAML file and model YAML file settings Here are the required Domain YAML file and model YAML file settings for Model in Image JRF domains:\n  Set configuration.model.domainType to JRF.\n  Set configuration.opss.walletPasswordSecret to reference a secret that defines a walletPassword key. This is used to encrypt the domain\u0026rsquo;s OPSS wallet file. This is a required field for JRF domains.\n  Set configuration.opss.walletFileSecret to reference a secret that contains your domain\u0026rsquo;s OPSS wallet file in its walletFile key. This assumes you have an OPSS wallet file from a previous start of the same domain. It enables a restarted or migrated domain to access its database information. This is an optional field for JRF domains, but must always be set if you want a restarted or migrated domain to access its database information.\n  Set the configuration.introspectorJobActiveDeadlineSeconds introspection job timeout to at least 600 seconds. This is in an optional field but is needed because domain home creation takes a considerable amount of time the first time a JRF domain is created (due to initializing the domain\u0026rsquo;s database tables), and because Model in Image creates your domain home for you using the introspection job.\n  Define an RCUDbInfo stanza in your model. Access to an database requires defining a RCUDbInfo stanza in your model\u0026rsquo;s domainInfo stanza with the necessary information for accessing the domain\u0026rsquo;s schema within the database. Usually this information should be supplied using a secret that you deploy and reference in your Domain YAML file\u0026rsquo;s configuration.secrets field. Here\u0026rsquo;s an example RCUDbInfo stanza:\ndomainInfo: RCUDbInfo: rcu_prefix: \u0026#39;@@SECRET:sample-domain1-rcu-access:rcu_prefix@@\u0026#39; rcu_schema_password: \u0026#39;@@SECRET:sample-domain1-rcu-access:rcu_schema_password@@\u0026#39; rcu_db_conn_string: \u0026#39;@@SECRET:sample-domain1-rcu-access:rcu_db_conn_string@@\u0026#39;   Saving and restoring JRF wallets It is important to save a JRF domain\u0026rsquo;s OPSS wallet password and wallet file so that you can restore them as needed. This ensures that a restart or migration of the domain can continue to access the domain\u0026rsquo;s FMW infrastructure database.\nWhen you deploy a JRF domain for the first time, the domain will add itself to its RCU database tables, and also create a \u0026lsquo;wallet\u0026rsquo; file in the domain\u0026rsquo;s home directory that enables access to the domain\u0026rsquo;s data in the RCU database. This wallet is encrypted using an OPSS key password that you supply to the domain using a Secret that is referenced by your Domain YAML file configuration.opss.walletPasswordSecret field.\nFor a domain that has been started by Model in Image, the operator will copy the wallet file from the domain home of a new JRF domain and store it in the domain\u0026rsquo;s introspector domain ConfigMap in file ewallet.p12. Here is how to export this wallet file from the introspector domain ConfigMap:\n  Option 1\n$ kubectl -n MY_DOMAIN_NAMESPACE \\  get configmap MY_DOMAIN_UID-weblogic-domain-introspect-cm \\  -o jsonpath=\u0026#39;{.data.ewallet\\.p12}\u0026#39; \\  \u0026gt; ewallet.p12   Option 2\nAlternatively, you can use the ./kubernetes/samples/scripts/create-weblogic-domain/model-in-image/utils/opss-wallet.sh -s command to export the wallet file (pass -? to this script\u0026rsquo;s command-line arguments and defaults).\n  Always back up your wallet file to a safe location that can be retrieved later. In addition, save your OPSS key password.\n To reuse the wallet:\n Create a secret with a key named walletPassword that contains the same OPSS password that you specified in the original domain. For example, assuming the password is welcome1: $ kubectl -n MY_DOMAIN_NAMESPACE \\  create secret generic MY_DOMAIN_UID-my-opss-wallet-password-secret \\  --from-literal=walletPassword=welcome1 $ kubectl -n MY_DOMAIN_NAMESPACE \\  label secret MY_DOMAIN_UID-my-opss-wallet-password-secret \\  weblogic.domainUID=sample-domain1  Create a secret with a key named walletFile that contains the OPSS wallet file that you exported above. For example, assuming the file is ewallet.p12: $ kubectl -n MY_DOMAIN_NAMESPACE \\  create secret generic MY_DOMAIN_UID-my-opss-wallet-file-secret \\  --from-file=walletFile=ewallet.p12 $ kubectl -n sample-domain1-ns \\  label secret MY_DOMAIN_UID-my-opss-wallet-file-secret \\  weblogic.domainUID=sample-domain1 Alternatively, you can use the ./kubernetes/samples/scripts/create-weblogic-domain/model-in-image/utils/opss-wallet.sh -r command to deploy a local wallet file as a secret (pass -? to get this script\u0026rsquo;s command-line arguments and defaults).\n Make sure that your Domain YAML file configuration.opss.walletPasswordSecret field names the OPSS password Secret, and make sure that your Domain YAML file configuration.opss.walletFileSecret field names the OPSS wallet file secret.  Instructions for changing a JRF domain\u0026rsquo;s database password Follow these steps to ensure that a JRF domain can continue to access its RCU data after changing its database password.\n  Before changing the database password, shut down all domains that access the database schema. For example, set their serverStartPolicy to NEVER.\n  Update the password in the database.\n  Update the Kubernetes Secret that contains your RCUDbInfo.rcu_schema_password for each domain.\n  Restart the domains. For example, change their serverStartPolicy from NEVER to IF_NEEDED.\n  Save your wallet files again, as changing your password generates a different wallet.\n  JRF references For an example of using JRF in combination with Model in Image, see the Model in Image sample.\nSee also, Specifying RCU connection information in the model in the WDT documentation.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/model-in-image/auxiliary-images/",
	"title": "Auxiliary images",
	"tags": [],
	"description": "Auxiliary images are an alternative approach for supplying a domain&#39;s model files or other types of files.",
	"content": "Contents  Introduction References Configuration  Auxiliary volumes and paths Auxiliary images Model in Image paths   Merge order  Expected merge order Performing replaces instead of merges Merge ordering example   Sample  Step 1: Prerequisites Step 2: Create the auxiliary image Step 3: Prepare and apply the domain resource Step 4: Invoke the web application    Introduction Auxiliary images are an alternative approach for including Model in Image model files, application archive files, WebLogic Deploying Tooling installation files, or other types of files, in your pods. This feature eliminates the need to provide these files in the image specified in domain.spec.image.\nInstead:\n The domain resource\u0026rsquo;s domain.spec.image directly references a base image that needs to include only a WebLogic installation and a Java installation. The domain resource\u0026rsquo;s auxiliary image related fields reference one or more smaller images that contain the desired Model in Image files. The domain resource\u0026rsquo;s domain.spec.configuration.model.wdtInstallHome and domain.spec.configuration.model.modelHome fields are set to reference a directory that contains the files from the smaller images.  The advantages of auxiliary images for Model In Image domains are:\n Use or patch a WebLogic installation image without needing to include a WDT installation, application archive, or model artifacts within the image. Share one WebLogic installation image with multiple different model configurations that are supplied in specific images. Distribute or update model files, application archives, and the WebLogic Deploy Tooling executable using specific images that do not contain a WebLogic installation.  Auxiliary images internally use a Kubernetes emptyDir volume and Kubernetes init containers to share files from additional images.\nReferences   Run the kubectl explain domain.spec.auxiliaryImageVolumes and kubectl explain domain.spec.serverPod.auxiliaryImages commands.\n  See the spec.auxiliaryImageVolumes and serverPod.auxiliaryImages sections in the domain resource schema and documentation.\n  Configuration This section describes a typical auxiliary image configuration for the Model in Image use case.\nAuxiliary volumes and paths A domain resource domain.spec.auxiliaryImageVolumes auxiliary image volume defines a mountPath, name, plus additional optional fields. The mountPath field is the location of a directory in an auxiliary image, and is also the location in the main pod container (which will automatically contain a recursive copy of the auxiliary image directory). The name field is arbitrary, and is in turn referenced by one or more auxiliary images that are defined separately using Auxiliary images configuration.\nFor example:\n spec: auxiliaryImageVolumes: - name: auxiliaryImageVolume1 mountPath: /auxiliary Auxiliary images One or more auxiliary images can be configured in a domain resource serverPod.auxiliaryImage array. Each array entry must define an image and volume where image is the name of an auxiliary image and the volume is the name of an auxiliary image volume as described previously. Optionally , you can also specify an imagePullPolicy, which defaults to Always if the image ends in :latest and to IfNotPresent, otherwise. Also, optionally, you can customize the command that is used to merge (copy) the auxiliary image\u0026rsquo;s files into the auxiliary image volume during pod startup (this is rarely needed, see Performing replaces instead of merges for an example). For details about each field, see the schema.\nA serverPod can be defined at the domain scope, which applies to every pod in the domain, plus the introspector job\u0026rsquo;s pod, at a specific WebLogic cluster\u0026rsquo;s scope, or at a specific WebLogic Server pod\u0026rsquo;s scope. Typically, the domain scope is the most applicable for the Model in Image use case; for example:\nspec: serverPod: auxiliaryImages: - image: model-in-image:v1 imagePullPolicy: IfNotPresent volume: auxiliaryImageVolume1  If image pull secrets are required for pulling auxiliary images, then the secrets must be referenced using domain.spec.imagePullSecrets.\n Model in Image paths For the Model In Image auxiliary image use case, you also need to configure the domain.spec.configuration.model.modelHome and domain.spec.configuration.model.wdtInstallHome attributes to specify the location of the domain\u0026rsquo;s WebLogic Deploy Tool (WDT) model files and the domain\u0026rsquo;s WDT installation. These default to /u01/wdt/models and /u01/wdt/weblogic-deploy respectively, and must be changed to specify a directory in domain.spec.auxiliaryImageVolumes.mountPath. For example:\n configuration: model: modelHome: \u0026quot;/auxiliary/models\u0026quot; wdtInstallHome: \u0026quot;/auxiliary/weblogic-deploy\u0026quot;  If multiple auxiliary images supply different versions of a WebLogic Deploy Tool installation to the same wdtInstallHome path, then it is recommended to ensure that the newer version completely replaces the older version instead of merges with it. See Performing replaces instead of merges.\n Merge order Refer to this section if you need to control the merge order of files from multiple auxiliary images that all reference the same volume, or if you want to customize the command that is used to copy the files from an auxiliary image into its volume. You can use command customization to force \u0026ldquo;replace\u0026rdquo; behavior instead of merge behavior.\nExpected merge order By default, the files from multiple auxiliary images that share the same volume are merged. Specifically:\n  Files from later images in the merge overwrite same named files from earlier images.\n  The contents of overlapping directories from multiple images are combined.\n  The expected merge order for auxiliary images that share the same auxiliary image volume is:\n  If you specify auxiliary images at different serverPod scopes, and they all share the same volume, then the files from the domain scope will be merged first, the cluster scope second, and the server scope last.\n  If you specify multiple auxiliary images at the same scope, and they all share the same volume, then the files will be merged in the order in which images appear under serverPod.auxiliaryImages.\n  Note: If the results of a merge yield two or more differently named domain.spec.configuration.model.ModelHome model files in an auxilliary image volume, then refer to Model file naming and loading order for the model files loading order.\nPerforming replaces instead of merges If multiple auxiliary images share the same volume and you prefer that a particular same named directory from a later image completely replaces a directory from a previous image instead of combining the two directories, then you can customize the command that the second image uses to populate the shared volume in order to force a replace.\nFor example, you can customize the the second image\u0026rsquo;s serverPod.auxiliaryImage.command field to first delete the directory that was already copied from the earlier image and then have it perform a normal copy:\n... spec: ... auxiliaryImageVolumes: - name: aivolume mountPath: /auxiliary ... serverPod: auxiliaryImages: - image: domain-image-A:v1 volume: aivolume - image: domain-image-B:v1 volume: aivolume # the following command replaces \u0026#39;mydir\u0026#39; instead of merging it: command: \u0026#39;rm -fr $TARGET_MOUNT_PATH/mydir; cp -R $COMMON_MOUNT_PATH/* $TARGET_MOUNT_PATH\u0026#39; Merge ordering example Assuming you have auxiliary images are defined at the domain, the WebLogic cluster, and the server scope (myserver is part of the mycluster):\n... spec: auxiliaryImageVolumes: - name: aivolume mountPath: /auxiliary serverPod: auxiliaryImages: - image: domain-image-A:v1 volume: aivolume - image: domain-image-B:v1 volume: aivolume cluster: - name: mycluster serverPod: auxiliaryImages: - image: cl-image-A:v1 volume: aivolume - image: cl-image-B:v1 volume: aivolume managedServers: - serverName: \u0026quot;myserver\u0026quot; serverPod: auxiliaryImages: - image: ms-image-A:v1 volume: aivolume Then the files from the images will be merged into their shared aivolume volume /auxiliary mount path in the following order:\ndomain-image-A:v1 (first) domain-image-B:v1 cl-image-A:v1 cl-image-B:v1 ms-image-A:v1 (last) Sample This sample demonstrates deploying a Model in Image domain that uses auxiliary images to supply the domain\u0026rsquo;s WDT model files, application archive ZIP files, and WDT installation in a small, separate container image.\nStep 1: Prerequisites   First, follow all of the steps in the Model in Image initial use case sample.\nThis will:\n Set up the operator and a namespace for the domain. Download a WebLogic Deploy Tool ZIP installation. Deploy a domain without auxiliary images.    Second, shut down the domain and wait for its pods to exit.\n You can use the wl-pod-wait.sh script to wait. For example, assuming that you have set up /tmp/mii-sample as your working directory: $ kubectl delete domain sample-domain1 -n sample-domain1-ns $ /tmp/mii-sample/utils/wl-pod-wait.sh -p 0     Step 2: Create the auxiliary image Follow these steps to create an auxiliary image containing Model In Image model files, application archives, and the WDT installation files:\n  Create a model ZIP application archive and place it in the same directory where the model YAML file and model properties files are already in place for the initial use case:\n$ rm -f /tmp/mii-sample/model-images/model-in-image__WLS-AI-v1/archive.zip $ cd /tmp/mii-sample/archives/archive-v1 $ zip -r /tmp/mii-sample/model-images/model-in-image__WLS-AI-v1/archive.zip wlsdeploy The rm -f command is included in case there\u0026rsquo;s an old version of the archive ZIP from a previous run of this sample.\n  Create a temporary directory for staging the auxiliary image\u0026rsquo;s files and cd to this directory:\n$ mkdir -p /tmp/mii-sample/ai-image/WLS-AI-v1 $ cd /tmp/mii-sample/ai-image/WLS-AI-v1 We call this directory WLS-AI-v1 to correspond with the image version tag that we plan to use for the auxiliary image.\n  Install WDT in the staging directory and remove its weblogic-deploy/bin/*.cmd files, which are not used in UNIX environments:\n$ unzip /tmp/mii-sample/model-images/weblogic-deploy.zip -d . $ rm ./weblogic-deploy/bin/*.cmd In a later step, we will specify a domain resource domain.spec.configuration.model.wdtInstallHome attribute that references this WDT installation directory.\nIf the weblogic-deploy.zip file is missing, then you have skipped a step in the prerequisites.\n  Create a models directory in the staging directory and copy the model YAML file, properties, and archive into it:\n$ mkdir ./models $ cp /tmp/mii-sample/model-images/model-in-image__WLS-AI-v1/model.10.yaml ./models $ cp /tmp/mii-sample/model-images/model-in-image__WLS-AI-v1/model.10.properties ./models $ cp /tmp/mii-sample/model-images/model-in-image__WLS-AI-v1/archive.zip ./models In a later step, we will specify a domain resource domain.spec.configuration.model.modelHome attribute that references this directory.\n  Run docker build using /tmp/mii-sample/ai-docker-file/Dockerfile to create your auxiliary image using a small busybox image as the base image.\n$ docker build -f /tmp/mii-sample/ai-docker-file/Dockerfile \\  --build-arg AUXILIARY_IMAGE_PATH=/auxiliary \\  --tag model-in-image:WLS-AI-v1 . See ./Dockerfile for an explanation of each build argument.\n  Click here to view the Dockerfile.   # Copyright (c) 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl. # This is a sample Dockerfile for supplying Model in Image model files # and a WDT installation in a small separate auxiliary # image. This is an alternative to supplying the files directly # in the domain resource `domain.spec.image` image. # AUXILIARY_IMAGE_PATH arg: # Parent location for Model in Image model and WDT installation files. # Must match domain resource 'domain.spec.auxiliaryImageVolumes.mountPath' # For model-in-image, the following two domain resource attributes can # be a directory in the mount path: # 1) 'domain.spec.configuration.model.modelHome' # 2) 'domain.spec.configuration.model.wdtInstallHome' # Default '/auxiliary'. # FROM busybox ARG AUXILIARY_IMAGE_PATH=/auxiliary ARG USER=oracle ARG USERID=1000 ARG GROUP=root ENV AUXILIARY_IMAGE_PATH=${AUXILIARY_IMAGE_PATH} RUN adduser -D -u ${USERID} -G $GROUP $USER COPY ./ ${AUXILIARY_IMAGE_PATH}/ RUN chown -R $USER:$GROUP ${AUXILIARY_IMAGE_PATH} USER $USER      After the image is created, it should have the WDT executables in /auxiliary/weblogic-deploy, and WDT model, property, and archive files in /auxiliary/models. You can run ls in the Docker image to verify this:\n$ docker run -it --rm model-in-image:WLS-AI-v1 ls -l /auxiliary total 8 drwxr-xr-x 1 oracle root 4096 Jun 1 21:53 models drwxr-xr-x 1 oracle root 4096 May 26 22:29 weblogic-deploy $ docker run -it --rm model-in-image:WLS-AI-v1 ls -l /auxiliary/models total 16 -rw-rw-r-- 1 oracle root 5112 Jun 1 21:52 archive.zip -rw-rw-r-- 1 oracle root 173 Jun 1 21:59 model.10.properties -rw-rw-r-- 1 oracle root 1515 Jun 1 21:59 model.10.yaml $ docker run -it --rm model-in-image:WLS-AI-v1 ls -l /auxiliary/weblogic-deploy total 28 -rw-r----- 1 oracle root 4673 Oct 22 2019 LICENSE.txt -rw-r----- 1 oracle root 30 May 25 11:40 VERSION.txt drwxr-x--- 1 oracle root 4096 May 26 22:29 bin drwxr-x--- 1 oracle root 4096 May 25 11:40 etc drwxr-x--- 1 oracle root 4096 May 25 11:40 lib drwxr-x--- 1 oracle root 4096 Jan 22 2019 samples   Step 3: Prepare and apply the domain resource Copy the following to a file called /tmp/mii-sample/mii-initial.yaml or similar, or you can directly use the file /tmp/mii-sample/domain-resources/WLS-AI/mii-initial-d1-WLS-AI-v1.yaml that is included in the sample source.\n  Click here to view the WLS Domain YAML file using auxiliary images.   # Copyright (c) 2021, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. # # This is an example of how to define a Domain resource. # apiVersion: \u0026#34;weblogic.oracle/v8\u0026#34; kind: Domain metadata: name: sample-domain1 namespace: sample-domain1-ns labels: weblogic.domainUID: sample-domain1 spec: # Set to \u0026#39;FromModel\u0026#39; to indicate \u0026#39;Model in Image\u0026#39;. domainHomeSourceType: FromModel # The WebLogic Domain Home, this must be a location within # the image for \u0026#39;Model in Image\u0026#39; domains. domainHome: /u01/domains/sample-domain1 # The WebLogic Server image that the Operator uses to start the domain image: \u0026#34;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#34; # Defaults to \u0026#34;Always\u0026#34; if image tag (version) is \u0026#39;:latest\u0026#39; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; # Identify which Secret contains the credentials for pulling an image #imagePullSecrets: #- name: regsecret # Identify which Secret contains the WebLogic Admin credentials, # the secret must contain \u0026#39;username\u0026#39; and \u0026#39;password\u0026#39; fields. webLogicCredentialsSecret: name: sample-domain1-weblogic-credentials # Whether to include the WebLogic Server stdout in the pod\u0026#39;s stdout, default is true includeServerOutInPodLog: true # Whether to enable overriding your log file location, see also \u0026#39;logHome\u0026#39; #logHomeEnabled: false # The location for domain log, server logs, server out, introspector out, and Node Manager log files # see also \u0026#39;logHomeEnabled\u0026#39;, \u0026#39;volumes\u0026#39;, and \u0026#39;volumeMounts\u0026#39;. #logHome: /shared/logs/sample-domain1 # Set which WebLogic Servers the Operator will start # - \u0026#34;NEVER\u0026#34; will not start any server in the domain # - \u0026#34;ADMIN_ONLY\u0026#34; will start up only the administration server (no managed servers will be started) # - \u0026#34;IF_NEEDED\u0026#34; will start all non-clustered servers, including the administration server, and clustered servers up to their replica count. serverStartPolicy: \u0026#34;IF_NEEDED\u0026#34; # Settings for auxiliary image volume(s), see also \u0026#39;serverPod.auxiliaryImages\u0026#39;. auxiliaryImageVolumes: - name: auxiliaryImageVolume1 mountPath: \u0026#34;/auxiliary\u0026#34; # Settings for all server pods in the domain including the introspector job pod serverPod: # Optional new or overridden environment variables for the domain\u0026#39;s pods # - This sample uses CUSTOM_DOMAIN_NAME in its image model file # to set the WebLogic domain name env: - name: CUSTOM_DOMAIN_NAME value: \u0026#34;domain1\u0026#34; - name: JAVA_OPTIONS value: \u0026#34;-Dweblogic.StdoutDebugEnabled=false\u0026#34; - name: USER_MEM_ARGS value: \u0026#34;-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx512m \u0026#34; resources: requests: cpu: \u0026#34;250m\u0026#34; memory: \u0026#34;768Mi\u0026#34; # Auxiliary image(s) containing WDT model, archives and install. See also: # \u0026#39;spec.auxiliaryImageVolumes\u0026#39;. # \u0026#39;spec.configuration.model.modelHome\u0026#39; # \u0026#39;spec.configuration.model.wdtInstallHome\u0026#39; auxiliaryImages: - image: \u0026#34;model-in-image:WLS-AI-v1\u0026#34; imagePullPolicy: IfNotPresent volume: auxiliaryImageVolume1 # Optional volumes and mounts for the domain\u0026#39;s pods. See also \u0026#39;logHome\u0026#39;. #volumes: #- name: weblogic-domain-storage-volume # persistentVolumeClaim: # claimName: sample-domain1-weblogic-sample-pvc #volumeMounts: #- mountPath: /shared # name: weblogic-domain-storage-volume # The desired behavior for starting the domain\u0026#39;s administration server. adminServer: # The serverStartState legal values are \u0026#34;RUNNING\u0026#34; or \u0026#34;ADMIN\u0026#34; # \u0026#34;RUNNING\u0026#34; means the listed server will be started up to \u0026#34;RUNNING\u0026#34; mode # \u0026#34;ADMIN\u0026#34; means the listed server will be start up to \u0026#34;ADMIN\u0026#34; mode serverStartState: \u0026#34;RUNNING\u0026#34; # Setup a Kubernetes node port for the administration server default channel #adminService: # channels: # - channelName: default # nodePort: 30701 # The number of managed servers to start for unlisted clusters replicas: 1 # The desired behavior for starting a specific cluster\u0026#39;s member servers clusters: - clusterName: cluster-1 serverStartState: \u0026#34;RUNNING\u0026#34; serverPod: # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not # already members of the same cluster. affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026#34;weblogic.clusterName\u0026#34; operator: In values: - $(CLUSTER_NAME) topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; # The number of managed servers to start for this cluster replicas: 2 # Change the restartVersion to force the introspector job to rerun # and apply any new model configuration, to also force a subsequent # roll of your domain\u0026#39;s WebLogic Server pods. restartVersion: \u0026#39;1\u0026#39; # Changes to this field cause the operator to repeat its introspection of the # WebLogic domain configuration. introspectVersion: \u0026#39;1\u0026#39; configuration: # Settings for domainHomeSourceType \u0026#39;FromModel\u0026#39; model: # Valid model domain types are \u0026#39;WLS\u0026#39;, \u0026#39;JRF\u0026#39;, and \u0026#39;RestrictedJRF\u0026#39;, default is \u0026#39;WLS\u0026#39; domainType: \u0026#34;WLS\u0026#34; modelHome: \u0026#34;/auxiliary/models\u0026#34; wdtInstallHome: \u0026#34;/auxiliary/weblogic-deploy\u0026#34; # Optional configmap for additional models and variable files #configMap: sample-domain1-wdt-config-map # All \u0026#39;FromModel\u0026#39; domains require a runtimeEncryptionSecret with a \u0026#39;password\u0026#39; field runtimeEncryptionSecret: sample-domain1-runtime-encryption-secret # Secrets that are referenced by model yaml macros # (the model yaml in the optional configMap or in the image) #secrets: #- sample-domain1-datasource-secret    You can compare this domain resource YAML file with the domain resource YAML file from the original initial use case (/tmp/mii-sample/domain-resources/WLS/mii-initial-d1-WLS-v1.yaml) to see the changes required for auxiliary images. For example:\n$ diff /tmp/mii-sample/domain-resources/WLS-AI/mii-initial-d1-WLS-AI-v1.yaml /tmp/mii-sample/domain-resources/WLS/mii-initial-d1-WLS-v1.yaml 1c1 \u0026lt; # Copyright (c) 2021, Oracle and/or its affiliates. --- \u0026gt; # Copyright (c) 2020, 2021, Oracle and/or its affiliates. 23c23 \u0026lt; image: \u0026quot;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026quot; --- \u0026gt; image: \u0026quot;model-in-image:WLS-v1\u0026quot; 53,57d52 \u0026lt; # Settings for auxiliary image volume(s), see also 'serverPod.auxiliaryImages'. \u0026lt; auxiliaryImageVolumes: \u0026lt; - name: auxiliaryImageVolume1 \u0026lt; mountPath: \u0026quot;/auxiliary\u0026quot; \u0026lt; 75,83d69 \u0026lt; # Auxiliary image(s) containing WDT model, archives and install. See also: \u0026lt; # 'spec.auxiliaryImageVolumes'. \u0026lt; # 'spec.configuration.model.modelHome' \u0026lt; # 'spec.configuration.model.wdtInstallHome' \u0026lt; auxiliaryImages: \u0026lt; - image: \u0026quot;model-in-image:WLS-AI-v1\u0026quot; \u0026lt; imagePullPolicy: IfNotPresent \u0026lt; volume: auxiliaryImageVolume1 \u0026lt; 145,146d130 \u0026lt; modelHome: \u0026quot;/auxiliary/models\u0026quot; \u0026lt; wdtInstallHome: \u0026quot;/auxiliary/weblogic-deploy\u0026quot; Run the following command to deploy the domain custom resource:\n$ kubectl apply -f /tmp/mii-sample/domain-resources/WLS-AI/mii-initial-d1-WLS-AI-v1.yaml Note: If you are choosing not to use the predefined Domain YAML file and instead created your own Domain YAML file earlier, then substitute your custom file name in the previous command. Previously, we suggested naming it /tmp/mii-sample/mii-initial.yaml.\nNow, if you run kubectl get pods -n sample-domain1-ns --watch, then you will see the introspector job run and your WebLogic Server pods start. The output will look something like this:\n  Click here to expand.   $ kubectl get pods -n sample-domain1-ns --watch NAME READY STATUS RESTARTS AGE sample-domain1-introspector-z5vmp 0/1 Pending 0 0s sample-domain1-introspector-z5vmp 0/1 Pending 0 0s sample-domain1-introspector-z5vmp 0/1 Init:0/1 0 0s sample-domain1-introspector-z5vmp 0/1 PodInitializing 0 2s sample-domain1-introspector-z5vmp 1/1 Running 0 3s sample-domain1-introspector-z5vmp 0/1 Completed 0 71s sample-domain1-admin-server 0/1 Pending 0 0s sample-domain1-admin-server 0/1 Pending 0 0s sample-domain1-admin-server 0/1 Init:0/1 0 0s sample-domain1-introspector-z5vmp 0/1 Terminating 0 71s sample-domain1-introspector-z5vmp 0/1 Terminating 0 71s sample-domain1-admin-server 0/1 PodInitializing 0 2s sample-domain1-admin-server 0/1 Running 0 3s sample-domain1-admin-server 1/1 Running 0 41s sample-domain1-managed-server1 0/1 Pending 0 0s sample-domain1-managed-server1 0/1 Pending 0 0s sample-domain1-managed-server1 0/1 Init:0/1 0 0s sample-domain1-managed-server2 0/1 Pending 0 0s sample-domain1-managed-server2 0/1 Pending 0 0s sample-domain1-managed-server2 0/1 Init:0/1 0 0s sample-domain1-managed-server2 0/1 Init:0/1 0 1s sample-domain1-managed-server1 0/1 Init:0/1 0 1s sample-domain1-managed-server1 0/1 PodInitializing 0 2s sample-domain1-managed-server2 0/1 PodInitializing 0 2s sample-domain1-managed-server2 0/1 Running 0 3s sample-domain1-managed-server1 0/1 Running 0 3s sample-domain1-managed-server2 1/1 Running 0 39s sample-domain1-managed-server1 1/1 Running 0 43s    Alternatively, you can run /tmp/mii-sample/utils/wl-pod-wait.sh -p 3. This utility script exits successfully when the designated number of WebLogic Server pods reach a ready state and have restartVersion, introspectVersion, spec.image, and spec.serverPod.auxiliaryImages.image values that match their corresponding values in their domain resource.\n  Click here to display the `wl-pod-wait.sh` usage.   $ ./wl-pod-wait.sh -? Usage: wl-pod-wait.sh [-n mynamespace] [-d mydomainuid] \\ [-p expected_pod_count] \\ [-t timeout_secs] \\ [-q] Exits non-zero if \u0026#39;timeout_secs\u0026#39; is reached before \u0026#39;pod_count\u0026#39; is reached. Parameters: -d \u0026lt;domain_uid\u0026gt; : Defaults to \u0026#39;sample-domain1\u0026#39;. -n \u0026lt;namespace\u0026gt; : Defaults to \u0026#39;sample-domain1-ns\u0026#39;. -p 0 : Wait until there are no running WebLogic Server pods for a domain. The default. -p \u0026lt;pod_count\u0026gt; : Wait until all of the following are true for exactly \u0026#39;pod_count\u0026#39; WebLogic Server pods in the domain: - ready - same \u0026#39;weblogic.domainRestartVersion\u0026#39; label value as the domain resource\u0026#39;s \u0026#39;spec.restartVersion\u0026#39; - same \u0026#39;weblogic.introspectVersion\u0026#39; label value as the domain resource\u0026#39;s \u0026#39;spec.introspectVersion\u0026#39; - same image as the domain resource\u0026#39;s \u0026#39;spec.image\u0026#39; - same auxiliary images as the domain resource\u0026#39;s \u0026#39;spec.serverPod.auxiliaryImages\u0026#39; -t \u0026lt;timeout\u0026gt; : Timeout in seconds. Defaults to \u0026#39;1000\u0026#39;. -q : Quiet mode. Show only a count of wl pods that have reached the desired criteria. -? : This help.      Click here to view sample output from `wl-pod-wait.sh`.   @@ [2021-06-14T20:35:35][seconds=0] Info: Waiting up to 1000 seconds for exactly \u0026#39;3\u0026#39; WebLogic Server pods to reach the following criteria: @@ [2021-06-14T20:35:35][seconds=0] Info: ready=\u0026#39;true\u0026#39; @@ [2021-06-14T20:35:35][seconds=0] Info: image=\u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; @@ [2021-06-14T20:35:35][seconds=0] Info: auxiliaryImages=\u0026#39;model-in-image:WLS-AI-v1\u0026#39; @@ [2021-06-14T20:35:35][seconds=0] Info: domainRestartVersion=\u0026#39;1\u0026#39; @@ [2021-06-14T20:35:35][seconds=0] Info: introspectVersion=\u0026#39;1\u0026#39; @@ [2021-06-14T20:35:35][seconds=0] Info: namespace=\u0026#39;sample-domain1-ns\u0026#39; @@ [2021-06-14T20:35:35][seconds=0] Info: domainUID=\u0026#39;sample-domain1\u0026#39; @@ [2021-06-14T20:35:35][seconds=0] Info: \u0026#39;0\u0026#39; WebLogic Server pods currently match all criteria, expecting \u0026#39;3\u0026#39;. @@ [2021-06-14T20:35:35][seconds=0] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME RVER IVER IMAGE CMIMAGES READY PHASE ---- ---- ---- ----- -------- ----- ----- \u0026#39;sample-domain1-introspector-wz8q6\u0026#39; \u0026#39;\u0026#39; \u0026#39;\u0026#39; \u0026#39;\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;\u0026#39; \u0026#39;Pending\u0026#39; @@ [2021-06-14T20:35:39][seconds=4] Info: \u0026#39;0\u0026#39; WebLogic Server pods currently match all criteria, expecting \u0026#39;3\u0026#39;. @@ [2021-06-14T20:35:39][seconds=4] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME RVER IVER IMAGE CMIMAGES READY PHASE ---- ---- ---- ----- -------- ----- ----- \u0026#39;sample-domain1-introspector-wz8q6\u0026#39; \u0026#39;\u0026#39; \u0026#39;\u0026#39; \u0026#39;\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;\u0026#39; \u0026#39;Running\u0026#39; @@ [2021-06-14T20:36:51][seconds=76] Info: \u0026#39;0\u0026#39; WebLogic Server pods currently match all criteria, expecting \u0026#39;3\u0026#39;. @@ [2021-06-14T20:36:51][seconds=76] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME RVER IVER IMAGE CMIMAGES READY PHASE ---- ---- ---- ----- -------- ----- ----- \u0026#39;sample-domain1-admin-server\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;false\u0026#39; \u0026#39;Pending\u0026#39; @@ [2021-06-14T20:36:55][seconds=80] Info: \u0026#39;0\u0026#39; WebLogic Server pods currently match all criteria, expecting \u0026#39;3\u0026#39;. @@ [2021-06-14T20:36:55][seconds=80] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME RVER IVER IMAGE CMIMAGES READY PHASE ---- ---- ---- ----- -------- ----- ----- \u0026#39;sample-domain1-admin-server\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;false\u0026#39; \u0026#39;Running\u0026#39; @@ [2021-06-14T20:37:34][seconds=119] Info: \u0026#39;1\u0026#39; WebLogic Server pods currently match all criteria, expecting \u0026#39;3\u0026#39;. @@ [2021-06-14T20:37:34][seconds=119] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME RVER IVER IMAGE CMIMAGES READY PHASE ---- ---- ---- ----- -------- ----- ----- \u0026#39;sample-domain1-admin-server\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;true\u0026#39; \u0026#39;Running\u0026#39; \u0026#39;sample-domain1-managed-server1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;false\u0026#39; \u0026#39;Pending\u0026#39; \u0026#39;sample-domain1-managed-server2\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;false\u0026#39; \u0026#39;Pending\u0026#39; @@ [2021-06-14T20:37:35][seconds=120] Info: \u0026#39;1\u0026#39; WebLogic Server pods currently match all criteria, expecting \u0026#39;3\u0026#39;. @@ [2021-06-14T20:37:35][seconds=120] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME RVER IVER IMAGE CMIMAGES READY PHASE ---- ---- ---- ----- -------- ----- ----- \u0026#39;sample-domain1-admin-server\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;true\u0026#39; \u0026#39;Running\u0026#39; \u0026#39;sample-domain1-managed-server1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;false\u0026#39; \u0026#39;Running\u0026#39; \u0026#39;sample-domain1-managed-server2\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;false\u0026#39; \u0026#39;Pending\u0026#39; @@ [2021-06-14T20:37:37][seconds=122] Info: \u0026#39;1\u0026#39; WebLogic Server pods currently match all criteria, expecting \u0026#39;3\u0026#39;. @@ [2021-06-14T20:37:37][seconds=122] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME RVER IVER IMAGE CMIMAGES READY PHASE ---- ---- ---- ----- -------- ----- ----- \u0026#39;sample-domain1-admin-server\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;true\u0026#39; \u0026#39;Running\u0026#39; \u0026#39;sample-domain1-managed-server1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;false\u0026#39; \u0026#39;Running\u0026#39; \u0026#39;sample-domain1-managed-server2\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;false\u0026#39; \u0026#39;Running\u0026#39; @@ [2021-06-14T20:38:18][seconds=163] Info: \u0026#39;2\u0026#39; WebLogic Server pods currently match all criteria, expecting \u0026#39;3\u0026#39;. @@ [2021-06-14T20:38:18][seconds=163] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME RVER IVER IMAGE CMIMAGES READY PHASE ---- ---- ---- ----- -------- ----- ----- \u0026#39;sample-domain1-admin-server\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;true\u0026#39; \u0026#39;Running\u0026#39; \u0026#39;sample-domain1-managed-server1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;false\u0026#39; \u0026#39;Running\u0026#39; \u0026#39;sample-domain1-managed-server2\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;true\u0026#39; \u0026#39;Running\u0026#39; @@ [2021-06-14T20:38:20][seconds=165] Info: \u0026#39;3\u0026#39; WebLogic Server pods currently match all criteria, expecting \u0026#39;3\u0026#39;. @@ [2021-06-14T20:38:20][seconds=165] Info: Introspector and WebLogic Server pods with same namespace and domain-uid: NAME RVER IVER IMAGE CMIMAGES READY PHASE ---- ---- ---- ----- -------- ----- ----- \u0026#39;sample-domain1-admin-server\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;true\u0026#39; \u0026#39;Running\u0026#39; \u0026#39;sample-domain1-managed-server1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;true\u0026#39; \u0026#39;Running\u0026#39; \u0026#39;sample-domain1-managed-server2\u0026#39; \u0026#39;1\u0026#39; \u0026#39;1\u0026#39; \u0026#39;container-registry.oracle.com/middleware/weblogic:12.2.1.4\u0026#39; \u0026#39;model-in-image:WLS-AI-v1\u0026#39; \u0026#39;true\u0026#39; \u0026#39;Running\u0026#39; @@ [2021-06-14T20:38:20][seconds=165] Info: Success!    If you see an error, then consult Debugging in the Model in Image user guide.\nStep 4: Invoke the web application To invoke the web application, follow the same steps as described in the Invoke the web application section of the initial use case.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/model-in-image/model-files/",
	"title": "Model files",
	"tags": [],
	"description": "Model file requirements, macros, and loading order.",
	"content": "Contents  Introduction Sample model file Important notes about Model in Image model files Model file naming and loading order Model file macros  Using secrets in model files Using environment variables in model files Combining secrets and environment variables in model files    Introduction This document describes basic Model in Image model file syntax, naming, and macros. For additional information, see the WebLogic Deploy Tool documentation.\nThe WDT Discover Domain Tool is particularly useful for generating model files from an existing domain home.\n Sample model file Here\u0026rsquo;s an example of a model YAML file that defines a WebLogic Server Administration Server and dynamic cluster.\ndomainInfo: AdminUserName: \u0026#39;@@SECRET:__weblogic-credentials__:username@@\u0026#39; AdminPassword: \u0026#39;@@SECRET:__weblogic-credentials__:password@@\u0026#39; ServerStartMode: \u0026#39;prod\u0026#39; topology: Name: \u0026#39;@@ENV:DOMAIN_UID@@\u0026#39; AdminServerName: \u0026#34;admin-server\u0026#34; Cluster: \u0026#34;cluster-1\u0026#34;: DynamicServers: ServerTemplate: \u0026#34;cluster-1-template\u0026#34; ServerNamePrefix: \u0026#34;managed-server\u0026#34; DynamicClusterSize: 5 MaxDynamicClusterSize: 5 CalculatedListenPorts: false Server: \u0026#34;admin-server\u0026#34;: ListenPort: 7001 ServerTemplate: \u0026#34;cluster-1-template\u0026#34;: Cluster: \u0026#34;cluster-1\u0026#34; ListenPort: 8001 This sample model file:\n Includes a WebLogic credentials stanza that is required by Model in Image. Derives its domain name from the predefined environment variable DOMAIN_UID, but note that this is not required.  For a description of model file macro references to secrets and environment variables, see Model file macros.\nImportant notes about Model in Image model files   Using model file macros\n  You can use model macros to reference arbitrary secrets from model files. This is recommended for handling mutable values such as database user names, passwords, and URLs. See Using secrets in model files.\n  All password fields in a model should use a secret macro. Passwords should not be directly included in property or model files because the files may appear in logs or debugging.\n  Model files encrypted with the WDT Encrypt Model Tool are not supported. Use secrets instead.\n    You can use model macros to reference arbitrary environment variables from model files. This is useful for handling plain text mutable values that you can define using an env stanza in your Domain YAML file, and is also useful for accessing the built in DOMAIN_UID environment variable. See Using environment variables in model files.\n  For most models, it\u0026rsquo;s useful to minimize or eliminate the usage of model variable files (also known as property files) and use secrets or environment variables instead.\n    A model must contain a domainInfo stanza that references your WebLogic administrative credentials. You can use the @@SECRET macro with the reserved secret name __weblogic-credentials__ to reference your Domain YAML file\u0026rsquo;s WebLogic credentials secret for this purpose. For example:\ndomainInfo: AdminUserName: \u0026#39;@@SECRET:__weblogic-credentials__:username@@\u0026#39; AdminPassword: \u0026#39;@@SECRET:__weblogic-credentials__:password@@\u0026#39;   A JRF domain type model must contain a domainInfo.RCUDbInfo stanza; see Requirements for JRF domain types.\n  You can control the order that WDT uses to load your model files, see Model file naming and loading order.\n  Model file naming and loading order Refer to this section if you need to control the order in which your model files are loaded. The order is important when two or more model files refer to the same configuration, because the last model that\u0026rsquo;s loaded has the highest precedence.\nDuring domain home creation, model and property files are first loaded from the configuration.models.modelHome directory within the image, which defaults to /u01/wdt/models. After the modelHome files are all loaded, the domain home creation then loads files from the optional WDT ConfigMap, described in Optional WDT model ConfigMap. If a modelHome file and ConfigMap file both have the same name, then both files are loaded.\nThe loading order within each of these locations is first determined using the convention filename.##.yaml and filename.##.properties, where ## are digits that specify the desired order when sorted numerically. Additional details:\n Embedding a .##. in a file name is optional.  When present, it must be placed just before the properties or yaml extension in order for it to take precedence over alphabetical precedence. The precedence of file names that include more than one .##. is undefined. The number can be any integer greater than or equal to zero.   File names that don\u0026rsquo;t include .##. sort before other files as if they implicitly have the lowest possible .##. If two files share the same number, the loading order is determined alphabetically as a tie-breaker.   Note: If configuration.models.modelHome files are supplied by combining multiple Auxiliary images, then the files in this directory are populated according to their Auxiliary image merge order before the loading order is determined.\n For example, if you have these files in the model home directory /u01/wdt/models:\njdbc.20.yaml main-model.10.yaml my-model.10.yaml y.yaml And you have these files in the ConfigMap:\njdbc-dev-urlprops.10.yaml z.yaml Then the combined model files list is passed to the WebLogic Deploy Tool as:\ny.yaml,main-model.10.yaml,my-model.10.yaml,jdbc.20.yaml,z.yaml,jdbc-dev-urlprops.10.yaml Property files (ending in .properties) use the same sorting algorithm, but they are appended together into a single file prior to passing them to the WebLogic Deploy Tool.\nModel file macros WDT models can have macros that reference secrets or environment variables.\nUsing secrets in model files You can use WDT model @@SECRET macros to reference the WebLogic administrator username and password keys that are stored in a Kubernetes Secret and to optionally reference additional secrets. Here is the macro pattern for accessing these secrets:\n   Domain Resource Attribute Corresponding WDT Model @@SECRET Macro     webLogicCredentialsSecret @@SECRET:__weblogic-credentials__:username@@ and @@SECRET:__weblogic-credentials__:password@@   configuration.secrets @@SECRET:mysecret:mykey@@    For example, you can reference the WebLogic credential user name using @@SECRET:__weblogic-credentials__:username@@, and you can reference a custom secret mysecret with key mykey using @@SECRET:mysecret:mykey@@.\nAny secrets that are referenced by an @@SECRET macro must be deployed to the same namespace as your Domain, and must be referenced in your Domain YAML file using the weblogicCredentialsSecret and configuration.secrets fields.\nHere\u0026rsquo;s a sample snippet from a Domain YAML file that sets a webLogicCredentialsSecret and two custom secrets my-custom-secret1 and my-custom-secret2.\n... spec: webLogicCredentialsSecret: name: my-weblogic-credentials-secret configuration: secrets: [ my-custom-secret1,my-custom-secret2 ] ... Using environment variables in model files You can reference operator environment variables in model files. This includes any that you define yourself in your Domain YAML file using domain.spec.serverPod.env or domain.spec.adminServer.serverPod.env, or the built-in DOMAIN_UID environment variable.\nFor example, the @@ENV:DOMAIN_UID@@ macro resolves to the current domain\u0026rsquo;s domain UID.\nCombining secrets and environment variables in model files You can embed an environment variable macro in a secret macro. This is useful for referencing secrets that you\u0026rsquo;ve named based on your domain\u0026rsquo;s domainUID.\nFor example, if your domainUID is domain1, then the macro @@SECRET:@@ENV:DOMAIN_UID@@-super-double-secret:mykey@@ resolves to the value stored in mykey for secret domain1-super-double-secret.\n"
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/model-in-image/runtime-updates/",
	"title": "Runtime updates",
	"tags": [],
	"description": "Updating a running Model in Image domain&#39;s images and model files.",
	"content": "Contents  Overview Supported updates Unsupported updates Updating an existing model Offline updates  Offline update steps Offline update sample   Online updates  Online update steps Online update handling of non-dynamic WebLogic configuration changes Online update handling of deletes Online update status and labels Online update scenarios Online update sample   Appendices  Using the WDT Discover and Compare Model Tools below. Changing a Domain restartVersion or introspectVersion    Overview If you want to make a WebLogic domain home configuration update to a running Model in Image domain, and you want the update to survive WebLogic Server pod restarts, then you must modify your existing model and instruct the WebLogic Kubernetes Operator to propagate the change.\nIf instead you make a direct runtime WebLogic configuration update of a Model in Image domain using the WebLogic Server Administration Console or WLST scripts, then the update will be ephemeral. This is because a Model in Image domain home is regenerated from the model on every pod restart.\nThere are two approaches for propagating model updates to a running Model in Image domain without first shutting down the domain:\n  Online updates: If model changes are configured to fully dynamic configuration MBean attributes, then you can optionally propagate changes to WebLogic pods without a roll using an online update. If an online update request includes non-dynamic model updates that can only be achieved using an offline update, then the resulting behavior is controlled by the domain resource YAML domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges attribute, which is discussed in detail later in this document.\n  Offline updates: Offline updates are propagated to WebLogic pods by updating your model and then initiating a domain roll, which generates a new domain configuration, restarts the domain\u0026rsquo;s WebLogic Administration Server with the updated configuration, and then restarts the other pods in the cluster.\n  The operator does not support all types of WebLogic configuration changes while a domain is still running. If a change is unsupported for an online or offline update, then propagating the change requires entirely shutting domain the domain, applying the change, and finally restarting the domain. Full domain restarts are discussed in Full domain restarts. Supported and unsupported changes are discussed in Supported and unsupported updates.\nIt is the administrator\u0026rsquo;s responsibility to make the necessary changes to a domain resource in order to initiate the correct approach for an update.\nCustom configuration overrides, which are WebLogic configuration overrides specified using a domain resource YAML file configuration.overridesConfigMap, as described in Configuration overrides, are not supported in combination with Model in Image. Model in Image will generate an error if custom overrides are specified. This should not be a concern because model file, secret, or model image updates are simpler and more flexible than custom configuration override updates. Unlike configuration overrides, the syntax for a model file update exactly matches the syntax for specifying your model file originally.\n Supported updates The following updates are supported for offline or online updates, except when they reference an area that is specifically documented as unsupported below:\n  You can add a new WebLogic cluster or standalone server.\n  You can increase the size of a dynamic WebLogic cluster.\n  You can add new MBeans or resources by specifying their corresponding model YAML file snippet along with their parent bean hierarchy. For example, you can add a data source.\n  You can change or add MBean attributes by specifying a YAML file snippet along with its parent bean hierarchy that references an existing MBean and the attribute. For example, to add or alter the maximum capacity of a data source named mynewdatasource:\nresources: JDBCSystemResource: mynewdatasource: JdbcResource: JDBCConnectionPoolParams: MaxCapacity: 5 For more information, see Using Multiple Models in the WebLogic Deploy Tooling documentation.\n  You can change or add secrets that your model macros reference (macros that use the @@SECRET:secretname:secretkey@@ syntax). For example, you can change a database password secret.\n  For offline updates only, you can change or add environment variables that your model macros reference (macros that use the @@ENV:myenvvar@@ syntax).\n  You can remove an MBean, application deployment, or resource by omitting any reference to it in your image model files and WDT config map. You can also remove a named MBean, application deployment, or resource by specifying an additional model file with an exclamation point (!) just before its name plus ensuring the new model file is loaded after the original model file that contains the original named configuration. For example, if you have a data source named mynewdatasource defined in your model, then it can be removed by specifying a small model file that loads after the model file that defines the data source, where the small model file looks like this:\nresources: JDBCSystemResource: !mynewdatasource: There are some exceptions for online updates.\nFor more information, see Declaring Named MBeans to Delete in the WebLogic Deploying Tooling documentation.\n  Unsupported updates It is important to avoid applying unsupported model updates to a running domain. An attempt to use an unsupported update may not always result in a clear error message, and the expected behavior may be undefined. If you need to make an unsupported update and no workaround is documented, then shut down your domain entirely before making the change. See Full domain restarts.\n The following summarizes the types of runtime update configuration that are not supported in this release of Model in Image unless a workaround or alternative is documented:\n Decreasing dynamic cluster size (see detailed discussion below for an alternative) Adding WebLogic Servers to a configured cluster or removing them Default and custom network channel configuration for an existing WebLogic cluster or server. Specifically:  Adding or removing Network Access Points (custom channels) for existing servers Changing a Default, SSL, Admin, or custom channel, Enabled, listen address, protocol, or port   Node Manager related configuration Log related settings (see the detailed discussion below for when this applies) Changing the domain name Deleting an MBean attribute (see the detailed discussion below for workaround) Changing any existing MBean name (see the detailed discussion below for workaround) Embedded LDAP entries (see detailed discussion below for alternatives) Any Model YAML topology: stanza changes Dependency deletion in combination with online updates Various security related changes in combination with online updates  Here is a detailed discussion of each unsupported runtime update and a description of workarounds and alternatives when applicable:\n  There is no way to directly delete an attribute from an MBean that\u0026rsquo;s already been specified by a model file. The workaround is to do this using two model files: add a model file that deletes the named bean/resource that is a parent to the attribute you want to delete using the ! syntax as described above, and add another model file that will be loaded after the first one, which fully defines the named bean/resource but without the attribute you want to delete.\n  There is no way to directly change the MBean name of an attribute. Instead, you can remove a named MBean using the ! syntax as described in Supported Updates, and then add a new one as a replacement.\n  You cannot change the domain name at runtime.\n  You have a limited ability to change an existing WebLogic cluster\u0026rsquo;s membership. Specifically, do not apply runtime updates for:\n Adding WebLogic Servers to a configured cluster. As an alternative, consider using dynamic clusters instead of configured clusters. Removing WebLogic Servers from a configured cluster. As an alternative, you can lower your cluster\u0026rsquo;s domain resource YAML replicas attribute. Decreasing the size of a dynamic cluster. As an alternative, you can lower your cluster\u0026rsquo;s domain resource YAML replicas attribute.    You cannot change, add, or remove network listen address, port, protocol, and enabled configuration for existing clusters or servers at runtime.\nSpecifically, do not apply runtime updates for:\n A Default, SSL, Admin channel Enabled, listen address, or port. A Network Access Point (custom channel) Enabled, listen address, protocol, or port.  Note that it is permitted to override network access point public or external addresses and ports. External access to JMX (MBean) or online WLST requires that the network access point internal port and external port match (external T3 or HTTP tunneling access to JMS, RMI, or EJBs don\u0026rsquo;t require port matching).\nDue to security considerations, we strongly recommend that T3 or any RMI protocol should not be exposed outside the cluster.\n   Changing, adding, or removing server and domain log related settings in an MBean at runtime when the domain resource is configured to override the same MBeans using the spec.logHome, spec.logHomeEnabled, or spec.httpAccessLogInLogHome attributes.\n  Embedded LDAP security entries for users, groups, roles, and credential mappings. For example, you cannot add a user to the default security realm. Online update attempts in this area will fail during the introspector job, and offline update attempts may result in inconsistent security checks during the offline update\u0026rsquo;s rolling cycle. If you need to make these kinds of updates, then shut down your domain entirely before making the change, or switch to an external security provider.\n  Any Model YAML topology: stanza changes, for example, ConsoleEnabled, RootDirectory, AdminServerName, and such. For a complete list, run /u01/wdt/weblogic-deploy/bin/modelHelp.sh -oracle_home $ORACLE_HOME topology (this assumes you have installed WDT in the /u01/wdt/weblogic-deploy directory).\n  Deleting Model entries by type. For example, you cannot delete an entire SelfTuning type stanza by omitting the stanza in an online update or by specifying an additional model with !SelfTuning in either an offline or an online update. Instead, you can delete the specific MBean by omitting the MBean itself while leaving its SelfTuning parent in place or by specifying an additional model using the ! syntax in combination with the name of the specific MBean. See online update handling of deletes for details.\n  Deleting multiple resources that have cross-references in combination with online updates. For example, concurrently deleting a persistent store and a data source referenced by the persistent store, For this type of failure, the introspection job will fail and log an error describing the failed reference, and the job will automatically retry up to its maximum retries. See online update handling of deletes for details.\n  Security related changes in combination with online updates. Such changes included security changes in domainInfo.Admin*, domainInfo.RCUDbinfo.*, topology.Security.*, and topology.SecurityConfiguration.*. Any online update changes in these sections will result in a failure.\n  Updating an existing model If you have verified your proposed model updates to a running Model in Image domain are supported by consulting Supported and unsupported updates, then you can use the following approaches.\nFor online or offline updates:\n  Specify a new or changed WDT ConfigMap that contains model files and use your domain resource YAML file configuration.model.configMap field to reference the map. The model files in the ConfigMap will be merged with any model files in the image. Ensure the ConfigMap is deployed to the same namespace as your domain.\n  Change, add, or delete secrets that are referenced by macros in your model files and use your domain resource YAML file configuration.secrets field to reference the secrets. Ensure the secrets are deployed to the same namespace as your domain.\n  For offline updates only, there are two additional options:\n  Supply a new image with new or changed model files.\n If the files are located in the image specified in the domain resource YAML file spec.image, then change this field to reference the image. If you are using auxiliary images to supply model files in an image, then change the corresponding serverPod.auxiliaryImages.image field value to reference the new image or add a new serverPod.auxiliaryImages mount for the new image.    Change, add, or delete environment variables that are referenced by macros in your model files. Environment variables are specified in the domain resource YAML file spec.serverPod.env or spec.serverPod.adminServer.env attributes.\n  It is advisable to defer the last two modification options, or similar domain resource YAML file changes to fields that cause servers to be restarted, until all of your other modifications are ready. This is because such changes automatically and immediately result in a rerun of your introspector job, and, if the job succeeds, then a roll of the domain, plus, an offline update, if there are any accompanying model changes.\n Model updates can include additions, changes, and deletions. For help generating model changes:\n  For a discussion of model file syntax, see the WebLogic Deploy Tool documentation and Model in Image Model files documentation.\n  For a discussion about helper tooling that you can use to generate model change YAML, see Using the WDT Discover and Compare Model Tools.\n  If you specify multiple model files in your image, volumes (including those based on images from the auxiliary images feature), or WDT ConfigMap, then the order in which they\u0026rsquo;re loaded and merged is determined as described in Model file naming and loading order.\n  If you are performing an online update and the update includes deletes, then see Online update handling of deletes.\n  After your model updates are prepared, you can instruct the operator to propagate the changed model to a running domain by following the steps in Offline updates or Online updates.\nOffline updates Offline update steps Use the following steps to initiate an offline configuration update to your model:\n Ensure your updates are supported by checking Supported and Unsupported updates. Modify, add, or delete your model resources as per Updating an existing model. Modify your domain resource YAML file:  If you have updated your image:  If the files are located in the image specified in the domain resource YAML file spec.image, then change this field to reference the image. If you are using auxiliary images to supply model files in an image, then change the corresponding serverPod.auxiliaryImages.image field value to reference the new image or add a new serverPod.auxiliaryImages mount for the new image.   If you are updating environment variables, change domain.spec.serverPod.env or domain.spec.adminServer.serverPod.env accordingly. If you are specifying a WDT ConfigMap, then set domain.spec.configuration.model.configMap to the name of the ConfigMap. If you are adding or deleting secrets as part of your change, then ensure the domain.spec.configuration.secrets array reflects all current secrets. If you have modified your image or environment variables, then no more domain resource YAML file changes are needed; otherwise, change an attribute that instructs the operator to roll the domain. For examples, see change the domain spec.restartVersion or change any of the other Domain resource YAML fields that cause servers to be restarted.    The operator will subsequently rerun the domain\u0026rsquo;s introspector job. This job will reload all of your secrets and environment variables, merge all of your model files, and generate a new domain home.\nIf the job succeeds, then the operator will make the updated domain home available to pods using a ConfigMap named DOMAIN_UID-weblogic-domain-introspect-cm and the operator will subsequently roll (restart) each running WebLogic Server pod in the domain so that it can load the new configuration. A domain roll begins by restarting the domain\u0026rsquo;s Administration Server and then proceeds to restart each Managed Server in the domain.\nIf the job reports a failure, see Debugging for advice.\nOffline update sample For an offline update sample which adds a data source, see the Update 1 use case in the Model in Image sample.\nOnline updates Online update steps Use the following steps to initiate an online configuration update to your model:\n Ensure your updates are supported by checking Supported and Unsupported updates. Modify, add, or delete your model secrets or WDT ConfigMap as per Updating an existing model. Modify your domain resource YAML file:   Do not change domain.spec.image, domain.spec.serverPod.env, or any other domain resource YAML fields that cause servers to be restarted; this will automatically and immediately result in a rerun of your introspector job, a roll if the job succeeds, plus an offline update if there are any accompanying model changes.\n  If you are specifying a WDT ConfigMap, then set domain.spec.configuration.model.configMap to the name of the ConfigMap.\n  If you are adding or deleting secrets as part of your change, then ensure the domain.spec.configuration.secrets array reflects all current secrets.\n  Set domain.spec.configuration.model.onlineUpdate.enabled to true (default is false).\n  Set domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges to one of CommitUpdateOnly (default), and CommitUpdateAndRoll. For details, see online update handling of non-dynamic WebLogic configuration changes.\n  Optionally, tune the WDT timeouts in domain.spec.configuration.model.onlineUpdate.wdtTimeouts.\n This is only necessary in the rare case when an introspector job\u0026rsquo;s WDT online update command timeout results in an error in the introspector job log or operator log. All timeouts are specified in milliseconds and default to two or three minutes. For a full list of timeouts, you can call kubectl explain domain.spec.configuration.model.onlineUpdate.wdtTimeouts.    Change domain.spec.introspectVersion to a different value. For examples, see change the domain spec.introspectVersion.\n    After you\u0026rsquo;ve completed these steps, the operator will subsequently run an introspector Job which generates a new merged model, compares the new merged model to the previously deployed merged model, and runs the WebLogic Deploy Tool to process the differences:\n  If the introspector job WDT determines that the differences are confined to fully dynamic WebLogic configuration MBean changes, then the operator will send delta online updates to the running WebLogic pods.\n  If WDT detects non-dynamic WebLogic configuration MBean changes, then the operator may ignore the updates, honor only the online updates, or initiate an offline update (roll) depending on whether you have configured domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges to CommitUpdateOnly (default), or CommitUpdateAndRoll. For details, see online update handling of non-dynamic WebLogic configuration changes.\n  If the introspector job reports a failure or any other failure occurs, then see Debugging for advice. When recovering from a failure, please keep the following points in mind:\n  The operator cannot automatically revert changes to resources that are under user control (just like with offline updates). For example, it is the administrator\u0026rsquo;s responsibility to revert problem changes to an image, configMap, secrets, and domain resource YAML file.\n  If there is any failure during an online update, then no WebLogic configuration changes are made to the running domain and the introspector job retries up to a maximum number of times. To correct the problem, modify and reapply your model resources (ConfigMap and/or secrets), plus, if the introspector job has stopped retrying, you must also change your domain resource domain.spec.introspectVersion again.\n  Sample domain resource YAML file for an online update:\n... kind: Domain metadata: name: sample-domain1 namespace: sample-domain1-ns ... spec: ... introspectVersion: 5 configuration: ... model: domainType: \u0026#34;WLS\u0026#34; configMap: sample-domain1-wdt-config-map runtimeEncryptionSecret: sample-domain1-runtime-encryption-secret onlineUpdate: enabled: true onNonDynamicChanges: \u0026#34;CommitUpdateAndRoll\u0026#34; secrets: - sample-domain1-datasource-secret - sample-domain1-another-secret Online update handling of non-dynamic WebLogic configuration changes The domain resource YAML domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges attribute controls behavior when non-dynamic WebLogic configuration changes are detected during an online update introspector job. Non-dynamic changes are changes that require a domain restart to take effect. Valid values are CommitUpdateOnly (default), or CommitUpdateAndRoll:\n  If set to CommitUpdateOnly (the default) and any non-dynamic changes are detected, then all changes will be committed, dynamic changes will take effect immediately, the domain will not automatically restart (roll), and any non-dynamic changes will become effective on a pod only when the pod is later restarted.\n  If set to CommitUpdateAndRoll and any non-dynamic changes are detected, then all changes will be committed, dynamic changes will take effect immediately, the domain will automatically restart (roll), and non-dynamic changes will take effect on each pod after the pod restarts.\n  When updating a domain with non-dynamic MBean changes with domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges=CommitUpdateOnly (the default), the non-dynamic changes are not effective on a WebLogic pod until the pod is restarted. However, if you scale up a cluster or otherwise start any new servers in the domain, then the new servers will start with the new non-dynamic changes and the domain will then be running in an inconsistent state until its older servers are restarted.\n Online update handling of deletes The primary use case for online updates is to make small additions, deletions of single resources or MBeans that have no dependencies, or changes to non-dynamic MBean attributes.\nDeletion can be problematic for online updates in two cases:\n Deleting multiple resources that have cross dependencies. Deleting the parent type section in an MBean hierarchy.  In general, complex deletion should be handled by offline updates in order to avoid these problems.\nNote: Implicitly removing a model\u0026rsquo;s parent type section may sometimes work depending on the type of the section. For example, if you have an application in the model under appDeployments: in a model.configMap and you subsequently update the ConfigMap using an online update so that it no longer includes the appDeployment section, then the online update will delete the application from the domain.\nMBean type section deletion\nFor an example of an MBean deletion, consider a WDT ConfigMap that starts with:\nresources: SelfTuning: WorkManager: wm1: Target: \u0026#39;cluster-1\u0026#39; wm2: Target: \u0026#39;cluster-1\u0026#39; JDBCSystemResource: ... If you want to online update to a new model without work-managers, then change the ConfigMap to the following:\nresources: SelfTuning: WorkManager: JDBCSystemResource: ... or supply an additional ConfigMap:\nresources: SelfTuning: WorkManager: \u0026#39;!wm1\u0026#39;: \u0026#39;!wm2\u0026#39;: The online update will fail if you try replace the ConfigMap with the SelfTuning section omitted:\nresources: JDBCSystemResource: ... The above will fail as this implicitly removes the MBean types SelfTuning and WorkManager.\nDeleting cross-referenced MBeans\nFor an example of an unsupported online update delete of MBeans with cross references, consider the case of a Work Manager configured with constraints where you want to delete the entire Work Manager:\nresources: SelfTuning: WorkManager: newWM: Target: \u0026#39;cluster-1\u0026#39; MinThreadsConstraint: \u0026#39;SampleMinThreads\u0026#39; MaxThreadsConstraint: \u0026#39;SampleMaxThreads\u0026#39; MinThreadsConstraint: SampleMinThreads: Count: 1 MaxThreadsConstraint: SampleMaxThreads: Count: 10 If you try to specify the updated model in the ConfigMap as:\nresources: SelfTuning: WorkManager: MinThreadsConstraint: MaxThreadsConstraint: Then, the operator will try use this delta to online update the domain:\nresources: SelfTuning: MaxThreadsConstraint: \u0026#39;!SampleMaxThreads\u0026#39;: WorkManager: \u0026#39;!newWM\u0026#39;: MinThreadsConstraint: \u0026#39;!SampleMinThreads\u0026#39;: This can fail because an online update might not delete all the referenced Constraints first before deleting the WorkManager.\nTo work around problems with online updates to objects with cross dependencies, you can use a series of online updates to make the change in stages. For example, continuing the previous Work Manager example, first perform an online update to omit the Work Manager but not the constraints:\nresources: SelfTuning: WorkManager: MinThreadsConstraint: SampleMinThreads: Count: 1 MaxThreadsConstraint: SampleMaxThreads: Count: 10 After that update completes, then perform another online update:\nresources: SelfTuning: WorkManager: MinThreadsConstraint: MaxThreadsConstraint: Online update status and labels During an online update, the operator will rerun the introspector job, which in turn attempts online WebLogic configuration changes to the running domain. You can monitor an update\u0026rsquo;s status using its domain resource\u0026rsquo;s status conditions and its WebLogic Server pod labels.\nFor example, for the domain status you can check the domain resource domain.status stanza using kubectl -n MY_NAMESPACE get domain MY_DOMAINUID -o yaml, and for the WebLogic pod labels you can use kubectl -n MY_NAMESPACE get pods --show-labels plus optionally add --watch to watch the pods as they change over time.\nHere is how to interpret each domain resource\u0026rsquo;s domain.status.conditions type:\n  The Progressing type.\n Status attribute is True when a domain resource change is being processed by the operator, such as when:  The operator is processing a new domain resource or a change to an existing domain resource. The introspector is running. Servers are starting. A roll is in progress   Status becomes False or unset after a failure, but can return to True if/when there is a retry. Status becomes False or unset after a success. Note: This condition may \u0026lsquo;blink\u0026rsquo; on and off during a roll or pod restart.    The Available type.\n Status attribute is True when:  Processing successfully completes without error (introspect job, syntax checks, and such) The operator is starting or has started all desired WebLogic Server pods (not including any servers that may be shutting down).   Status is False or unset:  Servers are rolling/starting or a failure has occurred.   Note: This condition may \u0026lsquo;blink\u0026rsquo; on and off while processing a domain resource change or during a roll. For example, after a successful online update, you will see something like this in the domain resource domain.status section: status: clusters: - clusterName: cluster-1 maximumReplicas: 5 minimumReplicas: 0 readyReplicas: 2 replicas: 2 replicasGoal: 2 conditions: - lastTransitionTime: \u0026#34;2021-01-26T18:43:14.377Z\u0026#34; reason: ServersReady status: \u0026#34;True\u0026#34; type: Available     The Failed type.\n Status attribute is True after a failure including validation errors or an introspector failure.  In this case, the condition\u0026rsquo;s message attribute will display an error message.   Status becomes False or unset after any succesful retry. Note: A Failed condition\u0026rsquo;s status is not set to True if the domain resource is successfully processed but one or more WebLogic Server pods is failing to start. In this case, Kubernetes will periodically try and restart the pod(s), and the Available type may remain True.    The ConfigChangesPendingRestart type.\n Status attribute is True if all of the following are true:  The domain resource attribute domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges is CommitUpdateOnly. The domain resource attribute domain.spec.configuration.model.onlineUpdate.enabled is True. There were model changes and these changes modify non-dynamic WebLogic configuration. Processing successfully completed, including the introspector job. The administrator has not subsequently rolled/restarted each WebLogic Server pod (in order to propagate the pending non-dynamic changes).  See the following discussion of WebLogic pod labels to see which pods are awaiting restart.     For example: Status: ... Conditions: Last Transition Time: 2021-01-20T15:09:15.209Z Message: Online update completed successfully, but the changes require restart and the domain resource specified 'spec.configuration.model.onlineUpdate.onNonDynamicChanges=CommitUpdateOnly' or not set. The changes are committed but the domain require manually restart to make the changes effective. The changes are: Server re-start is REQUIRED for the set of changes in progress. The following non-dynamic attribute(s) have been changed on MBeans that require server re-start: MBean Changed : com.bea:Name=oracle.jdbc.fanEnabled,Type=weblogic.j2ee.descriptor.wl.JDBCPropertyBean,Parent=[sample-domain1]/JDBCSystemResources[Bubba-DS],Path=JDBCResource[Bubba-DS]/JDBCDriverParams/Properties/Properties[oracle.jdbc.fanEnabled] Attributes changed : Value Reason: Online update applied, introspectVersion updated to 82 Status: True Type: ConfigChangesPendingRestart     Here are some of the expected WebLogic pod labels after an online update success:\n  Each WebLogic Server pod\u0026rsquo;s weblogic.introspectVersion label value will eventually match the domain.spec.introspectVersion value that you defined.\n If the domain resource attribute domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges is CommitUpdateOnly (the default), then the introspect version label on all pods is immediately updated after the introspect job successfully completes. If the domain resource attribute domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges is CommitUpdateAndRoll and there are no non-dynamic configuration changes to the model, then the introspect version label on all pods is immediately updated after the introspect job successfully completes. If the domain resource attribute domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges is CommitUpdateAndRoll and there are non-dynamic clabel onfiguration changes to the model, then the introspect version label on each pod is updated after the pod is rolled.    There will be a weblogic.configChangesPendingRestart=true label on each WebLogic Server pod until the pod is restarted (rolled) by an administrator if all of the following are true:\n The domain resource domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges attribute is CommitUpdateOnly (the default). Non-dynamic WebLogic configuration changes were included in a successful online model update.    Online update scenarios   Successful online update that includes only dynamic WebLogic MBean changes.\n Example dynamic WebLogic MBean changes:  Changing data source connection pool capacity, password, and targets. Changing application targets. Deleting or adding a data source. Deleting or adding an application. The MBean changes are committed in the running domain and effective immediately.   Expected outcome after the introspector job completes:  The domain Available condition status is set to True. The weblogic.introspectVersion label on all pods will be set to match the domain.spec.introspectVersion.   Actions required:  None.      Successful online update that includes non-dynamic WebLogic MBean attribute changes when domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges is CommitUpdateOnly (the default).\n Example non-dynamic WebLogic MBean change:  Changing a data source driver parameter property (such as username).   Expected outcome after the introspector job completes:  Any dynamic WebLogic configuration changes are committed in the running domain and effective immediately. Non-dynamic WebLogic configuration changes will not take effect on already running WebLogic Server pods until an administrator subsequently rolls the pod. The domain status Available condition will have a Status of True. The domain status ConfigChangesPendingRestart condition will have a Status of True until an administrator subsequently rolls all WebLogic Server pods that are already running. Each WebLogic Server pod\u0026rsquo;s weblogic.introspectVersion label will match domain.spec.introspectVersion. Each WebLogic Server pod that is already running will be given a weblogic.configChangesPendingRestart=true label until an administrator subsequently rolls the pod.   Actions required:  If you want the non-dynamic changes to take effect, then restart the pod(s) with the weblogic.configChangesPendingRestart=true label (such as by initiating a domain roll). See Online update handling of non-dynamic WebLogic configuration changes.      Successful online update that includes non-dynamic WebLogic MBean attribute changes when domain.spec.configuration.model.onlineUpdate.onNonDynamicChanges is CommitUpdateAndRoll.\n Expected outcome after the introspector job completes:  Any dynamic WebLogic configuration changes are committed in the running domain and effective immediately. The operator will initiate a domain roll. Non-dynamic WebLogic configuration changes will take effect on each pod when the pod is rolled. Each WebLogic Server pod\u0026rsquo;s weblogic.introspectVersion label will match domain.spec.introspectVersion after it is rolled. The domain status Available condition will have a Status of True after the roll completes.   Actions required:  None. All changes will complete after the operator initiated domain roll completes. See Online update handling of non-dynamic WebLogic configuration changes.      Changing any of the domain resource fields that cause servers to be restarted in addition to domain.spec.introspectVersion, spec.configuration.secrets, spec.configuration.model.onlineUpdate, or spec.configuration.model.configMap.\n Expected outcome after the introspector job completes:  No online update was attempted by the introspector job. All model changes are treated the same as offline updates (which may result in restarts/roll after job success).   Actions required:  None.      Changing any model attribute that is unsupported.\n Expected outcome:  The expected behavior is often undefined, but in some cases there will be helpful error in the introspector job, events, and/or domain status, and the job will periodically retry until the error is corrected or its maximum error count exceeded.   Actions required:  Use offline updates if they are supported, or, if not, shutdown the entire domain and restart it. See Debugging.      Errors in the model; for example, a syntax error.\n Expected outcome after the introspector job completes:  Error in the introspector job\u0026rsquo;s pod log, domain events, and domain status. The domain status Failed condition will have a Status of True. Periodic job retries until the error is corrected or until a maximum error count is exceeded.   Actions required:  Correct the model. If retries have halted, then alter the spec.introspectVersion.      Other errors while updating the domain.\n Expected outcome:  Error in the introspector job, domain events, and/or domain status. The domain status Failed condition will have a Status of True. If there\u0026rsquo;s a failed introspector job, the job will retry periodically until the error is corrected or until it exceeds its maximum error count. Other types of errors will also usually incur periodic retries.   Actions required:  See Debugging. Make corrections to the domain resource and/or model. If retries have halted, then alter the spec.introspectVersion.      Online update sample For an online update sample which alters a data source and Work Manager, see the Update 4 use case in the Model in Image sample.\nAppendices Using the WDT Discover Domain and Compare Model Tools Optionally, you can use the WDT Discover Domain and Compare Domain Tools to help generate your model file updates. The WebLogic Deploy Tooling Discover Domain Tool generates model files from an existing domain home, and its Compare Model Tool compares two domain models and generates the YAML file for updating the first domain to the second domain.\nFor example, assuming you\u0026rsquo;ve installed WDT in /u01/wdt/weblogic-deploy and assuming your domain type is WLS:\n# (1) Run discover for your existing domain home. $ /u01/wdt/weblogic-deploy/bin/discoverDomain.sh \\  -oracle_home $ORACLE_HOME \\  -domain_home $DOMAIN_HOME \\  -domain_type WLS \\  -archive_file old.zip \\  -model_file old.yaml \\  -variable_file old.properties # (2) Now make some WebLogic config changes using the console or WLST. # (3) Run discover for your changed domain home. $ /u01/wdt/weblogic-deploy/bin/discoverDomain.sh \\  -oracle_home $ORACLE_HOME \\  -domain_home $DOMAIN_HOME \\  -domain_type WLS \\  -archive_file new.zip \\  -model_file new.yaml \\  -variable_file new.properties # (4) Compare your old and new yaml using diff $ diff new.yaml old.yaml # (5) Compare your old and new yaml using compareDomain to generate # the YAML update file you can use for transforming the old to new. # /u01/wdt/weblogic-deploy/bin/compareModel.sh \\ -oracle_home $ORACLE_HOME \\ -output_dir /tmp \\ -variable_file old.properties \\ old.yaml \\ new.yaml # (6) The compareModel will generate these files: # /tmp/diffed_model.json # /tmp/diffed_model.yaml, and # /tmp/compare_model_stdout  Note: If your domain type isn\u0026rsquo;t WLS, remember to change the domain type to JRF or RestrictedJRF in the above discoverDomain.sh commands.\n Changing a Domain restartVersion or introspectVersion As was mentioned in the offline updates section, one way to tell the operator to apply offline configuration changes to a running domain is by altering the Domain spec.restartVersion. Similarly, an online update is initiated by altering the Domain spec.introspectVersion. Here are some common ways to alter either of these fields:\n  You can alter restartVersion or introspectVersion interactively using kubectl edit -n MY_NAMESPACE domain MY_DOMAINUID.\n  If you have your domain\u0026rsquo;s resource file, then you can alter this file and call kubectl apply -f on the file.\n  You can use the Kubernetes get and patch commands.\nHere\u0026rsquo;s a sample automation script for restartVersion that takes a namespace as the first parameter (default sample-domain1-ns) and a domainUID as the second parameter (default sample-domain1):\n#!/bin/bash NAMESPACE=${1:-sample-domain1-ns} DOMAINUID=${2:-sample-domain1} currentRV=$(kubectl -n ${NAMESPACE} get domain ${DOMAINUID} -o=jsonpath=\u0026#39;{.spec.restartVersion}\u0026#39;) if [ $? = 0 ]; then # we enter here only if the previous command succeeded nextRV=$((currentRV + 1)) echo \u0026#34;@@ Info: Rolling domain \u0026#39;${DOMAINUID}\u0026#39; in namespace \u0026#39;${NAMESPACE}\u0026#39; from restartVersion=\u0026#39;${currentRV}\u0026#39; to restartVersion=\u0026#39;${nextRV}\u0026#39;.\u0026#34; kubectl -n ${NAMESPACE} patch domain ${DOMAINUID} --type=\u0026#39;json\u0026#39; \\  -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/restartVersion\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\u0026#39;${nextRV}\u0026#39;\u0026#34; }]\u0026#39; fi Here\u0026rsquo;s a similar sample script for introspectVersion:\n#!/bin/bash NAMESPACE=${1:-sample-domain1-ns} DOMAINUID=${2:-sample-domain1} currentIV=$(kubectl -n ${NAMESPACE} get domain ${DOMAINUID} -o=jsonpath=\u0026#39;{.spec.introspectVersion}\u0026#39;) if [ $? = 0 ]; then # we enter here only if the previous command succeeded nextIV=$((currentIV + 1)) echo \u0026#34;@@ Info: Rolling domain \u0026#39;${DOMAINUID}\u0026#39; in namespace \u0026#39;${NAMESPACE}\u0026#39; from introspectVersion=\u0026#39;${currentIV}\u0026#39; to introspectVersion=\u0026#39;${nextIV}\u0026#39;.\u0026#34; kubectl -n ${NAMESPACE} patch domain ${DOMAINUID} --type=\u0026#39;json\u0026#39; \\  -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/introspectVersion\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;\u0026#39;${nextIV}\u0026#39;\u0026#34; }]\u0026#39; fi   You can use a WebLogic Kubernetes Operator sample script that invokes the same commands that are described in the previous bulleted item.\n See patch-restart-version.sh and patch-introspect-version.sh in the kubernetes/samples/scripts/create-weblogic-domain/model-in-image/utils/ directory. Or, see the more advanced introspectDomain.sh and rollDomain.sh among the Domain lifecycle sample scripts.    "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/userguide/managing-domains/model-in-image/debugging/",
	"title": "Debugging",
	"tags": [],
	"description": "Debugging a deployed Model in Image domain.",
	"content": "Here are some suggestions for debugging problems with Model in Image after your Domain YAML file is deployed.\nContents  Check the Domain status Check the Domain events Check the introspector job Check the WebLogic Server pods Check the operator log Check the FAQ  Check the Domain status To check the Domain status: kubectl -n MY_NAMESPACE describe domain MY_DOMAINUID.\nIf you are performing an online update to a running domain\u0026rsquo;s WebLogic configuration, then see Online update status and labels.\nCheck the Domain events To check events for the Domain: kubectl -n MY_NAMESPACE get events --sort-by='.lastTimestamp'.\nFor more information, see Domain events.\nCheck the introspector job If your introspector job failed, then examine the kubectl describe of the job and its pod, and also examine its log, if one exists.\nTo prevent the introspector job from retrying while you are debugging a failure, set the operator\u0026rsquo;s Helm domainPresenceFailureRetryMaxCount parameter to 0. For more information, see Manage operators -\u0026gt; Use the operator -\u0026gt; Use Helm.\n For example, assuming your domain UID is sample-domain1 and your domain namespace is sample-domain1-ns:\n$ # here we see a failed introspector job pod among the domain's pods: $ kubectl -n sample-domain1-ns get pods -l weblogic.domainUID=sample-domain1 NAME READY STATUS RESTARTS AGE sample-domain1-admin-server 1/1 Running 0 19h sample-domain1-introspector-v2l7k 0/1 Error 0 75m sample-domain1-managed-server1 1/1 Running 0 19h sample-domain1-managed-server2 1/1 Running 0 19h $ # let's look at the job's describe $ kubectl -n sample-domain1-ns describe job/sample-domain1-introspector $ # now let's look at the job's pod describe, in particular look at its 'events' $ kubectl -n sample-domain1-ns describe pod/sample-domain1-introspector-v2l7k $ # finally let's look at job's pod's log $ kubectl -n sample-domain1-ns logs job/sample-domain1-introspector $ # alternative log command (will have same output as previous) # kubectl -n sample-domain1-ns logs pod/sample-domain1-introspector-v2l7k A common reason for the introspector job to fail is because of an error in a model file. Here's some sample log output from an introspector job that shows such a failure: ... SEVERE Messages: 1. WLSDPLY-05007: Model file /u01/wdt/models/model1.yaml,/weblogic-operator/wdt-config-map/..2020_03_19_15_43_05.993607882/datasource.yaml contains an unrecognized section: TYPOresources. The recognized sections are domainInfo, topology, resources, appDeployments, kubernetes  The introspector log is mirrored to the Domain resource spec.logHome directory when spec.logHome is configured and spec.logHomeEnabled is true.\n If a model file error references a model file in your spec.configuration.model.configMap, then you can correct the error by redeploying the ConfigMap with a corrected model file and then initiating a domain restart or roll. Similarly, if a model file error references a model file in your model image, then you can correct the error by deploying a corrected image, modifying your Domain YAML file to reference the new image, and then initiating a domain restart or roll.\n Check the WebLogic Server pods If your introspector job succeeded, then there will be no introspector job or pod, the operator will create a MY_DOMAIN_UID-weblogic-domain-introspect-cm ConfigMap for your domain, and the operator will then run the domain\u0026rsquo;s WebLogic Server pods.\nIf kubectl -n MY_NAMESPACE get pods reveals that your WebLogic Server pods have errors, then use kubectl -n MY_NAMESPACE describe pod POD_NAME, kubectl -n MY_NAMESPACE logs POD_NAME, and/or kubectl -n MY_NAMESPACE get events --sort-by='.lastTimestamp' to debug.\nIf you are performing an online update to a running domain\u0026rsquo;s WebLogic configuration, then see Online update status and labels.\nCheck the operator log Look for SEVERE and ERROR level messages in your operator logs. For example:\n$ # find your operator $ kubectl get deployment --all-namespaces=true -l weblogic.operatorName NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE sample-weblogic-operator-ns weblogic-operator 1 1 1 1 20h $ # grep operator log for SEVERE and WARNING level messages $ kubectl logs deployment/weblogic-operator -n sample-weblogic-operator-ns \\  | egrep -e \u0026#34;level...(SEVERE|WARNING)\u0026#34; {\u0026#34;timestamp\u0026#34;:\u0026#34;03-18-2020T20:42:21.702+0000\u0026#34;,\u0026#34;thread\u0026#34;:11,\u0026#34;fiber\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;domainUID\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;WARNING\u0026#34;,\u0026#34;class\u0026#34;:\u0026#34;oracle.kubernetes.operator.helpers.HealthCheckHelper\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;createAndValidateKubernetesVersion\u0026#34;,\u0026#34;timeInMillis\u0026#34;:1584564141702,\u0026#34;message\u0026#34;:\u0026#34;Kubernetes minimum version check failed. Supported versions are 1.13.5+,1.14.8+,1.15.7+, but found version v1.12.3\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;code\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;headers\u0026#34;:{},\u0026#34;body\u0026#34;:\u0026#34;\u0026#34;} You can filter out operator log messages specific to your domainUID by piping the above logs command through grep \u0026quot;domainUID...MY_DOMAINUID\u0026quot;. For example, assuming your operator is running in namespace sample-weblogic-operator-ns and your domain UID is sample-domain1:\n$ kubectl logs deployment/weblogic-operator -n sample-weblogic-operator-ns \\  | egrep -e \u0026#34;level...(SEVERE|WARNING)\u0026#34; \\  | grep \u0026#34;domainUID...sample-domain1\u0026#34; Check the FAQ Common issues that have corresponding FAQ entries include:\n When a Domain YAML file is deployed and no introspector or WebLogic Server pods start, plus the operator log contains no mention of the domain, then check to make sure that the Domain\u0026rsquo;s namespace has been set up to be monitored by an operator. See the Managing domain namespaces FAQ. If a describe of an introspector job or WebLogic Server pod reveals image access errors, see the Cannot pull image FAQ.  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "WebLogic Kubernetes Operator The WebLogic Kubernetes Operator (the “operator”) supports running your WebLogic Server and Fusion Middleware Infrastructure domains on Kubernetes, an industry standard, cloud neutral deployment platform. It lets you encapsulate your entire WebLogic Server installation and layered applications into a portable set of cloud neutral images and simple resource description files. You can run them on any on-premises or public cloud that supports Kubernetes where you\u0026rsquo;ve deployed the operator.\nFurthermore, the operator is well suited to CI/CD processes. You can easily inject changes when moving between environments, such as from test to production. For example, you can externally inject database URLs and credentials during deployment or you can inject arbitrary changes to most WebLogic configurations.\nThe operator takes advantage of the Kubernetes operator pattern, which means that it uses Kubernetes APIs to provide support for operations, such as: provisioning, lifecycle management, application versioning, product patching, scaling, and security. The operator also enables the use of tooling that is native to this infrastructure for monitoring, logging, tracing, and security.\nYou can:\n Deploy an operator that manages all WebLogic domains in all namespaces in a Kubernetes cluster, or that only manages domains in a specific subset of the namespaces, or that manages only domains that are located in the same namespace as the operator. At most, a namespace can be managed by one operator. Supply WebLogic domain configuration using:  Domain in PV: Locates WebLogic domain homes in a Kubernetes PersistentVolume (PV). This PV can reside in an NFS file system or other Kubernetes volume types. Domain in Image: Includes a WebLogic domain home in a container image. Model in Image: Includes WebLogic Deploy Tooling models and archives in a container image.   Configure deployment of WebLogic domains as a Kubernetes resource (using a Kubernetes custom resource definition). Override certain aspects of the WebLogic domain configuration; for example, use a different database password for different deployments. Start and stop servers and clusters in the domain based on declarative startup parameters and desired states. Scale WebLogic domains by starting and stopping Managed Servers on demand, or by integrating with a REST API to initiate scaling based on the WebLogic Diagnostics Framework (WLDF), Prometheus, Grafana, or other rules. Expose the WebLogic Server Administration Console outside the Kubernetes cluster, if desired. Expose T3 channels outside the Kubernetes domain, if desired. Expose HTTP paths on a WebLogic domain outside the Kubernetes domain with load balancing, and automatically update the load balancer when Managed Servers in the WebLogic domain are started or stopped. Publish operator and WebLogic Server logs into Elasticsearch and interact with them in Kibana.  The fastest way to experience the operator is to follow the Quick Start guide, or you can peruse our documentation, read our blogs, or try out the samples. Also, you can step through the Tutorial using the operator to deploy and run a WebLogic domain container-packaged web application on an Oracle Cloud Infrastructure Container Engine for Kubernetes (OKE) cluster.\n  Current production release The current release of the operator is 3.3.2. This release was published on September 24, 2021. See the operator prerequisites and supported environments.\n Recent changes and known issues See the Release Notes for recent changes to the operator and known issues.\nOperator earlier versions Documentation for prior releases of the operator: 2.5.0, 2.6.0, 3.0.x, and 3.1.x.\nBackward compatibility guidelines Starting from the 2.0.1 release, operator releases are backward compatible with respect to the domain resource schema, operator Helm chart input values, configuration overrides template, Kubernetes resources created by the operator Helm chart, Kubernetes resources created by the operator, and the operator REST interface. We intend to maintain compatibility for three releases, except in the case of a clearly communicated deprecated feature, which will be maintained for one release after a replacement is available.\nGetting help See Get help.\nRelated projects  Oracle Fusion Middleware on Kubernetes WebLogic Deploy Tooling WebLogic Image Tool WebLogic Monitoring Exporter WebLogic Logging Exporter WebLogic Remote Console  "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/release-notes/",
	"title": "Release Notes",
	"tags": [],
	"description": "",
	"content": "Releases    Date Version Introduces backward incompatibilities? Change - See also, Change log.     September 24, 2021 v3.3.2 no Istio 1.10 support, enhanced liveness and readiness probe customization to support customizing failure thresholds, and additional validations.   August 23, 2021 v3.3.1 no Resolved an issue related to managed Coherence cluster formation when using Istio and another issue related to Secret and ConfigMap validation.   July 20, 2021 v3.3.0 no Auxiliary image support.   June 21, 2021 v3.2.5 no Updated Oracle Linux libraries and resolved an issue related to repeated introspection.   June 18, 2021 v3.2.4 no Resolved several issues related to Istio, diagnostics, and recovery.   May 21, 2021 v3.2.3 no Resolved several issues, including an issue related to preserving the operator-generated internal certificate, corrected the monitoring exporter integration to include the Administration Server, enhanced the model-in-image support to not require the use of configuration overrides, and updated the domain-home-in-image samples to support the WebLogic Image Tool.   April 27, 2021 v3.2.2 no Resolved a set of issues with many related to reducing the operator\u0026rsquo;s network utilization.   April 5, 2021 v3.2.1 no Updated several dependencies, including the Oracle Linux base for the container image.   March 31, 2021 v3.2.0 no Online updates for dynamic changes for Model in Image, injection of the WebLogic Monitoring Exporter, other features, and a significant number of additional fixes.   March 1, 2021 v3.1.4 no Resolved an issue where the operator would ignore live data that was older than cached data, such as following an etcd restore and updated Kubernetes Java Client and Bouncy Castle dependencies.   February 12, 2021 v3.1.3 no Resolved a pair of issues related to the operator running well in very large Kubernetes clusters.   January 22, 2021 v3.1.2 no Resolved an issue where the operator failed to start servers in which the pods were configured to have an annotation containing a forward slash.   December 17, 2020 v3.1.1 no Resolved an issue that caused unexpected server restarts when the domain had multiple WebLogic clusters.   November 24, 2020 v3.0.4 no This release contains a back-ported fix from 3.1.0 for Managed Server pods that do not properly restart following a rolling activity.   November 13, 2020 v3.1.0 no Enhanced options for specifying managed namespaces. Helm 3.1.3+ now required. Added support for Tanzu Kubernetes Service.   November 9, 2020 v3.0.3 no This release contains a fix for pods that are stuck in the Terminating state after an unexpected shut down of a worker node.   September 15, 2020 v3.0.2 no This release contains several fixes, including improvements to log rotation and a fix that avoids unnecessarily updating the domain status.   August 13, 2020 v3.0.1 no Fixed an issue preventing the REST interface from working after a Helm upgrade. Helm 3.1.3+ now required.   July 17, 2020 v3.0.0 yes; for more information, see Upgrade the operator. Adds Model in Image feature and support for applying topology and configuration override changes without downtime. Removal of support for Helm 2.x. Operator performance improvements to manage many domains in the same Kubernetes cluster.   June 22, 2020 v2.6.0 no Kubernetes 1.16, 1.17, and 1.18 support. Removal of support for Kubernetes 1.13 and earlier. This release can be run in the same cluster with operators of either 2.5.0 and below, or with 3.x providing an upgrade path. Certified support of Oracle Linux Cloud Native Environment (OLCNE) 1.1 with Kubernetes 1.17.0.   February 26, 2020 v2.5.0 no Support for Helm 3.x and OpenShift 4.3. Operator can be installed in a namespace-dedicated mode where operator requires no cluster-level Kubernetes privileges. This version is not supported on Kubernetes 1.16+; check the prerequisites.   November 15, 2019 v2.4.0 no Includes fixes for a variety of issues related to FMW infrastructure domains and pod variable substitution. Operator now uses WebLogic Deploy Tooling 1.6.0 and the latest version of the Kubernetes Java Client.   August 27, 2019 v2.3.0 no Added support for Coherence cluster rolling, pod templating and additional pod content, and experimental support for running under an Istio service mesh.   June 20, 2019 v2.2.1 no The operator now supports Kubernetes 1.14.0+. This release is primarily a bug fix release and resolves the following issues: Servers in domains, where the domain home is on a persistent volume, would sometimes fail to start. These failures would be during the introspection phase following a full domain shutdown. Now, the introspection script better handles the relevant error conditions. Also, now the Domain provides an option to pre-create Kubernetes Services for WebLogic Servers that are not yet running so that the DNS addresses of these services are resolvable. These services are now created as non-headless so that they have an IP address.   June 6, 2019 v2.2.0 no Added support for FMW Infrastructure domains. WebLogic Server instances are now gracefully shut down by default and shutdown options are configurable. Operator is now built and runs on JDK 11.   April 4, 2019 v2.1 no Customers can add init and sidecar containers to generated pods.   March 4, 2019 v2.0.1 no OpenShift support is now certified. Many bug fixes, including fixes for configuration overrides, cluster services, and domain status processing.   January 24, 2019 v2.0 yes; not compatible with 1.x releases, but is compatible with 2.0-rc2. Final version numbers and documentation updates.   January 16, 2019 v2.0-rc2 yes Schema updates are completed, and various bugs fixed.   December 20, 2018 v2.0-rc1 yes Operator is now installed using Helm charts, replacing the earlier scripts. The operator now supports the domain home on a persistent volume or in image use cases, which required a redesign of the domain schema. You can override the domain configuration using configuration override templates. Now load balancers and ingresses can be independently configured. You can direct WebLogic logs to a persistent volume or to the pod\u0026rsquo;s log. Added life cycle support for servers and significantly enhanced configurability for generated pods. The final v2.0 release will be the initial release where the operator team intends to provide backward compatibility as part of future releases.   September 11, 2018 v1.1 no Enhanced the documentation and fixed various bugs.   May 7, 2018 v1.0 no Added support for dynamic clusters, the Apache HTTP Server, the Voyager Ingress Controller, and for PV in NFS storage for multi-node environments.   April 4, 2018 0.2 yes Many Kubernetes artifact names and labels have changed. Also, the names of generated YAML files for creating a domain\u0026rsquo;s PV and PVC have changed. Because of these changes, customers must recreate their operators and domains.   March 20, 2018  yes Several files and input parameters have been renamed. This affects how operators and domains are created. It also changes generated Kubernetes artifacts, therefore customers must recreate their operators and domains.    Change log Operator 3.3.2  Support for the networking changes included with Istio 1.10 (#2538). Support for accessing the WebLogic Server Administration Console through kubectl port-forward (#2520). Prevent insecure file system warnings related to the \u0026ldquo;umask 027\u0026rdquo; requirement (#2533). Enhanced liveness and readiness probe customization to support customizing failure thresholds (#2521). Additional validation for container port names and WebLogic Network Access Point (NAP) names that will be used as container ports (#2542).  Operator 3.3.1  Resolved an issue related to managed Coherence cluster formation when using Istio (#2499). Resolved an issue related to generating the internal certificate when using Istio (#2486). Resolved an issue related to validating Secrets and ConfigMaps referenced by a Domain when the namespace has a larger number of such resources (#2500).  Operator 3.3.0  Auxiliary images support. Resolved an issue related to Event creation failure with the error: \u0026ldquo;StorageError: invalid object, Code: 4\u0026rdquo; (#2443). Improved the ability of the operator to use an existing introspection (#2430). Upgraded core dependency versions, including upgrading the Kubernetes Java Client to 13.0.0 (#2466). Corrected documentation of roles necessary for the scaling script (#2464). All fixes included in 3.2.1 through 3.2.5 are included in 3.3.0.  Operator 3.2.5  Updated the Dockerfile to ensure Oracle Linux libraries are at the latest versions. Resolved an issue related to unnecessary repeated introspection (#2418). Updated the default monitoring exporter sidecar container image to use the 2.0.3 version.  Operator 3.2.4  Added support for the sessionAffinity field for the clusterService (#2383). Moved several logging messages to the CONFIG level (#2387). Resolved an issue related to scalingAction.sh when there were multiple domains in the same namespace (#2388). Updated operator logging and related scripts to consistently use ISO-8601 timestamp formatting (#2386). Resolved an issue related to monitoring exporter integration and Istio (#2390). Additional diagnostics when container start scripts fail to start the WebLogic Server instance (#2393). Ensure Kubernetes API failures are logged after the final retry (#2406). Resolved an issue related to failing to recover when a node drain or repaving occurred while waiting for the Administration Server to start (#2398). Resolved an issue related to Istio and WDT models that use default listen and SSL ports (#2379).  Operator 3.2.3  Resolved an issue related to preserving the operator-generated internal certificate (#2374). Corrected the monitoring exporter integration to include the Administration Server (#2365). Enhanced the model-in-image support to not require the use of configuration overrides (#2344). Resolved an issue related to model-in-image domains performing a rolling restart when there had been no change to the model (#2348). Resolved an issue related to RCU schema password updates (#2357). Resolved an issue related to namespace starting and stopping events (#2345). Added support for several new events related to rolling restarts (#2364). Added support for customer-defined labels and annotations on the operator\u0026rsquo;s pod (#2370).  Operator 3.2.2  Resolved an issue where the operator would retry Kubernetes API requests that timed out without a backoff causing increased network utilization (#2300). Resolved an issue where the operator would select the incorrect WebLogic Server port for administrative traffic (#2301). Resolved an issue where the operator, when running in a large and heavily loaded Kubernetes cluster, would not properly detect when a domain had been deleted and recreated (#2305 and #2314). Resolved an issue where the operator would fail to recover and begin processing in a namespace if the operator did not immediately have privileges in that namespace when it was first detected (#2315). The operator logs a message when it cannot generate a NamespaceWatchingStopped Event in a namespace because the operator no longer has privileges in that namespace (#2323). Resolved an issue where the operator would repeatedly replace a ConfigMap causing increased network utilization (#2321). Resolved an issue where the operator would repeatedly read a Secret causing increased network utilization (#2326). Resolved an issue where the operator was not honoring domain.spec.adminServer.serverService (#2334).  Operator 3.2.1 Updated several dependencies, including the Oracle Linux base for the container image.\nOperator 3.2.0 Features  The operator\u0026rsquo;s container image is based on Oracle Linux 8. WebLogic Server container images based on Oracle Linux 8 are supported. Online updates of dynamic configuration changes for Model in Image. Automatic injection of the WebLogic Monitoring Exporter as a sidecar container. Events are generated at important moments in the life cycle of the operator or a domain. PodDisruptionBudgets are generated for clusters improving the ability to maintain cluster availability during planned node shutdowns and Kubernetes upgrade. Additional scripts to assist with common tasks, such as the scaleCluster.sh script. Support for TCP and UDP on the same channel when the SIP protocol is used.  Fixes for Bugs or Regressions  All fixes included in 3.1.1 through 3.1.4 are included in 3.2.0. Resolved an issue where clustered Managed Servers would not start when the Administration Server was not running (#2093). Model in Image generated domain home file systems that exceed 1 MB are supported (#2095). An event and status update are generated when a cluster can\u0026rsquo;t be scaled past the cluster\u0026rsquo;s maximum size (#2097). Improved the operator\u0026rsquo;s ability to recover from failures during its initialization (#2118). Improved the ability for scalingAction.sh to discover the latest API version (#2130). Resolved an issue where the operator\u0026rsquo;s log would show incorrect warnings related to missing RBAC permissions (#2138). Captured WDT errors related to validating the model (#2140). Resolved an issue where the operator incorrectly missed Secrets or ConfigMaps in namespaces with a large number of either resource (#2199). Resolved an issue where the operator could report incorrect information about an introspection job that failed (#2201). Resolved an issue where a Service with the older naming pattern from operator 3.0.x could be stranded (#2208). Resolved an issue in the domain and cluster start scripts related to overrides at specific Managed Servers (#2222). The operator supports logging rotation and maximum file size configurations through Helm chart values (#2229). Resolved an issue supporting session replication when Istio is in use (#2242). Resolved an issue where the operator could swallow exceptions related to SSL negotiation failure (#2251). Resolved an issue where introspection would detect the wrong SSL port (#2256). Resolved an issue where introspection would fail if a referenced Secret or ConfigMap name was too long (#2257).  Operator 3.1.4  Resolved an issue where the operator would ignore live data that was older than cached data, such as following an etcd restore (#2196). Updated Kubernetes Java Client and Bouncy Castle dependencies.  Operator 3.1.3  Resolved an issue that caused some WebLogic Servers to fail to start in large Kubernetes clusters where Kubernetes watch notifications were not reliably delivered (#2188). Resolved an issue that caused the operator to ignore some namespaces it was configured to manage in Kubernetes clusters that had more than 500 namespaces (#2189).  Operator 3.1.2  Resolved an issue where the operator failed to start servers in which the pods were configured to have an annotation containing a forward slash (#2089).  Operator 3.1.1  Resolved an issue that caused unexpected server restarts when the domain had multiple WebLogic clusters (#2109).  Operator 3.1.0  All fixes included in 3.0.1 through 3.0.4 are included in 3.1.0. Sample scripts to start and stop server instances (#2002). Support running with OpenShift restrictive SCC (#2007). Updated default resource and Java options (#1775). Introspection failures are logged to the operator\u0026rsquo;s log (#1787). Mirror introspector log to a rotating file in the log home (#1827). Reflect introspector status to domain status (#1832). Ensure operator detects pod state changes even when watch events are not delivered (#1811). Support configurable WDT model home (#1828). Namespace management enhancements (#1860). Limit concurrent pod shut down while scaling down a cluster (#1892). List continuation and watch bookmark support (#1881). Fix scaling script when used with dedicated namespace mode (#1921). Fix token substitution for mount paths (#1911). Validate existence of service accounts during Helm chart processing (#1939). Use Kubernetes Java Client 10.0.0 (#1937). Better validation and guidance when using longer domainUID values (#1979). Update pods with label for introspection version (#2012). Fix validation error during inrtrospector for certain static clusters (#2014). Correct issue in wl-pod-wait.sh sample script (#2018). Correct processing of ALWAYS serverStartPolicy (#2020).  Operator 3.0.4  The operator now correctly completes restarting Managed Server pods in order to complete a rolling activity. This fix is already present in 3.1.0.  Operator 3.0.3  The operator now responds to WebLogic Server instance pods that are stuck in the Terminating state when those pods are evicted from a node that has unexpectedly shut down and where Kubernetes has not removed the pod.  Operator 3.0.2  Removed unnecessary duplicated parameter in initialize-internal-operator-identity.sh script (#1867). Support nodeAffinity and nodeSelector for the operator in its Helm chart (#1869). Log file rotation enhancements and documentation (#1872, #1827). Production support for the NGINX ingress controller (#1878). Prevent unnecessary changes to Domain status that were causing churn to the resourceVersion (#1879). Better reflect introspector status in the Domain status (#1832). Create each pod after any previous pods have been scheduled to allow for correct anti-affinity behavior (#1855).  Operator 3.0.1  Resolved an issue where a Helm upgrade was incorrectly removing the operator\u0026rsquo;s private key thereby disabling the operator\u0026rsquo;s REST interface (#1846).  Known issues    Issue Description     None currently     "
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://oracle.github.io/weblogic-kubernetes-operator/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]