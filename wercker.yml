# Copyright 2017, 2018, Oracle Corporation and/or its affiliates.  All rights reserved.
# Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl.

#
#  Wercker build file for Oracle WebLogic Server Kubernetes Operator
#

#
#  Wercker application is at : https://app.wercker.com/Oracle/weblogic-kubernetes-operator
#
#  Werkcer workflow looks like this:
#
#  build -> integration-tests (1.7.9)
#        -> integration-tests (1.8.5)
#        -> quality
#

box:
  id: store/oracle/serverjre
  username: $DOCKER_USERNAME
  password: $DOCKER_PASSWORD
  tag: 8

# This is the main build pipeline that builds the codebase and runs unit tests.
build:
  steps:
  - mbrevda/github-status:
    token: $GITHUB_TOKEN
    context: wercker/build
    msg: Wercker build in progress
  - script:
    name: Hello
    code: |
      echo "Building Oracle WebLogic Server Kubernetes Operator..."
      echo "The branch and commit id are $WERCKER_GIT_BRANCH, $WERCKER_GIT_COMMIT"
  - script:
    name: Install pre-reqs
    code: |
      yum -y install tar gzip procps
  - wercker/maven:
    goals: clean install
    version: 3.5.2
    cache_repo: true
  - script:
    name: Copy built-artifacts into the image
    code: |
      mkdir /operator
      mkdir /operator/lib
      cp -R src/scripts/* /operator/
      cp operator/target/weblogic-kubernetes-operator-1.0.jar /operator/weblogic-kubernetes-operator.jar
      cp operator/target/lib/*.jar /operator/lib/
      export IMAGE_TAG_OPERATOR="${IMAGE_TAG_OPERATOR:-${WERCKER_GIT_BRANCH//[_\/]/-}}"
      if [ "$IMAGE_TAG_OPERATOR" = "master" ]; then
        export IMAGE_TAG_OPERATOR="latest"
      fi
  - script:
    name: Remove things we do not want in the Docker image in order to reduce size of image
    code: |
      rpm -e --nodeps tar 
      rpm -e --nodeps gzip 
      yum clean all
      rm -rf /var/cache/yum
  # push the image to Docker using the GIT branch as the tag
  # this image needs to be available to the integration-test pipeline for testing
  - internal/docker-push:
    username: $REPO_USERNAME
    password: $REPO_PASSWORD
    repository: $REPO_REPOSITORY
    registry: $REPO_REGISTRY
    tag: $IMAGE_TAG_OPERATOR
    working-dir: "/operator"
    cmd: "operator.sh"
    env: "PATH=$PATH:/operator"
  after-steps:
  - mbrevda/github-status:
    token: $GITHUB_TOKEN
    context: My App
    msg: Wercker build completed
    fail: Wercker build failed
    url: https://app.wercker.com/Oracle/weblogic-kubernetes-operator/runs/build/$WERCKER_RUN_ID
    
# This pipeline runs the integration tests against a k8s cluster on OCI.
command-timeout: 60
integration-test:
  steps:
  - mbrevda/github-status:
    token: $GITHUB_TOKEN
    context: wercker/$WERCKER_DEPLOYTARGET_NAME
    msg: Wercker $WERCKER_DEPLOYTARGET_NAME in progress
  - script:
    name: Run integration tests
    code: |
      #!/bin/bash
      function cleanup_and_store {
        # release lease in case run.sh failed to release it
        # (the following command only releases the release after confirming this pipeline still owns it)
        # TBD Calling this somehow seems to fail the wercker run
        # $WERCKER_SOURCE_DIR/src/integration-tests/bash/lease.sh -d "$LEASE_ID" > /tmp/junk 2>&1

        # clean up
        yum clean all

        # store the artifacts so we can download them easily 
        tar czvf ${WERCKER_REPORT_ARTIFACTS_DIR}/integration-test-data.tar.gz /pipeline/output/*
      }
      
      function finish {
        exit_code=$?
        export INTEGRATION_TEST_RESULT="$exit_code"
        
        cleanup_and_store
        exit 0
      }
      trap finish EXIT

      # Copy Docker file to OCI host and load into local Docker registry
      # yum install -y openssh-clients
      # echo -e $OCI_K8S_SSHKEY > /tmp/ssh_key
      # chmod 600 /tmp/ssh_key
      # scp -o StrictHostKeyChecking=no -i /tmp/ssh_key $WERCKER_OUTPUT_DIR/build.tar opc@$OCI_K8S_WORKER0_IP:/scratch/build.tar
      # ssh -o StrictHostKeyChecking=no -i /tmp/ssh_key opc@$OCI_K8S_WORKER0_IP "tar -xvf /scratch/operator.tar”
      # ssh -o StrictHostKeyChecking=no -i /tmp/ssh_key opc@$OCI_K8S_WORKER0_IP "sudo docker build -t weblogic-kubernetes-operator:$WERCKER_GIT_BRANCH --no-cache=true /scratch/”
      # ssh -o StrictHostKeyChecking=no -i /tmp/ssh_key opc@$OCI_K8S_WORKER0_IP "sudo docker save weblogic-kubernetes-operator:$WERCKER_GIT_BRANCH > /scratch/operator.tar”
      # ssh -o StrictHostKeyChecking=no -i /tmp/ssh_key opc@$OCI_K8S_WORKER1_IP "sudo docker load < /scratch/operator.tar”

      cp /etc/hosts $WERCKER_PIPELINE_DIR/hosts
      sed -i "$ a ${OCI_K8S_WORKER0_IP} ${OCI_K8S_WORKER0_HOSTNAME}" $WERCKER_PIPELINE_DIR/hosts
      cp $WERCKER_PIPELINE_DIR/hosts /etc/hosts

      # Update KUBECONFIG for K8S cluster
      export K8S_NODEPORT_HOST="${OCI_K8S_WORKER0_HOSTNAME}"
      sed -i -e "s,%ADDRESS%,https://$OCI_K8S_MASTER_IP:443,g" $WERCKER_SOURCE_DIR/build/kube.config
      sed -i -e "s,%CLIENT_CERT_DATA%,$OCI_K8S_CLIENT_CERT_DATA,g" $WERCKER_SOURCE_DIR/build/kube.config
      sed -i -e "s,%CLIENT_KEY_DATA%,$OCI_K8S_CLIENT_KEY_DATA,g" $WERCKER_SOURCE_DIR/build/kube.config
      export KUBECONFIG="$WERCKER_SOURCE_DIR/build/kube.config"

      # running on Wercker
      export WERCKER="true"

      # install kubectl
      curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
      chmod +x ./kubectl
      mv ./kubectl /usr/local/bin/kubectl

      # install maven, includes java as dependency
      curl -LO http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo
      mv epel-apache-maven.repo /etc/yum.repos.d/
      yum install -y apache-maven
      export M2_HOME="/usr/share/apache-maven"
      export PATH=$M2_HOME/bin:$PATH

      # install opensll
      yum install -y openssl

      echo @@ "Calling 'kubectl version'"
      kubectl version

      # obtain an exclusive k8s cluster lease using the 'lease.sh' helper script
      #   - first set LEASE_ID to a unique value
      #   - then try obtain the lease, block up to 100 minutes (wercker pipeline should timeout before then)
      #   - finally, run.sh will periodically try renew the lease as it runs (using $LEASE_ID)
      #   - if run.sh fails when it tries to renew the lease (as something else took it, etc), it will exit early
      #   - when run.sh exits, it will try release the lease if it's still the owner...
      export LEASE_ID="${WERCKER_STEP_ID}-pid$$"
      echo @@
      echo @@ "Obtaining lease!"
      echo @@
      echo @@ "About to block up to the 100 minutes trying to get exclusive access to the kubernetes cluster."
      echo @@ "If this blocks unexpectedly and you are sure that the kubernetes cluster isn't in use by "
      echo @@ "another Wercker pipeline, you can force the lease to free up via 'kubectl delete cm acceptance-test-lease'."
      echo @@ "See LEASE_ID in run.sh for details about this heuristic."
      echo @@ "LEASE_ID=$LEASE_ID host=$HOST date=`date` user=$USER."
      echo @@
      echo @@ "Current lease owner (if any):"
      $WERCKER_SOURCE_DIR/src/integration-tests/bash/lease.sh -s 
      echo @@
      echo @@ "About to try obtain lease:"
      $WERCKER_SOURCE_DIR/src/integration-tests/bash/lease.sh -o "$LEASE_ID" -t $((100 * 60))
      echo @@

      export HOST_PATH="/scratch"
      export PV_ROOT=$HOST_PATH
      export RESULT_ROOT="$WERCKER_OUTPUT_DIR/k8s_dir"
      mkdir -m 777 -p $RESULT_ROOT
      export PROJECT_ROOT="${WERCKER_SOURCE_DIR}"
      $WERCKER_SOURCE_DIR/src/integration-tests/bash/cleanup.sh

      export IMAGE_NAME_OPERATOR="${REPO_REPOSITORY}"
      export IMAGE_TAG_OPERATOR="${WERCKER_GIT_BRANCH//[_\/]/-}"
      if [ "$IMAGE_TAG_OPERATOR" = "master" ]; then
        export IMAGE_TAG_OPERATOR="latest"
      fi
      export IMAGE_PULL_POLICY_OPERATOR="Always"
      export IMAGE_PULL_SECRET_OPERATOR="ocir-registry"
      export IMAGE_PULL_SECRET_WEBLOGIC="docker-store"

      echo "Integration test suite against the test image which is:"
      echo "$IMAGE_NAME_OPERATOR:$IMAGE_TAG_OPERATOR"

      # integration tests
      $WERCKER_SOURCE_DIR/src/integration-tests/bash/run.sh
      RUN_SH_RC=$?

      if [ "$RUN_SH_RC" = "0" ]; then 
        echo "run.sh finished successfully"
      else
        echo "run.sh failed with return code ${RUN_SH_RC}"
        exit $RUN_SH_RC
      fi
    
      cleanup_and_store
  after-steps:
  - mbrevda/github-status:
    token: $GITHUB_TOKEN
    context: My App
    msg: Wercker $WERCKER_DEPLOYTARGET_NAME completed
    fail: Wercker $WERCKER_DEPLOYTARGET_NAME failed
    url: https://app.wercker.com/Oracle/weblogic-kubernetes-operator/runs/$WERCKER_DEPLOYTARGET_NAME/$WERCKER_RUN_ID

# This pipeline runs quality checks 
quality:
  steps:
  - script:
    name: Install pre-reqs
    code: |
      yum -y install tar gzip procps
  - wercker/maven: 
    profiles: build-sonar 
    maven_opts: -Dsonar.login=${SONAR_LOGIN} -Dsonar.password=${SONAR_PASSWORD} -Dsonar.host.url=${SONAR_HOST} 
    goals: clean install sonar:sonar
    cache_repo: true
    version: 3.5.2


dev:
  steps:
  - internal/shell
